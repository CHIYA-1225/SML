{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>使用資料: face_data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>函式說明:</h3>\n",
    "將資料 X 和標籤 以70%、30%分成訓練集和測試集，並且進行標準化。<br>\n",
    "利用主成分分析（PCA）將訓練集和測試集轉換為二維空間，並將轉換後的訓練集和測試集存儲在變數 Z_train 和 Z_test 中。<br>\n",
    "利用邏輯回歸、支持向量機和多層感知機三種分類器，針對原始資料和 PCA 轉換後的資料分別進行評估，計算測試資料的準確度並將其存儲在相應的變數中。<br>\n",
    "最後函數返回所有評估結果的陣列，包括原始資料的邏輯回歸、支持向量機和多層感知機的準確度，PCA 轉換後的資料的邏輯回歸、支持向量機和多層感知機的準確度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  classifier (X,y,dataname,h):\n",
    "    data=dataname\n",
    "    # Split data into training and testing data 7:3\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_ = scaler.fit_transform(X_train)\n",
    "    X_test_ = scaler.fit_transform(X_test)\n",
    "\n",
    "#PCA\n",
    "    pca = PCA(n_components = 2).fit(X_train_)\n",
    "    Z_train = pca.transform(X_train_)\n",
    "    Z_test = pca.transform(X_test_)\n",
    "\n",
    "#LogisticRegression\n",
    "    LR=np.zeros(3)\n",
    "    LR_CV=np.zeros(3)\n",
    "    LR_pca=np.zeros(3)\n",
    "    LR_CV_pca=np.zeros(3)\n",
    "\n",
    "    Cs = np.logspace(-5, 5, 20)\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "    solver = ['lbfgs','liblinear','newton-cg'] \n",
    "    for s in range(3):\n",
    "        clf_original = LogisticRegression(solver = solver[s], **opts)\n",
    "        clf_original.fit(X_train_, y_train)\n",
    "        y_pred = clf_original.predict(X_test_)\n",
    "        # 測 試 資 料 之 準 確 率 回 報\n",
    "        LR[s]= accuracy_score(y_test, y_pred)\n",
    "\n",
    "        clf_PCA = LogisticRegression(solver = solver[s], **opts)\n",
    "        clf_PCA.fit(Z_train, y_train)\n",
    "        LR_pca[s]=clf_PCA.score(Z_test, y_test)\n",
    "\n",
    " # SVM\n",
    "    SVM=np.zeros((2,3))\n",
    "    SVM_pca=np.zeros((2,3))\n",
    "    C = 1 # SVM regularization parameter\n",
    "    opts = [dict(C = C, tol = 1e-6, max_iter = int(1e6)),dict(C = C, decision_function_shape = 'ovo', tol = 1e-6, max_iter = int(1e6))]\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            clf_svm = [SVC(kernel=\"linear\", **opts[i]),\\\n",
    "            SVC(kernel=\"rbf\", gamma=0.2, **opts[i]),\\\n",
    "             SVC(kernel=\"poly\", degree=3, gamma=\"auto\", **opts[i])]\n",
    "             #LinearSVC(**opts[i]) ]\n",
    "\n",
    "            clf_svm[j].fit(X_train, y_train)\n",
    "            predictions = clf_svm[j].predict(X_test)\n",
    "            SVM[i][j]= accuracy_score(y_test, predictions)\n",
    "            \n",
    "            clf_svm[j].fit(Z_train, y_train) #pca\n",
    "            predictions = clf_svm[j].predict(Z_test)\n",
    "            SVM_pca[i][j]= accuracy_score(y_test, predictions)\n",
    "\n",
    "#MLPClassifier\n",
    "    MLP= np.zeros((2,2))\n",
    "    MLP_pca = np.zeros((2,2))\n",
    "    hidden_layers = (h,)\n",
    "    activation = ['logistic','relu']\n",
    "    solver = ['adam','sgd']\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "            activation = activation[i], tol = 1e-6, max_iter = int(1e6))\n",
    "            clf_MLP = MLPClassifier(solver = solver[j], **opts)\n",
    "            clf_MLP.fit(X_train, y_train)\n",
    "            predictions_mlp = clf_MLP.predict(X_test)\n",
    "            MLP[i][j]= accuracy_score(y_test, predictions_mlp)\n",
    "\n",
    "            clf_MLP.fit(Z_train, y_train) #pca\n",
    "            predictions_mlp = clf_MLP.predict(Z_test)\n",
    "            MLP_pca[i][j]= accuracy_score(y_test, predictions_mlp)\n",
    "\n",
    "    return data,LR,SVM,MLP,LR_pca,SVM_pca,MLP_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "D = scipy.io.loadmat('allFaces.mat')\n",
    "X = D['faces'] # 32256 x 2410, each column represents an image\n",
    "num = np.ndarray.flatten(D['nfaces']) #每位人臉的張數\n",
    "#每張影像的大小與人數\n",
    "m = int(D['m']) # 168\n",
    "n = int(D['n']) # 192\n",
    "n_persons = int(D['person']) # 38"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將y加入label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= []\n",
    "for i in range(len(num)):\n",
    "    for j in range(num[i]):\n",
    "        y.append(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從中隨機選取了2410個影像，作為機器學習模型的輸入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2292  378 1824 ... 1121 1605 1865]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(X.shape[1])\n",
    "_X = X[indices[:2410], :]\n",
    "y=np.array(y)\n",
    "_y = y[indices[:2410]]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   56.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "c:\\Users\\ian20\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ian20\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ian20\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ian20\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.71031890\n",
      "Iteration 2, loss = 3.68453823\n",
      "Iteration 3, loss = 3.66762072\n",
      "Iteration 4, loss = 3.65604378\n",
      "Iteration 5, loss = 3.64828677\n",
      "Iteration 6, loss = 3.64200657\n",
      "Iteration 7, loss = 3.63864245\n",
      "Iteration 8, loss = 3.63713875\n",
      "Iteration 9, loss = 3.63616718\n",
      "Iteration 10, loss = 3.63932296\n",
      "Iteration 11, loss = 3.63787916\n",
      "Iteration 12, loss = 3.63686447\n",
      "Iteration 13, loss = 3.63631425\n",
      "Iteration 14, loss = 3.63816405\n",
      "Iteration 15, loss = 3.63938431\n",
      "Iteration 16, loss = 3.63805600\n",
      "Iteration 17, loss = 3.63694957\n",
      "Iteration 18, loss = 3.63621825\n",
      "Iteration 19, loss = 3.63598689\n",
      "Iteration 20, loss = 3.63537714\n",
      "Iteration 21, loss = 3.63521245\n",
      "Iteration 22, loss = 3.63536257\n",
      "Iteration 23, loss = 3.63544277\n",
      "Iteration 24, loss = 3.63526832\n",
      "Iteration 25, loss = 3.63525232\n",
      "Iteration 26, loss = 3.63522201\n",
      "Iteration 27, loss = 3.63562696\n",
      "Iteration 28, loss = 3.63536492\n",
      "Iteration 29, loss = 3.63510992\n",
      "Iteration 30, loss = 3.63512970\n",
      "Iteration 31, loss = 3.63537891\n",
      "Iteration 32, loss = 3.63620803\n",
      "Iteration 33, loss = 3.63544918\n",
      "Iteration 34, loss = 3.63516849\n",
      "Iteration 35, loss = 3.63511285\n",
      "Iteration 36, loss = 3.63520236\n",
      "Iteration 37, loss = 3.63525502\n",
      "Iteration 38, loss = 3.63537665\n",
      "Iteration 39, loss = 3.63525921\n",
      "Iteration 40, loss = 3.63524929\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.66858006\n",
      "Iteration 2, loss = 3.59844730\n",
      "Iteration 3, loss = 3.53611845\n",
      "Iteration 4, loss = 3.48028647\n",
      "Iteration 5, loss = 3.42922263\n",
      "Iteration 6, loss = 3.38411473\n",
      "Iteration 7, loss = 3.34307465\n",
      "Iteration 8, loss = 3.30592863\n",
      "Iteration 9, loss = 3.27131793\n",
      "Iteration 10, loss = 3.23979291\n",
      "Iteration 11, loss = 3.21062370\n",
      "Iteration 12, loss = 3.18405679\n",
      "Iteration 13, loss = 3.15949181\n",
      "Iteration 14, loss = 3.13648976\n",
      "Iteration 15, loss = 3.11586460\n",
      "Iteration 16, loss = 3.09669418\n",
      "Iteration 17, loss = 3.07921763\n",
      "Iteration 18, loss = 3.06299538\n",
      "Iteration 19, loss = 3.04826117\n",
      "Iteration 20, loss = 3.03452328\n",
      "Iteration 21, loss = 3.02175249\n",
      "Iteration 22, loss = 3.00995800\n",
      "Iteration 23, loss = 2.99879166\n",
      "Iteration 24, loss = 2.98856416\n",
      "Iteration 25, loss = 2.97894445\n",
      "Iteration 26, loss = 2.97030405\n",
      "Iteration 27, loss = 2.96186904\n",
      "Iteration 28, loss = 2.95406099\n",
      "Iteration 29, loss = 2.94683634\n",
      "Iteration 30, loss = 2.93988382\n",
      "Iteration 31, loss = 2.93361041\n",
      "Iteration 32, loss = 2.92773949\n",
      "Iteration 33, loss = 2.92184671\n",
      "Iteration 34, loss = 2.91670659\n",
      "Iteration 35, loss = 2.91186564\n",
      "Iteration 36, loss = 2.90706272\n",
      "Iteration 37, loss = 2.90241663\n",
      "Iteration 38, loss = 2.89822184\n",
      "Iteration 39, loss = 2.89375613\n",
      "Iteration 40, loss = 2.88990913\n",
      "Iteration 41, loss = 2.88638279\n",
      "Iteration 42, loss = 2.88277143\n",
      "Iteration 43, loss = 2.87904415\n",
      "Iteration 44, loss = 2.87569419\n",
      "Iteration 45, loss = 2.87277105\n",
      "Iteration 46, loss = 2.86990451\n",
      "Iteration 47, loss = 2.86686649\n",
      "Iteration 48, loss = 2.86387723\n",
      "Iteration 49, loss = 2.86106952\n",
      "Iteration 50, loss = 2.85884444\n",
      "Iteration 51, loss = 2.85613203\n",
      "Iteration 52, loss = 2.85355373\n",
      "Iteration 53, loss = 2.85130150\n",
      "Iteration 54, loss = 2.84929022\n",
      "Iteration 55, loss = 2.84705909\n",
      "Iteration 56, loss = 2.84468626\n",
      "Iteration 57, loss = 2.84267147\n",
      "Iteration 58, loss = 2.84055255\n",
      "Iteration 59, loss = 2.83850090\n",
      "Iteration 60, loss = 2.83673877\n",
      "Iteration 61, loss = 2.83473831\n",
      "Iteration 62, loss = 2.83295715\n",
      "Iteration 63, loss = 2.83135260\n",
      "Iteration 64, loss = 2.82959175\n",
      "Iteration 65, loss = 2.82756956\n",
      "Iteration 66, loss = 2.82596673\n",
      "Iteration 67, loss = 2.82441388\n",
      "Iteration 68, loss = 2.82304106\n",
      "Iteration 69, loss = 2.82116295\n",
      "Iteration 70, loss = 2.81978305\n",
      "Iteration 71, loss = 2.81828788\n",
      "Iteration 72, loss = 2.81667386\n",
      "Iteration 73, loss = 2.81519659\n",
      "Iteration 74, loss = 2.81384064\n",
      "Iteration 75, loss = 2.81250659\n",
      "Iteration 76, loss = 2.81098695\n",
      "Iteration 77, loss = 2.80966701\n",
      "Iteration 78, loss = 2.80832320\n",
      "Iteration 79, loss = 2.80713110\n",
      "Iteration 80, loss = 2.80583313\n",
      "Iteration 81, loss = 2.80454970\n",
      "Iteration 82, loss = 2.80296964\n",
      "Iteration 83, loss = 2.80161304\n",
      "Iteration 84, loss = 2.80043694\n",
      "Iteration 85, loss = 2.79922162\n",
      "Iteration 86, loss = 2.79806399\n",
      "Iteration 87, loss = 2.79685638\n",
      "Iteration 88, loss = 2.79544707\n",
      "Iteration 89, loss = 2.79423115\n",
      "Iteration 90, loss = 2.79349452\n",
      "Iteration 91, loss = 2.79229904\n",
      "Iteration 92, loss = 2.79051040\n",
      "Iteration 93, loss = 2.78950248\n",
      "Iteration 94, loss = 2.78857791\n",
      "Iteration 95, loss = 2.78723241\n",
      "Iteration 96, loss = 2.78629942\n",
      "Iteration 97, loss = 2.78529659\n",
      "Iteration 98, loss = 2.78391693\n",
      "Iteration 99, loss = 2.78258034\n",
      "Iteration 100, loss = 2.78170771\n",
      "Iteration 101, loss = 2.78064550\n",
      "Iteration 102, loss = 2.77956655\n",
      "Iteration 103, loss = 2.77835077\n",
      "Iteration 104, loss = 2.77765978\n",
      "Iteration 105, loss = 2.77657401\n",
      "Iteration 106, loss = 2.77536885\n",
      "Iteration 107, loss = 2.77416870\n",
      "Iteration 108, loss = 2.77346110\n",
      "Iteration 109, loss = 2.77262739\n",
      "Iteration 110, loss = 2.77138606\n",
      "Iteration 111, loss = 2.77048922\n",
      "Iteration 112, loss = 2.76920622\n",
      "Iteration 113, loss = 2.76836506\n",
      "Iteration 114, loss = 2.76739742\n",
      "Iteration 115, loss = 2.76643985\n",
      "Iteration 116, loss = 2.76539871\n",
      "Iteration 117, loss = 2.76439283\n",
      "Iteration 118, loss = 2.76380864\n",
      "Iteration 119, loss = 2.76257641\n",
      "Iteration 120, loss = 2.76163292\n",
      "Iteration 121, loss = 2.76065558\n",
      "Iteration 122, loss = 2.75986129\n",
      "Iteration 123, loss = 2.75888980\n",
      "Iteration 124, loss = 2.75818327\n",
      "Iteration 125, loss = 2.75700529\n",
      "Iteration 126, loss = 2.75621835\n",
      "Iteration 127, loss = 2.75547670\n",
      "Iteration 128, loss = 2.75460238\n",
      "Iteration 129, loss = 2.75334236\n",
      "Iteration 130, loss = 2.75239608\n",
      "Iteration 131, loss = 2.75161393\n",
      "Iteration 132, loss = 2.75073437\n",
      "Iteration 133, loss = 2.74963851\n",
      "Iteration 134, loss = 2.74887829\n",
      "Iteration 135, loss = 2.74811164\n",
      "Iteration 136, loss = 2.74697903\n",
      "Iteration 137, loss = 2.74613207\n",
      "Iteration 138, loss = 2.74533586\n",
      "Iteration 139, loss = 2.74445449\n",
      "Iteration 140, loss = 2.74397398\n",
      "Iteration 141, loss = 2.74271410\n",
      "Iteration 142, loss = 2.74183423\n",
      "Iteration 143, loss = 2.74120125\n",
      "Iteration 144, loss = 2.74015209\n",
      "Iteration 145, loss = 2.73954271\n",
      "Iteration 146, loss = 2.73858356\n",
      "Iteration 147, loss = 2.73749883\n",
      "Iteration 148, loss = 2.73661722\n",
      "Iteration 149, loss = 2.73578378\n",
      "Iteration 150, loss = 2.73500362\n",
      "Iteration 151, loss = 2.73424145\n",
      "Iteration 152, loss = 2.73342234\n",
      "Iteration 153, loss = 2.73243119\n",
      "Iteration 154, loss = 2.73164641\n",
      "Iteration 155, loss = 2.73103909\n",
      "Iteration 156, loss = 2.72993466\n",
      "Iteration 157, loss = 2.72934520\n",
      "Iteration 158, loss = 2.72856530\n",
      "Iteration 159, loss = 2.72743815\n",
      "Iteration 160, loss = 2.72678461\n",
      "Iteration 161, loss = 2.72589027\n",
      "Iteration 162, loss = 2.72528168\n",
      "Iteration 163, loss = 2.72438345\n",
      "Iteration 164, loss = 2.72343057\n",
      "Iteration 165, loss = 2.72267830\n",
      "Iteration 166, loss = 2.72210123\n",
      "Iteration 167, loss = 2.72086584\n",
      "Iteration 168, loss = 2.72023396\n",
      "Iteration 169, loss = 2.71983502\n",
      "Iteration 170, loss = 2.71859400\n",
      "Iteration 171, loss = 2.71781965\n",
      "Iteration 172, loss = 2.71724569\n",
      "Iteration 173, loss = 2.71640209\n",
      "Iteration 174, loss = 2.71558052\n",
      "Iteration 175, loss = 2.71505159\n",
      "Iteration 176, loss = 2.71409850\n",
      "Iteration 177, loss = 2.71310594\n",
      "Iteration 178, loss = 2.71252275\n",
      "Iteration 179, loss = 2.71183535\n",
      "Iteration 180, loss = 2.71107271\n",
      "Iteration 181, loss = 2.71074698\n",
      "Iteration 182, loss = 2.70945048\n",
      "Iteration 183, loss = 2.70852116\n",
      "Iteration 184, loss = 2.70828262\n",
      "Iteration 185, loss = 2.70737766\n",
      "Iteration 186, loss = 2.70642032\n",
      "Iteration 187, loss = 2.70558567\n",
      "Iteration 188, loss = 2.70500646\n",
      "Iteration 189, loss = 2.70427726\n",
      "Iteration 190, loss = 2.70329737\n",
      "Iteration 191, loss = 2.70264961\n",
      "Iteration 192, loss = 2.70169761\n",
      "Iteration 193, loss = 2.70095455\n",
      "Iteration 194, loss = 2.70026456\n",
      "Iteration 195, loss = 2.69965165\n",
      "Iteration 196, loss = 2.69884152\n",
      "Iteration 197, loss = 2.69833758\n",
      "Iteration 198, loss = 2.69715947\n",
      "Iteration 199, loss = 2.69654650\n",
      "Iteration 200, loss = 2.69582660\n",
      "Iteration 201, loss = 2.69519922\n",
      "Iteration 202, loss = 2.69413788\n",
      "Iteration 203, loss = 2.69345722\n",
      "Iteration 204, loss = 2.69295064\n",
      "Iteration 205, loss = 2.69191366\n",
      "Iteration 206, loss = 2.69123225\n",
      "Iteration 207, loss = 2.69065393\n",
      "Iteration 208, loss = 2.68999442\n",
      "Iteration 209, loss = 2.68912570\n",
      "Iteration 210, loss = 2.68822896\n",
      "Iteration 211, loss = 2.68724324\n",
      "Iteration 212, loss = 2.68713867\n",
      "Iteration 213, loss = 2.68628394\n",
      "Iteration 214, loss = 2.68572836\n",
      "Iteration 215, loss = 2.68439727\n",
      "Iteration 216, loss = 2.68351872\n",
      "Iteration 217, loss = 2.68343114\n",
      "Iteration 218, loss = 2.68258025\n",
      "Iteration 219, loss = 2.68168090\n",
      "Iteration 220, loss = 2.68099080\n",
      "Iteration 221, loss = 2.68015604\n",
      "Iteration 222, loss = 2.67945821\n",
      "Iteration 223, loss = 2.67856459\n",
      "Iteration 224, loss = 2.67800795\n",
      "Iteration 225, loss = 2.67711082\n",
      "Iteration 226, loss = 2.67618387\n",
      "Iteration 227, loss = 2.67534697\n",
      "Iteration 228, loss = 2.67469989\n",
      "Iteration 229, loss = 2.67458475\n",
      "Iteration 230, loss = 2.67375755\n",
      "Iteration 231, loss = 2.67241445\n",
      "Iteration 232, loss = 2.67162613\n",
      "Iteration 233, loss = 2.67113741\n",
      "Iteration 234, loss = 2.67048955\n",
      "Iteration 235, loss = 2.66942564\n",
      "Iteration 236, loss = 2.66893851\n",
      "Iteration 237, loss = 2.66796436\n",
      "Iteration 238, loss = 2.66751121\n",
      "Iteration 239, loss = 2.66701751\n",
      "Iteration 240, loss = 2.66569252\n",
      "Iteration 241, loss = 2.66502004\n",
      "Iteration 242, loss = 2.66431807\n",
      "Iteration 243, loss = 2.66382431\n",
      "Iteration 244, loss = 2.66285271\n",
      "Iteration 245, loss = 2.66224271\n",
      "Iteration 246, loss = 2.66198237\n",
      "Iteration 247, loss = 2.66037401\n",
      "Iteration 248, loss = 2.65972204\n",
      "Iteration 249, loss = 2.65917202\n",
      "Iteration 250, loss = 2.65859432\n",
      "Iteration 251, loss = 2.65747440\n",
      "Iteration 252, loss = 2.65690155\n",
      "Iteration 253, loss = 2.65580639\n",
      "Iteration 254, loss = 2.65568702\n",
      "Iteration 255, loss = 2.65483612\n",
      "Iteration 256, loss = 2.65381114\n",
      "Iteration 257, loss = 2.65322742\n",
      "Iteration 258, loss = 2.65261377\n",
      "Iteration 259, loss = 2.65151156\n",
      "Iteration 260, loss = 2.65079991\n",
      "Iteration 261, loss = 2.65037102\n",
      "Iteration 262, loss = 2.64937029\n",
      "Iteration 263, loss = 2.64857793\n",
      "Iteration 264, loss = 2.64776788\n",
      "Iteration 265, loss = 2.64719955\n",
      "Iteration 266, loss = 2.64631363\n",
      "Iteration 267, loss = 2.64578477\n",
      "Iteration 268, loss = 2.64484622\n",
      "Iteration 269, loss = 2.64445021\n",
      "Iteration 270, loss = 2.64360866\n",
      "Iteration 271, loss = 2.64259332\n",
      "Iteration 272, loss = 2.64215761\n",
      "Iteration 273, loss = 2.64138185\n",
      "Iteration 274, loss = 2.64067189\n",
      "Iteration 275, loss = 2.63992523\n",
      "Iteration 276, loss = 2.63934415\n",
      "Iteration 277, loss = 2.63866445\n",
      "Iteration 278, loss = 2.63764667\n",
      "Iteration 279, loss = 2.63705322\n",
      "Iteration 280, loss = 2.63641491\n",
      "Iteration 281, loss = 2.63565698\n",
      "Iteration 282, loss = 2.63516757\n",
      "Iteration 283, loss = 2.63423143\n",
      "Iteration 284, loss = 2.63344805\n",
      "Iteration 285, loss = 2.63291693\n",
      "Iteration 286, loss = 2.63316303\n",
      "Iteration 287, loss = 2.63111662\n",
      "Iteration 288, loss = 2.63125237\n",
      "Iteration 289, loss = 2.62990356\n",
      "Iteration 290, loss = 2.62938079\n",
      "Iteration 291, loss = 2.62886632\n",
      "Iteration 292, loss = 2.62780985\n",
      "Iteration 293, loss = 2.62708303\n",
      "Iteration 294, loss = 2.62650014\n",
      "Iteration 295, loss = 2.62558002\n",
      "Iteration 296, loss = 2.62509067\n",
      "Iteration 297, loss = 2.62462773\n",
      "Iteration 298, loss = 2.62368794\n",
      "Iteration 299, loss = 2.62323381\n",
      "Iteration 300, loss = 2.62270443\n",
      "Iteration 301, loss = 2.62162333\n",
      "Iteration 302, loss = 2.62110345\n",
      "Iteration 303, loss = 2.62022339\n",
      "Iteration 304, loss = 2.61986995\n",
      "Iteration 305, loss = 2.61932444\n",
      "Iteration 306, loss = 2.61842747\n",
      "Iteration 307, loss = 2.61771258\n",
      "Iteration 308, loss = 2.61710298\n",
      "Iteration 309, loss = 2.61653632\n",
      "Iteration 310, loss = 2.61573923\n",
      "Iteration 311, loss = 2.61521544\n",
      "Iteration 312, loss = 2.61482888\n",
      "Iteration 313, loss = 2.61396878\n",
      "Iteration 314, loss = 2.61349018\n",
      "Iteration 315, loss = 2.61262450\n",
      "Iteration 316, loss = 2.61172229\n",
      "Iteration 317, loss = 2.61108165\n",
      "Iteration 318, loss = 2.61079630\n",
      "Iteration 319, loss = 2.60950831\n",
      "Iteration 320, loss = 2.60919805\n",
      "Iteration 321, loss = 2.60889870\n",
      "Iteration 322, loss = 2.60812794\n",
      "Iteration 323, loss = 2.60773334\n",
      "Iteration 324, loss = 2.60715699\n",
      "Iteration 325, loss = 2.60608893\n",
      "Iteration 326, loss = 2.60541109\n",
      "Iteration 327, loss = 2.60467437\n",
      "Iteration 328, loss = 2.60411609\n",
      "Iteration 329, loss = 2.60358976\n",
      "Iteration 330, loss = 2.60271936\n",
      "Iteration 331, loss = 2.60220483\n",
      "Iteration 332, loss = 2.60165496\n",
      "Iteration 333, loss = 2.60086641\n",
      "Iteration 334, loss = 2.60096208\n",
      "Iteration 335, loss = 2.59963118\n",
      "Iteration 336, loss = 2.59909299\n",
      "Iteration 337, loss = 2.59821336\n",
      "Iteration 338, loss = 2.59757925\n",
      "Iteration 339, loss = 2.59710146\n",
      "Iteration 340, loss = 2.59650238\n",
      "Iteration 341, loss = 2.59608258\n",
      "Iteration 342, loss = 2.59510037\n",
      "Iteration 343, loss = 2.59444796\n",
      "Iteration 344, loss = 2.59404176\n",
      "Iteration 345, loss = 2.59327442\n",
      "Iteration 346, loss = 2.59286602\n",
      "Iteration 347, loss = 2.59232980\n",
      "Iteration 348, loss = 2.59167930\n",
      "Iteration 349, loss = 2.59068274\n",
      "Iteration 350, loss = 2.59015229\n",
      "Iteration 351, loss = 2.58952559\n",
      "Iteration 352, loss = 2.58928461\n",
      "Iteration 353, loss = 2.58833965\n",
      "Iteration 354, loss = 2.58814761\n",
      "Iteration 355, loss = 2.58738670\n",
      "Iteration 356, loss = 2.58659011\n",
      "Iteration 357, loss = 2.58576138\n",
      "Iteration 358, loss = 2.58536378\n",
      "Iteration 359, loss = 2.58497643\n",
      "Iteration 360, loss = 2.58433667\n",
      "Iteration 361, loss = 2.58363917\n",
      "Iteration 362, loss = 2.58316221\n",
      "Iteration 363, loss = 2.58238189\n",
      "Iteration 364, loss = 2.58192662\n",
      "Iteration 365, loss = 2.58142042\n",
      "Iteration 366, loss = 2.58061513\n",
      "Iteration 367, loss = 2.57995295\n",
      "Iteration 368, loss = 2.57963982\n",
      "Iteration 369, loss = 2.57891211\n",
      "Iteration 370, loss = 2.57870201\n",
      "Iteration 371, loss = 2.57768600\n",
      "Iteration 372, loss = 2.57739823\n",
      "Iteration 373, loss = 2.57627956\n",
      "Iteration 374, loss = 2.57640643\n",
      "Iteration 375, loss = 2.57542111\n",
      "Iteration 376, loss = 2.57497659\n",
      "Iteration 377, loss = 2.57423357\n",
      "Iteration 378, loss = 2.57363591\n",
      "Iteration 379, loss = 2.57296728\n",
      "Iteration 380, loss = 2.57266734\n",
      "Iteration 381, loss = 2.57220196\n",
      "Iteration 382, loss = 2.57138860\n",
      "Iteration 383, loss = 2.57114083\n",
      "Iteration 384, loss = 2.57050627\n",
      "Iteration 385, loss = 2.56994815\n",
      "Iteration 386, loss = 2.56917962\n",
      "Iteration 387, loss = 2.56921554\n",
      "Iteration 388, loss = 2.56837702\n",
      "Iteration 389, loss = 2.56776566\n",
      "Iteration 390, loss = 2.56682567\n",
      "Iteration 391, loss = 2.56648764\n",
      "Iteration 392, loss = 2.56580563\n",
      "Iteration 393, loss = 2.56529072\n",
      "Iteration 394, loss = 2.56482400\n",
      "Iteration 395, loss = 2.56395088\n",
      "Iteration 396, loss = 2.56349073\n",
      "Iteration 397, loss = 2.56322993\n",
      "Iteration 398, loss = 2.56256504\n",
      "Iteration 399, loss = 2.56196466\n",
      "Iteration 400, loss = 2.56183762\n",
      "Iteration 401, loss = 2.56087225\n",
      "Iteration 402, loss = 2.56035027\n",
      "Iteration 403, loss = 2.55961299\n",
      "Iteration 404, loss = 2.55927519\n",
      "Iteration 405, loss = 2.55853184\n",
      "Iteration 406, loss = 2.55797624\n",
      "Iteration 407, loss = 2.55767165\n",
      "Iteration 408, loss = 2.55733380\n",
      "Iteration 409, loss = 2.55639017\n",
      "Iteration 410, loss = 2.55596341\n",
      "Iteration 411, loss = 2.55578250\n",
      "Iteration 412, loss = 2.55512957\n",
      "Iteration 413, loss = 2.55454812\n",
      "Iteration 414, loss = 2.55398593\n",
      "Iteration 415, loss = 2.55362059\n",
      "Iteration 416, loss = 2.55274244\n",
      "Iteration 417, loss = 2.55215480\n",
      "Iteration 418, loss = 2.55177926\n",
      "Iteration 419, loss = 2.55113369\n",
      "Iteration 420, loss = 2.55099385\n",
      "Iteration 421, loss = 2.55043121\n",
      "Iteration 422, loss = 2.54963174\n",
      "Iteration 423, loss = 2.54938187\n",
      "Iteration 424, loss = 2.54876328\n",
      "Iteration 425, loss = 2.54782717\n",
      "Iteration 426, loss = 2.54753393\n",
      "Iteration 427, loss = 2.54685129\n",
      "Iteration 428, loss = 2.54670723\n",
      "Iteration 429, loss = 2.54591435\n",
      "Iteration 430, loss = 2.54522843\n",
      "Iteration 431, loss = 2.54509350\n",
      "Iteration 432, loss = 2.54457583\n",
      "Iteration 433, loss = 2.54475372\n",
      "Iteration 434, loss = 2.54362362\n",
      "Iteration 435, loss = 2.54285289\n",
      "Iteration 436, loss = 2.54262460\n",
      "Iteration 437, loss = 2.54230615\n",
      "Iteration 438, loss = 2.54158252\n",
      "Iteration 439, loss = 2.54091364\n",
      "Iteration 440, loss = 2.54032200\n",
      "Iteration 441, loss = 2.53996295\n",
      "Iteration 442, loss = 2.53966741\n",
      "Iteration 443, loss = 2.53919167\n",
      "Iteration 444, loss = 2.53860298\n",
      "Iteration 445, loss = 2.53860062\n",
      "Iteration 446, loss = 2.53790581\n",
      "Iteration 447, loss = 2.53699579\n",
      "Iteration 448, loss = 2.53646582\n",
      "Iteration 449, loss = 2.53602215\n",
      "Iteration 450, loss = 2.53540286\n",
      "Iteration 451, loss = 2.53512316\n",
      "Iteration 452, loss = 2.53463310\n",
      "Iteration 453, loss = 2.53424920\n",
      "Iteration 454, loss = 2.53388988\n",
      "Iteration 455, loss = 2.53309588\n",
      "Iteration 456, loss = 2.53227068\n",
      "Iteration 457, loss = 2.53223070\n",
      "Iteration 458, loss = 2.53197084\n",
      "Iteration 459, loss = 2.53107274\n",
      "Iteration 460, loss = 2.53073409\n",
      "Iteration 461, loss = 2.53007554\n",
      "Iteration 462, loss = 2.52953048\n",
      "Iteration 463, loss = 2.52916010\n",
      "Iteration 464, loss = 2.52878345\n",
      "Iteration 465, loss = 2.52795299\n",
      "Iteration 466, loss = 2.52762653\n",
      "Iteration 467, loss = 2.52746043\n",
      "Iteration 468, loss = 2.52681077\n",
      "Iteration 469, loss = 2.52606201\n",
      "Iteration 470, loss = 2.52579502\n",
      "Iteration 471, loss = 2.52536256\n",
      "Iteration 472, loss = 2.52485654\n",
      "Iteration 473, loss = 2.52457203\n",
      "Iteration 474, loss = 2.52393252\n",
      "Iteration 475, loss = 2.52362586\n",
      "Iteration 476, loss = 2.52321154\n",
      "Iteration 477, loss = 2.52240216\n",
      "Iteration 478, loss = 2.52214496\n",
      "Iteration 479, loss = 2.52194173\n",
      "Iteration 480, loss = 2.52148058\n",
      "Iteration 481, loss = 2.52077315\n",
      "Iteration 482, loss = 2.52029075\n",
      "Iteration 483, loss = 2.51963903\n",
      "Iteration 484, loss = 2.51955438\n",
      "Iteration 485, loss = 2.51905740\n",
      "Iteration 486, loss = 2.51820708\n",
      "Iteration 487, loss = 2.51777059\n",
      "Iteration 488, loss = 2.51746382\n",
      "Iteration 489, loss = 2.51710439\n",
      "Iteration 490, loss = 2.51635505\n",
      "Iteration 491, loss = 2.51612947\n",
      "Iteration 492, loss = 2.51571740\n",
      "Iteration 493, loss = 2.51529619\n",
      "Iteration 494, loss = 2.51466612\n",
      "Iteration 495, loss = 2.51453710\n",
      "Iteration 496, loss = 2.51379598\n",
      "Iteration 497, loss = 2.51363986\n",
      "Iteration 498, loss = 2.51344477\n",
      "Iteration 499, loss = 2.51245292\n",
      "Iteration 500, loss = 2.51217133\n",
      "Iteration 501, loss = 2.51184340\n",
      "Iteration 502, loss = 2.51142088\n",
      "Iteration 503, loss = 2.51057092\n",
      "Iteration 504, loss = 2.51033914\n",
      "Iteration 505, loss = 2.50964914\n",
      "Iteration 506, loss = 2.50945027\n",
      "Iteration 507, loss = 2.50905274\n",
      "Iteration 508, loss = 2.50859814\n",
      "Iteration 509, loss = 2.50845947\n",
      "Iteration 510, loss = 2.50784792\n",
      "Iteration 511, loss = 2.50765161\n",
      "Iteration 512, loss = 2.50695425\n",
      "Iteration 513, loss = 2.50676626\n",
      "Iteration 514, loss = 2.50599930\n",
      "Iteration 515, loss = 2.50559449\n",
      "Iteration 516, loss = 2.50495135\n",
      "Iteration 517, loss = 2.50481230\n",
      "Iteration 518, loss = 2.50394192\n",
      "Iteration 519, loss = 2.50414765\n",
      "Iteration 520, loss = 2.50351032\n",
      "Iteration 521, loss = 2.50305996\n",
      "Iteration 522, loss = 2.50264682\n",
      "Iteration 523, loss = 2.50218470\n",
      "Iteration 524, loss = 2.50159801\n",
      "Iteration 525, loss = 2.50079418\n",
      "Iteration 526, loss = 2.50132099\n",
      "Iteration 527, loss = 2.50016081\n",
      "Iteration 528, loss = 2.50034758\n",
      "Iteration 529, loss = 2.49939651\n",
      "Iteration 530, loss = 2.49902471\n",
      "Iteration 531, loss = 2.49851627\n",
      "Iteration 532, loss = 2.49831549\n",
      "Iteration 533, loss = 2.49774648\n",
      "Iteration 534, loss = 2.49750799\n",
      "Iteration 535, loss = 2.49684946\n",
      "Iteration 536, loss = 2.49667644\n",
      "Iteration 537, loss = 2.49606801\n",
      "Iteration 538, loss = 2.49594079\n",
      "Iteration 539, loss = 2.49574910\n",
      "Iteration 540, loss = 2.49519908\n",
      "Iteration 541, loss = 2.49426139\n",
      "Iteration 542, loss = 2.49404958\n",
      "Iteration 543, loss = 2.49369798\n",
      "Iteration 544, loss = 2.49316940\n",
      "Iteration 545, loss = 2.49305994\n",
      "Iteration 546, loss = 2.49249439\n",
      "Iteration 547, loss = 2.49245837\n",
      "Iteration 548, loss = 2.49127609\n",
      "Iteration 549, loss = 2.49085321\n",
      "Iteration 550, loss = 2.49091771\n",
      "Iteration 551, loss = 2.49058939\n",
      "Iteration 552, loss = 2.48993326\n",
      "Iteration 553, loss = 2.48997822\n",
      "Iteration 554, loss = 2.48957423\n",
      "Iteration 555, loss = 2.48850293\n",
      "Iteration 556, loss = 2.48806887\n",
      "Iteration 557, loss = 2.48796211\n",
      "Iteration 558, loss = 2.48741916\n",
      "Iteration 559, loss = 2.48700000\n",
      "Iteration 560, loss = 2.48641310\n",
      "Iteration 561, loss = 2.48641065\n",
      "Iteration 562, loss = 2.48621683\n",
      "Iteration 563, loss = 2.48536575\n",
      "Iteration 564, loss = 2.48511603\n",
      "Iteration 565, loss = 2.48471212\n",
      "Iteration 566, loss = 2.48445159\n",
      "Iteration 567, loss = 2.48385062\n",
      "Iteration 568, loss = 2.48350904\n",
      "Iteration 569, loss = 2.48305242\n",
      "Iteration 570, loss = 2.48278621\n",
      "Iteration 571, loss = 2.48231154\n",
      "Iteration 572, loss = 2.48209815\n",
      "Iteration 573, loss = 2.48134312\n",
      "Iteration 574, loss = 2.48145716\n",
      "Iteration 575, loss = 2.48078215\n",
      "Iteration 576, loss = 2.48064252\n",
      "Iteration 577, loss = 2.47984815\n",
      "Iteration 578, loss = 2.47950959\n",
      "Iteration 579, loss = 2.47902546\n",
      "Iteration 580, loss = 2.47870165\n",
      "Iteration 581, loss = 2.47853606\n",
      "Iteration 582, loss = 2.47813936\n",
      "Iteration 583, loss = 2.47784636\n",
      "Iteration 584, loss = 2.47752597\n",
      "Iteration 585, loss = 2.47697376\n",
      "Iteration 586, loss = 2.47650008\n",
      "Iteration 587, loss = 2.47595022\n",
      "Iteration 588, loss = 2.47555846\n",
      "Iteration 589, loss = 2.47535259\n",
      "Iteration 590, loss = 2.47515875\n",
      "Iteration 591, loss = 2.47449737\n",
      "Iteration 592, loss = 2.47440280\n",
      "Iteration 593, loss = 2.47338338\n",
      "Iteration 594, loss = 2.47345850\n",
      "Iteration 595, loss = 2.47290074\n",
      "Iteration 596, loss = 2.47303842\n",
      "Iteration 597, loss = 2.47243029\n",
      "Iteration 598, loss = 2.47175147\n",
      "Iteration 599, loss = 2.47109270\n",
      "Iteration 600, loss = 2.47109299\n",
      "Iteration 601, loss = 2.47083512\n",
      "Iteration 602, loss = 2.47034818\n",
      "Iteration 603, loss = 2.46983443\n",
      "Iteration 604, loss = 2.46966852\n",
      "Iteration 605, loss = 2.46912956\n",
      "Iteration 606, loss = 2.46865791\n",
      "Iteration 607, loss = 2.46863991\n",
      "Iteration 608, loss = 2.46827303\n",
      "Iteration 609, loss = 2.46728503\n",
      "Iteration 610, loss = 2.46742260\n",
      "Iteration 611, loss = 2.46714658\n",
      "Iteration 612, loss = 2.46628499\n",
      "Iteration 613, loss = 2.46678292\n",
      "Iteration 614, loss = 2.46599824\n",
      "Iteration 615, loss = 2.46578819\n",
      "Iteration 616, loss = 2.46510189\n",
      "Iteration 617, loss = 2.46533172\n",
      "Iteration 618, loss = 2.46423251\n",
      "Iteration 619, loss = 2.46458848\n",
      "Iteration 620, loss = 2.46355365\n",
      "Iteration 621, loss = 2.46329625\n",
      "Iteration 622, loss = 2.46318202\n",
      "Iteration 623, loss = 2.46256864\n",
      "Iteration 624, loss = 2.46218238\n",
      "Iteration 625, loss = 2.46186907\n",
      "Iteration 626, loss = 2.46155142\n",
      "Iteration 627, loss = 2.46114324\n",
      "Iteration 628, loss = 2.46081444\n",
      "Iteration 629, loss = 2.46112899\n",
      "Iteration 630, loss = 2.45999885\n",
      "Iteration 631, loss = 2.45968882\n",
      "Iteration 632, loss = 2.45947586\n",
      "Iteration 633, loss = 2.45870441\n",
      "Iteration 634, loss = 2.45835817\n",
      "Iteration 635, loss = 2.45818493\n",
      "Iteration 636, loss = 2.45759050\n",
      "Iteration 637, loss = 2.45711308\n",
      "Iteration 638, loss = 2.45716934\n",
      "Iteration 639, loss = 2.45675021\n",
      "Iteration 640, loss = 2.45641338\n",
      "Iteration 641, loss = 2.45588173\n",
      "Iteration 642, loss = 2.45518948\n",
      "Iteration 643, loss = 2.45537878\n",
      "Iteration 644, loss = 2.45467492\n",
      "Iteration 645, loss = 2.45463667\n",
      "Iteration 646, loss = 2.45432103\n",
      "Iteration 647, loss = 2.45420545\n",
      "Iteration 648, loss = 2.45357912\n",
      "Iteration 649, loss = 2.45329522\n",
      "Iteration 650, loss = 2.45287651\n",
      "Iteration 651, loss = 2.45263335\n",
      "Iteration 652, loss = 2.45198852\n",
      "Iteration 653, loss = 2.45155307\n",
      "Iteration 654, loss = 2.45111523\n",
      "Iteration 655, loss = 2.45088169\n",
      "Iteration 656, loss = 2.45050007\n",
      "Iteration 657, loss = 2.45035576\n",
      "Iteration 658, loss = 2.45005294\n",
      "Iteration 659, loss = 2.44939606\n",
      "Iteration 660, loss = 2.45011149\n",
      "Iteration 661, loss = 2.44883093\n",
      "Iteration 662, loss = 2.44944784\n",
      "Iteration 663, loss = 2.44828440\n",
      "Iteration 664, loss = 2.44784536\n",
      "Iteration 665, loss = 2.44787395\n",
      "Iteration 666, loss = 2.44719722\n",
      "Iteration 667, loss = 2.44702174\n",
      "Iteration 668, loss = 2.44632343\n",
      "Iteration 669, loss = 2.44624876\n",
      "Iteration 670, loss = 2.44606466\n",
      "Iteration 671, loss = 2.44547823\n",
      "Iteration 672, loss = 2.44480127\n",
      "Iteration 673, loss = 2.44463545\n",
      "Iteration 674, loss = 2.44442985\n",
      "Iteration 675, loss = 2.44398039\n",
      "Iteration 676, loss = 2.44374353\n",
      "Iteration 677, loss = 2.44332794\n",
      "Iteration 678, loss = 2.44304610\n",
      "Iteration 679, loss = 2.44270513\n",
      "Iteration 680, loss = 2.44256758\n",
      "Iteration 681, loss = 2.44229053\n",
      "Iteration 682, loss = 2.44186812\n",
      "Iteration 683, loss = 2.44142393\n",
      "Iteration 684, loss = 2.44107486\n",
      "Iteration 685, loss = 2.44053522\n",
      "Iteration 686, loss = 2.44025109\n",
      "Iteration 687, loss = 2.44017303\n",
      "Iteration 688, loss = 2.43982029\n",
      "Iteration 689, loss = 2.43927237\n",
      "Iteration 690, loss = 2.43903636\n",
      "Iteration 691, loss = 2.43855060\n",
      "Iteration 692, loss = 2.43831097\n",
      "Iteration 693, loss = 2.43792930\n",
      "Iteration 694, loss = 2.43762863\n",
      "Iteration 695, loss = 2.43754750\n",
      "Iteration 696, loss = 2.43705209\n",
      "Iteration 697, loss = 2.43673024\n",
      "Iteration 698, loss = 2.43650164\n",
      "Iteration 699, loss = 2.43616023\n",
      "Iteration 700, loss = 2.43558483\n",
      "Iteration 701, loss = 2.43546554\n",
      "Iteration 702, loss = 2.43463707\n",
      "Iteration 703, loss = 2.43529106\n",
      "Iteration 704, loss = 2.43452539\n",
      "Iteration 705, loss = 2.43428819\n",
      "Iteration 706, loss = 2.43400025\n",
      "Iteration 707, loss = 2.43327294\n",
      "Iteration 708, loss = 2.43300786\n",
      "Iteration 709, loss = 2.43261086\n",
      "Iteration 710, loss = 2.43191279\n",
      "Iteration 711, loss = 2.43229383\n",
      "Iteration 712, loss = 2.43144207\n",
      "Iteration 713, loss = 2.43113295\n",
      "Iteration 714, loss = 2.43100300\n",
      "Iteration 715, loss = 2.43056236\n",
      "Iteration 716, loss = 2.43007527\n",
      "Iteration 717, loss = 2.42981285\n",
      "Iteration 718, loss = 2.42919646\n",
      "Iteration 719, loss = 2.42924917\n",
      "Iteration 720, loss = 2.42877370\n",
      "Iteration 721, loss = 2.42855078\n",
      "Iteration 722, loss = 2.42835794\n",
      "Iteration 723, loss = 2.42766145\n",
      "Iteration 724, loss = 2.42727742\n",
      "Iteration 725, loss = 2.42746555\n",
      "Iteration 726, loss = 2.42680193\n",
      "Iteration 727, loss = 2.42672859\n",
      "Iteration 728, loss = 2.42650115\n",
      "Iteration 729, loss = 2.42600479\n",
      "Iteration 730, loss = 2.42584009\n",
      "Iteration 731, loss = 2.42535867\n",
      "Iteration 732, loss = 2.42507815\n",
      "Iteration 733, loss = 2.42452585\n",
      "Iteration 734, loss = 2.42440439\n",
      "Iteration 735, loss = 2.42391004\n",
      "Iteration 736, loss = 2.42357459\n",
      "Iteration 737, loss = 2.42354513\n",
      "Iteration 738, loss = 2.42312284\n",
      "Iteration 739, loss = 2.42258017\n",
      "Iteration 740, loss = 2.42236132\n",
      "Iteration 741, loss = 2.42204584\n",
      "Iteration 742, loss = 2.42210821\n",
      "Iteration 743, loss = 2.42161341\n",
      "Iteration 744, loss = 2.42115042\n",
      "Iteration 745, loss = 2.42078243\n",
      "Iteration 746, loss = 2.42088053\n",
      "Iteration 747, loss = 2.42017040\n",
      "Iteration 748, loss = 2.42004246\n",
      "Iteration 749, loss = 2.41967271\n",
      "Iteration 750, loss = 2.41930268\n",
      "Iteration 751, loss = 2.41890684\n",
      "Iteration 752, loss = 2.41836175\n",
      "Iteration 753, loss = 2.41826141\n",
      "Iteration 754, loss = 2.41817778\n",
      "Iteration 755, loss = 2.41777909\n",
      "Iteration 756, loss = 2.41737848\n",
      "Iteration 757, loss = 2.41761175\n",
      "Iteration 758, loss = 2.41646423\n",
      "Iteration 759, loss = 2.41637426\n",
      "Iteration 760, loss = 2.41592968\n",
      "Iteration 761, loss = 2.41578061\n",
      "Iteration 762, loss = 2.41565611\n",
      "Iteration 763, loss = 2.41511758\n",
      "Iteration 764, loss = 2.41454187\n",
      "Iteration 765, loss = 2.41480255\n",
      "Iteration 766, loss = 2.41389742\n",
      "Iteration 767, loss = 2.41405758\n",
      "Iteration 768, loss = 2.41365584\n",
      "Iteration 769, loss = 2.41389492\n",
      "Iteration 770, loss = 2.41408761\n",
      "Iteration 771, loss = 2.41242830\n",
      "Iteration 772, loss = 2.41252905\n",
      "Iteration 773, loss = 2.41244045\n",
      "Iteration 774, loss = 2.41168140\n",
      "Iteration 775, loss = 2.41142740\n",
      "Iteration 776, loss = 2.41100413\n",
      "Iteration 777, loss = 2.41085080\n",
      "Iteration 778, loss = 2.41038231\n",
      "Iteration 779, loss = 2.41018983\n",
      "Iteration 780, loss = 2.40969927\n",
      "Iteration 781, loss = 2.40924715\n",
      "Iteration 782, loss = 2.40916327\n",
      "Iteration 783, loss = 2.40874877\n",
      "Iteration 784, loss = 2.40860001\n",
      "Iteration 785, loss = 2.40796544\n",
      "Iteration 786, loss = 2.40813100\n",
      "Iteration 787, loss = 2.40762500\n",
      "Iteration 788, loss = 2.40816604\n",
      "Iteration 789, loss = 2.40656231\n",
      "Iteration 790, loss = 2.40676900\n",
      "Iteration 791, loss = 2.40657427\n",
      "Iteration 792, loss = 2.40602802\n",
      "Iteration 793, loss = 2.40613667\n",
      "Iteration 794, loss = 2.40524193\n",
      "Iteration 795, loss = 2.40509636\n",
      "Iteration 796, loss = 2.40462755\n",
      "Iteration 797, loss = 2.40469477\n",
      "Iteration 798, loss = 2.40464898\n",
      "Iteration 799, loss = 2.40427751\n",
      "Iteration 800, loss = 2.40338460\n",
      "Iteration 801, loss = 2.40327102\n",
      "Iteration 802, loss = 2.40287963\n",
      "Iteration 803, loss = 2.40254437\n",
      "Iteration 804, loss = 2.40245260\n",
      "Iteration 805, loss = 2.40193167\n",
      "Iteration 806, loss = 2.40183593\n",
      "Iteration 807, loss = 2.40175666\n",
      "Iteration 808, loss = 2.40151702\n",
      "Iteration 809, loss = 2.40121748\n",
      "Iteration 810, loss = 2.40053613\n",
      "Iteration 811, loss = 2.40021843\n",
      "Iteration 812, loss = 2.40047551\n",
      "Iteration 813, loss = 2.39978146\n",
      "Iteration 814, loss = 2.39937471\n",
      "Iteration 815, loss = 2.39906868\n",
      "Iteration 816, loss = 2.39912619\n",
      "Iteration 817, loss = 2.39836063\n",
      "Iteration 818, loss = 2.39818812\n",
      "Iteration 819, loss = 2.39794800\n",
      "Iteration 820, loss = 2.39765134\n",
      "Iteration 821, loss = 2.39708955\n",
      "Iteration 822, loss = 2.39742441\n",
      "Iteration 823, loss = 2.39675966\n",
      "Iteration 824, loss = 2.39630278\n",
      "Iteration 825, loss = 2.39596546\n",
      "Iteration 826, loss = 2.39574026\n",
      "Iteration 827, loss = 2.39592026\n",
      "Iteration 828, loss = 2.39518986\n",
      "Iteration 829, loss = 2.39484431\n",
      "Iteration 830, loss = 2.39469075\n",
      "Iteration 831, loss = 2.39427889\n",
      "Iteration 832, loss = 2.39389876\n",
      "Iteration 833, loss = 2.39347188\n",
      "Iteration 834, loss = 2.39325685\n",
      "Iteration 835, loss = 2.39344541\n",
      "Iteration 836, loss = 2.39270754\n",
      "Iteration 837, loss = 2.39258774\n",
      "Iteration 838, loss = 2.39223742\n",
      "Iteration 839, loss = 2.39210647\n",
      "Iteration 840, loss = 2.39158656\n",
      "Iteration 841, loss = 2.39151584\n",
      "Iteration 842, loss = 2.39085883\n",
      "Iteration 843, loss = 2.39072179\n",
      "Iteration 844, loss = 2.39056632\n",
      "Iteration 845, loss = 2.39021327\n",
      "Iteration 846, loss = 2.38975334\n",
      "Iteration 847, loss = 2.38944698\n",
      "Iteration 848, loss = 2.38917420\n",
      "Iteration 849, loss = 2.38894763\n",
      "Iteration 850, loss = 2.38867667\n",
      "Iteration 851, loss = 2.38828975\n",
      "Iteration 852, loss = 2.38806578\n",
      "Iteration 853, loss = 2.38750000\n",
      "Iteration 854, loss = 2.38775424\n",
      "Iteration 855, loss = 2.38709104\n",
      "Iteration 856, loss = 2.38685034\n",
      "Iteration 857, loss = 2.38656651\n",
      "Iteration 858, loss = 2.38621717\n",
      "Iteration 859, loss = 2.38601842\n",
      "Iteration 860, loss = 2.38568493\n",
      "Iteration 861, loss = 2.38540247\n",
      "Iteration 862, loss = 2.38508244\n",
      "Iteration 863, loss = 2.38484195\n",
      "Iteration 864, loss = 2.38469490\n",
      "Iteration 865, loss = 2.38455400\n",
      "Iteration 866, loss = 2.38412509\n",
      "Iteration 867, loss = 2.38425853\n",
      "Iteration 868, loss = 2.38320444\n",
      "Iteration 869, loss = 2.38289115\n",
      "Iteration 870, loss = 2.38287054\n",
      "Iteration 871, loss = 2.38222544\n",
      "Iteration 872, loss = 2.38254905\n",
      "Iteration 873, loss = 2.38192292\n",
      "Iteration 874, loss = 2.38184650\n",
      "Iteration 875, loss = 2.38142328\n",
      "Iteration 876, loss = 2.38114400\n",
      "Iteration 877, loss = 2.38085428\n",
      "Iteration 878, loss = 2.38025231\n",
      "Iteration 879, loss = 2.38043196\n",
      "Iteration 880, loss = 2.37974777\n",
      "Iteration 881, loss = 2.37967490\n",
      "Iteration 882, loss = 2.37951303\n",
      "Iteration 883, loss = 2.37926705\n",
      "Iteration 884, loss = 2.37875309\n",
      "Iteration 885, loss = 2.37850309\n",
      "Iteration 886, loss = 2.37834277\n",
      "Iteration 887, loss = 2.37776747\n",
      "Iteration 888, loss = 2.37752971\n",
      "Iteration 889, loss = 2.37760505\n",
      "Iteration 890, loss = 2.37700616\n",
      "Iteration 891, loss = 2.37656575\n",
      "Iteration 892, loss = 2.37674111\n",
      "Iteration 893, loss = 2.37610454\n",
      "Iteration 894, loss = 2.37605811\n",
      "Iteration 895, loss = 2.37579167\n",
      "Iteration 896, loss = 2.37544468\n",
      "Iteration 897, loss = 2.37482911\n",
      "Iteration 898, loss = 2.37515512\n",
      "Iteration 899, loss = 2.37450419\n",
      "Iteration 900, loss = 2.37440304\n",
      "Iteration 901, loss = 2.37417483\n",
      "Iteration 902, loss = 2.37354227\n",
      "Iteration 903, loss = 2.37350536\n",
      "Iteration 904, loss = 2.37336863\n",
      "Iteration 905, loss = 2.37295699\n",
      "Iteration 906, loss = 2.37239262\n",
      "Iteration 907, loss = 2.37199249\n",
      "Iteration 908, loss = 2.37215058\n",
      "Iteration 909, loss = 2.37169370\n",
      "Iteration 910, loss = 2.37145623\n",
      "Iteration 911, loss = 2.37109084\n",
      "Iteration 912, loss = 2.37082237\n",
      "Iteration 913, loss = 2.37101669\n",
      "Iteration 914, loss = 2.37000351\n",
      "Iteration 915, loss = 2.37000532\n",
      "Iteration 916, loss = 2.36973677\n",
      "Iteration 917, loss = 2.36916511\n",
      "Iteration 918, loss = 2.36910663\n",
      "Iteration 919, loss = 2.36924616\n",
      "Iteration 920, loss = 2.36871992\n",
      "Iteration 921, loss = 2.36825144\n",
      "Iteration 922, loss = 2.36792673\n",
      "Iteration 923, loss = 2.36759004\n",
      "Iteration 924, loss = 2.36767024\n",
      "Iteration 925, loss = 2.36755603\n",
      "Iteration 926, loss = 2.36698549\n",
      "Iteration 927, loss = 2.36668012\n",
      "Iteration 928, loss = 2.36634424\n",
      "Iteration 929, loss = 2.36566585\n",
      "Iteration 930, loss = 2.36562361\n",
      "Iteration 931, loss = 2.36543760\n",
      "Iteration 932, loss = 2.36575135\n",
      "Iteration 933, loss = 2.36491423\n",
      "Iteration 934, loss = 2.36457565\n",
      "Iteration 935, loss = 2.36440810\n",
      "Iteration 936, loss = 2.36383775\n",
      "Iteration 937, loss = 2.36364807\n",
      "Iteration 938, loss = 2.36371586\n",
      "Iteration 939, loss = 2.36273569\n",
      "Iteration 940, loss = 2.36274248\n",
      "Iteration 941, loss = 2.36265479\n",
      "Iteration 942, loss = 2.36236682\n",
      "Iteration 943, loss = 2.36221956\n",
      "Iteration 944, loss = 2.36242919\n",
      "Iteration 945, loss = 2.36173625\n",
      "Iteration 946, loss = 2.36143350\n",
      "Iteration 947, loss = 2.36071917\n",
      "Iteration 948, loss = 2.36086130\n",
      "Iteration 949, loss = 2.36047557\n",
      "Iteration 950, loss = 2.36069653\n",
      "Iteration 951, loss = 2.35987756\n",
      "Iteration 952, loss = 2.35977107\n",
      "Iteration 953, loss = 2.35957377\n",
      "Iteration 954, loss = 2.35970320\n",
      "Iteration 955, loss = 2.35876738\n",
      "Iteration 956, loss = 2.35877400\n",
      "Iteration 957, loss = 2.35854112\n",
      "Iteration 958, loss = 2.35840861\n",
      "Iteration 959, loss = 2.35781213\n",
      "Iteration 960, loss = 2.35800486\n",
      "Iteration 961, loss = 2.35710633\n",
      "Iteration 962, loss = 2.35684411\n",
      "Iteration 963, loss = 2.35634167\n",
      "Iteration 964, loss = 2.35661737\n",
      "Iteration 965, loss = 2.35633295\n",
      "Iteration 966, loss = 2.35586115\n",
      "Iteration 967, loss = 2.35581754\n",
      "Iteration 968, loss = 2.35519205\n",
      "Iteration 969, loss = 2.35486995\n",
      "Iteration 970, loss = 2.35470343\n",
      "Iteration 971, loss = 2.35475637\n",
      "Iteration 972, loss = 2.35400376\n",
      "Iteration 973, loss = 2.35395185\n",
      "Iteration 974, loss = 2.35381396\n",
      "Iteration 975, loss = 2.35297757\n",
      "Iteration 976, loss = 2.35288385\n",
      "Iteration 977, loss = 2.35260759\n",
      "Iteration 978, loss = 2.35244705\n",
      "Iteration 979, loss = 2.35171491\n",
      "Iteration 980, loss = 2.35169064\n",
      "Iteration 981, loss = 2.35163650\n",
      "Iteration 982, loss = 2.35113952\n",
      "Iteration 983, loss = 2.35099336\n",
      "Iteration 984, loss = 2.35128942\n",
      "Iteration 985, loss = 2.35158894\n",
      "Iteration 986, loss = 2.35138107\n",
      "Iteration 987, loss = 2.35026054\n",
      "Iteration 988, loss = 2.34982625\n",
      "Iteration 989, loss = 2.34956763\n",
      "Iteration 990, loss = 2.34919727\n",
      "Iteration 991, loss = 2.34890242\n",
      "Iteration 992, loss = 2.34828896\n",
      "Iteration 993, loss = 2.34816228\n",
      "Iteration 994, loss = 2.34812897\n",
      "Iteration 995, loss = 2.34837338\n",
      "Iteration 996, loss = 2.34811071\n",
      "Iteration 997, loss = 2.34722869\n",
      "Iteration 998, loss = 2.34683450\n",
      "Iteration 999, loss = 2.34718277\n",
      "Iteration 1000, loss = 2.34660091\n",
      "Iteration 1001, loss = 2.34647549\n",
      "Iteration 1002, loss = 2.34569464\n",
      "Iteration 1003, loss = 2.34574335\n",
      "Iteration 1004, loss = 2.34519642\n",
      "Iteration 1005, loss = 2.34506549\n",
      "Iteration 1006, loss = 2.34520829\n",
      "Iteration 1007, loss = 2.34442038\n",
      "Iteration 1008, loss = 2.34459881\n",
      "Iteration 1009, loss = 2.34401135\n",
      "Iteration 1010, loss = 2.34341586\n",
      "Iteration 1011, loss = 2.34330479\n",
      "Iteration 1012, loss = 2.34323154\n",
      "Iteration 1013, loss = 2.34295398\n",
      "Iteration 1014, loss = 2.34268999\n",
      "Iteration 1015, loss = 2.34240975\n",
      "Iteration 1016, loss = 2.34208973\n",
      "Iteration 1017, loss = 2.34194046\n",
      "Iteration 1018, loss = 2.34145367\n",
      "Iteration 1019, loss = 2.34122264\n",
      "Iteration 1020, loss = 2.34115768\n",
      "Iteration 1021, loss = 2.34047853\n",
      "Iteration 1022, loss = 2.34047524\n",
      "Iteration 1023, loss = 2.33983378\n",
      "Iteration 1024, loss = 2.33990738\n",
      "Iteration 1025, loss = 2.33965097\n",
      "Iteration 1026, loss = 2.33965907\n",
      "Iteration 1027, loss = 2.33887646\n",
      "Iteration 1028, loss = 2.33867503\n",
      "Iteration 1029, loss = 2.33838436\n",
      "Iteration 1030, loss = 2.33788518\n",
      "Iteration 1031, loss = 2.33815551\n",
      "Iteration 1032, loss = 2.33800934\n",
      "Iteration 1033, loss = 2.33767649\n",
      "Iteration 1034, loss = 2.33707291\n",
      "Iteration 1035, loss = 2.33685966\n",
      "Iteration 1036, loss = 2.33650788\n",
      "Iteration 1037, loss = 2.33608620\n",
      "Iteration 1038, loss = 2.33598139\n",
      "Iteration 1039, loss = 2.33600972\n",
      "Iteration 1040, loss = 2.33587523\n",
      "Iteration 1041, loss = 2.33506821\n",
      "Iteration 1042, loss = 2.33461801\n",
      "Iteration 1043, loss = 2.33475338\n",
      "Iteration 1044, loss = 2.33435025\n",
      "Iteration 1045, loss = 2.33402503\n",
      "Iteration 1046, loss = 2.33380468\n",
      "Iteration 1047, loss = 2.33367978\n",
      "Iteration 1048, loss = 2.33385436\n",
      "Iteration 1049, loss = 2.33310924\n",
      "Iteration 1050, loss = 2.33371759\n",
      "Iteration 1051, loss = 2.33265012\n",
      "Iteration 1052, loss = 2.33240888\n",
      "Iteration 1053, loss = 2.33204963\n",
      "Iteration 1054, loss = 2.33191963\n",
      "Iteration 1055, loss = 2.33137802\n",
      "Iteration 1056, loss = 2.33096820\n",
      "Iteration 1057, loss = 2.33101005\n",
      "Iteration 1058, loss = 2.33081865\n",
      "Iteration 1059, loss = 2.33026880\n",
      "Iteration 1060, loss = 2.32978665\n",
      "Iteration 1061, loss = 2.32997920\n",
      "Iteration 1062, loss = 2.32940997\n",
      "Iteration 1063, loss = 2.32961311\n",
      "Iteration 1064, loss = 2.32884908\n",
      "Iteration 1065, loss = 2.32886360\n",
      "Iteration 1066, loss = 2.32831464\n",
      "Iteration 1067, loss = 2.32816651\n",
      "Iteration 1068, loss = 2.32781631\n",
      "Iteration 1069, loss = 2.32793675\n",
      "Iteration 1070, loss = 2.32739230\n",
      "Iteration 1071, loss = 2.32715002\n",
      "Iteration 1072, loss = 2.32673409\n",
      "Iteration 1073, loss = 2.32657216\n",
      "Iteration 1074, loss = 2.32631434\n",
      "Iteration 1075, loss = 2.32595444\n",
      "Iteration 1076, loss = 2.32530402\n",
      "Iteration 1077, loss = 2.32531616\n",
      "Iteration 1078, loss = 2.32499687\n",
      "Iteration 1079, loss = 2.32462241\n",
      "Iteration 1080, loss = 2.32480204\n",
      "Iteration 1081, loss = 2.32430962\n",
      "Iteration 1082, loss = 2.32432354\n",
      "Iteration 1083, loss = 2.32385988\n",
      "Iteration 1084, loss = 2.32321160\n",
      "Iteration 1085, loss = 2.32327413\n",
      "Iteration 1086, loss = 2.32300479\n",
      "Iteration 1087, loss = 2.32295943\n",
      "Iteration 1088, loss = 2.32222755\n",
      "Iteration 1089, loss = 2.32188150\n",
      "Iteration 1090, loss = 2.32165733\n",
      "Iteration 1091, loss = 2.32132302\n",
      "Iteration 1092, loss = 2.32167240\n",
      "Iteration 1093, loss = 2.32115046\n",
      "Iteration 1094, loss = 2.32123796\n",
      "Iteration 1095, loss = 2.32064733\n",
      "Iteration 1096, loss = 2.32005468\n",
      "Iteration 1097, loss = 2.31951703\n",
      "Iteration 1098, loss = 2.31949659\n",
      "Iteration 1099, loss = 2.31935534\n",
      "Iteration 1100, loss = 2.31915336\n",
      "Iteration 1101, loss = 2.31879985\n",
      "Iteration 1102, loss = 2.31847389\n",
      "Iteration 1103, loss = 2.31826698\n",
      "Iteration 1104, loss = 2.31769290\n",
      "Iteration 1105, loss = 2.31740132\n",
      "Iteration 1106, loss = 2.31731119\n",
      "Iteration 1107, loss = 2.31712728\n",
      "Iteration 1108, loss = 2.31673314\n",
      "Iteration 1109, loss = 2.31666719\n",
      "Iteration 1110, loss = 2.31683645\n",
      "Iteration 1111, loss = 2.31586865\n",
      "Iteration 1112, loss = 2.31636555\n",
      "Iteration 1113, loss = 2.31545634\n",
      "Iteration 1114, loss = 2.31536688\n",
      "Iteration 1115, loss = 2.31500801\n",
      "Iteration 1116, loss = 2.31474694\n",
      "Iteration 1117, loss = 2.31445245\n",
      "Iteration 1118, loss = 2.31434838\n",
      "Iteration 1119, loss = 2.31370286\n",
      "Iteration 1120, loss = 2.31375909\n",
      "Iteration 1121, loss = 2.31316283\n",
      "Iteration 1122, loss = 2.31332819\n",
      "Iteration 1123, loss = 2.31281744\n",
      "Iteration 1124, loss = 2.31222222\n",
      "Iteration 1125, loss = 2.31229463\n",
      "Iteration 1126, loss = 2.31220498\n",
      "Iteration 1127, loss = 2.31186668\n",
      "Iteration 1128, loss = 2.31130442\n",
      "Iteration 1129, loss = 2.31091284\n",
      "Iteration 1130, loss = 2.31048605\n",
      "Iteration 1131, loss = 2.31041344\n",
      "Iteration 1132, loss = 2.31044959\n",
      "Iteration 1133, loss = 2.30981627\n",
      "Iteration 1134, loss = 2.30985602\n",
      "Iteration 1135, loss = 2.30959671\n",
      "Iteration 1136, loss = 2.30893249\n",
      "Iteration 1137, loss = 2.30892791\n",
      "Iteration 1138, loss = 2.30881440\n",
      "Iteration 1139, loss = 2.30828586\n",
      "Iteration 1140, loss = 2.30814939\n",
      "Iteration 1141, loss = 2.30773800\n",
      "Iteration 1142, loss = 2.30743259\n",
      "Iteration 1143, loss = 2.30733542\n",
      "Iteration 1144, loss = 2.30698850\n",
      "Iteration 1145, loss = 2.30659715\n",
      "Iteration 1146, loss = 2.30634531\n",
      "Iteration 1147, loss = 2.30633825\n",
      "Iteration 1148, loss = 2.30600352\n",
      "Iteration 1149, loss = 2.30531376\n",
      "Iteration 1150, loss = 2.30482303\n",
      "Iteration 1151, loss = 2.30483844\n",
      "Iteration 1152, loss = 2.30478579\n",
      "Iteration 1153, loss = 2.30425557\n",
      "Iteration 1154, loss = 2.30432173\n",
      "Iteration 1155, loss = 2.30399344\n",
      "Iteration 1156, loss = 2.30354331\n",
      "Iteration 1157, loss = 2.30299147\n",
      "Iteration 1158, loss = 2.30328798\n",
      "Iteration 1159, loss = 2.30244141\n",
      "Iteration 1160, loss = 2.30271833\n",
      "Iteration 1161, loss = 2.30218664\n",
      "Iteration 1162, loss = 2.30155557\n",
      "Iteration 1163, loss = 2.30183998\n",
      "Iteration 1164, loss = 2.30174448\n",
      "Iteration 1165, loss = 2.30107266\n",
      "Iteration 1166, loss = 2.30072567\n",
      "Iteration 1167, loss = 2.30032300\n",
      "Iteration 1168, loss = 2.30005624\n",
      "Iteration 1169, loss = 2.29957328\n",
      "Iteration 1170, loss = 2.29927678\n",
      "Iteration 1171, loss = 2.29931743\n",
      "Iteration 1172, loss = 2.29907552\n",
      "Iteration 1173, loss = 2.29875003\n",
      "Iteration 1174, loss = 2.29848098\n",
      "Iteration 1175, loss = 2.29831331\n",
      "Iteration 1176, loss = 2.29781706\n",
      "Iteration 1177, loss = 2.29772753\n",
      "Iteration 1178, loss = 2.29760777\n",
      "Iteration 1179, loss = 2.29664120\n",
      "Iteration 1180, loss = 2.29705124\n",
      "Iteration 1181, loss = 2.29646357\n",
      "Iteration 1182, loss = 2.29597173\n",
      "Iteration 1183, loss = 2.29554572\n",
      "Iteration 1184, loss = 2.29535335\n",
      "Iteration 1185, loss = 2.29535587\n",
      "Iteration 1186, loss = 2.29466428\n",
      "Iteration 1187, loss = 2.29454703\n",
      "Iteration 1188, loss = 2.29436868\n",
      "Iteration 1189, loss = 2.29398014\n",
      "Iteration 1190, loss = 2.29352983\n",
      "Iteration 1191, loss = 2.29336788\n",
      "Iteration 1192, loss = 2.29322866\n",
      "Iteration 1193, loss = 2.29328687\n",
      "Iteration 1194, loss = 2.29217114\n",
      "Iteration 1195, loss = 2.29213557\n",
      "Iteration 1196, loss = 2.29241884\n",
      "Iteration 1197, loss = 2.29196268\n",
      "Iteration 1198, loss = 2.29117367\n",
      "Iteration 1199, loss = 2.29094943\n",
      "Iteration 1200, loss = 2.29069846\n",
      "Iteration 1201, loss = 2.29020914\n",
      "Iteration 1202, loss = 2.29083348\n",
      "Iteration 1203, loss = 2.29001118\n",
      "Iteration 1204, loss = 2.28984809\n",
      "Iteration 1205, loss = 2.28918411\n",
      "Iteration 1206, loss = 2.28894542\n",
      "Iteration 1207, loss = 2.28854272\n",
      "Iteration 1208, loss = 2.28873252\n",
      "Iteration 1209, loss = 2.28837951\n",
      "Iteration 1210, loss = 2.28793232\n",
      "Iteration 1211, loss = 2.28761168\n",
      "Iteration 1212, loss = 2.28694090\n",
      "Iteration 1213, loss = 2.28656552\n",
      "Iteration 1214, loss = 2.28634757\n",
      "Iteration 1215, loss = 2.28636311\n",
      "Iteration 1216, loss = 2.28684965\n",
      "Iteration 1217, loss = 2.28564590\n",
      "Iteration 1218, loss = 2.28552565\n",
      "Iteration 1219, loss = 2.28529625\n",
      "Iteration 1220, loss = 2.28458159\n",
      "Iteration 1221, loss = 2.28442604\n",
      "Iteration 1222, loss = 2.28432296\n",
      "Iteration 1223, loss = 2.28405119\n",
      "Iteration 1224, loss = 2.28343149\n",
      "Iteration 1225, loss = 2.28377940\n",
      "Iteration 1226, loss = 2.28334741\n",
      "Iteration 1227, loss = 2.28306381\n",
      "Iteration 1228, loss = 2.28233442\n",
      "Iteration 1229, loss = 2.28222102\n",
      "Iteration 1230, loss = 2.28183871\n",
      "Iteration 1231, loss = 2.28149770\n",
      "Iteration 1232, loss = 2.28108222\n",
      "Iteration 1233, loss = 2.28111175\n",
      "Iteration 1234, loss = 2.28088659\n",
      "Iteration 1235, loss = 2.28022903\n",
      "Iteration 1236, loss = 2.28017146\n",
      "Iteration 1237, loss = 2.28032152\n",
      "Iteration 1238, loss = 2.27968271\n",
      "Iteration 1239, loss = 2.27944350\n",
      "Iteration 1240, loss = 2.27877358\n",
      "Iteration 1241, loss = 2.27911829\n",
      "Iteration 1242, loss = 2.27849870\n",
      "Iteration 1243, loss = 2.27815394\n",
      "Iteration 1244, loss = 2.27781257\n",
      "Iteration 1245, loss = 2.27724866\n",
      "Iteration 1246, loss = 2.27697162\n",
      "Iteration 1247, loss = 2.27683086\n",
      "Iteration 1248, loss = 2.27666785\n",
      "Iteration 1249, loss = 2.27637653\n",
      "Iteration 1250, loss = 2.27607373\n",
      "Iteration 1251, loss = 2.27625118\n",
      "Iteration 1252, loss = 2.27585880\n",
      "Iteration 1253, loss = 2.27514662\n",
      "Iteration 1254, loss = 2.27505509\n",
      "Iteration 1255, loss = 2.27449883\n",
      "Iteration 1256, loss = 2.27427042\n",
      "Iteration 1257, loss = 2.27411526\n",
      "Iteration 1258, loss = 2.27372528\n",
      "Iteration 1259, loss = 2.27337941\n",
      "Iteration 1260, loss = 2.27338309\n",
      "Iteration 1261, loss = 2.27289872\n",
      "Iteration 1262, loss = 2.27303433\n",
      "Iteration 1263, loss = 2.27264992\n",
      "Iteration 1264, loss = 2.27192514\n",
      "Iteration 1265, loss = 2.27163946\n",
      "Iteration 1266, loss = 2.27188700\n",
      "Iteration 1267, loss = 2.27148725\n",
      "Iteration 1268, loss = 2.27073689\n",
      "Iteration 1269, loss = 2.27070888\n",
      "Iteration 1270, loss = 2.27043953\n",
      "Iteration 1271, loss = 2.27037882\n",
      "Iteration 1272, loss = 2.27005971\n",
      "Iteration 1273, loss = 2.26958500\n",
      "Iteration 1274, loss = 2.26924739\n",
      "Iteration 1275, loss = 2.26953346\n",
      "Iteration 1276, loss = 2.26833990\n",
      "Iteration 1277, loss = 2.26843966\n",
      "Iteration 1278, loss = 2.26792433\n",
      "Iteration 1279, loss = 2.26811809\n",
      "Iteration 1280, loss = 2.26745711\n",
      "Iteration 1281, loss = 2.26710661\n",
      "Iteration 1282, loss = 2.26698945\n",
      "Iteration 1283, loss = 2.26657102\n",
      "Iteration 1284, loss = 2.26621901\n",
      "Iteration 1285, loss = 2.26625091\n",
      "Iteration 1286, loss = 2.26562630\n",
      "Iteration 1287, loss = 2.26565039\n",
      "Iteration 1288, loss = 2.26533041\n",
      "Iteration 1289, loss = 2.26501606\n",
      "Iteration 1290, loss = 2.26438752\n",
      "Iteration 1291, loss = 2.26431736\n",
      "Iteration 1292, loss = 2.26409314\n",
      "Iteration 1293, loss = 2.26425503\n",
      "Iteration 1294, loss = 2.26316615\n",
      "Iteration 1295, loss = 2.26378102\n",
      "Iteration 1296, loss = 2.26308901\n",
      "Iteration 1297, loss = 2.26328813\n",
      "Iteration 1298, loss = 2.26290325\n",
      "Iteration 1299, loss = 2.26231232\n",
      "Iteration 1300, loss = 2.26165613\n",
      "Iteration 1301, loss = 2.26168831\n",
      "Iteration 1302, loss = 2.26147650\n",
      "Iteration 1303, loss = 2.26142195\n",
      "Iteration 1304, loss = 2.26053438\n",
      "Iteration 1305, loss = 2.26035145\n",
      "Iteration 1306, loss = 2.26033382\n",
      "Iteration 1307, loss = 2.26015631\n",
      "Iteration 1308, loss = 2.26028835\n",
      "Iteration 1309, loss = 2.25969008\n",
      "Iteration 1310, loss = 2.25885932\n",
      "Iteration 1311, loss = 2.25879630\n",
      "Iteration 1312, loss = 2.25878580\n",
      "Iteration 1313, loss = 2.25827828\n",
      "Iteration 1314, loss = 2.25823457\n",
      "Iteration 1315, loss = 2.25760152\n",
      "Iteration 1316, loss = 2.25774463\n",
      "Iteration 1317, loss = 2.25712656\n",
      "Iteration 1318, loss = 2.25711009\n",
      "Iteration 1319, loss = 2.25651432\n",
      "Iteration 1320, loss = 2.25631358\n",
      "Iteration 1321, loss = 2.25606562\n",
      "Iteration 1322, loss = 2.25627839\n",
      "Iteration 1323, loss = 2.25549224\n",
      "Iteration 1324, loss = 2.25558493\n",
      "Iteration 1325, loss = 2.25485078\n",
      "Iteration 1326, loss = 2.25488884\n",
      "Iteration 1327, loss = 2.25434949\n",
      "Iteration 1328, loss = 2.25417837\n",
      "Iteration 1329, loss = 2.25427937\n",
      "Iteration 1330, loss = 2.25371335\n",
      "Iteration 1331, loss = 2.25338081\n",
      "Iteration 1332, loss = 2.25309753\n",
      "Iteration 1333, loss = 2.25260136\n",
      "Iteration 1334, loss = 2.25315602\n",
      "Iteration 1335, loss = 2.25310321\n",
      "Iteration 1336, loss = 2.25216485\n",
      "Iteration 1337, loss = 2.25204465\n",
      "Iteration 1338, loss = 2.25171548\n",
      "Iteration 1339, loss = 2.25155308\n",
      "Iteration 1340, loss = 2.25128651\n",
      "Iteration 1341, loss = 2.25099594\n",
      "Iteration 1342, loss = 2.25063558\n",
      "Iteration 1343, loss = 2.25035502\n",
      "Iteration 1344, loss = 2.25004867\n",
      "Iteration 1345, loss = 2.24931115\n",
      "Iteration 1346, loss = 2.24984112\n",
      "Iteration 1347, loss = 2.24902301\n",
      "Iteration 1348, loss = 2.24911042\n",
      "Iteration 1349, loss = 2.24872720\n",
      "Iteration 1350, loss = 2.24851052\n",
      "Iteration 1351, loss = 2.24799443\n",
      "Iteration 1352, loss = 2.24783444\n",
      "Iteration 1353, loss = 2.24766695\n",
      "Iteration 1354, loss = 2.24713394\n",
      "Iteration 1355, loss = 2.24737761\n",
      "Iteration 1356, loss = 2.24697319\n",
      "Iteration 1357, loss = 2.24660217\n",
      "Iteration 1358, loss = 2.24650429\n",
      "Iteration 1359, loss = 2.24616851\n",
      "Iteration 1360, loss = 2.24578929\n",
      "Iteration 1361, loss = 2.24576433\n",
      "Iteration 1362, loss = 2.24518343\n",
      "Iteration 1363, loss = 2.24577232\n",
      "Iteration 1364, loss = 2.24456448\n",
      "Iteration 1365, loss = 2.24494502\n",
      "Iteration 1366, loss = 2.24408195\n",
      "Iteration 1367, loss = 2.24393344\n",
      "Iteration 1368, loss = 2.24378100\n",
      "Iteration 1369, loss = 2.24338145\n",
      "Iteration 1370, loss = 2.24333347\n",
      "Iteration 1371, loss = 2.24327591\n",
      "Iteration 1372, loss = 2.24256257\n",
      "Iteration 1373, loss = 2.24294829\n",
      "Iteration 1374, loss = 2.24186059\n",
      "Iteration 1375, loss = 2.24170137\n",
      "Iteration 1376, loss = 2.24159369\n",
      "Iteration 1377, loss = 2.24138336\n",
      "Iteration 1378, loss = 2.24160898\n",
      "Iteration 1379, loss = 2.24114264\n",
      "Iteration 1380, loss = 2.24145474\n",
      "Iteration 1381, loss = 2.24091640\n",
      "Iteration 1382, loss = 2.24075316\n",
      "Iteration 1383, loss = 2.24011639\n",
      "Iteration 1384, loss = 2.23968910\n",
      "Iteration 1385, loss = 2.23949805\n",
      "Iteration 1386, loss = 2.23910324\n",
      "Iteration 1387, loss = 2.23901617\n",
      "Iteration 1388, loss = 2.23890905\n",
      "Iteration 1389, loss = 2.23842476\n",
      "Iteration 1390, loss = 2.23819368\n",
      "Iteration 1391, loss = 2.23757724\n",
      "Iteration 1392, loss = 2.23751406\n",
      "Iteration 1393, loss = 2.23721468\n",
      "Iteration 1394, loss = 2.23699304\n",
      "Iteration 1395, loss = 2.23667691\n",
      "Iteration 1396, loss = 2.23668818\n",
      "Iteration 1397, loss = 2.23627807\n",
      "Iteration 1398, loss = 2.23579759\n",
      "Iteration 1399, loss = 2.23592160\n",
      "Iteration 1400, loss = 2.23558492\n",
      "Iteration 1401, loss = 2.23521356\n",
      "Iteration 1402, loss = 2.23515259\n",
      "Iteration 1403, loss = 2.23495802\n",
      "Iteration 1404, loss = 2.23447099\n",
      "Iteration 1405, loss = 2.23401634\n",
      "Iteration 1406, loss = 2.23416500\n",
      "Iteration 1407, loss = 2.23370892\n",
      "Iteration 1408, loss = 2.23400009\n",
      "Iteration 1409, loss = 2.23339640\n",
      "Iteration 1410, loss = 2.23272855\n",
      "Iteration 1411, loss = 2.23262693\n",
      "Iteration 1412, loss = 2.23245261\n",
      "Iteration 1413, loss = 2.23256437\n",
      "Iteration 1414, loss = 2.23179984\n",
      "Iteration 1415, loss = 2.23168269\n",
      "Iteration 1416, loss = 2.23130013\n",
      "Iteration 1417, loss = 2.23098296\n",
      "Iteration 1418, loss = 2.23117577\n",
      "Iteration 1419, loss = 2.23060281\n",
      "Iteration 1420, loss = 2.23076223\n",
      "Iteration 1421, loss = 2.23007992\n",
      "Iteration 1422, loss = 2.23002282\n",
      "Iteration 1423, loss = 2.22968620\n",
      "Iteration 1424, loss = 2.22944968\n",
      "Iteration 1425, loss = 2.22944363\n",
      "Iteration 1426, loss = 2.22866131\n",
      "Iteration 1427, loss = 2.22919509\n",
      "Iteration 1428, loss = 2.22836225\n",
      "Iteration 1429, loss = 2.22822729\n",
      "Iteration 1430, loss = 2.22799310\n",
      "Iteration 1431, loss = 2.22859195\n",
      "Iteration 1432, loss = 2.22754474\n",
      "Iteration 1433, loss = 2.22728669\n",
      "Iteration 1434, loss = 2.22677064\n",
      "Iteration 1435, loss = 2.22672945\n",
      "Iteration 1436, loss = 2.22632780\n",
      "Iteration 1437, loss = 2.22630120\n",
      "Iteration 1438, loss = 2.22596294\n",
      "Iteration 1439, loss = 2.22599536\n",
      "Iteration 1440, loss = 2.22555089\n",
      "Iteration 1441, loss = 2.22503050\n",
      "Iteration 1442, loss = 2.22529409\n",
      "Iteration 1443, loss = 2.22470856\n",
      "Iteration 1444, loss = 2.22458431\n",
      "Iteration 1445, loss = 2.22392371\n",
      "Iteration 1446, loss = 2.22400735\n",
      "Iteration 1447, loss = 2.22369299\n",
      "Iteration 1448, loss = 2.22368181\n",
      "Iteration 1449, loss = 2.22388403\n",
      "Iteration 1450, loss = 2.22298449\n",
      "Iteration 1451, loss = 2.22312388\n",
      "Iteration 1452, loss = 2.22240426\n",
      "Iteration 1453, loss = 2.22238968\n",
      "Iteration 1454, loss = 2.22204084\n",
      "Iteration 1455, loss = 2.22166434\n",
      "Iteration 1456, loss = 2.22146336\n",
      "Iteration 1457, loss = 2.22101016\n",
      "Iteration 1458, loss = 2.22082348\n",
      "Iteration 1459, loss = 2.22071282\n",
      "Iteration 1460, loss = 2.22109324\n",
      "Iteration 1461, loss = 2.22036690\n",
      "Iteration 1462, loss = 2.22021209\n",
      "Iteration 1463, loss = 2.22013563\n",
      "Iteration 1464, loss = 2.21987067\n",
      "Iteration 1465, loss = 2.21901519\n",
      "Iteration 1466, loss = 2.21915102\n",
      "Iteration 1467, loss = 2.21948667\n",
      "Iteration 1468, loss = 2.21874481\n",
      "Iteration 1469, loss = 2.21823027\n",
      "Iteration 1470, loss = 2.21815074\n",
      "Iteration 1471, loss = 2.21816636\n",
      "Iteration 1472, loss = 2.21769268\n",
      "Iteration 1473, loss = 2.21722458\n",
      "Iteration 1474, loss = 2.21718121\n",
      "Iteration 1475, loss = 2.21702807\n",
      "Iteration 1476, loss = 2.21665152\n",
      "Iteration 1477, loss = 2.21663094\n",
      "Iteration 1478, loss = 2.21616256\n",
      "Iteration 1479, loss = 2.21588387\n",
      "Iteration 1480, loss = 2.21561592\n",
      "Iteration 1481, loss = 2.21551598\n",
      "Iteration 1482, loss = 2.21561336\n",
      "Iteration 1483, loss = 2.21555255\n",
      "Iteration 1484, loss = 2.21494507\n",
      "Iteration 1485, loss = 2.21493008\n",
      "Iteration 1486, loss = 2.21446322\n",
      "Iteration 1487, loss = 2.21389407\n",
      "Iteration 1488, loss = 2.21381082\n",
      "Iteration 1489, loss = 2.21324623\n",
      "Iteration 1490, loss = 2.21368600\n",
      "Iteration 1491, loss = 2.21298790\n",
      "Iteration 1492, loss = 2.21304517\n",
      "Iteration 1493, loss = 2.21281359\n",
      "Iteration 1494, loss = 2.21249059\n",
      "Iteration 1495, loss = 2.21203476\n",
      "Iteration 1496, loss = 2.21227166\n",
      "Iteration 1497, loss = 2.21195561\n",
      "Iteration 1498, loss = 2.21149911\n",
      "Iteration 1499, loss = 2.21123072\n",
      "Iteration 1500, loss = 2.21109462\n",
      "Iteration 1501, loss = 2.21066984\n",
      "Iteration 1502, loss = 2.21129835\n",
      "Iteration 1503, loss = 2.21054864\n",
      "Iteration 1504, loss = 2.21014901\n",
      "Iteration 1505, loss = 2.20986294\n",
      "Iteration 1506, loss = 2.20985799\n",
      "Iteration 1507, loss = 2.20957988\n",
      "Iteration 1508, loss = 2.20892977\n",
      "Iteration 1509, loss = 2.20894091\n",
      "Iteration 1510, loss = 2.20861719\n",
      "Iteration 1511, loss = 2.20866476\n",
      "Iteration 1512, loss = 2.20836368\n",
      "Iteration 1513, loss = 2.20832880\n",
      "Iteration 1514, loss = 2.20761121\n",
      "Iteration 1515, loss = 2.20788914\n",
      "Iteration 1516, loss = 2.20728675\n",
      "Iteration 1517, loss = 2.20687783\n",
      "Iteration 1518, loss = 2.20704804\n",
      "Iteration 1519, loss = 2.20730240\n",
      "Iteration 1520, loss = 2.20707935\n",
      "Iteration 1521, loss = 2.20624031\n",
      "Iteration 1522, loss = 2.20575018\n",
      "Iteration 1523, loss = 2.20563830\n",
      "Iteration 1524, loss = 2.20563006\n",
      "Iteration 1525, loss = 2.20524979\n",
      "Iteration 1526, loss = 2.20506896\n",
      "Iteration 1527, loss = 2.20495917\n",
      "Iteration 1528, loss = 2.20453845\n",
      "Iteration 1529, loss = 2.20435311\n",
      "Iteration 1530, loss = 2.20401287\n",
      "Iteration 1531, loss = 2.20403128\n",
      "Iteration 1532, loss = 2.20379851\n",
      "Iteration 1533, loss = 2.20348659\n",
      "Iteration 1534, loss = 2.20364040\n",
      "Iteration 1535, loss = 2.20297928\n",
      "Iteration 1536, loss = 2.20300676\n",
      "Iteration 1537, loss = 2.20258023\n",
      "Iteration 1538, loss = 2.20240554\n",
      "Iteration 1539, loss = 2.20221054\n",
      "Iteration 1540, loss = 2.20185126\n",
      "Iteration 1541, loss = 2.20196590\n",
      "Iteration 1542, loss = 2.20169953\n",
      "Iteration 1543, loss = 2.20151551\n",
      "Iteration 1544, loss = 2.20127223\n",
      "Iteration 1545, loss = 2.20085510\n",
      "Iteration 1546, loss = 2.20070901\n",
      "Iteration 1547, loss = 2.20047836\n",
      "Iteration 1548, loss = 2.20042496\n",
      "Iteration 1549, loss = 2.19998367\n",
      "Iteration 1550, loss = 2.19985157\n",
      "Iteration 1551, loss = 2.19994582\n",
      "Iteration 1552, loss = 2.19935084\n",
      "Iteration 1553, loss = 2.19942793\n",
      "Iteration 1554, loss = 2.19867984\n",
      "Iteration 1555, loss = 2.19860843\n",
      "Iteration 1556, loss = 2.19873924\n",
      "Iteration 1557, loss = 2.19836219\n",
      "Iteration 1558, loss = 2.19828626\n",
      "Iteration 1559, loss = 2.19773810\n",
      "Iteration 1560, loss = 2.19764316\n",
      "Iteration 1561, loss = 2.19704538\n",
      "Iteration 1562, loss = 2.19729103\n",
      "Iteration 1563, loss = 2.19697964\n",
      "Iteration 1564, loss = 2.19653217\n",
      "Iteration 1565, loss = 2.19668977\n",
      "Iteration 1566, loss = 2.19593660\n",
      "Iteration 1567, loss = 2.19599309\n",
      "Iteration 1568, loss = 2.19577266\n",
      "Iteration 1569, loss = 2.19563993\n",
      "Iteration 1570, loss = 2.19564060\n",
      "Iteration 1571, loss = 2.19494737\n",
      "Iteration 1572, loss = 2.19599852\n",
      "Iteration 1573, loss = 2.19491477\n",
      "Iteration 1574, loss = 2.19496484\n",
      "Iteration 1575, loss = 2.19479869\n",
      "Iteration 1576, loss = 2.19416974\n",
      "Iteration 1577, loss = 2.19427580\n",
      "Iteration 1578, loss = 2.19379017\n",
      "Iteration 1579, loss = 2.19328799\n",
      "Iteration 1580, loss = 2.19332514\n",
      "Iteration 1581, loss = 2.19261227\n",
      "Iteration 1582, loss = 2.19294379\n",
      "Iteration 1583, loss = 2.19244570\n",
      "Iteration 1584, loss = 2.19238167\n",
      "Iteration 1585, loss = 2.19207459\n",
      "Iteration 1586, loss = 2.19216746\n",
      "Iteration 1587, loss = 2.19172647\n",
      "Iteration 1588, loss = 2.19188452\n",
      "Iteration 1589, loss = 2.19151737\n",
      "Iteration 1590, loss = 2.19105168\n",
      "Iteration 1591, loss = 2.19068921\n",
      "Iteration 1592, loss = 2.19073372\n",
      "Iteration 1593, loss = 2.19077253\n",
      "Iteration 1594, loss = 2.19024894\n",
      "Iteration 1595, loss = 2.19019327\n",
      "Iteration 1596, loss = 2.18971529\n",
      "Iteration 1597, loss = 2.18956649\n",
      "Iteration 1598, loss = 2.18912472\n",
      "Iteration 1599, loss = 2.18939063\n",
      "Iteration 1600, loss = 2.18893016\n",
      "Iteration 1601, loss = 2.18883833\n",
      "Iteration 1602, loss = 2.18816962\n",
      "Iteration 1603, loss = 2.18882391\n",
      "Iteration 1604, loss = 2.18789408\n",
      "Iteration 1605, loss = 2.18805969\n",
      "Iteration 1606, loss = 2.18762763\n",
      "Iteration 1607, loss = 2.18774476\n",
      "Iteration 1608, loss = 2.18708162\n",
      "Iteration 1609, loss = 2.18704937\n",
      "Iteration 1610, loss = 2.18724166\n",
      "Iteration 1611, loss = 2.18668650\n",
      "Iteration 1612, loss = 2.18658482\n",
      "Iteration 1613, loss = 2.18610059\n",
      "Iteration 1614, loss = 2.18640602\n",
      "Iteration 1615, loss = 2.18571049\n",
      "Iteration 1616, loss = 2.18573335\n",
      "Iteration 1617, loss = 2.18538493\n",
      "Iteration 1618, loss = 2.18524247\n",
      "Iteration 1619, loss = 2.18508447\n",
      "Iteration 1620, loss = 2.18469872\n",
      "Iteration 1621, loss = 2.18438603\n",
      "Iteration 1622, loss = 2.18442912\n",
      "Iteration 1623, loss = 2.18446751\n",
      "Iteration 1624, loss = 2.18393994\n",
      "Iteration 1625, loss = 2.18397946\n",
      "Iteration 1626, loss = 2.18329270\n",
      "Iteration 1627, loss = 2.18335515\n",
      "Iteration 1628, loss = 2.18340988\n",
      "Iteration 1629, loss = 2.18278101\n",
      "Iteration 1630, loss = 2.18266920\n",
      "Iteration 1631, loss = 2.18235063\n",
      "Iteration 1632, loss = 2.18218071\n",
      "Iteration 1633, loss = 2.18203803\n",
      "Iteration 1634, loss = 2.18176093\n",
      "Iteration 1635, loss = 2.18188797\n",
      "Iteration 1636, loss = 2.18180179\n",
      "Iteration 1637, loss = 2.18140572\n",
      "Iteration 1638, loss = 2.18140320\n",
      "Iteration 1639, loss = 2.18084487\n",
      "Iteration 1640, loss = 2.18092734\n",
      "Iteration 1641, loss = 2.18043600\n",
      "Iteration 1642, loss = 2.18015007\n",
      "Iteration 1643, loss = 2.17998175\n",
      "Iteration 1644, loss = 2.17991628\n",
      "Iteration 1645, loss = 2.18030200\n",
      "Iteration 1646, loss = 2.17916729\n",
      "Iteration 1647, loss = 2.17890789\n",
      "Iteration 1648, loss = 2.17891038\n",
      "Iteration 1649, loss = 2.17891471\n",
      "Iteration 1650, loss = 2.17851612\n",
      "Iteration 1651, loss = 2.17854596\n",
      "Iteration 1652, loss = 2.17790478\n",
      "Iteration 1653, loss = 2.17862908\n",
      "Iteration 1654, loss = 2.17808531\n",
      "Iteration 1655, loss = 2.17739183\n",
      "Iteration 1656, loss = 2.17782494\n",
      "Iteration 1657, loss = 2.17765249\n",
      "Iteration 1658, loss = 2.17732330\n",
      "Iteration 1659, loss = 2.17722946\n",
      "Iteration 1660, loss = 2.17693730\n",
      "Iteration 1661, loss = 2.17663070\n",
      "Iteration 1662, loss = 2.17604170\n",
      "Iteration 1663, loss = 2.17628424\n",
      "Iteration 1664, loss = 2.17612390\n",
      "Iteration 1665, loss = 2.17551304\n",
      "Iteration 1666, loss = 2.17535145\n",
      "Iteration 1667, loss = 2.17496108\n",
      "Iteration 1668, loss = 2.17502257\n",
      "Iteration 1669, loss = 2.17496683\n",
      "Iteration 1670, loss = 2.17471500\n",
      "Iteration 1671, loss = 2.17438977\n",
      "Iteration 1672, loss = 2.17415112\n",
      "Iteration 1673, loss = 2.17445589\n",
      "Iteration 1674, loss = 2.17441971\n",
      "Iteration 1675, loss = 2.17402999\n",
      "Iteration 1676, loss = 2.17343323\n",
      "Iteration 1677, loss = 2.17324472\n",
      "Iteration 1678, loss = 2.17265376\n",
      "Iteration 1679, loss = 2.17326307\n",
      "Iteration 1680, loss = 2.17264279\n",
      "Iteration 1681, loss = 2.17239648\n",
      "Iteration 1682, loss = 2.17257051\n",
      "Iteration 1683, loss = 2.17164341\n",
      "Iteration 1684, loss = 2.17181359\n",
      "Iteration 1685, loss = 2.17161704\n",
      "Iteration 1686, loss = 2.17111045\n",
      "Iteration 1687, loss = 2.17129077\n",
      "Iteration 1688, loss = 2.17092443\n",
      "Iteration 1689, loss = 2.17095371\n",
      "Iteration 1690, loss = 2.17116639\n",
      "Iteration 1691, loss = 2.17148545\n",
      "Iteration 1692, loss = 2.17027468\n",
      "Iteration 1693, loss = 2.16989368\n",
      "Iteration 1694, loss = 2.17013832\n",
      "Iteration 1695, loss = 2.16948088\n",
      "Iteration 1696, loss = 2.16946876\n",
      "Iteration 1697, loss = 2.16941012\n",
      "Iteration 1698, loss = 2.16937912\n",
      "Iteration 1699, loss = 2.16910699\n",
      "Iteration 1700, loss = 2.16896386\n",
      "Iteration 1701, loss = 2.16848762\n",
      "Iteration 1702, loss = 2.16858730\n",
      "Iteration 1703, loss = 2.16842832\n",
      "Iteration 1704, loss = 2.16831110\n",
      "Iteration 1705, loss = 2.16776070\n",
      "Iteration 1706, loss = 2.16742708\n",
      "Iteration 1707, loss = 2.16769993\n",
      "Iteration 1708, loss = 2.16702461\n",
      "Iteration 1709, loss = 2.16711125\n",
      "Iteration 1710, loss = 2.16743995\n",
      "Iteration 1711, loss = 2.16685242\n",
      "Iteration 1712, loss = 2.16661071\n",
      "Iteration 1713, loss = 2.16683595\n",
      "Iteration 1714, loss = 2.16588521\n",
      "Iteration 1715, loss = 2.16599134\n",
      "Iteration 1716, loss = 2.16589174\n",
      "Iteration 1717, loss = 2.16554365\n",
      "Iteration 1718, loss = 2.16560658\n",
      "Iteration 1719, loss = 2.16497870\n",
      "Iteration 1720, loss = 2.16508983\n",
      "Iteration 1721, loss = 2.16478769\n",
      "Iteration 1722, loss = 2.16453293\n",
      "Iteration 1723, loss = 2.16412251\n",
      "Iteration 1724, loss = 2.16390224\n",
      "Iteration 1725, loss = 2.16394965\n",
      "Iteration 1726, loss = 2.16389186\n",
      "Iteration 1727, loss = 2.16365980\n",
      "Iteration 1728, loss = 2.16308630\n",
      "Iteration 1729, loss = 2.16374792\n",
      "Iteration 1730, loss = 2.16328879\n",
      "Iteration 1731, loss = 2.16307275\n",
      "Iteration 1732, loss = 2.16248709\n",
      "Iteration 1733, loss = 2.16244736\n",
      "Iteration 1734, loss = 2.16215535\n",
      "Iteration 1735, loss = 2.16195206\n",
      "Iteration 1736, loss = 2.16237061\n",
      "Iteration 1737, loss = 2.16168902\n",
      "Iteration 1738, loss = 2.16169119\n",
      "Iteration 1739, loss = 2.16115404\n",
      "Iteration 1740, loss = 2.16108058\n",
      "Iteration 1741, loss = 2.16109334\n",
      "Iteration 1742, loss = 2.16095244\n",
      "Iteration 1743, loss = 2.16064383\n",
      "Iteration 1744, loss = 2.16059550\n",
      "Iteration 1745, loss = 2.16021898\n",
      "Iteration 1746, loss = 2.15997441\n",
      "Iteration 1747, loss = 2.15975234\n",
      "Iteration 1748, loss = 2.15965065\n",
      "Iteration 1749, loss = 2.15957738\n",
      "Iteration 1750, loss = 2.15925307\n",
      "Iteration 1751, loss = 2.15915759\n",
      "Iteration 1752, loss = 2.15871026\n",
      "Iteration 1753, loss = 2.15893189\n",
      "Iteration 1754, loss = 2.15861169\n",
      "Iteration 1755, loss = 2.15861387\n",
      "Iteration 1756, loss = 2.15866956\n",
      "Iteration 1757, loss = 2.15823459\n",
      "Iteration 1758, loss = 2.15785423\n",
      "Iteration 1759, loss = 2.15731346\n",
      "Iteration 1760, loss = 2.15783858\n",
      "Iteration 1761, loss = 2.15761310\n",
      "Iteration 1762, loss = 2.15785471\n",
      "Iteration 1763, loss = 2.15721591\n",
      "Iteration 1764, loss = 2.15645247\n",
      "Iteration 1765, loss = 2.15679190\n",
      "Iteration 1766, loss = 2.15626074\n",
      "Iteration 1767, loss = 2.15608510\n",
      "Iteration 1768, loss = 2.15579792\n",
      "Iteration 1769, loss = 2.15557798\n",
      "Iteration 1770, loss = 2.15568381\n",
      "Iteration 1771, loss = 2.15489318\n",
      "Iteration 1772, loss = 2.15543275\n",
      "Iteration 1773, loss = 2.15500609\n",
      "Iteration 1774, loss = 2.15501736\n",
      "Iteration 1775, loss = 2.15440743\n",
      "Iteration 1776, loss = 2.15428520\n",
      "Iteration 1777, loss = 2.15452271\n",
      "Iteration 1778, loss = 2.15408816\n",
      "Iteration 1779, loss = 2.15388406\n",
      "Iteration 1780, loss = 2.15359358\n",
      "Iteration 1781, loss = 2.15344076\n",
      "Iteration 1782, loss = 2.15375995\n",
      "Iteration 1783, loss = 2.15339666\n",
      "Iteration 1784, loss = 2.15270092\n",
      "Iteration 1785, loss = 2.15301698\n",
      "Iteration 1786, loss = 2.15292171\n",
      "Iteration 1787, loss = 2.15247521\n",
      "Iteration 1788, loss = 2.15220193\n",
      "Iteration 1789, loss = 2.15219681\n",
      "Iteration 1790, loss = 2.15209648\n",
      "Iteration 1791, loss = 2.15224364\n",
      "Iteration 1792, loss = 2.15148795\n",
      "Iteration 1793, loss = 2.15147509\n",
      "Iteration 1794, loss = 2.15150788\n",
      "Iteration 1795, loss = 2.15097988\n",
      "Iteration 1796, loss = 2.15068990\n",
      "Iteration 1797, loss = 2.15103127\n",
      "Iteration 1798, loss = 2.15079479\n",
      "Iteration 1799, loss = 2.15035494\n",
      "Iteration 1800, loss = 2.15109264\n",
      "Iteration 1801, loss = 2.14985598\n",
      "Iteration 1802, loss = 2.14992767\n",
      "Iteration 1803, loss = 2.14961446\n",
      "Iteration 1804, loss = 2.14937616\n",
      "Iteration 1805, loss = 2.14946124\n",
      "Iteration 1806, loss = 2.14952888\n",
      "Iteration 1807, loss = 2.14907310\n",
      "Iteration 1808, loss = 2.14860925\n",
      "Iteration 1809, loss = 2.14902244\n",
      "Iteration 1810, loss = 2.14862057\n",
      "Iteration 1811, loss = 2.14852799\n",
      "Iteration 1812, loss = 2.14876771\n",
      "Iteration 1813, loss = 2.14786214\n",
      "Iteration 1814, loss = 2.14797396\n",
      "Iteration 1815, loss = 2.14736110\n",
      "Iteration 1816, loss = 2.14769344\n",
      "Iteration 1817, loss = 2.14731803\n",
      "Iteration 1818, loss = 2.14730506\n",
      "Iteration 1819, loss = 2.14680920\n",
      "Iteration 1820, loss = 2.14698246\n",
      "Iteration 1821, loss = 2.14624593\n",
      "Iteration 1822, loss = 2.14645102\n",
      "Iteration 1823, loss = 2.14638960\n",
      "Iteration 1824, loss = 2.14608860\n",
      "Iteration 1825, loss = 2.14552663\n",
      "Iteration 1826, loss = 2.14603511\n",
      "Iteration 1827, loss = 2.14559931\n",
      "Iteration 1828, loss = 2.14550225\n",
      "Iteration 1829, loss = 2.14485855\n",
      "Iteration 1830, loss = 2.14500717\n",
      "Iteration 1831, loss = 2.14464755\n",
      "Iteration 1832, loss = 2.14471217\n",
      "Iteration 1833, loss = 2.14455983\n",
      "Iteration 1834, loss = 2.14432206\n",
      "Iteration 1835, loss = 2.14443007\n",
      "Iteration 1836, loss = 2.14362433\n",
      "Iteration 1837, loss = 2.14391807\n",
      "Iteration 1838, loss = 2.14344920\n",
      "Iteration 1839, loss = 2.14370717\n",
      "Iteration 1840, loss = 2.14280007\n",
      "Iteration 1841, loss = 2.14297589\n",
      "Iteration 1842, loss = 2.14264792\n",
      "Iteration 1843, loss = 2.14252362\n",
      "Iteration 1844, loss = 2.14307972\n",
      "Iteration 1845, loss = 2.14263284\n",
      "Iteration 1846, loss = 2.14200187\n",
      "Iteration 1847, loss = 2.14160509\n",
      "Iteration 1848, loss = 2.14170940\n",
      "Iteration 1849, loss = 2.14163993\n",
      "Iteration 1850, loss = 2.14215606\n",
      "Iteration 1851, loss = 2.14119604\n",
      "Iteration 1852, loss = 2.14120614\n",
      "Iteration 1853, loss = 2.14089856\n",
      "Iteration 1854, loss = 2.14090619\n",
      "Iteration 1855, loss = 2.14117127\n",
      "Iteration 1856, loss = 2.14045336\n",
      "Iteration 1857, loss = 2.14050830\n",
      "Iteration 1858, loss = 2.14017659\n",
      "Iteration 1859, loss = 2.13991998\n",
      "Iteration 1860, loss = 2.13974256\n",
      "Iteration 1861, loss = 2.13982699\n",
      "Iteration 1862, loss = 2.13924947\n",
      "Iteration 1863, loss = 2.13884570\n",
      "Iteration 1864, loss = 2.13905095\n",
      "Iteration 1865, loss = 2.13884712\n",
      "Iteration 1866, loss = 2.13860476\n",
      "Iteration 1867, loss = 2.13860687\n",
      "Iteration 1868, loss = 2.13805469\n",
      "Iteration 1869, loss = 2.13839909\n",
      "Iteration 1870, loss = 2.13827054\n",
      "Iteration 1871, loss = 2.13785130\n",
      "Iteration 1872, loss = 2.13765770\n",
      "Iteration 1873, loss = 2.13791417\n",
      "Iteration 1874, loss = 2.13774345\n",
      "Iteration 1875, loss = 2.13750301\n",
      "Iteration 1876, loss = 2.13707535\n",
      "Iteration 1877, loss = 2.13695500\n",
      "Iteration 1878, loss = 2.13664469\n",
      "Iteration 1879, loss = 2.13667827\n",
      "Iteration 1880, loss = 2.13634296\n",
      "Iteration 1881, loss = 2.13665012\n",
      "Iteration 1882, loss = 2.13609197\n",
      "Iteration 1883, loss = 2.13573922\n",
      "Iteration 1884, loss = 2.13588221\n",
      "Iteration 1885, loss = 2.13585976\n",
      "Iteration 1886, loss = 2.13519774\n",
      "Iteration 1887, loss = 2.13514161\n",
      "Iteration 1888, loss = 2.13489577\n",
      "Iteration 1889, loss = 2.13509517\n",
      "Iteration 1890, loss = 2.13469656\n",
      "Iteration 1891, loss = 2.13426423\n",
      "Iteration 1892, loss = 2.13454162\n",
      "Iteration 1893, loss = 2.13426410\n",
      "Iteration 1894, loss = 2.13428870\n",
      "Iteration 1895, loss = 2.13369539\n",
      "Iteration 1896, loss = 2.13384757\n",
      "Iteration 1897, loss = 2.13365015\n",
      "Iteration 1898, loss = 2.13347053\n",
      "Iteration 1899, loss = 2.13338274\n",
      "Iteration 1900, loss = 2.13351880\n",
      "Iteration 1901, loss = 2.13340495\n",
      "Iteration 1902, loss = 2.13306474\n",
      "Iteration 1903, loss = 2.13307565\n",
      "Iteration 1904, loss = 2.13250657\n",
      "Iteration 1905, loss = 2.13188937\n",
      "Iteration 1906, loss = 2.13204790\n",
      "Iteration 1907, loss = 2.13175057\n",
      "Iteration 1908, loss = 2.13185071\n",
      "Iteration 1909, loss = 2.13168310\n",
      "Iteration 1910, loss = 2.13152771\n",
      "Iteration 1911, loss = 2.13122418\n",
      "Iteration 1912, loss = 2.13113236\n",
      "Iteration 1913, loss = 2.13086316\n",
      "Iteration 1914, loss = 2.13066794\n",
      "Iteration 1915, loss = 2.13088616\n",
      "Iteration 1916, loss = 2.13037099\n",
      "Iteration 1917, loss = 2.13051935\n",
      "Iteration 1918, loss = 2.13013058\n",
      "Iteration 1919, loss = 2.13018098\n",
      "Iteration 1920, loss = 2.12952622\n",
      "Iteration 1921, loss = 2.12968682\n",
      "Iteration 1922, loss = 2.12959624\n",
      "Iteration 1923, loss = 2.12945273\n",
      "Iteration 1924, loss = 2.12920497\n",
      "Iteration 1925, loss = 2.12923715\n",
      "Iteration 1926, loss = 2.12888944\n",
      "Iteration 1927, loss = 2.12880411\n",
      "Iteration 1928, loss = 2.12848820\n",
      "Iteration 1929, loss = 2.12861608\n",
      "Iteration 1930, loss = 2.12845588\n",
      "Iteration 1931, loss = 2.12838983\n",
      "Iteration 1932, loss = 2.12787315\n",
      "Iteration 1933, loss = 2.12819528\n",
      "Iteration 1934, loss = 2.12751064\n",
      "Iteration 1935, loss = 2.12741479\n",
      "Iteration 1936, loss = 2.12730502\n",
      "Iteration 1937, loss = 2.12690759\n",
      "Iteration 1938, loss = 2.12688771\n",
      "Iteration 1939, loss = 2.12758302\n",
      "Iteration 1940, loss = 2.12661906\n",
      "Iteration 1941, loss = 2.12670185\n",
      "Iteration 1942, loss = 2.12723711\n",
      "Iteration 1943, loss = 2.12672982\n",
      "Iteration 1944, loss = 2.12625943\n",
      "Iteration 1945, loss = 2.12555307\n",
      "Iteration 1946, loss = 2.12582091\n",
      "Iteration 1947, loss = 2.12540084\n",
      "Iteration 1948, loss = 2.12517280\n",
      "Iteration 1949, loss = 2.12517961\n",
      "Iteration 1950, loss = 2.12504248\n",
      "Iteration 1951, loss = 2.12483185\n",
      "Iteration 1952, loss = 2.12512354\n",
      "Iteration 1953, loss = 2.12459741\n",
      "Iteration 1954, loss = 2.12462555\n",
      "Iteration 1955, loss = 2.12448412\n",
      "Iteration 1956, loss = 2.12462223\n",
      "Iteration 1957, loss = 2.12399857\n",
      "Iteration 1958, loss = 2.12407171\n",
      "Iteration 1959, loss = 2.12358640\n",
      "Iteration 1960, loss = 2.12370983\n",
      "Iteration 1961, loss = 2.12326849\n",
      "Iteration 1962, loss = 2.12300377\n",
      "Iteration 1963, loss = 2.12265386\n",
      "Iteration 1964, loss = 2.12240206\n",
      "Iteration 1965, loss = 2.12248633\n",
      "Iteration 1966, loss = 2.12233687\n",
      "Iteration 1967, loss = 2.12261893\n",
      "Iteration 1968, loss = 2.12275235\n",
      "Iteration 1969, loss = 2.12216684\n",
      "Iteration 1970, loss = 2.12223924\n",
      "Iteration 1971, loss = 2.12168345\n",
      "Iteration 1972, loss = 2.12185230\n",
      "Iteration 1973, loss = 2.12178675\n",
      "Iteration 1974, loss = 2.12106381\n",
      "Iteration 1975, loss = 2.12118320\n",
      "Iteration 1976, loss = 2.12109356\n",
      "Iteration 1977, loss = 2.12083498\n",
      "Iteration 1978, loss = 2.12079391\n",
      "Iteration 1979, loss = 2.12038744\n",
      "Iteration 1980, loss = 2.12045484\n",
      "Iteration 1981, loss = 2.12027196\n",
      "Iteration 1982, loss = 2.12040987\n",
      "Iteration 1983, loss = 2.12019711\n",
      "Iteration 1984, loss = 2.11950323\n",
      "Iteration 1985, loss = 2.11963660\n",
      "Iteration 1986, loss = 2.11983463\n",
      "Iteration 1987, loss = 2.11902866\n",
      "Iteration 1988, loss = 2.11911728\n",
      "Iteration 1989, loss = 2.11878443\n",
      "Iteration 1990, loss = 2.11866710\n",
      "Iteration 1991, loss = 2.11862225\n",
      "Iteration 1992, loss = 2.11876095\n",
      "Iteration 1993, loss = 2.11860780\n",
      "Iteration 1994, loss = 2.11816876\n",
      "Iteration 1995, loss = 2.11788533\n",
      "Iteration 1996, loss = 2.11804675\n",
      "Iteration 1997, loss = 2.11765739\n",
      "Iteration 1998, loss = 2.11762571\n",
      "Iteration 1999, loss = 2.11779886\n",
      "Iteration 2000, loss = 2.11818616\n",
      "Iteration 2001, loss = 2.11734364\n",
      "Iteration 2002, loss = 2.11689807\n",
      "Iteration 2003, loss = 2.11675108\n",
      "Iteration 2004, loss = 2.11663973\n",
      "Iteration 2005, loss = 2.11651070\n",
      "Iteration 2006, loss = 2.11620726\n",
      "Iteration 2007, loss = 2.11629086\n",
      "Iteration 2008, loss = 2.11601137\n",
      "Iteration 2009, loss = 2.11589763\n",
      "Iteration 2010, loss = 2.11565165\n",
      "Iteration 2011, loss = 2.11565610\n",
      "Iteration 2012, loss = 2.11538946\n",
      "Iteration 2013, loss = 2.11530143\n",
      "Iteration 2014, loss = 2.11498338\n",
      "Iteration 2015, loss = 2.11504310\n",
      "Iteration 2016, loss = 2.11532682\n",
      "Iteration 2017, loss = 2.11476187\n",
      "Iteration 2018, loss = 2.11515003\n",
      "Iteration 2019, loss = 2.11467571\n",
      "Iteration 2020, loss = 2.11492536\n",
      "Iteration 2021, loss = 2.11416502\n",
      "Iteration 2022, loss = 2.11413715\n",
      "Iteration 2023, loss = 2.11398097\n",
      "Iteration 2024, loss = 2.11354501\n",
      "Iteration 2025, loss = 2.11360677\n",
      "Iteration 2026, loss = 2.11355814\n",
      "Iteration 2027, loss = 2.11314924\n",
      "Iteration 2028, loss = 2.11364801\n",
      "Iteration 2029, loss = 2.11366911\n",
      "Iteration 2030, loss = 2.11282204\n",
      "Iteration 2031, loss = 2.11277877\n",
      "Iteration 2032, loss = 2.11247008\n",
      "Iteration 2033, loss = 2.11260573\n",
      "Iteration 2034, loss = 2.11253965\n",
      "Iteration 2035, loss = 2.11248538\n",
      "Iteration 2036, loss = 2.11173759\n",
      "Iteration 2037, loss = 2.11178557\n",
      "Iteration 2038, loss = 2.11144411\n",
      "Iteration 2039, loss = 2.11156476\n",
      "Iteration 2040, loss = 2.11139136\n",
      "Iteration 2041, loss = 2.11107609\n",
      "Iteration 2042, loss = 2.11092276\n",
      "Iteration 2043, loss = 2.11103000\n",
      "Iteration 2044, loss = 2.11051303\n",
      "Iteration 2045, loss = 2.11033148\n",
      "Iteration 2046, loss = 2.11054136\n",
      "Iteration 2047, loss = 2.11065471\n",
      "Iteration 2048, loss = 2.10992079\n",
      "Iteration 2049, loss = 2.11011463\n",
      "Iteration 2050, loss = 2.10976389\n",
      "Iteration 2051, loss = 2.10958832\n",
      "Iteration 2052, loss = 2.10988877\n",
      "Iteration 2053, loss = 2.10936204\n",
      "Iteration 2054, loss = 2.10970684\n",
      "Iteration 2055, loss = 2.10902430\n",
      "Iteration 2056, loss = 2.10860003\n",
      "Iteration 2057, loss = 2.10859250\n",
      "Iteration 2058, loss = 2.10850124\n",
      "Iteration 2059, loss = 2.10846038\n",
      "Iteration 2060, loss = 2.10877331\n",
      "Iteration 2061, loss = 2.10831034\n",
      "Iteration 2062, loss = 2.10786724\n",
      "Iteration 2063, loss = 2.10781797\n",
      "Iteration 2064, loss = 2.10775994\n",
      "Iteration 2065, loss = 2.10791144\n",
      "Iteration 2066, loss = 2.10748674\n",
      "Iteration 2067, loss = 2.10763131\n",
      "Iteration 2068, loss = 2.10725144\n",
      "Iteration 2069, loss = 2.10716294\n",
      "Iteration 2070, loss = 2.10729543\n",
      "Iteration 2071, loss = 2.10670715\n",
      "Iteration 2072, loss = 2.10652067\n",
      "Iteration 2073, loss = 2.10659722\n",
      "Iteration 2074, loss = 2.10642275\n",
      "Iteration 2075, loss = 2.10627923\n",
      "Iteration 2076, loss = 2.10593766\n",
      "Iteration 2077, loss = 2.10591104\n",
      "Iteration 2078, loss = 2.10559335\n",
      "Iteration 2079, loss = 2.10606317\n",
      "Iteration 2080, loss = 2.10594487\n",
      "Iteration 2081, loss = 2.10503891\n",
      "Iteration 2082, loss = 2.10487184\n",
      "Iteration 2083, loss = 2.10539336\n",
      "Iteration 2084, loss = 2.10498392\n",
      "Iteration 2085, loss = 2.10485839\n",
      "Iteration 2086, loss = 2.10567948\n",
      "Iteration 2087, loss = 2.10467707\n",
      "Iteration 2088, loss = 2.10468914\n",
      "Iteration 2089, loss = 2.10387092\n",
      "Iteration 2090, loss = 2.10402306\n",
      "Iteration 2091, loss = 2.10387619\n",
      "Iteration 2092, loss = 2.10347786\n",
      "Iteration 2093, loss = 2.10354221\n",
      "Iteration 2094, loss = 2.10388336\n",
      "Iteration 2095, loss = 2.10322831\n",
      "Iteration 2096, loss = 2.10299476\n",
      "Iteration 2097, loss = 2.10315144\n",
      "Iteration 2098, loss = 2.10317908\n",
      "Iteration 2099, loss = 2.10275827\n",
      "Iteration 2100, loss = 2.10245665\n",
      "Iteration 2101, loss = 2.10216284\n",
      "Iteration 2102, loss = 2.10213914\n",
      "Iteration 2103, loss = 2.10183616\n",
      "Iteration 2104, loss = 2.10261959\n",
      "Iteration 2105, loss = 2.10175913\n",
      "Iteration 2106, loss = 2.10187917\n",
      "Iteration 2107, loss = 2.10162190\n",
      "Iteration 2108, loss = 2.10141104\n",
      "Iteration 2109, loss = 2.10121532\n",
      "Iteration 2110, loss = 2.10121264\n",
      "Iteration 2111, loss = 2.10067815\n",
      "Iteration 2112, loss = 2.10059789\n",
      "Iteration 2113, loss = 2.10055096\n",
      "Iteration 2114, loss = 2.10044964\n",
      "Iteration 2115, loss = 2.10098063\n",
      "Iteration 2116, loss = 2.10076583\n",
      "Iteration 2117, loss = 2.10047641\n",
      "Iteration 2118, loss = 2.10020668\n",
      "Iteration 2119, loss = 2.10018988\n",
      "Iteration 2120, loss = 2.10013390\n",
      "Iteration 2121, loss = 2.09993000\n",
      "Iteration 2122, loss = 2.09931077\n",
      "Iteration 2123, loss = 2.09950127\n",
      "Iteration 2124, loss = 2.09926770\n",
      "Iteration 2125, loss = 2.09913598\n",
      "Iteration 2126, loss = 2.09920485\n",
      "Iteration 2127, loss = 2.09854354\n",
      "Iteration 2128, loss = 2.09895488\n",
      "Iteration 2129, loss = 2.09813335\n",
      "Iteration 2130, loss = 2.09855340\n",
      "Iteration 2131, loss = 2.09816192\n",
      "Iteration 2132, loss = 2.09810008\n",
      "Iteration 2133, loss = 2.09799433\n",
      "Iteration 2134, loss = 2.09788112\n",
      "Iteration 2135, loss = 2.09757494\n",
      "Iteration 2136, loss = 2.09782764\n",
      "Iteration 2137, loss = 2.09722008\n",
      "Iteration 2138, loss = 2.09726345\n",
      "Iteration 2139, loss = 2.09738010\n",
      "Iteration 2140, loss = 2.09684345\n",
      "Iteration 2141, loss = 2.09653390\n",
      "Iteration 2142, loss = 2.09697218\n",
      "Iteration 2143, loss = 2.09695224\n",
      "Iteration 2144, loss = 2.09640536\n",
      "Iteration 2145, loss = 2.09670383\n",
      "Iteration 2146, loss = 2.09630291\n",
      "Iteration 2147, loss = 2.09611205\n",
      "Iteration 2148, loss = 2.09602383\n",
      "Iteration 2149, loss = 2.09567419\n",
      "Iteration 2150, loss = 2.09557428\n",
      "Iteration 2151, loss = 2.09524585\n",
      "Iteration 2152, loss = 2.09529055\n",
      "Iteration 2153, loss = 2.09505573\n",
      "Iteration 2154, loss = 2.09467616\n",
      "Iteration 2155, loss = 2.09472768\n",
      "Iteration 2156, loss = 2.09475884\n",
      "Iteration 2157, loss = 2.09460970\n",
      "Iteration 2158, loss = 2.09458316\n",
      "Iteration 2159, loss = 2.09429110\n",
      "Iteration 2160, loss = 2.09437605\n",
      "Iteration 2161, loss = 2.09355850\n",
      "Iteration 2162, loss = 2.09431907\n",
      "Iteration 2163, loss = 2.09358083\n",
      "Iteration 2164, loss = 2.09387106\n",
      "Iteration 2165, loss = 2.09398357\n",
      "Iteration 2166, loss = 2.09374538\n",
      "Iteration 2167, loss = 2.09333256\n",
      "Iteration 2168, loss = 2.09313541\n",
      "Iteration 2169, loss = 2.09283210\n",
      "Iteration 2170, loss = 2.09323487\n",
      "Iteration 2171, loss = 2.09290174\n",
      "Iteration 2172, loss = 2.09303774\n",
      "Iteration 2173, loss = 2.09256688\n",
      "Iteration 2174, loss = 2.09255006\n",
      "Iteration 2175, loss = 2.09208788\n",
      "Iteration 2176, loss = 2.09207214\n",
      "Iteration 2177, loss = 2.09188047\n",
      "Iteration 2178, loss = 2.09204876\n",
      "Iteration 2179, loss = 2.09175448\n",
      "Iteration 2180, loss = 2.09174812\n",
      "Iteration 2181, loss = 2.09153122\n",
      "Iteration 2182, loss = 2.09133094\n",
      "Iteration 2183, loss = 2.09130108\n",
      "Iteration 2184, loss = 2.09123597\n",
      "Iteration 2185, loss = 2.09078744\n",
      "Iteration 2186, loss = 2.09059286\n",
      "Iteration 2187, loss = 2.09062626\n",
      "Iteration 2188, loss = 2.09074843\n",
      "Iteration 2189, loss = 2.09034201\n",
      "Iteration 2190, loss = 2.09012821\n",
      "Iteration 2191, loss = 2.08986701\n",
      "Iteration 2192, loss = 2.08955893\n",
      "Iteration 2193, loss = 2.08959507\n",
      "Iteration 2194, loss = 2.08975286\n",
      "Iteration 2195, loss = 2.08939879\n",
      "Iteration 2196, loss = 2.08946816\n",
      "Iteration 2197, loss = 2.08911817\n",
      "Iteration 2198, loss = 2.08907124\n",
      "Iteration 2199, loss = 2.08873260\n",
      "Iteration 2200, loss = 2.08887054\n",
      "Iteration 2201, loss = 2.08884516\n",
      "Iteration 2202, loss = 2.08877059\n",
      "Iteration 2203, loss = 2.08866232\n",
      "Iteration 2204, loss = 2.08824899\n",
      "Iteration 2205, loss = 2.08848325\n",
      "Iteration 2206, loss = 2.08786188\n",
      "Iteration 2207, loss = 2.08789394\n",
      "Iteration 2208, loss = 2.08755741\n",
      "Iteration 2209, loss = 2.08779556\n",
      "Iteration 2210, loss = 2.08775097\n",
      "Iteration 2211, loss = 2.08712966\n",
      "Iteration 2212, loss = 2.08715193\n",
      "Iteration 2213, loss = 2.08739051\n",
      "Iteration 2214, loss = 2.08701400\n",
      "Iteration 2215, loss = 2.08665561\n",
      "Iteration 2216, loss = 2.08735393\n",
      "Iteration 2217, loss = 2.08671686\n",
      "Iteration 2218, loss = 2.08647375\n",
      "Iteration 2219, loss = 2.08612697\n",
      "Iteration 2220, loss = 2.08620319\n",
      "Iteration 2221, loss = 2.08604707\n",
      "Iteration 2222, loss = 2.08624569\n",
      "Iteration 2223, loss = 2.08553985\n",
      "Iteration 2224, loss = 2.08556963\n",
      "Iteration 2225, loss = 2.08554478\n",
      "Iteration 2226, loss = 2.08555254\n",
      "Iteration 2227, loss = 2.08581831\n",
      "Iteration 2228, loss = 2.08523012\n",
      "Iteration 2229, loss = 2.08600117\n",
      "Iteration 2230, loss = 2.08473521\n",
      "Iteration 2231, loss = 2.08471129\n",
      "Iteration 2232, loss = 2.08469255\n",
      "Iteration 2233, loss = 2.08419843\n",
      "Iteration 2234, loss = 2.08405901\n",
      "Iteration 2235, loss = 2.08414212\n",
      "Iteration 2236, loss = 2.08425367\n",
      "Iteration 2237, loss = 2.08408519\n",
      "Iteration 2238, loss = 2.08408107\n",
      "Iteration 2239, loss = 2.08360528\n",
      "Iteration 2240, loss = 2.08354746\n",
      "Iteration 2241, loss = 2.08364067\n",
      "Iteration 2242, loss = 2.08305086\n",
      "Iteration 2243, loss = 2.08347982\n",
      "Iteration 2244, loss = 2.08281625\n",
      "Iteration 2245, loss = 2.08289466\n",
      "Iteration 2246, loss = 2.08251640\n",
      "Iteration 2247, loss = 2.08308303\n",
      "Iteration 2248, loss = 2.08237741\n",
      "Iteration 2249, loss = 2.08242690\n",
      "Iteration 2250, loss = 2.08309861\n",
      "Iteration 2251, loss = 2.08243390\n",
      "Iteration 2252, loss = 2.08214841\n",
      "Iteration 2253, loss = 2.08255283\n",
      "Iteration 2254, loss = 2.08172665\n",
      "Iteration 2255, loss = 2.08172244\n",
      "Iteration 2256, loss = 2.08152944\n",
      "Iteration 2257, loss = 2.08143900\n",
      "Iteration 2258, loss = 2.08108905\n",
      "Iteration 2259, loss = 2.08101089\n",
      "Iteration 2260, loss = 2.08129310\n",
      "Iteration 2261, loss = 2.08053448\n",
      "Iteration 2262, loss = 2.08069115\n",
      "Iteration 2263, loss = 2.08136909\n",
      "Iteration 2264, loss = 2.08048645\n",
      "Iteration 2265, loss = 2.08010912\n",
      "Iteration 2266, loss = 2.08023632\n",
      "Iteration 2267, loss = 2.08000300\n",
      "Iteration 2268, loss = 2.08036800\n",
      "Iteration 2269, loss = 2.08003057\n",
      "Iteration 2270, loss = 2.07980472\n",
      "Iteration 2271, loss = 2.07967222\n",
      "Iteration 2272, loss = 2.07927174\n",
      "Iteration 2273, loss = 2.07935458\n",
      "Iteration 2274, loss = 2.07930047\n",
      "Iteration 2275, loss = 2.07926602\n",
      "Iteration 2276, loss = 2.07892774\n",
      "Iteration 2277, loss = 2.07859271\n",
      "Iteration 2278, loss = 2.07877898\n",
      "Iteration 2279, loss = 2.07856046\n",
      "Iteration 2280, loss = 2.07845969\n",
      "Iteration 2281, loss = 2.07813513\n",
      "Iteration 2282, loss = 2.07852033\n",
      "Iteration 2283, loss = 2.07828773\n",
      "Iteration 2284, loss = 2.07853269\n",
      "Iteration 2285, loss = 2.07760098\n",
      "Iteration 2286, loss = 2.07806947\n",
      "Iteration 2287, loss = 2.07753056\n",
      "Iteration 2288, loss = 2.07729581\n",
      "Iteration 2289, loss = 2.07713044\n",
      "Iteration 2290, loss = 2.07756242\n",
      "Iteration 2291, loss = 2.07700547\n",
      "Iteration 2292, loss = 2.07748499\n",
      "Iteration 2293, loss = 2.07691607\n",
      "Iteration 2294, loss = 2.07715494\n",
      "Iteration 2295, loss = 2.07645895\n",
      "Iteration 2296, loss = 2.07643679\n",
      "Iteration 2297, loss = 2.07618740\n",
      "Iteration 2298, loss = 2.07642039\n",
      "Iteration 2299, loss = 2.07606170\n",
      "Iteration 2300, loss = 2.07562710\n",
      "Iteration 2301, loss = 2.07575973\n",
      "Iteration 2302, loss = 2.07628031\n",
      "Iteration 2303, loss = 2.07570259\n",
      "Iteration 2304, loss = 2.07634586\n",
      "Iteration 2305, loss = 2.07529302\n",
      "Iteration 2306, loss = 2.07546124\n",
      "Iteration 2307, loss = 2.07497853\n",
      "Iteration 2308, loss = 2.07480680\n",
      "Iteration 2309, loss = 2.07474502\n",
      "Iteration 2310, loss = 2.07487313\n",
      "Iteration 2311, loss = 2.07459395\n",
      "Iteration 2312, loss = 2.07449572\n",
      "Iteration 2313, loss = 2.07415122\n",
      "Iteration 2314, loss = 2.07415387\n",
      "Iteration 2315, loss = 2.07408487\n",
      "Iteration 2316, loss = 2.07435521\n",
      "Iteration 2317, loss = 2.07368361\n",
      "Iteration 2318, loss = 2.07377862\n",
      "Iteration 2319, loss = 2.07390510\n",
      "Iteration 2320, loss = 2.07403167\n",
      "Iteration 2321, loss = 2.07369388\n",
      "Iteration 2322, loss = 2.07315284\n",
      "Iteration 2323, loss = 2.07344686\n",
      "Iteration 2324, loss = 2.07324382\n",
      "Iteration 2325, loss = 2.07302255\n",
      "Iteration 2326, loss = 2.07295837\n",
      "Iteration 2327, loss = 2.07296929\n",
      "Iteration 2328, loss = 2.07258283\n",
      "Iteration 2329, loss = 2.07268575\n",
      "Iteration 2330, loss = 2.07257827\n",
      "Iteration 2331, loss = 2.07243840\n",
      "Iteration 2332, loss = 2.07249854\n",
      "Iteration 2333, loss = 2.07232074\n",
      "Iteration 2334, loss = 2.07151739\n",
      "Iteration 2335, loss = 2.07196945\n",
      "Iteration 2336, loss = 2.07156987\n",
      "Iteration 2337, loss = 2.07180854\n",
      "Iteration 2338, loss = 2.07096520\n",
      "Iteration 2339, loss = 2.07171610\n",
      "Iteration 2340, loss = 2.07112613\n",
      "Iteration 2341, loss = 2.07088715\n",
      "Iteration 2342, loss = 2.07107439\n",
      "Iteration 2343, loss = 2.07049791\n",
      "Iteration 2344, loss = 2.07049278\n",
      "Iteration 2345, loss = 2.07022736\n",
      "Iteration 2346, loss = 2.07030663\n",
      "Iteration 2347, loss = 2.07028392\n",
      "Iteration 2348, loss = 2.07005431\n",
      "Iteration 2349, loss = 2.07023797\n",
      "Iteration 2350, loss = 2.06947842\n",
      "Iteration 2351, loss = 2.06973973\n",
      "Iteration 2352, loss = 2.06934137\n",
      "Iteration 2353, loss = 2.07007300\n",
      "Iteration 2354, loss = 2.06964834\n",
      "Iteration 2355, loss = 2.06925898\n",
      "Iteration 2356, loss = 2.06902110\n",
      "Iteration 2357, loss = 2.06890794\n",
      "Iteration 2358, loss = 2.06883446\n",
      "Iteration 2359, loss = 2.06883250\n",
      "Iteration 2360, loss = 2.06854892\n",
      "Iteration 2361, loss = 2.06854368\n",
      "Iteration 2362, loss = 2.06837311\n",
      "Iteration 2363, loss = 2.06863004\n",
      "Iteration 2364, loss = 2.06802735\n",
      "Iteration 2365, loss = 2.06759748\n",
      "Iteration 2366, loss = 2.06834470\n",
      "Iteration 2367, loss = 2.06740319\n",
      "Iteration 2368, loss = 2.06786537\n",
      "Iteration 2369, loss = 2.06742264\n",
      "Iteration 2370, loss = 2.06765134\n",
      "Iteration 2371, loss = 2.06789315\n",
      "Iteration 2372, loss = 2.06696487\n",
      "Iteration 2373, loss = 2.06749531\n",
      "Iteration 2374, loss = 2.06661628\n",
      "Iteration 2375, loss = 2.06737343\n",
      "Iteration 2376, loss = 2.06690175\n",
      "Iteration 2377, loss = 2.06678561\n",
      "Iteration 2378, loss = 2.06640540\n",
      "Iteration 2379, loss = 2.06620006\n",
      "Iteration 2380, loss = 2.06610214\n",
      "Iteration 2381, loss = 2.06630967\n",
      "Iteration 2382, loss = 2.06564137\n",
      "Iteration 2383, loss = 2.06575364\n",
      "Iteration 2384, loss = 2.06561549\n",
      "Iteration 2385, loss = 2.06569101\n",
      "Iteration 2386, loss = 2.06515185\n",
      "Iteration 2387, loss = 2.06535136\n",
      "Iteration 2388, loss = 2.06537312\n",
      "Iteration 2389, loss = 2.06506204\n",
      "Iteration 2390, loss = 2.06510640\n",
      "Iteration 2391, loss = 2.06483349\n",
      "Iteration 2392, loss = 2.06496787\n",
      "Iteration 2393, loss = 2.06455721\n",
      "Iteration 2394, loss = 2.06465706\n",
      "Iteration 2395, loss = 2.06443780\n",
      "Iteration 2396, loss = 2.06426373\n",
      "Iteration 2397, loss = 2.06441704\n",
      "Iteration 2398, loss = 2.06463382\n",
      "Iteration 2399, loss = 2.06407664\n",
      "Iteration 2400, loss = 2.06392482\n",
      "Iteration 2401, loss = 2.06379723\n",
      "Iteration 2402, loss = 2.06374397\n",
      "Iteration 2403, loss = 2.06342366\n",
      "Iteration 2404, loss = 2.06378309\n",
      "Iteration 2405, loss = 2.06317991\n",
      "Iteration 2406, loss = 2.06321966\n",
      "Iteration 2407, loss = 2.06301475\n",
      "Iteration 2408, loss = 2.06288160\n",
      "Iteration 2409, loss = 2.06257617\n",
      "Iteration 2410, loss = 2.06298049\n",
      "Iteration 2411, loss = 2.06220358\n",
      "Iteration 2412, loss = 2.06239910\n",
      "Iteration 2413, loss = 2.06248499\n",
      "Iteration 2414, loss = 2.06239599\n",
      "Iteration 2415, loss = 2.06191125\n",
      "Iteration 2416, loss = 2.06207152\n",
      "Iteration 2417, loss = 2.06182548\n",
      "Iteration 2418, loss = 2.06179420\n",
      "Iteration 2419, loss = 2.06159199\n",
      "Iteration 2420, loss = 2.06171798\n",
      "Iteration 2421, loss = 2.06196557\n",
      "Iteration 2422, loss = 2.06126489\n",
      "Iteration 2423, loss = 2.06152300\n",
      "Iteration 2424, loss = 2.06102767\n",
      "Iteration 2425, loss = 2.06108018\n",
      "Iteration 2426, loss = 2.06104206\n",
      "Iteration 2427, loss = 2.06064958\n",
      "Iteration 2428, loss = 2.06075204\n",
      "Iteration 2429, loss = 2.06053251\n",
      "Iteration 2430, loss = 2.06062594\n",
      "Iteration 2431, loss = 2.06029894\n",
      "Iteration 2432, loss = 2.06045787\n",
      "Iteration 2433, loss = 2.06038955\n",
      "Iteration 2434, loss = 2.06027644\n",
      "Iteration 2435, loss = 2.05976018\n",
      "Iteration 2436, loss = 2.06003976\n",
      "Iteration 2437, loss = 2.05968188\n",
      "Iteration 2438, loss = 2.05926069\n",
      "Iteration 2439, loss = 2.05944072\n",
      "Iteration 2440, loss = 2.05909992\n",
      "Iteration 2441, loss = 2.05919482\n",
      "Iteration 2442, loss = 2.05891968\n",
      "Iteration 2443, loss = 2.05914552\n",
      "Iteration 2444, loss = 2.05885661\n",
      "Iteration 2445, loss = 2.05844691\n",
      "Iteration 2446, loss = 2.05856296\n",
      "Iteration 2447, loss = 2.05843158\n",
      "Iteration 2448, loss = 2.05846283\n",
      "Iteration 2449, loss = 2.05815192\n",
      "Iteration 2450, loss = 2.05798495\n",
      "Iteration 2451, loss = 2.05768066\n",
      "Iteration 2452, loss = 2.05759481\n",
      "Iteration 2453, loss = 2.05766192\n",
      "Iteration 2454, loss = 2.05764466\n",
      "Iteration 2455, loss = 2.05742897\n",
      "Iteration 2456, loss = 2.05734811\n",
      "Iteration 2457, loss = 2.05726315\n",
      "Iteration 2458, loss = 2.05745350\n",
      "Iteration 2459, loss = 2.05695319\n",
      "Iteration 2460, loss = 2.05688313\n",
      "Iteration 2461, loss = 2.05651565\n",
      "Iteration 2462, loss = 2.05649739\n",
      "Iteration 2463, loss = 2.05667752\n",
      "Iteration 2464, loss = 2.05628465\n",
      "Iteration 2465, loss = 2.05634295\n",
      "Iteration 2466, loss = 2.05654967\n",
      "Iteration 2467, loss = 2.05604235\n",
      "Iteration 2468, loss = 2.05597631\n",
      "Iteration 2469, loss = 2.05614135\n",
      "Iteration 2470, loss = 2.05587718\n",
      "Iteration 2471, loss = 2.05590926\n",
      "Iteration 2472, loss = 2.05547477\n",
      "Iteration 2473, loss = 2.05613826\n",
      "Iteration 2474, loss = 2.05544596\n",
      "Iteration 2475, loss = 2.05543227\n",
      "Iteration 2476, loss = 2.05554866\n",
      "Iteration 2477, loss = 2.05504779\n",
      "Iteration 2478, loss = 2.05558169\n",
      "Iteration 2479, loss = 2.05468159\n",
      "Iteration 2480, loss = 2.05466505\n",
      "Iteration 2481, loss = 2.05434399\n",
      "Iteration 2482, loss = 2.05497227\n",
      "Iteration 2483, loss = 2.05439003\n",
      "Iteration 2484, loss = 2.05430349\n",
      "Iteration 2485, loss = 2.05410444\n",
      "Iteration 2486, loss = 2.05391709\n",
      "Iteration 2487, loss = 2.05379888\n",
      "Iteration 2488, loss = 2.05364420\n",
      "Iteration 2489, loss = 2.05363819\n",
      "Iteration 2490, loss = 2.05362165\n",
      "Iteration 2491, loss = 2.05329904\n",
      "Iteration 2492, loss = 2.05333925\n",
      "Iteration 2493, loss = 2.05345557\n",
      "Iteration 2494, loss = 2.05290229\n",
      "Iteration 2495, loss = 2.05307414\n",
      "Iteration 2496, loss = 2.05283438\n",
      "Iteration 2497, loss = 2.05287019\n",
      "Iteration 2498, loss = 2.05277985\n",
      "Iteration 2499, loss = 2.05284052\n",
      "Iteration 2500, loss = 2.05236874\n",
      "Iteration 2501, loss = 2.05297211\n",
      "Iteration 2502, loss = 2.05236089\n",
      "Iteration 2503, loss = 2.05235588\n",
      "Iteration 2504, loss = 2.05200758\n",
      "Iteration 2505, loss = 2.05185938\n",
      "Iteration 2506, loss = 2.05190087\n",
      "Iteration 2507, loss = 2.05205812\n",
      "Iteration 2508, loss = 2.05151758\n",
      "Iteration 2509, loss = 2.05182094\n",
      "Iteration 2510, loss = 2.05102022\n",
      "Iteration 2511, loss = 2.05224789\n",
      "Iteration 2512, loss = 2.05119189\n",
      "Iteration 2513, loss = 2.05135069\n",
      "Iteration 2514, loss = 2.05122779\n",
      "Iteration 2515, loss = 2.05065557\n",
      "Iteration 2516, loss = 2.05059207\n",
      "Iteration 2517, loss = 2.05041262\n",
      "Iteration 2518, loss = 2.05070776\n",
      "Iteration 2519, loss = 2.05097154\n",
      "Iteration 2520, loss = 2.05065175\n",
      "Iteration 2521, loss = 2.05034662\n",
      "Iteration 2522, loss = 2.05022867\n",
      "Iteration 2523, loss = 2.05001128\n",
      "Iteration 2524, loss = 2.04956224\n",
      "Iteration 2525, loss = 2.05057990\n",
      "Iteration 2526, loss = 2.04952724\n",
      "Iteration 2527, loss = 2.04942957\n",
      "Iteration 2528, loss = 2.04962849\n",
      "Iteration 2529, loss = 2.04930171\n",
      "Iteration 2530, loss = 2.04910670\n",
      "Iteration 2531, loss = 2.04930790\n",
      "Iteration 2532, loss = 2.04970209\n",
      "Iteration 2533, loss = 2.04890186\n",
      "Iteration 2534, loss = 2.04872673\n",
      "Iteration 2535, loss = 2.04835577\n",
      "Iteration 2536, loss = 2.04883558\n",
      "Iteration 2537, loss = 2.04883409\n",
      "Iteration 2538, loss = 2.04858847\n",
      "Iteration 2539, loss = 2.04889289\n",
      "Iteration 2540, loss = 2.04843458\n",
      "Iteration 2541, loss = 2.04847365\n",
      "Iteration 2542, loss = 2.04809372\n",
      "Iteration 2543, loss = 2.04783468\n",
      "Iteration 2544, loss = 2.04803597\n",
      "Iteration 2545, loss = 2.04722077\n",
      "Iteration 2546, loss = 2.04781773\n",
      "Iteration 2547, loss = 2.04777763\n",
      "Iteration 2548, loss = 2.04717208\n",
      "Iteration 2549, loss = 2.04734128\n",
      "Iteration 2550, loss = 2.04697667\n",
      "Iteration 2551, loss = 2.04687551\n",
      "Iteration 2552, loss = 2.04666443\n",
      "Iteration 2553, loss = 2.04730189\n",
      "Iteration 2554, loss = 2.04629585\n",
      "Iteration 2555, loss = 2.04712718\n",
      "Iteration 2556, loss = 2.04633415\n",
      "Iteration 2557, loss = 2.04670944\n",
      "Iteration 2558, loss = 2.04576592\n",
      "Iteration 2559, loss = 2.04608521\n",
      "Iteration 2560, loss = 2.04596561\n",
      "Iteration 2561, loss = 2.04566982\n",
      "Iteration 2562, loss = 2.04625430\n",
      "Iteration 2563, loss = 2.04576942\n",
      "Iteration 2564, loss = 2.04566005\n",
      "Iteration 2565, loss = 2.04608059\n",
      "Iteration 2566, loss = 2.04522297\n",
      "Iteration 2567, loss = 2.04572142\n",
      "Iteration 2568, loss = 2.04490866\n",
      "Iteration 2569, loss = 2.04499065\n",
      "Iteration 2570, loss = 2.04474899\n",
      "Iteration 2571, loss = 2.04483533\n",
      "Iteration 2572, loss = 2.04500202\n",
      "Iteration 2573, loss = 2.04472504\n",
      "Iteration 2574, loss = 2.04469996\n",
      "Iteration 2575, loss = 2.04425997\n",
      "Iteration 2576, loss = 2.04431337\n",
      "Iteration 2577, loss = 2.04408110\n",
      "Iteration 2578, loss = 2.04392599\n",
      "Iteration 2579, loss = 2.04376942\n",
      "Iteration 2580, loss = 2.04392130\n",
      "Iteration 2581, loss = 2.04354750\n",
      "Iteration 2582, loss = 2.04457629\n",
      "Iteration 2583, loss = 2.04364465\n",
      "Iteration 2584, loss = 2.04351895\n",
      "Iteration 2585, loss = 2.04313274\n",
      "Iteration 2586, loss = 2.04330489\n",
      "Iteration 2587, loss = 2.04282526\n",
      "Iteration 2588, loss = 2.04266285\n",
      "Iteration 2589, loss = 2.04282449\n",
      "Iteration 2590, loss = 2.04273649\n",
      "Iteration 2591, loss = 2.04268051\n",
      "Iteration 2592, loss = 2.04246077\n",
      "Iteration 2593, loss = 2.04242214\n",
      "Iteration 2594, loss = 2.04239638\n",
      "Iteration 2595, loss = 2.04225300\n",
      "Iteration 2596, loss = 2.04169724\n",
      "Iteration 2597, loss = 2.04171949\n",
      "Iteration 2598, loss = 2.04208913\n",
      "Iteration 2599, loss = 2.04171882\n",
      "Iteration 2600, loss = 2.04173586\n",
      "Iteration 2601, loss = 2.04152271\n",
      "Iteration 2602, loss = 2.04110127\n",
      "Iteration 2603, loss = 2.04150369\n",
      "Iteration 2604, loss = 2.04134297\n",
      "Iteration 2605, loss = 2.04107836\n",
      "Iteration 2606, loss = 2.04083884\n",
      "Iteration 2607, loss = 2.04109580\n",
      "Iteration 2608, loss = 2.04120358\n",
      "Iteration 2609, loss = 2.04073586\n",
      "Iteration 2610, loss = 2.04102458\n",
      "Iteration 2611, loss = 2.04041290\n",
      "Iteration 2612, loss = 2.04042081\n",
      "Iteration 2613, loss = 2.04006787\n",
      "Iteration 2614, loss = 2.04006198\n",
      "Iteration 2615, loss = 2.03991315\n",
      "Iteration 2616, loss = 2.04030728\n",
      "Iteration 2617, loss = 2.03978536\n",
      "Iteration 2618, loss = 2.03962270\n",
      "Iteration 2619, loss = 2.03952794\n",
      "Iteration 2620, loss = 2.03987500\n",
      "Iteration 2621, loss = 2.03964532\n",
      "Iteration 2622, loss = 2.03949923\n",
      "Iteration 2623, loss = 2.03946224\n",
      "Iteration 2624, loss = 2.03952939\n",
      "Iteration 2625, loss = 2.03922065\n",
      "Iteration 2626, loss = 2.03896580\n",
      "Iteration 2627, loss = 2.03896063\n",
      "Iteration 2628, loss = 2.03852308\n",
      "Iteration 2629, loss = 2.03871827\n",
      "Iteration 2630, loss = 2.03904764\n",
      "Iteration 2631, loss = 2.03938428\n",
      "Iteration 2632, loss = 2.03849460\n",
      "Iteration 2633, loss = 2.03839030\n",
      "Iteration 2634, loss = 2.03823710\n",
      "Iteration 2635, loss = 2.03798639\n",
      "Iteration 2636, loss = 2.03800521\n",
      "Iteration 2637, loss = 2.03804855\n",
      "Iteration 2638, loss = 2.03746575\n",
      "Iteration 2639, loss = 2.03791929\n",
      "Iteration 2640, loss = 2.03792001\n",
      "Iteration 2641, loss = 2.03757526\n",
      "Iteration 2642, loss = 2.03803807\n",
      "Iteration 2643, loss = 2.03724967\n",
      "Iteration 2644, loss = 2.03735148\n",
      "Iteration 2645, loss = 2.03761783\n",
      "Iteration 2646, loss = 2.03718667\n",
      "Iteration 2647, loss = 2.03689177\n",
      "Iteration 2648, loss = 2.03681202\n",
      "Iteration 2649, loss = 2.03660757\n",
      "Iteration 2650, loss = 2.03635085\n",
      "Iteration 2651, loss = 2.03609382\n",
      "Iteration 2652, loss = 2.03635291\n",
      "Iteration 2653, loss = 2.03601115\n",
      "Iteration 2654, loss = 2.03598263\n",
      "Iteration 2655, loss = 2.03616764\n",
      "Iteration 2656, loss = 2.03614202\n",
      "Iteration 2657, loss = 2.03591993\n",
      "Iteration 2658, loss = 2.03565543\n",
      "Iteration 2659, loss = 2.03597290\n",
      "Iteration 2660, loss = 2.03541339\n",
      "Iteration 2661, loss = 2.03546714\n",
      "Iteration 2662, loss = 2.03485560\n",
      "Iteration 2663, loss = 2.03567434\n",
      "Iteration 2664, loss = 2.03505873\n",
      "Iteration 2665, loss = 2.03477721\n",
      "Iteration 2666, loss = 2.03456427\n",
      "Iteration 2667, loss = 2.03484622\n",
      "Iteration 2668, loss = 2.03515522\n",
      "Iteration 2669, loss = 2.03471994\n",
      "Iteration 2670, loss = 2.03426197\n",
      "Iteration 2671, loss = 2.03431311\n",
      "Iteration 2672, loss = 2.03409289\n",
      "Iteration 2673, loss = 2.03413809\n",
      "Iteration 2674, loss = 2.03422500\n",
      "Iteration 2675, loss = 2.03409782\n",
      "Iteration 2676, loss = 2.03410359\n",
      "Iteration 2677, loss = 2.03440973\n",
      "Iteration 2678, loss = 2.03360308\n",
      "Iteration 2679, loss = 2.03377872\n",
      "Iteration 2680, loss = 2.03364157\n",
      "Iteration 2681, loss = 2.03349812\n",
      "Iteration 2682, loss = 2.03299699\n",
      "Iteration 2683, loss = 2.03320386\n",
      "Iteration 2684, loss = 2.03306873\n",
      "Iteration 2685, loss = 2.03282595\n",
      "Iteration 2686, loss = 2.03260420\n",
      "Iteration 2687, loss = 2.03269483\n",
      "Iteration 2688, loss = 2.03288360\n",
      "Iteration 2689, loss = 2.03258009\n",
      "Iteration 2690, loss = 2.03209346\n",
      "Iteration 2691, loss = 2.03292117\n",
      "Iteration 2692, loss = 2.03231800\n",
      "Iteration 2693, loss = 2.03261383\n",
      "Iteration 2694, loss = 2.03209578\n",
      "Iteration 2695, loss = 2.03197388\n",
      "Iteration 2696, loss = 2.03208120\n",
      "Iteration 2697, loss = 2.03178078\n",
      "Iteration 2698, loss = 2.03141106\n",
      "Iteration 2699, loss = 2.03132379\n",
      "Iteration 2700, loss = 2.03135693\n",
      "Iteration 2701, loss = 2.03162039\n",
      "Iteration 2702, loss = 2.03122318\n",
      "Iteration 2703, loss = 2.03131376\n",
      "Iteration 2704, loss = 2.03086536\n",
      "Iteration 2705, loss = 2.03087036\n",
      "Iteration 2706, loss = 2.03121490\n",
      "Iteration 2707, loss = 2.03121378\n",
      "Iteration 2708, loss = 2.03130719\n",
      "Iteration 2709, loss = 2.03033068\n",
      "Iteration 2710, loss = 2.03034793\n",
      "Iteration 2711, loss = 2.03030412\n",
      "Iteration 2712, loss = 2.03093823\n",
      "Iteration 2713, loss = 2.02980651\n",
      "Iteration 2714, loss = 2.03017624\n",
      "Iteration 2715, loss = 2.02980141\n",
      "Iteration 2716, loss = 2.02988246\n",
      "Iteration 2717, loss = 2.02946769\n",
      "Iteration 2718, loss = 2.02946390\n",
      "Iteration 2719, loss = 2.02941404\n",
      "Iteration 2720, loss = 2.02911579\n",
      "Iteration 2721, loss = 2.02939065\n",
      "Iteration 2722, loss = 2.02999251\n",
      "Iteration 2723, loss = 2.02950755\n",
      "Iteration 2724, loss = 2.02891311\n",
      "Iteration 2725, loss = 2.02861133\n",
      "Iteration 2726, loss = 2.02922875\n",
      "Iteration 2727, loss = 2.02967521\n",
      "Iteration 2728, loss = 2.02878595\n",
      "Iteration 2729, loss = 2.02836900\n",
      "Iteration 2730, loss = 2.02813628\n",
      "Iteration 2731, loss = 2.02849130\n",
      "Iteration 2732, loss = 2.02818898\n",
      "Iteration 2733, loss = 2.02805065\n",
      "Iteration 2734, loss = 2.02872542\n",
      "Iteration 2735, loss = 2.02782830\n",
      "Iteration 2736, loss = 2.02794688\n",
      "Iteration 2737, loss = 2.02785506\n",
      "Iteration 2738, loss = 2.02783285\n",
      "Iteration 2739, loss = 2.02788919\n",
      "Iteration 2740, loss = 2.02784340\n",
      "Iteration 2741, loss = 2.02774525\n",
      "Iteration 2742, loss = 2.02714987\n",
      "Iteration 2743, loss = 2.02712361\n",
      "Iteration 2744, loss = 2.02698692\n",
      "Iteration 2745, loss = 2.02693768\n",
      "Iteration 2746, loss = 2.02697856\n",
      "Iteration 2747, loss = 2.02657886\n",
      "Iteration 2748, loss = 2.02658128\n",
      "Iteration 2749, loss = 2.02666003\n",
      "Iteration 2750, loss = 2.02666222\n",
      "Iteration 2751, loss = 2.02622796\n",
      "Iteration 2752, loss = 2.02630825\n",
      "Iteration 2753, loss = 2.02623653\n",
      "Iteration 2754, loss = 2.02614089\n",
      "Iteration 2755, loss = 2.02586261\n",
      "Iteration 2756, loss = 2.02561111\n",
      "Iteration 2757, loss = 2.02600974\n",
      "Iteration 2758, loss = 2.02580198\n",
      "Iteration 2759, loss = 2.02538462\n",
      "Iteration 2760, loss = 2.02550920\n",
      "Iteration 2761, loss = 2.02563176\n",
      "Iteration 2762, loss = 2.02543893\n",
      "Iteration 2763, loss = 2.02564672\n",
      "Iteration 2764, loss = 2.02536434\n",
      "Iteration 2765, loss = 2.02506595\n",
      "Iteration 2766, loss = 2.02455253\n",
      "Iteration 2767, loss = 2.02472631\n",
      "Iteration 2768, loss = 2.02499189\n",
      "Iteration 2769, loss = 2.02481092\n",
      "Iteration 2770, loss = 2.02414992\n",
      "Iteration 2771, loss = 2.02452582\n",
      "Iteration 2772, loss = 2.02470708\n",
      "Iteration 2773, loss = 2.02404223\n",
      "Iteration 2774, loss = 2.02405280\n",
      "Iteration 2775, loss = 2.02405381\n",
      "Iteration 2776, loss = 2.02429977\n",
      "Iteration 2777, loss = 2.02391389\n",
      "Iteration 2778, loss = 2.02383210\n",
      "Iteration 2779, loss = 2.02356308\n",
      "Iteration 2780, loss = 2.02397122\n",
      "Iteration 2781, loss = 2.02311859\n",
      "Iteration 2782, loss = 2.02467770\n",
      "Iteration 2783, loss = 2.02371245\n",
      "Iteration 2784, loss = 2.02360486\n",
      "Iteration 2785, loss = 2.02293426\n",
      "Iteration 2786, loss = 2.02281513\n",
      "Iteration 2787, loss = 2.02273689\n",
      "Iteration 2788, loss = 2.02257720\n",
      "Iteration 2789, loss = 2.02279016\n",
      "Iteration 2790, loss = 2.02256394\n",
      "Iteration 2791, loss = 2.02250892\n",
      "Iteration 2792, loss = 2.02256613\n",
      "Iteration 2793, loss = 2.02211577\n",
      "Iteration 2794, loss = 2.02268119\n",
      "Iteration 2795, loss = 2.02196309\n",
      "Iteration 2796, loss = 2.02162763\n",
      "Iteration 2797, loss = 2.02182019\n",
      "Iteration 2798, loss = 2.02176540\n",
      "Iteration 2799, loss = 2.02182878\n",
      "Iteration 2800, loss = 2.02173106\n",
      "Iteration 2801, loss = 2.02193802\n",
      "Iteration 2802, loss = 2.02146208\n",
      "Iteration 2803, loss = 2.02090399\n",
      "Iteration 2804, loss = 2.02100376\n",
      "Iteration 2805, loss = 2.02114400\n",
      "Iteration 2806, loss = 2.02065990\n",
      "Iteration 2807, loss = 2.02112849\n",
      "Iteration 2808, loss = 2.02058711\n",
      "Iteration 2809, loss = 2.02088927\n",
      "Iteration 2810, loss = 2.02087585\n",
      "Iteration 2811, loss = 2.02036928\n",
      "Iteration 2812, loss = 2.02010212\n",
      "Iteration 2813, loss = 2.02019249\n",
      "Iteration 2814, loss = 2.01986038\n",
      "Iteration 2815, loss = 2.02008350\n",
      "Iteration 2816, loss = 2.01968508\n",
      "Iteration 2817, loss = 2.02012415\n",
      "Iteration 2818, loss = 2.02010942\n",
      "Iteration 2819, loss = 2.01961752\n",
      "Iteration 2820, loss = 2.01972782\n",
      "Iteration 2821, loss = 2.01937424\n",
      "Iteration 2822, loss = 2.01954356\n",
      "Iteration 2823, loss = 2.01922312\n",
      "Iteration 2824, loss = 2.01908601\n",
      "Iteration 2825, loss = 2.01897492\n",
      "Iteration 2826, loss = 2.01899167\n",
      "Iteration 2827, loss = 2.01924215\n",
      "Iteration 2828, loss = 2.01906602\n",
      "Iteration 2829, loss = 2.01898084\n",
      "Iteration 2830, loss = 2.01831853\n",
      "Iteration 2831, loss = 2.01848231\n",
      "Iteration 2832, loss = 2.01861035\n",
      "Iteration 2833, loss = 2.01863572\n",
      "Iteration 2834, loss = 2.01820454\n",
      "Iteration 2835, loss = 2.01914475\n",
      "Iteration 2836, loss = 2.01837533\n",
      "Iteration 2837, loss = 2.01910799\n",
      "Iteration 2838, loss = 2.01807753\n",
      "Iteration 2839, loss = 2.01752068\n",
      "Iteration 2840, loss = 2.01788937\n",
      "Iteration 2841, loss = 2.01807591\n",
      "Iteration 2842, loss = 2.01744416\n",
      "Iteration 2843, loss = 2.01732780\n",
      "Iteration 2844, loss = 2.01769101\n",
      "Iteration 2845, loss = 2.01729925\n",
      "Iteration 2846, loss = 2.01704407\n",
      "Iteration 2847, loss = 2.01715064\n",
      "Iteration 2848, loss = 2.01672119\n",
      "Iteration 2849, loss = 2.01673044\n",
      "Iteration 2850, loss = 2.01646564\n",
      "Iteration 2851, loss = 2.01660729\n",
      "Iteration 2852, loss = 2.01629680\n",
      "Iteration 2853, loss = 2.01648605\n",
      "Iteration 2854, loss = 2.01621227\n",
      "Iteration 2855, loss = 2.01622140\n",
      "Iteration 2856, loss = 2.01588813\n",
      "Iteration 2857, loss = 2.01607487\n",
      "Iteration 2858, loss = 2.01591289\n",
      "Iteration 2859, loss = 2.01591095\n",
      "Iteration 2860, loss = 2.01574538\n",
      "Iteration 2861, loss = 2.01591831\n",
      "Iteration 2862, loss = 2.01607480\n",
      "Iteration 2863, loss = 2.01577967\n",
      "Iteration 2864, loss = 2.01518306\n",
      "Iteration 2865, loss = 2.01518445\n",
      "Iteration 2866, loss = 2.01536346\n",
      "Iteration 2867, loss = 2.01595984\n",
      "Iteration 2868, loss = 2.01490029\n",
      "Iteration 2869, loss = 2.01518653\n",
      "Iteration 2870, loss = 2.01502740\n",
      "Iteration 2871, loss = 2.01469933\n",
      "Iteration 2872, loss = 2.01446285\n",
      "Iteration 2873, loss = 2.01459202\n",
      "Iteration 2874, loss = 2.01426018\n",
      "Iteration 2875, loss = 2.01453059\n",
      "Iteration 2876, loss = 2.01470728\n",
      "Iteration 2877, loss = 2.01418868\n",
      "Iteration 2878, loss = 2.01397494\n",
      "Iteration 2879, loss = 2.01372995\n",
      "Iteration 2880, loss = 2.01408019\n",
      "Iteration 2881, loss = 2.01350402\n",
      "Iteration 2882, loss = 2.01349696\n",
      "Iteration 2883, loss = 2.01335352\n",
      "Iteration 2884, loss = 2.01379040\n",
      "Iteration 2885, loss = 2.01316946\n",
      "Iteration 2886, loss = 2.01351332\n",
      "Iteration 2887, loss = 2.01321869\n",
      "Iteration 2888, loss = 2.01293585\n",
      "Iteration 2889, loss = 2.01362863\n",
      "Iteration 2890, loss = 2.01315858\n",
      "Iteration 2891, loss = 2.01320295\n",
      "Iteration 2892, loss = 2.01309470\n",
      "Iteration 2893, loss = 2.01304864\n",
      "Iteration 2894, loss = 2.01362299\n",
      "Iteration 2895, loss = 2.01283791\n",
      "Iteration 2896, loss = 2.01263236\n",
      "Iteration 2897, loss = 2.01217662\n",
      "Iteration 2898, loss = 2.01233624\n",
      "Iteration 2899, loss = 2.01242662\n",
      "Iteration 2900, loss = 2.01188429\n",
      "Iteration 2901, loss = 2.01245964\n",
      "Iteration 2902, loss = 2.01213308\n",
      "Iteration 2903, loss = 2.01182251\n",
      "Iteration 2904, loss = 2.01227096\n",
      "Iteration 2905, loss = 2.01200931\n",
      "Iteration 2906, loss = 2.01162187\n",
      "Iteration 2907, loss = 2.01141802\n",
      "Iteration 2908, loss = 2.01114264\n",
      "Iteration 2909, loss = 2.01125958\n",
      "Iteration 2910, loss = 2.01102210\n",
      "Iteration 2911, loss = 2.01074411\n",
      "Iteration 2912, loss = 2.01098605\n",
      "Iteration 2913, loss = 2.01089305\n",
      "Iteration 2914, loss = 2.01102741\n",
      "Iteration 2915, loss = 2.01032728\n",
      "Iteration 2916, loss = 2.01045467\n",
      "Iteration 2917, loss = 2.01042190\n",
      "Iteration 2918, loss = 2.00989604\n",
      "Iteration 2919, loss = 2.01064203\n",
      "Iteration 2920, loss = 2.00973753\n",
      "Iteration 2921, loss = 2.01021327\n",
      "Iteration 2922, loss = 2.01045854\n",
      "Iteration 2923, loss = 2.01008109\n",
      "Iteration 2924, loss = 2.00993403\n",
      "Iteration 2925, loss = 2.00971649\n",
      "Iteration 2926, loss = 2.00931308\n",
      "Iteration 2927, loss = 2.00936390\n",
      "Iteration 2928, loss = 2.00963365\n",
      "Iteration 2929, loss = 2.00964167\n",
      "Iteration 2930, loss = 2.00912930\n",
      "Iteration 2931, loss = 2.00971351\n",
      "Iteration 2932, loss = 2.00898904\n",
      "Iteration 2933, loss = 2.00935376\n",
      "Iteration 2934, loss = 2.00865611\n",
      "Iteration 2935, loss = 2.00917453\n",
      "Iteration 2936, loss = 2.00858238\n",
      "Iteration 2937, loss = 2.00851340\n",
      "Iteration 2938, loss = 2.00903430\n",
      "Iteration 2939, loss = 2.00875856\n",
      "Iteration 2940, loss = 2.00843759\n",
      "Iteration 2941, loss = 2.00813805\n",
      "Iteration 2942, loss = 2.00852225\n",
      "Iteration 2943, loss = 2.00890915\n",
      "Iteration 2944, loss = 2.00861778\n",
      "Iteration 2945, loss = 2.00832107\n",
      "Iteration 2946, loss = 2.00837790\n",
      "Iteration 2947, loss = 2.00775022\n",
      "Iteration 2948, loss = 2.00767480\n",
      "Iteration 2949, loss = 2.00745445\n",
      "Iteration 2950, loss = 2.00767325\n",
      "Iteration 2951, loss = 2.00727621\n",
      "Iteration 2952, loss = 2.00738608\n",
      "Iteration 2953, loss = 2.00759045\n",
      "Iteration 2954, loss = 2.00721844\n",
      "Iteration 2955, loss = 2.00747399\n",
      "Iteration 2956, loss = 2.00651267\n",
      "Iteration 2957, loss = 2.00715549\n",
      "Iteration 2958, loss = 2.00692501\n",
      "Iteration 2959, loss = 2.00642582\n",
      "Iteration 2960, loss = 2.00664331\n",
      "Iteration 2961, loss = 2.00627454\n",
      "Iteration 2962, loss = 2.00630623\n",
      "Iteration 2963, loss = 2.00602296\n",
      "Iteration 2964, loss = 2.00682467\n",
      "Iteration 2965, loss = 2.00603664\n",
      "Iteration 2966, loss = 2.00644684\n",
      "Iteration 2967, loss = 2.00597587\n",
      "Iteration 2968, loss = 2.00617880\n",
      "Iteration 2969, loss = 2.00577638\n",
      "Iteration 2970, loss = 2.00596009\n",
      "Iteration 2971, loss = 2.00587426\n",
      "Iteration 2972, loss = 2.00550232\n",
      "Iteration 2973, loss = 2.00543956\n",
      "Iteration 2974, loss = 2.00511810\n",
      "Iteration 2975, loss = 2.00550598\n",
      "Iteration 2976, loss = 2.00513864\n",
      "Iteration 2977, loss = 2.00503880\n",
      "Iteration 2978, loss = 2.00482607\n",
      "Iteration 2979, loss = 2.00575162\n",
      "Iteration 2980, loss = 2.00518077\n",
      "Iteration 2981, loss = 2.00537033\n",
      "Iteration 2982, loss = 2.00474600\n",
      "Iteration 2983, loss = 2.00455785\n",
      "Iteration 2984, loss = 2.00465616\n",
      "Iteration 2985, loss = 2.00467749\n",
      "Iteration 2986, loss = 2.00395955\n",
      "Iteration 2987, loss = 2.00377220\n",
      "Iteration 2988, loss = 2.00420611\n",
      "Iteration 2989, loss = 2.00379757\n",
      "Iteration 2990, loss = 2.00360938\n",
      "Iteration 2991, loss = 2.00384596\n",
      "Iteration 2992, loss = 2.00424465\n",
      "Iteration 2993, loss = 2.00404205\n",
      "Iteration 2994, loss = 2.00375532\n",
      "Iteration 2995, loss = 2.00424088\n",
      "Iteration 2996, loss = 2.00362604\n",
      "Iteration 2997, loss = 2.00324389\n",
      "Iteration 2998, loss = 2.00326992\n",
      "Iteration 2999, loss = 2.00343353\n",
      "Iteration 3000, loss = 2.00312652\n",
      "Iteration 3001, loss = 2.00258626\n",
      "Iteration 3002, loss = 2.00307562\n",
      "Iteration 3003, loss = 2.00240783\n",
      "Iteration 3004, loss = 2.00300368\n",
      "Iteration 3005, loss = 2.00235184\n",
      "Iteration 3006, loss = 2.00245679\n",
      "Iteration 3007, loss = 2.00251429\n",
      "Iteration 3008, loss = 2.00258523\n",
      "Iteration 3009, loss = 2.00225902\n",
      "Iteration 3010, loss = 2.00254614\n",
      "Iteration 3011, loss = 2.00250493\n",
      "Iteration 3012, loss = 2.00192881\n",
      "Iteration 3013, loss = 2.00174610\n",
      "Iteration 3014, loss = 2.00183241\n",
      "Iteration 3015, loss = 2.00151764\n",
      "Iteration 3016, loss = 2.00173190\n",
      "Iteration 3017, loss = 2.00197593\n",
      "Iteration 3018, loss = 2.00156614\n",
      "Iteration 3019, loss = 2.00109897\n",
      "Iteration 3020, loss = 2.00129783\n",
      "Iteration 3021, loss = 2.00133980\n",
      "Iteration 3022, loss = 2.00088813\n",
      "Iteration 3023, loss = 2.00078455\n",
      "Iteration 3024, loss = 2.00121604\n",
      "Iteration 3025, loss = 2.00089720\n",
      "Iteration 3026, loss = 2.00117830\n",
      "Iteration 3027, loss = 2.00115506\n",
      "Iteration 3028, loss = 2.00041311\n",
      "Iteration 3029, loss = 2.00039388\n",
      "Iteration 3030, loss = 2.00037380\n",
      "Iteration 3031, loss = 2.00034417\n",
      "Iteration 3032, loss = 2.00017931\n",
      "Iteration 3033, loss = 2.00039279\n",
      "Iteration 3034, loss = 1.99992934\n",
      "Iteration 3035, loss = 1.99957297\n",
      "Iteration 3036, loss = 1.99962799\n",
      "Iteration 3037, loss = 2.00018992\n",
      "Iteration 3038, loss = 2.00025609\n",
      "Iteration 3039, loss = 2.00022353\n",
      "Iteration 3040, loss = 1.99936289\n",
      "Iteration 3041, loss = 1.99963466\n",
      "Iteration 3042, loss = 1.99948647\n",
      "Iteration 3043, loss = 1.99951222\n",
      "Iteration 3044, loss = 1.99927015\n",
      "Iteration 3045, loss = 1.99890508\n",
      "Iteration 3046, loss = 1.99893293\n",
      "Iteration 3047, loss = 1.99854681\n",
      "Iteration 3048, loss = 1.99891400\n",
      "Iteration 3049, loss = 1.99865229\n",
      "Iteration 3050, loss = 1.99839400\n",
      "Iteration 3051, loss = 1.99855507\n",
      "Iteration 3052, loss = 1.99841661\n",
      "Iteration 3053, loss = 1.99865221\n",
      "Iteration 3054, loss = 1.99823316\n",
      "Iteration 3055, loss = 1.99835390\n",
      "Iteration 3056, loss = 1.99820963\n",
      "Iteration 3057, loss = 1.99774902\n",
      "Iteration 3058, loss = 1.99801319\n",
      "Iteration 3059, loss = 1.99827049\n",
      "Iteration 3060, loss = 1.99766718\n",
      "Iteration 3061, loss = 1.99776491\n",
      "Iteration 3062, loss = 1.99804346\n",
      "Iteration 3063, loss = 1.99742408\n",
      "Iteration 3064, loss = 1.99738337\n",
      "Iteration 3065, loss = 1.99746537\n",
      "Iteration 3066, loss = 1.99717660\n",
      "Iteration 3067, loss = 1.99731714\n",
      "Iteration 3068, loss = 1.99732626\n",
      "Iteration 3069, loss = 1.99788563\n",
      "Iteration 3070, loss = 1.99739318\n",
      "Iteration 3071, loss = 1.99770688\n",
      "Iteration 3072, loss = 1.99749778\n",
      "Iteration 3073, loss = 1.99664789\n",
      "Iteration 3074, loss = 1.99639234\n",
      "Iteration 3075, loss = 1.99681741\n",
      "Iteration 3076, loss = 1.99645137\n",
      "Iteration 3077, loss = 1.99643173\n",
      "Iteration 3078, loss = 1.99647031\n",
      "Iteration 3079, loss = 1.99661005\n",
      "Iteration 3080, loss = 1.99608934\n",
      "Iteration 3081, loss = 1.99596020\n",
      "Iteration 3082, loss = 1.99555443\n",
      "Iteration 3083, loss = 1.99589945\n",
      "Iteration 3084, loss = 1.99571720\n",
      "Iteration 3085, loss = 1.99593843\n",
      "Iteration 3086, loss = 1.99555806\n",
      "Iteration 3087, loss = 1.99606854\n",
      "Iteration 3088, loss = 1.99679918\n",
      "Iteration 3089, loss = 1.99595935\n",
      "Iteration 3090, loss = 1.99497773\n",
      "Iteration 3091, loss = 1.99528818\n",
      "Iteration 3092, loss = 1.99491697\n",
      "Iteration 3093, loss = 1.99511942\n",
      "Iteration 3094, loss = 1.99496847\n",
      "Iteration 3095, loss = 1.99460563\n",
      "Iteration 3096, loss = 1.99452330\n",
      "Iteration 3097, loss = 1.99438825\n",
      "Iteration 3098, loss = 1.99447290\n",
      "Iteration 3099, loss = 1.99447803\n",
      "Iteration 3100, loss = 1.99419370\n",
      "Iteration 3101, loss = 1.99462537\n",
      "Iteration 3102, loss = 1.99428964\n",
      "Iteration 3103, loss = 1.99457623\n",
      "Iteration 3104, loss = 1.99410822\n",
      "Iteration 3105, loss = 1.99398871\n",
      "Iteration 3106, loss = 1.99366711\n",
      "Iteration 3107, loss = 1.99341332\n",
      "Iteration 3108, loss = 1.99383464\n",
      "Iteration 3109, loss = 1.99377472\n",
      "Iteration 3110, loss = 1.99350202\n",
      "Iteration 3111, loss = 1.99321240\n",
      "Iteration 3112, loss = 1.99362838\n",
      "Iteration 3113, loss = 1.99297529\n",
      "Iteration 3114, loss = 1.99312626\n",
      "Iteration 3115, loss = 1.99310246\n",
      "Iteration 3116, loss = 1.99313458\n",
      "Iteration 3117, loss = 1.99337117\n",
      "Iteration 3118, loss = 1.99280720\n",
      "Iteration 3119, loss = 1.99291287\n",
      "Iteration 3120, loss = 1.99326895\n",
      "Iteration 3121, loss = 1.99260399\n",
      "Iteration 3122, loss = 1.99241425\n",
      "Iteration 3123, loss = 1.99249343\n",
      "Iteration 3124, loss = 1.99206685\n",
      "Iteration 3125, loss = 1.99270797\n",
      "Iteration 3126, loss = 1.99234040\n",
      "Iteration 3127, loss = 1.99226745\n",
      "Iteration 3128, loss = 1.99200862\n",
      "Iteration 3129, loss = 1.99186480\n",
      "Iteration 3130, loss = 1.99189111\n",
      "Iteration 3131, loss = 1.99209171\n",
      "Iteration 3132, loss = 1.99222327\n",
      "Iteration 3133, loss = 1.99152151\n",
      "Iteration 3134, loss = 1.99185704\n",
      "Iteration 3135, loss = 1.99149085\n",
      "Iteration 3136, loss = 1.99137785\n",
      "Iteration 3137, loss = 1.99125562\n",
      "Iteration 3138, loss = 1.99112609\n",
      "Iteration 3139, loss = 1.99097537\n",
      "Iteration 3140, loss = 1.99093155\n",
      "Iteration 3141, loss = 1.99118176\n",
      "Iteration 3142, loss = 1.99067282\n",
      "Iteration 3143, loss = 1.99073054\n",
      "Iteration 3144, loss = 1.99076364\n",
      "Iteration 3145, loss = 1.99129270\n",
      "Iteration 3146, loss = 1.99121373\n",
      "Iteration 3147, loss = 1.99111848\n",
      "Iteration 3148, loss = 1.99044538\n",
      "Iteration 3149, loss = 1.99018489\n",
      "Iteration 3150, loss = 1.99027241\n",
      "Iteration 3151, loss = 1.99020827\n",
      "Iteration 3152, loss = 1.98992663\n",
      "Iteration 3153, loss = 1.98975130\n",
      "Iteration 3154, loss = 1.98952940\n",
      "Iteration 3155, loss = 1.99000908\n",
      "Iteration 3156, loss = 1.99002634\n",
      "Iteration 3157, loss = 1.98997743\n",
      "Iteration 3158, loss = 1.98947011\n",
      "Iteration 3159, loss = 1.98943810\n",
      "Iteration 3160, loss = 1.98925371\n",
      "Iteration 3161, loss = 1.98974443\n",
      "Iteration 3162, loss = 1.98910745\n",
      "Iteration 3163, loss = 1.98912142\n",
      "Iteration 3164, loss = 1.98889797\n",
      "Iteration 3165, loss = 1.98912723\n",
      "Iteration 3166, loss = 1.98903189\n",
      "Iteration 3167, loss = 1.98874022\n",
      "Iteration 3168, loss = 1.98875529\n",
      "Iteration 3169, loss = 1.98880224\n",
      "Iteration 3170, loss = 1.98810206\n",
      "Iteration 3171, loss = 1.98838682\n",
      "Iteration 3172, loss = 1.98824549\n",
      "Iteration 3173, loss = 1.98850271\n",
      "Iteration 3174, loss = 1.98846303\n",
      "Iteration 3175, loss = 1.98824515\n",
      "Iteration 3176, loss = 1.98809845\n",
      "Iteration 3177, loss = 1.98841045\n",
      "Iteration 3178, loss = 1.98806215\n",
      "Iteration 3179, loss = 1.98799374\n",
      "Iteration 3180, loss = 1.98834829\n",
      "Iteration 3181, loss = 1.98826989\n",
      "Iteration 3182, loss = 1.98754388\n",
      "Iteration 3183, loss = 1.98771794\n",
      "Iteration 3184, loss = 1.98718635\n",
      "Iteration 3185, loss = 1.98717190\n",
      "Iteration 3186, loss = 1.98732019\n",
      "Iteration 3187, loss = 1.98731032\n",
      "Iteration 3188, loss = 1.98705727\n",
      "Iteration 3189, loss = 1.98754668\n",
      "Iteration 3190, loss = 1.98711285\n",
      "Iteration 3191, loss = 1.98690227\n",
      "Iteration 3192, loss = 1.98675378\n",
      "Iteration 3193, loss = 1.98723196\n",
      "Iteration 3194, loss = 1.98692613\n",
      "Iteration 3195, loss = 1.98664298\n",
      "Iteration 3196, loss = 1.98711620\n",
      "Iteration 3197, loss = 1.98643271\n",
      "Iteration 3198, loss = 1.98636417\n",
      "Iteration 3199, loss = 1.98611220\n",
      "Iteration 3200, loss = 1.98623635\n",
      "Iteration 3201, loss = 1.98628619\n",
      "Iteration 3202, loss = 1.98592455\n",
      "Iteration 3203, loss = 1.98585159\n",
      "Iteration 3204, loss = 1.98674488\n",
      "Iteration 3205, loss = 1.98593188\n",
      "Iteration 3206, loss = 1.98591471\n",
      "Iteration 3207, loss = 1.98580562\n",
      "Iteration 3208, loss = 1.98573112\n",
      "Iteration 3209, loss = 1.98526789\n",
      "Iteration 3210, loss = 1.98539017\n",
      "Iteration 3211, loss = 1.98525925\n",
      "Iteration 3212, loss = 1.98522541\n",
      "Iteration 3213, loss = 1.98450571\n",
      "Iteration 3214, loss = 1.98550730\n",
      "Iteration 3215, loss = 1.98496907\n",
      "Iteration 3216, loss = 1.98466741\n",
      "Iteration 3217, loss = 1.98469811\n",
      "Iteration 3218, loss = 1.98438438\n",
      "Iteration 3219, loss = 1.98449720\n",
      "Iteration 3220, loss = 1.98447696\n",
      "Iteration 3221, loss = 1.98435117\n",
      "Iteration 3222, loss = 1.98453817\n",
      "Iteration 3223, loss = 1.98453421\n",
      "Iteration 3224, loss = 1.98412233\n",
      "Iteration 3225, loss = 1.98389736\n",
      "Iteration 3226, loss = 1.98452180\n",
      "Iteration 3227, loss = 1.98396998\n",
      "Iteration 3228, loss = 1.98398154\n",
      "Iteration 3229, loss = 1.98368782\n",
      "Iteration 3230, loss = 1.98431829\n",
      "Iteration 3231, loss = 1.98363347\n",
      "Iteration 3232, loss = 1.98368538\n",
      "Iteration 3233, loss = 1.98388808\n",
      "Iteration 3234, loss = 1.98349619\n",
      "Iteration 3235, loss = 1.98351808\n",
      "Iteration 3236, loss = 1.98317899\n",
      "Iteration 3237, loss = 1.98358786\n",
      "Iteration 3238, loss = 1.98365400\n",
      "Iteration 3239, loss = 1.98303002\n",
      "Iteration 3240, loss = 1.98332487\n",
      "Iteration 3241, loss = 1.98246602\n",
      "Iteration 3242, loss = 1.98302438\n",
      "Iteration 3243, loss = 1.98256490\n",
      "Iteration 3244, loss = 1.98276519\n",
      "Iteration 3245, loss = 1.98259311\n",
      "Iteration 3246, loss = 1.98284885\n",
      "Iteration 3247, loss = 1.98282100\n",
      "Iteration 3248, loss = 1.98244462\n",
      "Iteration 3249, loss = 1.98230666\n",
      "Iteration 3250, loss = 1.98183803\n",
      "Iteration 3251, loss = 1.98227027\n",
      "Iteration 3252, loss = 1.98193541\n",
      "Iteration 3253, loss = 1.98177781\n",
      "Iteration 3254, loss = 1.98159409\n",
      "Iteration 3255, loss = 1.98186932\n",
      "Iteration 3256, loss = 1.98216672\n",
      "Iteration 3257, loss = 1.98181784\n",
      "Iteration 3258, loss = 1.98136593\n",
      "Iteration 3259, loss = 1.98129453\n",
      "Iteration 3260, loss = 1.98160746\n",
      "Iteration 3261, loss = 1.98152240\n",
      "Iteration 3262, loss = 1.98155373\n",
      "Iteration 3263, loss = 1.98145448\n",
      "Iteration 3264, loss = 1.98160895\n",
      "Iteration 3265, loss = 1.98178992\n",
      "Iteration 3266, loss = 1.98061894\n",
      "Iteration 3267, loss = 1.98045406\n",
      "Iteration 3268, loss = 1.98100567\n",
      "Iteration 3269, loss = 1.98062064\n",
      "Iteration 3270, loss = 1.98028651\n",
      "Iteration 3271, loss = 1.98033053\n",
      "Iteration 3272, loss = 1.98019252\n",
      "Iteration 3273, loss = 1.98080798\n",
      "Iteration 3274, loss = 1.98047837\n",
      "Iteration 3275, loss = 1.97984502\n",
      "Iteration 3276, loss = 1.98098513\n",
      "Iteration 3277, loss = 1.97974903\n",
      "Iteration 3278, loss = 1.98026396\n",
      "Iteration 3279, loss = 1.98024271\n",
      "Iteration 3280, loss = 1.98010453\n",
      "Iteration 3281, loss = 1.97971898\n",
      "Iteration 3282, loss = 1.98000196\n",
      "Iteration 3283, loss = 1.97999317\n",
      "Iteration 3284, loss = 1.97947311\n",
      "Iteration 3285, loss = 1.97978945\n",
      "Iteration 3286, loss = 1.97929545\n",
      "Iteration 3287, loss = 1.97956747\n",
      "Iteration 3288, loss = 1.97896680\n",
      "Iteration 3289, loss = 1.97947377\n",
      "Iteration 3290, loss = 1.97878277\n",
      "Iteration 3291, loss = 1.97906034\n",
      "Iteration 3292, loss = 1.97922369\n",
      "Iteration 3293, loss = 1.97897810\n",
      "Iteration 3294, loss = 1.97885879\n",
      "Iteration 3295, loss = 1.97906329\n",
      "Iteration 3296, loss = 1.97887249\n",
      "Iteration 3297, loss = 1.97879320\n",
      "Iteration 3298, loss = 1.97839319\n",
      "Iteration 3299, loss = 1.97859649\n",
      "Iteration 3300, loss = 1.97856410\n",
      "Iteration 3301, loss = 1.97861845\n",
      "Iteration 3302, loss = 1.97820652\n",
      "Iteration 3303, loss = 1.97799542\n",
      "Iteration 3304, loss = 1.97787562\n",
      "Iteration 3305, loss = 1.97758169\n",
      "Iteration 3306, loss = 1.97774815\n",
      "Iteration 3307, loss = 1.97782003\n",
      "Iteration 3308, loss = 1.97817375\n",
      "Iteration 3309, loss = 1.97744477\n",
      "Iteration 3310, loss = 1.97768050\n",
      "Iteration 3311, loss = 1.97718634\n",
      "Iteration 3312, loss = 1.97744176\n",
      "Iteration 3313, loss = 1.97752564\n",
      "Iteration 3314, loss = 1.97713350\n",
      "Iteration 3315, loss = 1.97751090\n",
      "Iteration 3316, loss = 1.97694783\n",
      "Iteration 3317, loss = 1.97702517\n",
      "Iteration 3318, loss = 1.97693135\n",
      "Iteration 3319, loss = 1.97728626\n",
      "Iteration 3320, loss = 1.97681148\n",
      "Iteration 3321, loss = 1.97642323\n",
      "Iteration 3322, loss = 1.97641982\n",
      "Iteration 3323, loss = 1.97652405\n",
      "Iteration 3324, loss = 1.97638634\n",
      "Iteration 3325, loss = 1.97632585\n",
      "Iteration 3326, loss = 1.97645623\n",
      "Iteration 3327, loss = 1.97671423\n",
      "Iteration 3328, loss = 1.97648817\n",
      "Iteration 3329, loss = 1.97620161\n",
      "Iteration 3330, loss = 1.97618886\n",
      "Iteration 3331, loss = 1.97614564\n",
      "Iteration 3332, loss = 1.97560276\n",
      "Iteration 3333, loss = 1.97632790\n",
      "Iteration 3334, loss = 1.97584958\n",
      "Iteration 3335, loss = 1.97619170\n",
      "Iteration 3336, loss = 1.97588284\n",
      "Iteration 3337, loss = 1.97550169\n",
      "Iteration 3338, loss = 1.97566403\n",
      "Iteration 3339, loss = 1.97553536\n",
      "Iteration 3340, loss = 1.97527492\n",
      "Iteration 3341, loss = 1.97533200\n",
      "Iteration 3342, loss = 1.97529999\n",
      "Iteration 3343, loss = 1.97552591\n",
      "Iteration 3344, loss = 1.97511614\n",
      "Iteration 3345, loss = 1.97546026\n",
      "Iteration 3346, loss = 1.97472377\n",
      "Iteration 3347, loss = 1.97493157\n",
      "Iteration 3348, loss = 1.97492067\n",
      "Iteration 3349, loss = 1.97468817\n",
      "Iteration 3350, loss = 1.97474026\n",
      "Iteration 3351, loss = 1.97479295\n",
      "Iteration 3352, loss = 1.97492136\n",
      "Iteration 3353, loss = 1.97402971\n",
      "Iteration 3354, loss = 1.97453832\n",
      "Iteration 3355, loss = 1.97425412\n",
      "Iteration 3356, loss = 1.97459144\n",
      "Iteration 3357, loss = 1.97442533\n",
      "Iteration 3358, loss = 1.97462710\n",
      "Iteration 3359, loss = 1.97419561\n",
      "Iteration 3360, loss = 1.97379910\n",
      "Iteration 3361, loss = 1.97351236\n",
      "Iteration 3362, loss = 1.97368219\n",
      "Iteration 3363, loss = 1.97388356\n",
      "Iteration 3364, loss = 1.97340423\n",
      "Iteration 3365, loss = 1.97335394\n",
      "Iteration 3366, loss = 1.97359595\n",
      "Iteration 3367, loss = 1.97325981\n",
      "Iteration 3368, loss = 1.97321328\n",
      "Iteration 3369, loss = 1.97324548\n",
      "Iteration 3370, loss = 1.97368314\n",
      "Iteration 3371, loss = 1.97351170\n",
      "Iteration 3372, loss = 1.97284465\n",
      "Iteration 3373, loss = 1.97319808\n",
      "Iteration 3374, loss = 1.97255873\n",
      "Iteration 3375, loss = 1.97269824\n",
      "Iteration 3376, loss = 1.97286661\n",
      "Iteration 3377, loss = 1.97298758\n",
      "Iteration 3378, loss = 1.97230748\n",
      "Iteration 3379, loss = 1.97254880\n",
      "Iteration 3380, loss = 1.97267672\n",
      "Iteration 3381, loss = 1.97184320\n",
      "Iteration 3382, loss = 1.97235308\n",
      "Iteration 3383, loss = 1.97241785\n",
      "Iteration 3384, loss = 1.97224657\n",
      "Iteration 3385, loss = 1.97208905\n",
      "Iteration 3386, loss = 1.97223811\n",
      "Iteration 3387, loss = 1.97203573\n",
      "Iteration 3388, loss = 1.97161415\n",
      "Iteration 3389, loss = 1.97154002\n",
      "Iteration 3390, loss = 1.97178462\n",
      "Iteration 3391, loss = 1.97233565\n",
      "Iteration 3392, loss = 1.97123504\n",
      "Iteration 3393, loss = 1.97127962\n",
      "Iteration 3394, loss = 1.97138324\n",
      "Iteration 3395, loss = 1.97159771\n",
      "Iteration 3396, loss = 1.97122243\n",
      "Iteration 3397, loss = 1.97123316\n",
      "Iteration 3398, loss = 1.97120880\n",
      "Iteration 3399, loss = 1.97130232\n",
      "Iteration 3400, loss = 1.97086898\n",
      "Iteration 3401, loss = 1.97099410\n",
      "Iteration 3402, loss = 1.97076559\n",
      "Iteration 3403, loss = 1.97110408\n",
      "Iteration 3404, loss = 1.97066678\n",
      "Iteration 3405, loss = 1.97095153\n",
      "Iteration 3406, loss = 1.97029340\n",
      "Iteration 3407, loss = 1.97063195\n",
      "Iteration 3408, loss = 1.97066366\n",
      "Iteration 3409, loss = 1.97007135\n",
      "Iteration 3410, loss = 1.97041898\n",
      "Iteration 3411, loss = 1.97023180\n",
      "Iteration 3412, loss = 1.96994974\n",
      "Iteration 3413, loss = 1.96994668\n",
      "Iteration 3414, loss = 1.97038858\n",
      "Iteration 3415, loss = 1.97003420\n",
      "Iteration 3416, loss = 1.97066984\n",
      "Iteration 3417, loss = 1.96982276\n",
      "Iteration 3418, loss = 1.96981130\n",
      "Iteration 3419, loss = 1.96955693\n",
      "Iteration 3420, loss = 1.96921864\n",
      "Iteration 3421, loss = 1.96954492\n",
      "Iteration 3422, loss = 1.96917668\n",
      "Iteration 3423, loss = 1.96982258\n",
      "Iteration 3424, loss = 1.96908606\n",
      "Iteration 3425, loss = 1.96907883\n",
      "Iteration 3426, loss = 1.96897749\n",
      "Iteration 3427, loss = 1.96921740\n",
      "Iteration 3428, loss = 1.96892962\n",
      "Iteration 3429, loss = 1.96895681\n",
      "Iteration 3430, loss = 1.96845701\n",
      "Iteration 3431, loss = 1.96930313\n",
      "Iteration 3432, loss = 1.96843469\n",
      "Iteration 3433, loss = 1.96892537\n",
      "Iteration 3434, loss = 1.96875036\n",
      "Iteration 3435, loss = 1.96802076\n",
      "Iteration 3436, loss = 1.96885193\n",
      "Iteration 3437, loss = 1.96821068\n",
      "Iteration 3438, loss = 1.96862112\n",
      "Iteration 3439, loss = 1.96802010\n",
      "Iteration 3440, loss = 1.96828936\n",
      "Iteration 3441, loss = 1.96805500\n",
      "Iteration 3442, loss = 1.96832451\n",
      "Iteration 3443, loss = 1.96793675\n",
      "Iteration 3444, loss = 1.96749330\n",
      "Iteration 3445, loss = 1.96783876\n",
      "Iteration 3446, loss = 1.96797568\n",
      "Iteration 3447, loss = 1.96803765\n",
      "Iteration 3448, loss = 1.96759622\n",
      "Iteration 3449, loss = 1.96718612\n",
      "Iteration 3450, loss = 1.96739226\n",
      "Iteration 3451, loss = 1.96716596\n",
      "Iteration 3452, loss = 1.96693036\n",
      "Iteration 3453, loss = 1.96776719\n",
      "Iteration 3454, loss = 1.96769397\n",
      "Iteration 3455, loss = 1.96714794\n",
      "Iteration 3456, loss = 1.96677972\n",
      "Iteration 3457, loss = 1.96655431\n",
      "Iteration 3458, loss = 1.96694692\n",
      "Iteration 3459, loss = 1.96661087\n",
      "Iteration 3460, loss = 1.96638340\n",
      "Iteration 3461, loss = 1.96671777\n",
      "Iteration 3462, loss = 1.96647476\n",
      "Iteration 3463, loss = 1.96657062\n",
      "Iteration 3464, loss = 1.96648090\n",
      "Iteration 3465, loss = 1.96628588\n",
      "Iteration 3466, loss = 1.96623856\n",
      "Iteration 3467, loss = 1.96587950\n",
      "Iteration 3468, loss = 1.96622372\n",
      "Iteration 3469, loss = 1.96597789\n",
      "Iteration 3470, loss = 1.96613845\n",
      "Iteration 3471, loss = 1.96585627\n",
      "Iteration 3472, loss = 1.96581625\n",
      "Iteration 3473, loss = 1.96576341\n",
      "Iteration 3474, loss = 1.96556615\n",
      "Iteration 3475, loss = 1.96552860\n",
      "Iteration 3476, loss = 1.96548665\n",
      "Iteration 3477, loss = 1.96503041\n",
      "Iteration 3478, loss = 1.96520735\n",
      "Iteration 3479, loss = 1.96497500\n",
      "Iteration 3480, loss = 1.96543163\n",
      "Iteration 3481, loss = 1.96478019\n",
      "Iteration 3482, loss = 1.96515431\n",
      "Iteration 3483, loss = 1.96495619\n",
      "Iteration 3484, loss = 1.96498820\n",
      "Iteration 3485, loss = 1.96509607\n",
      "Iteration 3486, loss = 1.96481594\n",
      "Iteration 3487, loss = 1.96494252\n",
      "Iteration 3488, loss = 1.96475845\n",
      "Iteration 3489, loss = 1.96524210\n",
      "Iteration 3490, loss = 1.96442853\n",
      "Iteration 3491, loss = 1.96458854\n",
      "Iteration 3492, loss = 1.96475850\n",
      "Iteration 3493, loss = 1.96469369\n",
      "Iteration 3494, loss = 1.96422191\n",
      "Iteration 3495, loss = 1.96415488\n",
      "Iteration 3496, loss = 1.96412298\n",
      "Iteration 3497, loss = 1.96388169\n",
      "Iteration 3498, loss = 1.96387913\n",
      "Iteration 3499, loss = 1.96403457\n",
      "Iteration 3500, loss = 1.96390616\n",
      "Iteration 3501, loss = 1.96376262\n",
      "Iteration 3502, loss = 1.96388740\n",
      "Iteration 3503, loss = 1.96360769\n",
      "Iteration 3504, loss = 1.96348302\n",
      "Iteration 3505, loss = 1.96333998\n",
      "Iteration 3506, loss = 1.96345163\n",
      "Iteration 3507, loss = 1.96310712\n",
      "Iteration 3508, loss = 1.96361645\n",
      "Iteration 3509, loss = 1.96397243\n",
      "Iteration 3510, loss = 1.96390953\n",
      "Iteration 3511, loss = 1.96359701\n",
      "Iteration 3512, loss = 1.96260585\n",
      "Iteration 3513, loss = 1.96292443\n",
      "Iteration 3514, loss = 1.96304570\n",
      "Iteration 3515, loss = 1.96294243\n",
      "Iteration 3516, loss = 1.96246707\n",
      "Iteration 3517, loss = 1.96267960\n",
      "Iteration 3518, loss = 1.96237170\n",
      "Iteration 3519, loss = 1.96271929\n",
      "Iteration 3520, loss = 1.96247473\n",
      "Iteration 3521, loss = 1.96274429\n",
      "Iteration 3522, loss = 1.96250733\n",
      "Iteration 3523, loss = 1.96236735\n",
      "Iteration 3524, loss = 1.96251398\n",
      "Iteration 3525, loss = 1.96205376\n",
      "Iteration 3526, loss = 1.96188353\n",
      "Iteration 3527, loss = 1.96191268\n",
      "Iteration 3528, loss = 1.96231305\n",
      "Iteration 3529, loss = 1.96160366\n",
      "Iteration 3530, loss = 1.96185664\n",
      "Iteration 3531, loss = 1.96216478\n",
      "Iteration 3532, loss = 1.96188629\n",
      "Iteration 3533, loss = 1.96217838\n",
      "Iteration 3534, loss = 1.96146041\n",
      "Iteration 3535, loss = 1.96120948\n",
      "Iteration 3536, loss = 1.96116888\n",
      "Iteration 3537, loss = 1.96128379\n",
      "Iteration 3538, loss = 1.96130497\n",
      "Iteration 3539, loss = 1.96123524\n",
      "Iteration 3540, loss = 1.96080702\n",
      "Iteration 3541, loss = 1.96157371\n",
      "Iteration 3542, loss = 1.96096058\n",
      "Iteration 3543, loss = 1.96100855\n",
      "Iteration 3544, loss = 1.96109052\n",
      "Iteration 3545, loss = 1.96115612\n",
      "Iteration 3546, loss = 1.96094708\n",
      "Iteration 3547, loss = 1.96100123\n",
      "Iteration 3548, loss = 1.96124534\n",
      "Iteration 3549, loss = 1.96082637\n",
      "Iteration 3550, loss = 1.96046959\n",
      "Iteration 3551, loss = 1.96025473\n",
      "Iteration 3552, loss = 1.96029670\n",
      "Iteration 3553, loss = 1.96023968\n",
      "Iteration 3554, loss = 1.95980162\n",
      "Iteration 3555, loss = 1.96004103\n",
      "Iteration 3556, loss = 1.95991724\n",
      "Iteration 3557, loss = 1.96004497\n",
      "Iteration 3558, loss = 1.95989286\n",
      "Iteration 3559, loss = 1.96000817\n",
      "Iteration 3560, loss = 1.95997559\n",
      "Iteration 3561, loss = 1.95969727\n",
      "Iteration 3562, loss = 1.95978122\n",
      "Iteration 3563, loss = 1.95948854\n",
      "Iteration 3564, loss = 1.95928940\n",
      "Iteration 3565, loss = 1.95948447\n",
      "Iteration 3566, loss = 1.95922701\n",
      "Iteration 3567, loss = 1.95963487\n",
      "Iteration 3568, loss = 1.95928718\n",
      "Iteration 3569, loss = 1.95882053\n",
      "Iteration 3570, loss = 1.95890441\n",
      "Iteration 3571, loss = 1.95882385\n",
      "Iteration 3572, loss = 1.95852148\n",
      "Iteration 3573, loss = 1.95896344\n",
      "Iteration 3574, loss = 1.95867317\n",
      "Iteration 3575, loss = 1.95881577\n",
      "Iteration 3576, loss = 1.95866593\n",
      "Iteration 3577, loss = 1.95852810\n",
      "Iteration 3578, loss = 1.95843078\n",
      "Iteration 3579, loss = 1.95874945\n",
      "Iteration 3580, loss = 1.95866459\n",
      "Iteration 3581, loss = 1.95819630\n",
      "Iteration 3582, loss = 1.95817840\n",
      "Iteration 3583, loss = 1.95795345\n",
      "Iteration 3584, loss = 1.95845231\n",
      "Iteration 3585, loss = 1.95829086\n",
      "Iteration 3586, loss = 1.95797239\n",
      "Iteration 3587, loss = 1.95880904\n",
      "Iteration 3588, loss = 1.95747991\n",
      "Iteration 3589, loss = 1.95810157\n",
      "Iteration 3590, loss = 1.95752715\n",
      "Iteration 3591, loss = 1.95799257\n",
      "Iteration 3592, loss = 1.95784061\n",
      "Iteration 3593, loss = 1.95838663\n",
      "Iteration 3594, loss = 1.95739508\n",
      "Iteration 3595, loss = 1.95772258\n",
      "Iteration 3596, loss = 1.95750312\n",
      "Iteration 3597, loss = 1.95759312\n",
      "Iteration 3598, loss = 1.95705227\n",
      "Iteration 3599, loss = 1.95719347\n",
      "Iteration 3600, loss = 1.95754069\n",
      "Iteration 3601, loss = 1.95693460\n",
      "Iteration 3602, loss = 1.95731970\n",
      "Iteration 3603, loss = 1.95708253\n",
      "Iteration 3604, loss = 1.95699414\n",
      "Iteration 3605, loss = 1.95680250\n",
      "Iteration 3606, loss = 1.95684979\n",
      "Iteration 3607, loss = 1.95669181\n",
      "Iteration 3608, loss = 1.95718745\n",
      "Iteration 3609, loss = 1.95697991\n",
      "Iteration 3610, loss = 1.95637887\n",
      "Iteration 3611, loss = 1.95615100\n",
      "Iteration 3612, loss = 1.95693330\n",
      "Iteration 3613, loss = 1.95644498\n",
      "Iteration 3614, loss = 1.95645760\n",
      "Iteration 3615, loss = 1.95666581\n",
      "Iteration 3616, loss = 1.95602236\n",
      "Iteration 3617, loss = 1.95607015\n",
      "Iteration 3618, loss = 1.95581822\n",
      "Iteration 3619, loss = 1.95611791\n",
      "Iteration 3620, loss = 1.95556440\n",
      "Iteration 3621, loss = 1.95579166\n",
      "Iteration 3622, loss = 1.95590228\n",
      "Iteration 3623, loss = 1.95572653\n",
      "Iteration 3624, loss = 1.95554983\n",
      "Iteration 3625, loss = 1.95560769\n",
      "Iteration 3626, loss = 1.95505860\n",
      "Iteration 3627, loss = 1.95525073\n",
      "Iteration 3628, loss = 1.95510240\n",
      "Iteration 3629, loss = 1.95523996\n",
      "Iteration 3630, loss = 1.95546046\n",
      "Iteration 3631, loss = 1.95532481\n",
      "Iteration 3632, loss = 1.95533111\n",
      "Iteration 3633, loss = 1.95506033\n",
      "Iteration 3634, loss = 1.95540473\n",
      "Iteration 3635, loss = 1.95520616\n",
      "Iteration 3636, loss = 1.95477771\n",
      "Iteration 3637, loss = 1.95463022\n",
      "Iteration 3638, loss = 1.95492237\n",
      "Iteration 3639, loss = 1.95448316\n",
      "Iteration 3640, loss = 1.95452033\n",
      "Iteration 3641, loss = 1.95413718\n",
      "Iteration 3642, loss = 1.95449162\n",
      "Iteration 3643, loss = 1.95404088\n",
      "Iteration 3644, loss = 1.95403592\n",
      "Iteration 3645, loss = 1.95412585\n",
      "Iteration 3646, loss = 1.95406963\n",
      "Iteration 3647, loss = 1.95492242\n",
      "Iteration 3648, loss = 1.95450051\n",
      "Iteration 3649, loss = 1.95438701\n",
      "Iteration 3650, loss = 1.95364575\n",
      "Iteration 3651, loss = 1.95442373\n",
      "Iteration 3652, loss = 1.95395857\n",
      "Iteration 3653, loss = 1.95370928\n",
      "Iteration 3654, loss = 1.95342991\n",
      "Iteration 3655, loss = 1.95330450\n",
      "Iteration 3656, loss = 1.95323554\n",
      "Iteration 3657, loss = 1.95347005\n",
      "Iteration 3658, loss = 1.95341480\n",
      "Iteration 3659, loss = 1.95312423\n",
      "Iteration 3660, loss = 1.95340137\n",
      "Iteration 3661, loss = 1.95321839\n",
      "Iteration 3662, loss = 1.95315995\n",
      "Iteration 3663, loss = 1.95281591\n",
      "Iteration 3664, loss = 1.95302757\n",
      "Iteration 3665, loss = 1.95271192\n",
      "Iteration 3666, loss = 1.95276250\n",
      "Iteration 3667, loss = 1.95296535\n",
      "Iteration 3668, loss = 1.95250415\n",
      "Iteration 3669, loss = 1.95230227\n",
      "Iteration 3670, loss = 1.95234395\n",
      "Iteration 3671, loss = 1.95254005\n",
      "Iteration 3672, loss = 1.95224995\n",
      "Iteration 3673, loss = 1.95201089\n",
      "Iteration 3674, loss = 1.95218226\n",
      "Iteration 3675, loss = 1.95238678\n",
      "Iteration 3676, loss = 1.95190351\n",
      "Iteration 3677, loss = 1.95208980\n",
      "Iteration 3678, loss = 1.95202090\n",
      "Iteration 3679, loss = 1.95182144\n",
      "Iteration 3680, loss = 1.95138711\n",
      "Iteration 3681, loss = 1.95186612\n",
      "Iteration 3682, loss = 1.95142565\n",
      "Iteration 3683, loss = 1.95165095\n",
      "Iteration 3684, loss = 1.95171621\n",
      "Iteration 3685, loss = 1.95182319\n",
      "Iteration 3686, loss = 1.95193140\n",
      "Iteration 3687, loss = 1.95135599\n",
      "Iteration 3688, loss = 1.95272064\n",
      "Iteration 3689, loss = 1.95128232\n",
      "Iteration 3690, loss = 1.95121816\n",
      "Iteration 3691, loss = 1.95078544\n",
      "Iteration 3692, loss = 1.95127619\n",
      "Iteration 3693, loss = 1.95095727\n",
      "Iteration 3694, loss = 1.95124977\n",
      "Iteration 3695, loss = 1.95086468\n",
      "Iteration 3696, loss = 1.95084189\n",
      "Iteration 3697, loss = 1.95086851\n",
      "Iteration 3698, loss = 1.95055555\n",
      "Iteration 3699, loss = 1.95032329\n",
      "Iteration 3700, loss = 1.95043495\n",
      "Iteration 3701, loss = 1.95059615\n",
      "Iteration 3702, loss = 1.95049624\n",
      "Iteration 3703, loss = 1.95032134\n",
      "Iteration 3704, loss = 1.95054303\n",
      "Iteration 3705, loss = 1.95034591\n",
      "Iteration 3706, loss = 1.94997161\n",
      "Iteration 3707, loss = 1.95096417\n",
      "Iteration 3708, loss = 1.95019399\n",
      "Iteration 3709, loss = 1.94999865\n",
      "Iteration 3710, loss = 1.95019842\n",
      "Iteration 3711, loss = 1.95031061\n",
      "Iteration 3712, loss = 1.95016905\n",
      "Iteration 3713, loss = 1.94975711\n",
      "Iteration 3714, loss = 1.94963734\n",
      "Iteration 3715, loss = 1.94941269\n",
      "Iteration 3716, loss = 1.94951350\n",
      "Iteration 3717, loss = 1.94980812\n",
      "Iteration 3718, loss = 1.94924901\n",
      "Iteration 3719, loss = 1.94948449\n",
      "Iteration 3720, loss = 1.94949222\n",
      "Iteration 3721, loss = 1.94949421\n",
      "Iteration 3722, loss = 1.94930491\n",
      "Iteration 3723, loss = 1.94921101\n",
      "Iteration 3724, loss = 1.94886128\n",
      "Iteration 3725, loss = 1.94873799\n",
      "Iteration 3726, loss = 1.94915525\n",
      "Iteration 3727, loss = 1.94891244\n",
      "Iteration 3728, loss = 1.94932940\n",
      "Iteration 3729, loss = 1.94868215\n",
      "Iteration 3730, loss = 1.94865685\n",
      "Iteration 3731, loss = 1.94874554\n",
      "Iteration 3732, loss = 1.94867691\n",
      "Iteration 3733, loss = 1.94829062\n",
      "Iteration 3734, loss = 1.94855631\n",
      "Iteration 3735, loss = 1.94835891\n",
      "Iteration 3736, loss = 1.94821449\n",
      "Iteration 3737, loss = 1.94833107\n",
      "Iteration 3738, loss = 1.94856637\n",
      "Iteration 3739, loss = 1.94794132\n",
      "Iteration 3740, loss = 1.94809848\n",
      "Iteration 3741, loss = 1.94810335\n",
      "Iteration 3742, loss = 1.94805827\n",
      "Iteration 3743, loss = 1.94795153\n",
      "Iteration 3744, loss = 1.94770702\n",
      "Iteration 3745, loss = 1.94812777\n",
      "Iteration 3746, loss = 1.94793598\n",
      "Iteration 3747, loss = 1.94760629\n",
      "Iteration 3748, loss = 1.94738484\n",
      "Iteration 3749, loss = 1.94722596\n",
      "Iteration 3750, loss = 1.94712562\n",
      "Iteration 3751, loss = 1.94728896\n",
      "Iteration 3752, loss = 1.94706193\n",
      "Iteration 3753, loss = 1.94755013\n",
      "Iteration 3754, loss = 1.94741057\n",
      "Iteration 3755, loss = 1.94699246\n",
      "Iteration 3756, loss = 1.94720452\n",
      "Iteration 3757, loss = 1.94790143\n",
      "Iteration 3758, loss = 1.94767573\n",
      "Iteration 3759, loss = 1.94748046\n",
      "Iteration 3760, loss = 1.94675583\n",
      "Iteration 3761, loss = 1.94659415\n",
      "Iteration 3762, loss = 1.94667436\n",
      "Iteration 3763, loss = 1.94625940\n",
      "Iteration 3764, loss = 1.94682007\n",
      "Iteration 3765, loss = 1.94679319\n",
      "Iteration 3766, loss = 1.94649103\n",
      "Iteration 3767, loss = 1.94665350\n",
      "Iteration 3768, loss = 1.94669605\n",
      "Iteration 3769, loss = 1.94637864\n",
      "Iteration 3770, loss = 1.94653001\n",
      "Iteration 3771, loss = 1.94652289\n",
      "Iteration 3772, loss = 1.94647564\n",
      "Iteration 3773, loss = 1.94634201\n",
      "Iteration 3774, loss = 1.94650471\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.71060099\n",
      "Iteration 2, loss = 3.68982577\n",
      "Iteration 3, loss = 3.68960896\n",
      "Iteration 4, loss = 3.68110280\n",
      "Iteration 5, loss = 3.67573013\n",
      "Iteration 6, loss = 3.67297290\n",
      "Iteration 7, loss = 3.66761722\n",
      "Iteration 8, loss = 3.66342447\n",
      "Iteration 9, loss = 3.66087346\n",
      "Iteration 10, loss = 3.65859294\n",
      "Iteration 11, loss = 3.65894251\n",
      "Iteration 12, loss = 3.65795973\n",
      "Iteration 13, loss = 3.65476366\n",
      "Iteration 14, loss = 3.65189522\n",
      "Iteration 15, loss = 3.64705941\n",
      "Iteration 16, loss = 3.64607954\n",
      "Iteration 17, loss = 3.64533542\n",
      "Iteration 18, loss = 3.64594175\n",
      "Iteration 19, loss = 3.64483880\n",
      "Iteration 20, loss = 3.64071155\n",
      "Iteration 21, loss = 3.63526889\n",
      "Iteration 22, loss = 3.63083373\n",
      "Iteration 23, loss = 3.63172766\n",
      "Iteration 24, loss = 3.62995036\n",
      "Iteration 25, loss = 3.62783050\n",
      "Iteration 26, loss = 3.62497040\n",
      "Iteration 27, loss = 3.62324931\n",
      "Iteration 28, loss = 3.62760045\n",
      "Iteration 29, loss = 3.62412115\n",
      "Iteration 30, loss = 3.62088047\n",
      "Iteration 31, loss = 3.61827140\n",
      "Iteration 32, loss = 3.61618522\n",
      "Iteration 33, loss = 3.61792752\n",
      "Iteration 34, loss = 3.61752931\n",
      "Iteration 35, loss = 3.61262989\n",
      "Iteration 36, loss = 3.60985121\n",
      "Iteration 37, loss = 3.60939275\n",
      "Iteration 38, loss = 3.60765714\n",
      "Iteration 39, loss = 3.60765662\n",
      "Iteration 40, loss = 3.60589016\n",
      "Iteration 41, loss = 3.60629362\n",
      "Iteration 42, loss = 3.60727674\n",
      "Iteration 43, loss = 3.60280997\n",
      "Iteration 44, loss = 3.60167276\n",
      "Iteration 45, loss = 3.60008247\n",
      "Iteration 46, loss = 3.60051580\n",
      "Iteration 47, loss = 3.59886164\n",
      "Iteration 48, loss = 3.59668663\n",
      "Iteration 49, loss = 3.59650523\n",
      "Iteration 50, loss = 3.59577060\n",
      "Iteration 51, loss = 3.59476150\n",
      "Iteration 52, loss = 3.59198817\n",
      "Iteration 53, loss = 3.59539462\n",
      "Iteration 54, loss = 3.60139637\n",
      "Iteration 55, loss = 3.59282765\n",
      "Iteration 56, loss = 3.59151982\n",
      "Iteration 57, loss = 3.59077586\n",
      "Iteration 58, loss = 3.58699406\n",
      "Iteration 59, loss = 3.58845579\n",
      "Iteration 60, loss = 3.58642847\n",
      "Iteration 61, loss = 3.58471758\n",
      "Iteration 62, loss = 3.58609672\n",
      "Iteration 63, loss = 3.58330442\n",
      "Iteration 64, loss = 3.58343428\n",
      "Iteration 65, loss = 3.58066148\n",
      "Iteration 66, loss = 3.58197221\n",
      "Iteration 67, loss = 3.57978966\n",
      "Iteration 68, loss = 3.58033771\n",
      "Iteration 69, loss = 3.58009151\n",
      "Iteration 70, loss = 3.58298906\n",
      "Iteration 71, loss = 3.57848234\n",
      "Iteration 72, loss = 3.57665494\n",
      "Iteration 73, loss = 3.57547412\n",
      "Iteration 74, loss = 3.57550103\n",
      "Iteration 75, loss = 3.57381475\n",
      "Iteration 76, loss = 3.57484072\n",
      "Iteration 77, loss = 3.57462202\n",
      "Iteration 78, loss = 3.57290731\n",
      "Iteration 79, loss = 3.57180126\n",
      "Iteration 80, loss = 3.56975957\n",
      "Iteration 81, loss = 3.57405951\n",
      "Iteration 82, loss = 3.56733806\n",
      "Iteration 83, loss = 3.56657124\n",
      "Iteration 84, loss = 3.56648598\n",
      "Iteration 85, loss = 3.56565163\n",
      "Iteration 86, loss = 3.56458802\n",
      "Iteration 87, loss = 3.56804780\n",
      "Iteration 88, loss = 3.56363918\n",
      "Iteration 89, loss = 3.56281650\n",
      "Iteration 90, loss = 3.56394002\n",
      "Iteration 91, loss = 3.56381025\n",
      "Iteration 92, loss = 3.56099901\n",
      "Iteration 93, loss = 3.56131516\n",
      "Iteration 94, loss = 3.55992480\n",
      "Iteration 95, loss = 3.55836923\n",
      "Iteration 96, loss = 3.55891255\n",
      "Iteration 97, loss = 3.55676586\n",
      "Iteration 98, loss = 3.55584124\n",
      "Iteration 99, loss = 3.55483140\n",
      "Iteration 100, loss = 3.55596196\n",
      "Iteration 101, loss = 3.55450305\n",
      "Iteration 102, loss = 3.55495080\n",
      "Iteration 103, loss = 3.55266899\n",
      "Iteration 104, loss = 3.55063339\n",
      "Iteration 105, loss = 3.55539275\n",
      "Iteration 106, loss = 3.54989129\n",
      "Iteration 107, loss = 3.55183910\n",
      "Iteration 108, loss = 3.55092656\n",
      "Iteration 109, loss = 3.54860127\n",
      "Iteration 110, loss = 3.54765287\n",
      "Iteration 111, loss = 3.54703835\n",
      "Iteration 112, loss = 3.54637459\n",
      "Iteration 113, loss = 3.54721689\n",
      "Iteration 114, loss = 3.54916153\n",
      "Iteration 115, loss = 3.54466823\n",
      "Iteration 116, loss = 3.54653253\n",
      "Iteration 117, loss = 3.54520527\n",
      "Iteration 118, loss = 3.54202473\n",
      "Iteration 119, loss = 3.54124126\n",
      "Iteration 120, loss = 3.54093089\n",
      "Iteration 121, loss = 3.54066419\n",
      "Iteration 122, loss = 3.54095097\n",
      "Iteration 123, loss = 3.54066076\n",
      "Iteration 124, loss = 3.53852070\n",
      "Iteration 125, loss = 3.53769980\n",
      "Iteration 126, loss = 3.53654580\n",
      "Iteration 127, loss = 3.53519320\n",
      "Iteration 128, loss = 3.54206942\n",
      "Iteration 129, loss = 3.54475427\n",
      "Iteration 130, loss = 3.53445371\n",
      "Iteration 131, loss = 3.53318230\n",
      "Iteration 132, loss = 3.53374300\n",
      "Iteration 133, loss = 3.53236753\n",
      "Iteration 134, loss = 3.53164811\n",
      "Iteration 135, loss = 3.53272372\n",
      "Iteration 136, loss = 3.53099453\n",
      "Iteration 137, loss = 3.53010479\n",
      "Iteration 138, loss = 3.52716400\n",
      "Iteration 139, loss = 3.52898344\n",
      "Iteration 140, loss = 3.52592192\n",
      "Iteration 141, loss = 3.52416532\n",
      "Iteration 142, loss = 3.52313324\n",
      "Iteration 143, loss = 3.52396043\n",
      "Iteration 144, loss = 3.52139790\n",
      "Iteration 145, loss = 3.52187002\n",
      "Iteration 146, loss = 3.52094415\n",
      "Iteration 147, loss = 3.51909171\n",
      "Iteration 148, loss = 3.52127119\n",
      "Iteration 149, loss = 3.51802793\n",
      "Iteration 150, loss = 3.51893224\n",
      "Iteration 151, loss = 3.51710485\n",
      "Iteration 152, loss = 3.51637040\n",
      "Iteration 153, loss = 3.51713339\n",
      "Iteration 154, loss = 3.51672700\n",
      "Iteration 155, loss = 3.51408730\n",
      "Iteration 156, loss = 3.51291410\n",
      "Iteration 157, loss = 3.51244428\n",
      "Iteration 158, loss = 3.51374289\n",
      "Iteration 159, loss = 3.51077821\n",
      "Iteration 160, loss = 3.51093333\n",
      "Iteration 161, loss = 3.50834995\n",
      "Iteration 162, loss = 3.50822808\n",
      "Iteration 163, loss = 3.50652696\n",
      "Iteration 164, loss = 3.50700189\n",
      "Iteration 165, loss = 3.50415634\n",
      "Iteration 166, loss = 3.50434255\n",
      "Iteration 167, loss = 3.50338001\n",
      "Iteration 168, loss = 3.50504878\n",
      "Iteration 169, loss = 3.50441573\n",
      "Iteration 170, loss = 3.50274349\n",
      "Iteration 171, loss = 3.50169821\n",
      "Iteration 172, loss = 3.49977292\n",
      "Iteration 173, loss = 3.49856535\n",
      "Iteration 174, loss = 3.49923340\n",
      "Iteration 175, loss = 3.49836402\n",
      "Iteration 176, loss = 3.49991800\n",
      "Iteration 177, loss = 3.49686965\n",
      "Iteration 178, loss = 3.49783672\n",
      "Iteration 179, loss = 3.49452777\n",
      "Iteration 180, loss = 3.49214402\n",
      "Iteration 181, loss = 3.49354610\n",
      "Iteration 182, loss = 3.49542832\n",
      "Iteration 183, loss = 3.50181185\n",
      "Iteration 184, loss = 3.49254116\n",
      "Iteration 185, loss = 3.49601662\n",
      "Iteration 186, loss = 3.49031691\n",
      "Iteration 187, loss = 3.48952323\n",
      "Iteration 188, loss = 3.48799277\n",
      "Iteration 189, loss = 3.48748038\n",
      "Iteration 190, loss = 3.48613370\n",
      "Iteration 191, loss = 3.48723256\n",
      "Iteration 192, loss = 3.48387804\n",
      "Iteration 193, loss = 3.48211951\n",
      "Iteration 194, loss = 3.48202814\n",
      "Iteration 195, loss = 3.48058735\n",
      "Iteration 196, loss = 3.47905447\n",
      "Iteration 197, loss = 3.48006481\n",
      "Iteration 198, loss = 3.47946796\n",
      "Iteration 199, loss = 3.48000744\n",
      "Iteration 200, loss = 3.48064792\n",
      "Iteration 201, loss = 3.47742725\n",
      "Iteration 202, loss = 3.47704293\n",
      "Iteration 203, loss = 3.47443191\n",
      "Iteration 204, loss = 3.47413199\n",
      "Iteration 205, loss = 3.47446530\n",
      "Iteration 206, loss = 3.47276126\n",
      "Iteration 207, loss = 3.47466584\n",
      "Iteration 208, loss = 3.47140579\n",
      "Iteration 209, loss = 3.47174244\n",
      "Iteration 210, loss = 3.47087977\n",
      "Iteration 211, loss = 3.46786180\n",
      "Iteration 212, loss = 3.46850754\n",
      "Iteration 213, loss = 3.47029674\n",
      "Iteration 214, loss = 3.46603624\n",
      "Iteration 215, loss = 3.46481768\n",
      "Iteration 216, loss = 3.46558394\n",
      "Iteration 217, loss = 3.46258912\n",
      "Iteration 218, loss = 3.46212904\n",
      "Iteration 219, loss = 3.46161066\n",
      "Iteration 220, loss = 3.46270600\n",
      "Iteration 221, loss = 3.46130393\n",
      "Iteration 222, loss = 3.46069674\n",
      "Iteration 223, loss = 3.45672545\n",
      "Iteration 224, loss = 3.45949966\n",
      "Iteration 225, loss = 3.45882707\n",
      "Iteration 226, loss = 3.45574884\n",
      "Iteration 227, loss = 3.45434255\n",
      "Iteration 228, loss = 3.45408915\n",
      "Iteration 229, loss = 3.45393421\n",
      "Iteration 230, loss = 3.45377092\n",
      "Iteration 231, loss = 3.45404562\n",
      "Iteration 232, loss = 3.45757837\n",
      "Iteration 233, loss = 3.45103076\n",
      "Iteration 234, loss = 3.45087954\n",
      "Iteration 235, loss = 3.44974644\n",
      "Iteration 236, loss = 3.44811336\n",
      "Iteration 237, loss = 3.44489156\n",
      "Iteration 238, loss = 3.45063552\n",
      "Iteration 239, loss = 3.44723224\n",
      "Iteration 240, loss = 3.44594440\n",
      "Iteration 241, loss = 3.44382904\n",
      "Iteration 242, loss = 3.44203570\n",
      "Iteration 243, loss = 3.44088101\n",
      "Iteration 244, loss = 3.44503253\n",
      "Iteration 245, loss = 3.44296554\n",
      "Iteration 246, loss = 3.43944538\n",
      "Iteration 247, loss = 3.43902117\n",
      "Iteration 248, loss = 3.44117811\n",
      "Iteration 249, loss = 3.43708044\n",
      "Iteration 250, loss = 3.43739570\n",
      "Iteration 251, loss = 3.44619362\n",
      "Iteration 252, loss = 3.43575069\n",
      "Iteration 253, loss = 3.43415129\n",
      "Iteration 254, loss = 3.44206823\n",
      "Iteration 255, loss = 3.43380348\n",
      "Iteration 256, loss = 3.43533503\n",
      "Iteration 257, loss = 3.43189151\n",
      "Iteration 258, loss = 3.42984164\n",
      "Iteration 259, loss = 3.43077913\n",
      "Iteration 260, loss = 3.43230971\n",
      "Iteration 261, loss = 3.43107709\n",
      "Iteration 262, loss = 3.43361937\n",
      "Iteration 263, loss = 3.42932294\n",
      "Iteration 264, loss = 3.42761055\n",
      "Iteration 265, loss = 3.42454350\n",
      "Iteration 266, loss = 3.42254325\n",
      "Iteration 267, loss = 3.42511690\n",
      "Iteration 268, loss = 3.42604339\n",
      "Iteration 269, loss = 3.42182254\n",
      "Iteration 270, loss = 3.42460305\n",
      "Iteration 271, loss = 3.41893148\n",
      "Iteration 272, loss = 3.41843054\n",
      "Iteration 273, loss = 3.42464086\n",
      "Iteration 274, loss = 3.43150653\n",
      "Iteration 275, loss = 3.42284200\n",
      "Iteration 276, loss = 3.42256970\n",
      "Iteration 277, loss = 3.41649727\n",
      "Iteration 278, loss = 3.42378001\n",
      "Iteration 279, loss = 3.41778444\n",
      "Iteration 280, loss = 3.41650264\n",
      "Iteration 281, loss = 3.41420465\n",
      "Iteration 282, loss = 3.41602232\n",
      "Iteration 283, loss = 3.41170862\n",
      "Iteration 284, loss = 3.41209529\n",
      "Iteration 285, loss = 3.41518858\n",
      "Iteration 286, loss = 3.42154044\n",
      "Iteration 287, loss = 3.41539411\n",
      "Iteration 288, loss = 3.41569977\n",
      "Iteration 289, loss = 3.41059317\n",
      "Iteration 290, loss = 3.40749478\n",
      "Iteration 291, loss = 3.41066248\n",
      "Iteration 292, loss = 3.41534774\n",
      "Iteration 293, loss = 3.41058486\n",
      "Iteration 294, loss = 3.40439450\n",
      "Iteration 295, loss = 3.41751366\n",
      "Iteration 296, loss = 3.40969656\n",
      "Iteration 297, loss = 3.40670070\n",
      "Iteration 298, loss = 3.40557072\n",
      "Iteration 299, loss = 3.40012319\n",
      "Iteration 300, loss = 3.40234859\n",
      "Iteration 301, loss = 3.40242332\n",
      "Iteration 302, loss = 3.40460405\n",
      "Iteration 303, loss = 3.40243639\n",
      "Iteration 304, loss = 3.39905959\n",
      "Iteration 305, loss = 3.39564544\n",
      "Iteration 306, loss = 3.39401794\n",
      "Iteration 307, loss = 3.39440959\n",
      "Iteration 308, loss = 3.39607410\n",
      "Iteration 309, loss = 3.39438996\n",
      "Iteration 310, loss = 3.39773517\n",
      "Iteration 311, loss = 3.39265382\n",
      "Iteration 312, loss = 3.39328851\n",
      "Iteration 313, loss = 3.39109412\n",
      "Iteration 314, loss = 3.39003429\n",
      "Iteration 315, loss = 3.38730700\n",
      "Iteration 316, loss = 3.38565870\n",
      "Iteration 317, loss = 3.39124117\n",
      "Iteration 318, loss = 3.39190066\n",
      "Iteration 319, loss = 3.38542355\n",
      "Iteration 320, loss = 3.38270588\n",
      "Iteration 321, loss = 3.38076449\n",
      "Iteration 322, loss = 3.38311939\n",
      "Iteration 323, loss = 3.38572190\n",
      "Iteration 324, loss = 3.38876336\n",
      "Iteration 325, loss = 3.38088713\n",
      "Iteration 326, loss = 3.38000242\n",
      "Iteration 327, loss = 3.37734478\n",
      "Iteration 328, loss = 3.37703550\n",
      "Iteration 329, loss = 3.37531251\n",
      "Iteration 330, loss = 3.37545723\n",
      "Iteration 331, loss = 3.37600809\n",
      "Iteration 332, loss = 3.37377044\n",
      "Iteration 333, loss = 3.37327225\n",
      "Iteration 334, loss = 3.38040944\n",
      "Iteration 335, loss = 3.38401707\n",
      "Iteration 336, loss = 3.40139283\n",
      "Iteration 337, loss = 3.37209664\n",
      "Iteration 338, loss = 3.37096269\n",
      "Iteration 339, loss = 3.36502207\n",
      "Iteration 340, loss = 3.36357821\n",
      "Iteration 341, loss = 3.36140443\n",
      "Iteration 342, loss = 3.36063640\n",
      "Iteration 343, loss = 3.37580809\n",
      "Iteration 344, loss = 3.36682752\n",
      "Iteration 345, loss = 3.35927141\n",
      "Iteration 346, loss = 3.35615284\n",
      "Iteration 347, loss = 3.35822845\n",
      "Iteration 348, loss = 3.35507269\n",
      "Iteration 349, loss = 3.35519886\n",
      "Iteration 350, loss = 3.35343764\n",
      "Iteration 351, loss = 3.35573769\n",
      "Iteration 352, loss = 3.35883361\n",
      "Iteration 353, loss = 3.36157098\n",
      "Iteration 354, loss = 3.35087136\n",
      "Iteration 355, loss = 3.34940790\n",
      "Iteration 356, loss = 3.34976759\n",
      "Iteration 357, loss = 3.34777270\n",
      "Iteration 358, loss = 3.34999880\n",
      "Iteration 359, loss = 3.35478983\n",
      "Iteration 360, loss = 3.34457826\n",
      "Iteration 361, loss = 3.34393270\n",
      "Iteration 362, loss = 3.35151893\n",
      "Iteration 363, loss = 3.34522136\n",
      "Iteration 364, loss = 3.35111317\n",
      "Iteration 365, loss = 3.34481390\n",
      "Iteration 366, loss = 3.33837142\n",
      "Iteration 367, loss = 3.33458514\n",
      "Iteration 368, loss = 3.33703778\n",
      "Iteration 369, loss = 3.33856601\n",
      "Iteration 370, loss = 3.33772461\n",
      "Iteration 371, loss = 3.33334395\n",
      "Iteration 372, loss = 3.33198478\n",
      "Iteration 373, loss = 3.33916071\n",
      "Iteration 374, loss = 3.33315170\n",
      "Iteration 375, loss = 3.33389586\n",
      "Iteration 376, loss = 3.33561986\n",
      "Iteration 377, loss = 3.32868121\n",
      "Iteration 378, loss = 3.33097949\n",
      "Iteration 379, loss = 3.33517783\n",
      "Iteration 380, loss = 3.33810312\n",
      "Iteration 381, loss = 3.33179369\n",
      "Iteration 382, loss = 3.32850729\n",
      "Iteration 383, loss = 3.32675506\n",
      "Iteration 384, loss = 3.32484579\n",
      "Iteration 385, loss = 3.32709789\n",
      "Iteration 386, loss = 3.32404023\n",
      "Iteration 387, loss = 3.32655068\n",
      "Iteration 388, loss = 3.32513869\n",
      "Iteration 389, loss = 3.33238156\n",
      "Iteration 390, loss = 3.32674147\n",
      "Iteration 391, loss = 3.32992846\n",
      "Iteration 392, loss = 3.31820832\n",
      "Iteration 393, loss = 3.31619021\n",
      "Iteration 394, loss = 3.32107720\n",
      "Iteration 395, loss = 3.32106384\n",
      "Iteration 396, loss = 3.31903446\n",
      "Iteration 397, loss = 3.31833322\n",
      "Iteration 398, loss = 3.31224709\n",
      "Iteration 399, loss = 3.31178988\n",
      "Iteration 400, loss = 3.31740792\n",
      "Iteration 401, loss = 3.31659271\n",
      "Iteration 402, loss = 3.30995276\n",
      "Iteration 403, loss = 3.31114680\n",
      "Iteration 404, loss = 3.30730719\n",
      "Iteration 405, loss = 3.30708428\n",
      "Iteration 406, loss = 3.30426346\n",
      "Iteration 407, loss = 3.31049121\n",
      "Iteration 408, loss = 3.30428300\n",
      "Iteration 409, loss = 3.30451500\n",
      "Iteration 410, loss = 3.30134199\n",
      "Iteration 411, loss = 3.30488879\n",
      "Iteration 412, loss = 3.29944941\n",
      "Iteration 413, loss = 3.29899535\n",
      "Iteration 414, loss = 3.29585540\n",
      "Iteration 415, loss = 3.29419407\n",
      "Iteration 416, loss = 3.29884645\n",
      "Iteration 417, loss = 3.29747303\n",
      "Iteration 418, loss = 3.30295900\n",
      "Iteration 419, loss = 3.29580176\n",
      "Iteration 420, loss = 3.30244893\n",
      "Iteration 421, loss = 3.29925661\n",
      "Iteration 422, loss = 3.28859837\n",
      "Iteration 423, loss = 3.28644878\n",
      "Iteration 424, loss = 3.29604308\n",
      "Iteration 425, loss = 3.28782365\n",
      "Iteration 426, loss = 3.28616006\n",
      "Iteration 427, loss = 3.28420300\n",
      "Iteration 428, loss = 3.28398472\n",
      "Iteration 429, loss = 3.28151534\n",
      "Iteration 430, loss = 3.28276900\n",
      "Iteration 431, loss = 3.28644902\n",
      "Iteration 432, loss = 3.27546307\n",
      "Iteration 433, loss = 3.29110913\n",
      "Iteration 434, loss = 3.28075512\n",
      "Iteration 435, loss = 3.27932437\n",
      "Iteration 436, loss = 3.29100591\n",
      "Iteration 437, loss = 3.28237573\n",
      "Iteration 438, loss = 3.27520024\n",
      "Iteration 439, loss = 3.27410793\n",
      "Iteration 440, loss = 3.27542021\n",
      "Iteration 441, loss = 3.27349579\n",
      "Iteration 442, loss = 3.27276000\n",
      "Iteration 443, loss = 3.27326900\n",
      "Iteration 444, loss = 3.27420172\n",
      "Iteration 445, loss = 3.28123355\n",
      "Iteration 446, loss = 3.26366322\n",
      "Iteration 447, loss = 3.26802088\n",
      "Iteration 448, loss = 3.28072189\n",
      "Iteration 449, loss = 3.26388855\n",
      "Iteration 450, loss = 3.26606578\n",
      "Iteration 451, loss = 3.27345886\n",
      "Iteration 452, loss = 3.26526884\n",
      "Iteration 453, loss = 3.28207550\n",
      "Iteration 454, loss = 3.26539134\n",
      "Iteration 455, loss = 3.26802147\n",
      "Iteration 456, loss = 3.27989123\n",
      "Iteration 457, loss = 3.27272346\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.68537758\n",
      "Iteration 2, loss = 3.67822310\n",
      "Iteration 3, loss = 3.66844406\n",
      "Iteration 4, loss = 3.65774238\n",
      "Iteration 5, loss = 3.64698487\n",
      "Iteration 6, loss = 3.63630884\n",
      "Iteration 7, loss = 3.62574219\n",
      "Iteration 8, loss = 3.61539693\n",
      "Iteration 9, loss = 3.60538443\n",
      "Iteration 10, loss = 3.59541446\n",
      "Iteration 11, loss = 3.58592587\n",
      "Iteration 12, loss = 3.57635656\n",
      "Iteration 13, loss = 3.56711466\n",
      "Iteration 14, loss = 3.55806538\n",
      "Iteration 15, loss = 3.54926847\n",
      "Iteration 16, loss = 3.54062310\n",
      "Iteration 17, loss = 3.53214198\n",
      "Iteration 18, loss = 3.52387515\n",
      "Iteration 19, loss = 3.51574014\n",
      "Iteration 20, loss = 3.50789117\n",
      "Iteration 21, loss = 3.50012373\n",
      "Iteration 22, loss = 3.49260510\n",
      "Iteration 23, loss = 3.48519439\n",
      "Iteration 24, loss = 3.47803370\n",
      "Iteration 25, loss = 3.47115256\n",
      "Iteration 26, loss = 3.46449929\n",
      "Iteration 27, loss = 3.45800102\n",
      "Iteration 28, loss = 3.45170908\n",
      "Iteration 29, loss = 3.44557086\n",
      "Iteration 30, loss = 3.43968369\n",
      "Iteration 31, loss = 3.43389281\n",
      "Iteration 32, loss = 3.42822419\n",
      "Iteration 33, loss = 3.42264275\n",
      "Iteration 34, loss = 3.41718320\n",
      "Iteration 35, loss = 3.41182382\n",
      "Iteration 36, loss = 3.40655294\n",
      "Iteration 37, loss = 3.40140032\n",
      "Iteration 38, loss = 3.39627101\n",
      "Iteration 39, loss = 3.39124416\n",
      "Iteration 40, loss = 3.38628735\n",
      "Iteration 41, loss = 3.38139793\n",
      "Iteration 42, loss = 3.37666450\n",
      "Iteration 43, loss = 3.37195971\n",
      "Iteration 44, loss = 3.36731400\n",
      "Iteration 45, loss = 3.36272467\n",
      "Iteration 46, loss = 3.35823402\n",
      "Iteration 47, loss = 3.35373707\n",
      "Iteration 48, loss = 3.34929914\n",
      "Iteration 49, loss = 3.34500352\n",
      "Iteration 50, loss = 3.34064488\n",
      "Iteration 51, loss = 3.33641970\n",
      "Iteration 52, loss = 3.33226441\n",
      "Iteration 53, loss = 3.32813295\n",
      "Iteration 54, loss = 3.32402415\n",
      "Iteration 55, loss = 3.32000993\n",
      "Iteration 56, loss = 3.31600759\n",
      "Iteration 57, loss = 3.31205702\n",
      "Iteration 58, loss = 3.30811077\n",
      "Iteration 59, loss = 3.30426658\n",
      "Iteration 60, loss = 3.30037521\n",
      "Iteration 61, loss = 3.29659758\n",
      "Iteration 62, loss = 3.29281253\n",
      "Iteration 63, loss = 3.28909998\n",
      "Iteration 64, loss = 3.28541963\n",
      "Iteration 65, loss = 3.28176590\n",
      "Iteration 66, loss = 3.27825466\n",
      "Iteration 67, loss = 3.27466970\n",
      "Iteration 68, loss = 3.27112537\n",
      "Iteration 69, loss = 3.26769813\n",
      "Iteration 70, loss = 3.26425358\n",
      "Iteration 71, loss = 3.26084824\n",
      "Iteration 72, loss = 3.25754925\n",
      "Iteration 73, loss = 3.25420239\n",
      "Iteration 74, loss = 3.25093507\n",
      "Iteration 75, loss = 3.24763466\n",
      "Iteration 76, loss = 3.24442859\n",
      "Iteration 77, loss = 3.24124321\n",
      "Iteration 78, loss = 3.23808128\n",
      "Iteration 79, loss = 3.23497403\n",
      "Iteration 80, loss = 3.23190807\n",
      "Iteration 81, loss = 3.22890056\n",
      "Iteration 82, loss = 3.22593120\n",
      "Iteration 83, loss = 3.22295598\n",
      "Iteration 84, loss = 3.22005724\n",
      "Iteration 85, loss = 3.21715172\n",
      "Iteration 86, loss = 3.21430441\n",
      "Iteration 87, loss = 3.21146129\n",
      "Iteration 88, loss = 3.20863080\n",
      "Iteration 89, loss = 3.20583827\n",
      "Iteration 90, loss = 3.20308690\n",
      "Iteration 91, loss = 3.20039199\n",
      "Iteration 92, loss = 3.19769784\n",
      "Iteration 93, loss = 3.19500492\n",
      "Iteration 94, loss = 3.19237663\n",
      "Iteration 95, loss = 3.18977488\n",
      "Iteration 96, loss = 3.18719264\n",
      "Iteration 97, loss = 3.18461260\n",
      "Iteration 98, loss = 3.18205750\n",
      "Iteration 99, loss = 3.17960141\n",
      "Iteration 100, loss = 3.17707238\n",
      "Iteration 101, loss = 3.17464478\n",
      "Iteration 102, loss = 3.17223624\n",
      "Iteration 103, loss = 3.16982031\n",
      "Iteration 104, loss = 3.16747036\n",
      "Iteration 105, loss = 3.16510916\n",
      "Iteration 106, loss = 3.16279269\n",
      "Iteration 107, loss = 3.16048689\n",
      "Iteration 108, loss = 3.15822263\n",
      "Iteration 109, loss = 3.15597154\n",
      "Iteration 110, loss = 3.15373210\n",
      "Iteration 111, loss = 3.15151197\n",
      "Iteration 112, loss = 3.14932876\n",
      "Iteration 113, loss = 3.14717538\n",
      "Iteration 114, loss = 3.14503656\n",
      "Iteration 115, loss = 3.14289566\n",
      "Iteration 116, loss = 3.14082711\n",
      "Iteration 117, loss = 3.13874290\n",
      "Iteration 118, loss = 3.13668898\n",
      "Iteration 119, loss = 3.13461244\n",
      "Iteration 120, loss = 3.13259259\n",
      "Iteration 121, loss = 3.13062585\n",
      "Iteration 122, loss = 3.12861661\n",
      "Iteration 123, loss = 3.12667590\n",
      "Iteration 124, loss = 3.12476257\n",
      "Iteration 125, loss = 3.12281689\n",
      "Iteration 126, loss = 3.12088479\n",
      "Iteration 127, loss = 3.11899668\n",
      "Iteration 128, loss = 3.11711149\n",
      "Iteration 129, loss = 3.11525718\n",
      "Iteration 130, loss = 3.11340823\n",
      "Iteration 131, loss = 3.11162796\n",
      "Iteration 132, loss = 3.10980627\n",
      "Iteration 133, loss = 3.10801490\n",
      "Iteration 134, loss = 3.10625883\n",
      "Iteration 135, loss = 3.10452262\n",
      "Iteration 136, loss = 3.10277036\n",
      "Iteration 137, loss = 3.10106865\n",
      "Iteration 138, loss = 3.09937913\n",
      "Iteration 139, loss = 3.09768503\n",
      "Iteration 140, loss = 3.09600193\n",
      "Iteration 141, loss = 3.09437063\n",
      "Iteration 142, loss = 3.09273540\n",
      "Iteration 143, loss = 3.09111513\n",
      "Iteration 144, loss = 3.08949379\n",
      "Iteration 145, loss = 3.08791013\n",
      "Iteration 146, loss = 3.08632712\n",
      "Iteration 147, loss = 3.08476691\n",
      "Iteration 148, loss = 3.08319398\n",
      "Iteration 149, loss = 3.08161292\n",
      "Iteration 150, loss = 3.08011893\n",
      "Iteration 151, loss = 3.07857999\n",
      "Iteration 152, loss = 3.07707372\n",
      "Iteration 153, loss = 3.07558648\n",
      "Iteration 154, loss = 3.07411501\n",
      "Iteration 155, loss = 3.07263684\n",
      "Iteration 156, loss = 3.07117077\n",
      "Iteration 157, loss = 3.06975018\n",
      "Iteration 158, loss = 3.06831152\n",
      "Iteration 159, loss = 3.06692054\n",
      "Iteration 160, loss = 3.06550663\n",
      "Iteration 161, loss = 3.06412929\n",
      "Iteration 162, loss = 3.06273037\n",
      "Iteration 163, loss = 3.06137335\n",
      "Iteration 164, loss = 3.05999930\n",
      "Iteration 165, loss = 3.05868128\n",
      "Iteration 166, loss = 3.05734139\n",
      "Iteration 167, loss = 3.05596824\n",
      "Iteration 168, loss = 3.05468262\n",
      "Iteration 169, loss = 3.05334803\n",
      "Iteration 170, loss = 3.05205129\n",
      "Iteration 171, loss = 3.05075901\n",
      "Iteration 172, loss = 3.04947289\n",
      "Iteration 173, loss = 3.04820672\n",
      "Iteration 174, loss = 3.04696663\n",
      "Iteration 175, loss = 3.04567308\n",
      "Iteration 176, loss = 3.04443843\n",
      "Iteration 177, loss = 3.04320685\n",
      "Iteration 178, loss = 3.04193133\n",
      "Iteration 179, loss = 3.04069591\n",
      "Iteration 180, loss = 3.03948080\n",
      "Iteration 181, loss = 3.03823814\n",
      "Iteration 182, loss = 3.03701662\n",
      "Iteration 183, loss = 3.03576249\n",
      "Iteration 184, loss = 3.03456855\n",
      "Iteration 185, loss = 3.03329502\n",
      "Iteration 186, loss = 3.03200419\n",
      "Iteration 187, loss = 3.03076549\n",
      "Iteration 188, loss = 3.02951088\n",
      "Iteration 189, loss = 3.02826409\n",
      "Iteration 190, loss = 3.02697944\n",
      "Iteration 191, loss = 3.02571978\n",
      "Iteration 192, loss = 3.02448772\n",
      "Iteration 193, loss = 3.02320913\n",
      "Iteration 194, loss = 3.02199851\n",
      "Iteration 195, loss = 3.02080339\n",
      "Iteration 196, loss = 3.01961471\n",
      "Iteration 197, loss = 3.01846087\n",
      "Iteration 198, loss = 3.01730898\n",
      "Iteration 199, loss = 3.01618615\n",
      "Iteration 200, loss = 3.01510568\n",
      "Iteration 201, loss = 3.01400894\n",
      "Iteration 202, loss = 3.01294869\n",
      "Iteration 203, loss = 3.01191763\n",
      "Iteration 204, loss = 3.01087046\n",
      "Iteration 205, loss = 3.00987503\n",
      "Iteration 206, loss = 3.00886963\n",
      "Iteration 207, loss = 3.00787666\n",
      "Iteration 208, loss = 3.00687825\n",
      "Iteration 209, loss = 3.00593908\n",
      "Iteration 210, loss = 3.00497156\n",
      "Iteration 211, loss = 3.00400554\n",
      "Iteration 212, loss = 3.00308785\n",
      "Iteration 213, loss = 3.00213986\n",
      "Iteration 214, loss = 3.00125484\n",
      "Iteration 215, loss = 3.00029410\n",
      "Iteration 216, loss = 2.99938794\n",
      "Iteration 217, loss = 2.99849465\n",
      "Iteration 218, loss = 2.99755821\n",
      "Iteration 219, loss = 2.99668941\n",
      "Iteration 220, loss = 2.99579318\n",
      "Iteration 221, loss = 2.99491338\n",
      "Iteration 222, loss = 2.99405274\n",
      "Iteration 223, loss = 2.99315396\n",
      "Iteration 224, loss = 2.99229876\n",
      "Iteration 225, loss = 2.99142765\n",
      "Iteration 226, loss = 2.99059826\n",
      "Iteration 227, loss = 2.98979443\n",
      "Iteration 228, loss = 2.98891725\n",
      "Iteration 229, loss = 2.98810743\n",
      "Iteration 230, loss = 2.98729997\n",
      "Iteration 231, loss = 2.98644139\n",
      "Iteration 232, loss = 2.98563457\n",
      "Iteration 233, loss = 2.98482519\n",
      "Iteration 234, loss = 2.98406075\n",
      "Iteration 235, loss = 2.98325628\n",
      "Iteration 236, loss = 2.98246159\n",
      "Iteration 237, loss = 2.98167155\n",
      "Iteration 238, loss = 2.98088921\n",
      "Iteration 239, loss = 2.98009600\n",
      "Iteration 240, loss = 2.97934728\n",
      "Iteration 241, loss = 2.97857414\n",
      "Iteration 242, loss = 2.97782287\n",
      "Iteration 243, loss = 2.97705320\n",
      "Iteration 244, loss = 2.97631236\n",
      "Iteration 245, loss = 2.97558708\n",
      "Iteration 246, loss = 2.97483436\n",
      "Iteration 247, loss = 2.97408258\n",
      "Iteration 248, loss = 2.97336256\n",
      "Iteration 249, loss = 2.97265927\n",
      "Iteration 250, loss = 2.97193431\n",
      "Iteration 251, loss = 2.97121323\n",
      "Iteration 252, loss = 2.97053598\n",
      "Iteration 253, loss = 2.96982985\n",
      "Iteration 254, loss = 2.96909905\n",
      "Iteration 255, loss = 2.96845178\n",
      "Iteration 256, loss = 2.96775849\n",
      "Iteration 257, loss = 2.96705392\n",
      "Iteration 258, loss = 2.96636231\n",
      "Iteration 259, loss = 2.96571304\n",
      "Iteration 260, loss = 2.96501236\n",
      "Iteration 261, loss = 2.96437590\n",
      "Iteration 262, loss = 2.96368360\n",
      "Iteration 263, loss = 2.96305261\n",
      "Iteration 264, loss = 2.96239825\n",
      "Iteration 265, loss = 2.96173242\n",
      "Iteration 266, loss = 2.96108251\n",
      "Iteration 267, loss = 2.96044876\n",
      "Iteration 268, loss = 2.95983968\n",
      "Iteration 269, loss = 2.95920519\n",
      "Iteration 270, loss = 2.95855935\n",
      "Iteration 271, loss = 2.95792273\n",
      "Iteration 272, loss = 2.95731304\n",
      "Iteration 273, loss = 2.95669688\n",
      "Iteration 274, loss = 2.95607839\n",
      "Iteration 275, loss = 2.95546178\n",
      "Iteration 276, loss = 2.95487567\n",
      "Iteration 277, loss = 2.95426913\n",
      "Iteration 278, loss = 2.95364852\n",
      "Iteration 279, loss = 2.95307329\n",
      "Iteration 280, loss = 2.95247096\n",
      "Iteration 281, loss = 2.95189470\n",
      "Iteration 282, loss = 2.95132712\n",
      "Iteration 283, loss = 2.95073262\n",
      "Iteration 284, loss = 2.95015602\n",
      "Iteration 285, loss = 2.94959165\n",
      "Iteration 286, loss = 2.94900714\n",
      "Iteration 287, loss = 2.94843372\n",
      "Iteration 288, loss = 2.94789401\n",
      "Iteration 289, loss = 2.94735476\n",
      "Iteration 290, loss = 2.94676369\n",
      "Iteration 291, loss = 2.94621651\n",
      "Iteration 292, loss = 2.94566274\n",
      "Iteration 293, loss = 2.94510800\n",
      "Iteration 294, loss = 2.94453703\n",
      "Iteration 295, loss = 2.94402125\n",
      "Iteration 296, loss = 2.94350540\n",
      "Iteration 297, loss = 2.94297407\n",
      "Iteration 298, loss = 2.94239388\n",
      "Iteration 299, loss = 2.94188725\n",
      "Iteration 300, loss = 2.94135098\n",
      "Iteration 301, loss = 2.94085116\n",
      "Iteration 302, loss = 2.94032482\n",
      "Iteration 303, loss = 2.93982169\n",
      "Iteration 304, loss = 2.93931040\n",
      "Iteration 305, loss = 2.93877499\n",
      "Iteration 306, loss = 2.93826016\n",
      "Iteration 307, loss = 2.93776725\n",
      "Iteration 308, loss = 2.93728275\n",
      "Iteration 309, loss = 2.93675269\n",
      "Iteration 310, loss = 2.93628792\n",
      "Iteration 311, loss = 2.93577598\n",
      "Iteration 312, loss = 2.93528804\n",
      "Iteration 313, loss = 2.93481916\n",
      "Iteration 314, loss = 2.93433019\n",
      "Iteration 315, loss = 2.93384305\n",
      "Iteration 316, loss = 2.93333527\n",
      "Iteration 317, loss = 2.93289098\n",
      "Iteration 318, loss = 2.93242913\n",
      "Iteration 319, loss = 2.93194178\n",
      "Iteration 320, loss = 2.93147326\n",
      "Iteration 321, loss = 2.93102043\n",
      "Iteration 322, loss = 2.93051387\n",
      "Iteration 323, loss = 2.93008104\n",
      "Iteration 324, loss = 2.92962389\n",
      "Iteration 325, loss = 2.92916010\n",
      "Iteration 326, loss = 2.92870550\n",
      "Iteration 327, loss = 2.92825730\n",
      "Iteration 328, loss = 2.92780700\n",
      "Iteration 329, loss = 2.92737232\n",
      "Iteration 330, loss = 2.92693422\n",
      "Iteration 331, loss = 2.92648235\n",
      "Iteration 332, loss = 2.92606603\n",
      "Iteration 333, loss = 2.92563560\n",
      "Iteration 334, loss = 2.92517107\n",
      "Iteration 335, loss = 2.92474813\n",
      "Iteration 336, loss = 2.92430086\n",
      "Iteration 337, loss = 2.92386185\n",
      "Iteration 338, loss = 2.92344601\n",
      "Iteration 339, loss = 2.92301956\n",
      "Iteration 340, loss = 2.92262838\n",
      "Iteration 341, loss = 2.92218694\n",
      "Iteration 342, loss = 2.92179663\n",
      "Iteration 343, loss = 2.92138191\n",
      "Iteration 344, loss = 2.92094981\n",
      "Iteration 345, loss = 2.92053344\n",
      "Iteration 346, loss = 2.92014145\n",
      "Iteration 347, loss = 2.91973635\n",
      "Iteration 348, loss = 2.91931960\n",
      "Iteration 349, loss = 2.91893426\n",
      "Iteration 350, loss = 2.91852973\n",
      "Iteration 351, loss = 2.91812471\n",
      "Iteration 352, loss = 2.91773363\n",
      "Iteration 353, loss = 2.91733958\n",
      "Iteration 354, loss = 2.91695133\n",
      "Iteration 355, loss = 2.91656377\n",
      "Iteration 356, loss = 2.91616469\n",
      "Iteration 357, loss = 2.91579602\n",
      "Iteration 358, loss = 2.91537082\n",
      "Iteration 359, loss = 2.91505473\n",
      "Iteration 360, loss = 2.91461921\n",
      "Iteration 361, loss = 2.91425822\n",
      "Iteration 362, loss = 2.91387047\n",
      "Iteration 363, loss = 2.91351310\n",
      "Iteration 364, loss = 2.91310826\n",
      "Iteration 365, loss = 2.91276100\n",
      "Iteration 366, loss = 2.91237071\n",
      "Iteration 367, loss = 2.91198860\n",
      "Iteration 368, loss = 2.91162267\n",
      "Iteration 369, loss = 2.91125522\n",
      "Iteration 370, loss = 2.91088951\n",
      "Iteration 371, loss = 2.91057323\n",
      "Iteration 372, loss = 2.91018490\n",
      "Iteration 373, loss = 2.90983484\n",
      "Iteration 374, loss = 2.90946217\n",
      "Iteration 375, loss = 2.90913744\n",
      "Iteration 376, loss = 2.90877145\n",
      "Iteration 377, loss = 2.90842315\n",
      "Iteration 378, loss = 2.90808933\n",
      "Iteration 379, loss = 2.90770459\n",
      "Iteration 380, loss = 2.90737317\n",
      "Iteration 381, loss = 2.90703008\n",
      "Iteration 382, loss = 2.90670471\n",
      "Iteration 383, loss = 2.90635247\n",
      "Iteration 384, loss = 2.90602050\n",
      "Iteration 385, loss = 2.90570316\n",
      "Iteration 386, loss = 2.90530291\n",
      "Iteration 387, loss = 2.90500914\n",
      "Iteration 388, loss = 2.90467700\n",
      "Iteration 389, loss = 2.90434495\n",
      "Iteration 390, loss = 2.90398365\n",
      "Iteration 391, loss = 2.90367488\n",
      "Iteration 392, loss = 2.90332713\n",
      "Iteration 393, loss = 2.90304018\n",
      "Iteration 394, loss = 2.90270980\n",
      "Iteration 395, loss = 2.90237016\n",
      "Iteration 396, loss = 2.90202504\n",
      "Iteration 397, loss = 2.90171242\n",
      "Iteration 398, loss = 2.90137723\n",
      "Iteration 399, loss = 2.90109284\n",
      "Iteration 400, loss = 2.90074365\n",
      "Iteration 401, loss = 2.90046496\n",
      "Iteration 402, loss = 2.90012589\n",
      "Iteration 403, loss = 2.89984101\n",
      "Iteration 404, loss = 2.89950728\n",
      "Iteration 405, loss = 2.89919391\n",
      "Iteration 406, loss = 2.89887535\n",
      "Iteration 407, loss = 2.89856743\n",
      "Iteration 408, loss = 2.89826087\n",
      "Iteration 409, loss = 2.89799070\n",
      "Iteration 410, loss = 2.89767892\n",
      "Iteration 411, loss = 2.89738171\n",
      "Iteration 412, loss = 2.89707800\n",
      "Iteration 413, loss = 2.89677897\n",
      "Iteration 414, loss = 2.89647052\n",
      "Iteration 415, loss = 2.89618075\n",
      "Iteration 416, loss = 2.89589093\n",
      "Iteration 417, loss = 2.89558291\n",
      "Iteration 418, loss = 2.89528534\n",
      "Iteration 419, loss = 2.89497148\n",
      "Iteration 420, loss = 2.89468554\n",
      "Iteration 421, loss = 2.89438395\n",
      "Iteration 422, loss = 2.89412808\n",
      "Iteration 423, loss = 2.89381414\n",
      "Iteration 424, loss = 2.89352571\n",
      "Iteration 425, loss = 2.89325433\n",
      "Iteration 426, loss = 2.89299861\n",
      "Iteration 427, loss = 2.89271634\n",
      "Iteration 428, loss = 2.89239732\n",
      "Iteration 429, loss = 2.89212185\n",
      "Iteration 430, loss = 2.89183418\n",
      "Iteration 431, loss = 2.89155820\n",
      "Iteration 432, loss = 2.89132872\n",
      "Iteration 433, loss = 2.89102794\n",
      "Iteration 434, loss = 2.89073499\n",
      "Iteration 435, loss = 2.89046154\n",
      "Iteration 436, loss = 2.89019192\n",
      "Iteration 437, loss = 2.88989706\n",
      "Iteration 438, loss = 2.88965629\n",
      "Iteration 439, loss = 2.88937480\n",
      "Iteration 440, loss = 2.88910392\n",
      "Iteration 441, loss = 2.88884767\n",
      "Iteration 442, loss = 2.88855965\n",
      "Iteration 443, loss = 2.88830566\n",
      "Iteration 444, loss = 2.88804259\n",
      "Iteration 445, loss = 2.88777650\n",
      "Iteration 446, loss = 2.88751955\n",
      "Iteration 447, loss = 2.88724287\n",
      "Iteration 448, loss = 2.88699732\n",
      "Iteration 449, loss = 2.88673415\n",
      "Iteration 450, loss = 2.88649244\n",
      "Iteration 451, loss = 2.88624354\n",
      "Iteration 452, loss = 2.88598823\n",
      "Iteration 453, loss = 2.88570016\n",
      "Iteration 454, loss = 2.88546969\n",
      "Iteration 455, loss = 2.88522577\n",
      "Iteration 456, loss = 2.88497039\n",
      "Iteration 457, loss = 2.88468888\n",
      "Iteration 458, loss = 2.88445247\n",
      "Iteration 459, loss = 2.88421791\n",
      "Iteration 460, loss = 2.88398399\n",
      "Iteration 461, loss = 2.88373616\n",
      "Iteration 462, loss = 2.88345084\n",
      "Iteration 463, loss = 2.88320421\n",
      "Iteration 464, loss = 2.88296567\n",
      "Iteration 465, loss = 2.88272722\n",
      "Iteration 466, loss = 2.88248419\n",
      "Iteration 467, loss = 2.88224598\n",
      "Iteration 468, loss = 2.88198052\n",
      "Iteration 469, loss = 2.88173783\n",
      "Iteration 470, loss = 2.88152606\n",
      "Iteration 471, loss = 2.88128392\n",
      "Iteration 472, loss = 2.88108330\n",
      "Iteration 473, loss = 2.88079483\n",
      "Iteration 474, loss = 2.88059202\n",
      "Iteration 475, loss = 2.88032629\n",
      "Iteration 476, loss = 2.88010916\n",
      "Iteration 477, loss = 2.87987910\n",
      "Iteration 478, loss = 2.87964028\n",
      "Iteration 479, loss = 2.87941399\n",
      "Iteration 480, loss = 2.87914901\n",
      "Iteration 481, loss = 2.87892319\n",
      "Iteration 482, loss = 2.87870440\n",
      "Iteration 483, loss = 2.87847954\n",
      "Iteration 484, loss = 2.87826647\n",
      "Iteration 485, loss = 2.87802724\n",
      "Iteration 486, loss = 2.87778255\n",
      "Iteration 487, loss = 2.87758572\n",
      "Iteration 488, loss = 2.87735101\n",
      "Iteration 489, loss = 2.87713599\n",
      "Iteration 490, loss = 2.87690876\n",
      "Iteration 491, loss = 2.87669525\n",
      "Iteration 492, loss = 2.87648381\n",
      "Iteration 493, loss = 2.87622483\n",
      "Iteration 494, loss = 2.87600951\n",
      "Iteration 495, loss = 2.87578432\n",
      "Iteration 496, loss = 2.87560225\n",
      "Iteration 497, loss = 2.87538331\n",
      "Iteration 498, loss = 2.87514223\n",
      "Iteration 499, loss = 2.87492408\n",
      "Iteration 500, loss = 2.87474352\n",
      "Iteration 501, loss = 2.87449784\n",
      "Iteration 502, loss = 2.87429866\n",
      "Iteration 503, loss = 2.87405505\n",
      "Iteration 504, loss = 2.87385443\n",
      "Iteration 505, loss = 2.87364701\n",
      "Iteration 506, loss = 2.87341115\n",
      "Iteration 507, loss = 2.87321941\n",
      "Iteration 508, loss = 2.87300683\n",
      "Iteration 509, loss = 2.87280475\n",
      "Iteration 510, loss = 2.87257150\n",
      "Iteration 511, loss = 2.87240084\n",
      "Iteration 512, loss = 2.87216718\n",
      "Iteration 513, loss = 2.87193354\n",
      "Iteration 514, loss = 2.87177041\n",
      "Iteration 515, loss = 2.87152774\n",
      "Iteration 516, loss = 2.87132778\n",
      "Iteration 517, loss = 2.87112590\n",
      "Iteration 518, loss = 2.87094844\n",
      "Iteration 519, loss = 2.87070477\n",
      "Iteration 520, loss = 2.87051141\n",
      "Iteration 521, loss = 2.87033648\n",
      "Iteration 522, loss = 2.87013287\n",
      "Iteration 523, loss = 2.86993015\n",
      "Iteration 524, loss = 2.86970043\n",
      "Iteration 525, loss = 2.86952501\n",
      "Iteration 526, loss = 2.86931464\n",
      "Iteration 527, loss = 2.86911412\n",
      "Iteration 528, loss = 2.86890209\n",
      "Iteration 529, loss = 2.86873383\n",
      "Iteration 530, loss = 2.86851593\n",
      "Iteration 531, loss = 2.86830944\n",
      "Iteration 532, loss = 2.86812927\n",
      "Iteration 533, loss = 2.86795085\n",
      "Iteration 534, loss = 2.86775118\n",
      "Iteration 535, loss = 2.86757487\n",
      "Iteration 536, loss = 2.86736501\n",
      "Iteration 537, loss = 2.86715915\n",
      "Iteration 538, loss = 2.86696304\n",
      "Iteration 539, loss = 2.86678338\n",
      "Iteration 540, loss = 2.86660150\n",
      "Iteration 541, loss = 2.86638743\n",
      "Iteration 542, loss = 2.86621099\n",
      "Iteration 543, loss = 2.86600792\n",
      "Iteration 544, loss = 2.86583496\n",
      "Iteration 545, loss = 2.86565003\n",
      "Iteration 546, loss = 2.86543441\n",
      "Iteration 547, loss = 2.86526750\n",
      "Iteration 548, loss = 2.86507249\n",
      "Iteration 549, loss = 2.86489483\n",
      "Iteration 550, loss = 2.86470790\n",
      "Iteration 551, loss = 2.86451460\n",
      "Iteration 552, loss = 2.86434648\n",
      "Iteration 553, loss = 2.86416541\n",
      "Iteration 554, loss = 2.86395077\n",
      "Iteration 555, loss = 2.86382587\n",
      "Iteration 556, loss = 2.86360539\n",
      "Iteration 557, loss = 2.86345800\n",
      "Iteration 558, loss = 2.86325595\n",
      "Iteration 559, loss = 2.86307848\n",
      "Iteration 560, loss = 2.86289451\n",
      "Iteration 561, loss = 2.86269782\n",
      "Iteration 562, loss = 2.86253976\n",
      "Iteration 563, loss = 2.86235404\n",
      "Iteration 564, loss = 2.86217177\n",
      "Iteration 565, loss = 2.86199080\n",
      "Iteration 566, loss = 2.86181310\n",
      "Iteration 567, loss = 2.86164595\n",
      "Iteration 568, loss = 2.86146282\n",
      "Iteration 569, loss = 2.86128609\n",
      "Iteration 570, loss = 2.86110378\n",
      "Iteration 571, loss = 2.86095071\n",
      "Iteration 572, loss = 2.86075113\n",
      "Iteration 573, loss = 2.86058164\n",
      "Iteration 574, loss = 2.86041491\n",
      "Iteration 575, loss = 2.86022712\n",
      "Iteration 576, loss = 2.86004850\n",
      "Iteration 577, loss = 2.85988834\n",
      "Iteration 578, loss = 2.85972871\n",
      "Iteration 579, loss = 2.85955427\n",
      "Iteration 580, loss = 2.85938823\n",
      "Iteration 581, loss = 2.85921495\n",
      "Iteration 582, loss = 2.85906079\n",
      "Iteration 583, loss = 2.85889732\n",
      "Iteration 584, loss = 2.85871424\n",
      "Iteration 585, loss = 2.85855078\n",
      "Iteration 586, loss = 2.85838066\n",
      "Iteration 587, loss = 2.85819774\n",
      "Iteration 588, loss = 2.85805245\n",
      "Iteration 589, loss = 2.85786419\n",
      "Iteration 590, loss = 2.85767132\n",
      "Iteration 591, loss = 2.85753081\n",
      "Iteration 592, loss = 2.85737682\n",
      "Iteration 593, loss = 2.85719807\n",
      "Iteration 594, loss = 2.85705407\n",
      "Iteration 595, loss = 2.85687737\n",
      "Iteration 596, loss = 2.85670439\n",
      "Iteration 597, loss = 2.85656689\n",
      "Iteration 598, loss = 2.85640007\n",
      "Iteration 599, loss = 2.85621728\n",
      "Iteration 600, loss = 2.85609230\n",
      "Iteration 601, loss = 2.85588683\n",
      "Iteration 602, loss = 2.85574855\n",
      "Iteration 603, loss = 2.85556517\n",
      "Iteration 604, loss = 2.85542619\n",
      "Iteration 605, loss = 2.85527103\n",
      "Iteration 606, loss = 2.85508115\n",
      "Iteration 607, loss = 2.85493239\n",
      "Iteration 608, loss = 2.85478667\n",
      "Iteration 609, loss = 2.85460379\n",
      "Iteration 610, loss = 2.85447359\n",
      "Iteration 611, loss = 2.85432100\n",
      "Iteration 612, loss = 2.85414069\n",
      "Iteration 613, loss = 2.85400132\n",
      "Iteration 614, loss = 2.85385726\n",
      "Iteration 615, loss = 2.85365351\n",
      "Iteration 616, loss = 2.85352158\n",
      "Iteration 617, loss = 2.85334274\n",
      "Iteration 618, loss = 2.85320318\n",
      "Iteration 619, loss = 2.85305555\n",
      "Iteration 620, loss = 2.85288358\n",
      "Iteration 621, loss = 2.85274726\n",
      "Iteration 622, loss = 2.85261150\n",
      "Iteration 623, loss = 2.85248133\n",
      "Iteration 624, loss = 2.85228154\n",
      "Iteration 625, loss = 2.85212299\n",
      "Iteration 626, loss = 2.85199508\n",
      "Iteration 627, loss = 2.85182708\n",
      "Iteration 628, loss = 2.85168122\n",
      "Iteration 629, loss = 2.85153022\n",
      "Iteration 630, loss = 2.85139591\n",
      "Iteration 631, loss = 2.85121080\n",
      "Iteration 632, loss = 2.85106455\n",
      "Iteration 633, loss = 2.85094182\n",
      "Iteration 634, loss = 2.85077173\n",
      "Iteration 635, loss = 2.85063935\n",
      "Iteration 636, loss = 2.85046644\n",
      "Iteration 637, loss = 2.85032523\n",
      "Iteration 638, loss = 2.85017530\n",
      "Iteration 639, loss = 2.85002726\n",
      "Iteration 640, loss = 2.84990556\n",
      "Iteration 641, loss = 2.84975642\n",
      "Iteration 642, loss = 2.84960181\n",
      "Iteration 643, loss = 2.84945742\n",
      "Iteration 644, loss = 2.84931065\n",
      "Iteration 645, loss = 2.84917908\n",
      "Iteration 646, loss = 2.84903370\n",
      "Iteration 647, loss = 2.84886269\n",
      "Iteration 648, loss = 2.84871034\n",
      "Iteration 649, loss = 2.84856811\n",
      "Iteration 650, loss = 2.84846698\n",
      "Iteration 651, loss = 2.84828492\n",
      "Iteration 652, loss = 2.84815537\n",
      "Iteration 653, loss = 2.84801219\n",
      "Iteration 654, loss = 2.84786483\n",
      "Iteration 655, loss = 2.84770717\n",
      "Iteration 656, loss = 2.84757795\n",
      "Iteration 657, loss = 2.84745808\n",
      "Iteration 658, loss = 2.84732579\n",
      "Iteration 659, loss = 2.84716091\n",
      "Iteration 660, loss = 2.84702219\n",
      "Iteration 661, loss = 2.84688168\n",
      "Iteration 662, loss = 2.84676291\n",
      "Iteration 663, loss = 2.84658073\n",
      "Iteration 664, loss = 2.84643959\n",
      "Iteration 665, loss = 2.84631500\n",
      "Iteration 666, loss = 2.84617298\n",
      "Iteration 667, loss = 2.84602284\n",
      "Iteration 668, loss = 2.84589711\n",
      "Iteration 669, loss = 2.84572801\n",
      "Iteration 670, loss = 2.84561510\n",
      "Iteration 671, loss = 2.84546943\n",
      "Iteration 672, loss = 2.84534552\n",
      "Iteration 673, loss = 2.84522972\n",
      "Iteration 674, loss = 2.84503703\n",
      "Iteration 675, loss = 2.84490932\n",
      "Iteration 676, loss = 2.84477268\n",
      "Iteration 677, loss = 2.84464858\n",
      "Iteration 678, loss = 2.84451599\n",
      "Iteration 679, loss = 2.84438888\n",
      "Iteration 680, loss = 2.84427631\n",
      "Iteration 681, loss = 2.84412450\n",
      "Iteration 682, loss = 2.84395274\n",
      "Iteration 683, loss = 2.84384974\n",
      "Iteration 684, loss = 2.84371631\n",
      "Iteration 685, loss = 2.84358527\n",
      "Iteration 686, loss = 2.84341070\n",
      "Iteration 687, loss = 2.84328100\n",
      "Iteration 688, loss = 2.84317442\n",
      "Iteration 689, loss = 2.84305312\n",
      "Iteration 690, loss = 2.84292837\n",
      "Iteration 691, loss = 2.84276374\n",
      "Iteration 692, loss = 2.84263572\n",
      "Iteration 693, loss = 2.84247832\n",
      "Iteration 694, loss = 2.84235646\n",
      "Iteration 695, loss = 2.84222485\n",
      "Iteration 696, loss = 2.84208590\n",
      "Iteration 697, loss = 2.84201608\n",
      "Iteration 698, loss = 2.84187118\n",
      "Iteration 699, loss = 2.84170537\n",
      "Iteration 700, loss = 2.84159656\n",
      "Iteration 701, loss = 2.84144644\n",
      "Iteration 702, loss = 2.84133366\n",
      "Iteration 703, loss = 2.84120685\n",
      "Iteration 704, loss = 2.84106075\n",
      "Iteration 705, loss = 2.84092136\n",
      "Iteration 706, loss = 2.84079917\n",
      "Iteration 707, loss = 2.84068227\n",
      "Iteration 708, loss = 2.84055106\n",
      "Iteration 709, loss = 2.84044576\n",
      "Iteration 710, loss = 2.84028949\n",
      "Iteration 711, loss = 2.84018121\n",
      "Iteration 712, loss = 2.84005576\n",
      "Iteration 713, loss = 2.83989921\n",
      "Iteration 714, loss = 2.83976284\n",
      "Iteration 715, loss = 2.83965570\n",
      "Iteration 716, loss = 2.83954223\n",
      "Iteration 717, loss = 2.83939408\n",
      "Iteration 718, loss = 2.83928651\n",
      "Iteration 719, loss = 2.83914784\n",
      "Iteration 720, loss = 2.83901560\n",
      "Iteration 721, loss = 2.83894518\n",
      "Iteration 722, loss = 2.83878527\n",
      "Iteration 723, loss = 2.83865995\n",
      "Iteration 724, loss = 2.83857997\n",
      "Iteration 725, loss = 2.83840162\n",
      "Iteration 726, loss = 2.83827615\n",
      "Iteration 727, loss = 2.83817859\n",
      "Iteration 728, loss = 2.83804865\n",
      "Iteration 729, loss = 2.83790409\n",
      "Iteration 730, loss = 2.83778753\n",
      "Iteration 731, loss = 2.83768409\n",
      "Iteration 732, loss = 2.83752727\n",
      "Iteration 733, loss = 2.83742184\n",
      "Iteration 734, loss = 2.83732229\n",
      "Iteration 735, loss = 2.83715802\n",
      "Iteration 736, loss = 2.83707715\n",
      "Iteration 737, loss = 2.83691658\n",
      "Iteration 738, loss = 2.83680268\n",
      "Iteration 739, loss = 2.83669260\n",
      "Iteration 740, loss = 2.83654748\n",
      "Iteration 741, loss = 2.83644384\n",
      "Iteration 742, loss = 2.83632610\n",
      "Iteration 743, loss = 2.83619293\n",
      "Iteration 744, loss = 2.83608525\n",
      "Iteration 745, loss = 2.83596569\n",
      "Iteration 746, loss = 2.83585149\n",
      "Iteration 747, loss = 2.83571135\n",
      "Iteration 748, loss = 2.83562106\n",
      "Iteration 749, loss = 2.83547382\n",
      "Iteration 750, loss = 2.83539400\n",
      "Iteration 751, loss = 2.83525512\n",
      "Iteration 752, loss = 2.83513440\n",
      "Iteration 753, loss = 2.83501080\n",
      "Iteration 754, loss = 2.83488946\n",
      "Iteration 755, loss = 2.83477141\n",
      "Iteration 756, loss = 2.83467084\n",
      "Iteration 757, loss = 2.83457780\n",
      "Iteration 758, loss = 2.83443108\n",
      "Iteration 759, loss = 2.83430648\n",
      "Iteration 760, loss = 2.83419112\n",
      "Iteration 761, loss = 2.83405059\n",
      "Iteration 762, loss = 2.83396428\n",
      "Iteration 763, loss = 2.83384853\n",
      "Iteration 764, loss = 2.83372632\n",
      "Iteration 765, loss = 2.83358659\n",
      "Iteration 766, loss = 2.83347415\n",
      "Iteration 767, loss = 2.83335774\n",
      "Iteration 768, loss = 2.83324347\n",
      "Iteration 769, loss = 2.83314064\n",
      "Iteration 770, loss = 2.83300642\n",
      "Iteration 771, loss = 2.83287014\n",
      "Iteration 772, loss = 2.83277384\n",
      "Iteration 773, loss = 2.83265401\n",
      "Iteration 774, loss = 2.83258446\n",
      "Iteration 775, loss = 2.83244527\n",
      "Iteration 776, loss = 2.83234139\n",
      "Iteration 777, loss = 2.83220897\n",
      "Iteration 778, loss = 2.83209693\n",
      "Iteration 779, loss = 2.83199073\n",
      "Iteration 780, loss = 2.83187240\n",
      "Iteration 781, loss = 2.83174886\n",
      "Iteration 782, loss = 2.83164250\n",
      "Iteration 783, loss = 2.83153006\n",
      "Iteration 784, loss = 2.83140127\n",
      "Iteration 785, loss = 2.83132854\n",
      "Iteration 786, loss = 2.83119537\n",
      "Iteration 787, loss = 2.83108467\n",
      "Iteration 788, loss = 2.83094998\n",
      "Iteration 789, loss = 2.83087025\n",
      "Iteration 790, loss = 2.83072923\n",
      "Iteration 791, loss = 2.83062146\n",
      "Iteration 792, loss = 2.83049354\n",
      "Iteration 793, loss = 2.83043205\n",
      "Iteration 794, loss = 2.83028481\n",
      "Iteration 795, loss = 2.83017673\n",
      "Iteration 796, loss = 2.83006833\n",
      "Iteration 797, loss = 2.82995310\n",
      "Iteration 798, loss = 2.82985446\n",
      "Iteration 799, loss = 2.82974049\n",
      "Iteration 800, loss = 2.82961362\n",
      "Iteration 801, loss = 2.82952075\n",
      "Iteration 802, loss = 2.82940304\n",
      "Iteration 803, loss = 2.82929255\n",
      "Iteration 804, loss = 2.82917975\n",
      "Iteration 805, loss = 2.82906886\n",
      "Iteration 806, loss = 2.82899389\n",
      "Iteration 807, loss = 2.82886863\n",
      "Iteration 808, loss = 2.82874310\n",
      "Iteration 809, loss = 2.82864337\n",
      "Iteration 810, loss = 2.82853293\n",
      "Iteration 811, loss = 2.82845071\n",
      "Iteration 812, loss = 2.82830797\n",
      "Iteration 813, loss = 2.82820050\n",
      "Iteration 814, loss = 2.82810225\n",
      "Iteration 815, loss = 2.82799030\n",
      "Iteration 816, loss = 2.82790496\n",
      "Iteration 817, loss = 2.82779323\n",
      "Iteration 818, loss = 2.82766505\n",
      "Iteration 819, loss = 2.82759388\n",
      "Iteration 820, loss = 2.82748514\n",
      "Iteration 821, loss = 2.82733044\n",
      "Iteration 822, loss = 2.82726810\n",
      "Iteration 823, loss = 2.82716139\n",
      "Iteration 824, loss = 2.82702730\n",
      "Iteration 825, loss = 2.82692680\n",
      "Iteration 826, loss = 2.82683818\n",
      "Iteration 827, loss = 2.82674361\n",
      "Iteration 828, loss = 2.82665394\n",
      "Iteration 829, loss = 2.82649414\n",
      "Iteration 830, loss = 2.82639068\n",
      "Iteration 831, loss = 2.82630528\n",
      "Iteration 832, loss = 2.82619492\n",
      "Iteration 833, loss = 2.82608934\n",
      "Iteration 834, loss = 2.82600669\n",
      "Iteration 835, loss = 2.82589656\n",
      "Iteration 836, loss = 2.82577210\n",
      "Iteration 837, loss = 2.82564805\n",
      "Iteration 838, loss = 2.82556200\n",
      "Iteration 839, loss = 2.82548054\n",
      "Iteration 840, loss = 2.82538909\n",
      "Iteration 841, loss = 2.82527957\n",
      "Iteration 842, loss = 2.82513949\n",
      "Iteration 843, loss = 2.82508801\n",
      "Iteration 844, loss = 2.82494135\n",
      "Iteration 845, loss = 2.82485369\n",
      "Iteration 846, loss = 2.82471721\n",
      "Iteration 847, loss = 2.82465869\n",
      "Iteration 848, loss = 2.82453239\n",
      "Iteration 849, loss = 2.82442583\n",
      "Iteration 850, loss = 2.82430770\n",
      "Iteration 851, loss = 2.82424018\n",
      "Iteration 852, loss = 2.82410273\n",
      "Iteration 853, loss = 2.82401337\n",
      "Iteration 854, loss = 2.82390147\n",
      "Iteration 855, loss = 2.82381179\n",
      "Iteration 856, loss = 2.82369821\n",
      "Iteration 857, loss = 2.82359302\n",
      "Iteration 858, loss = 2.82349990\n",
      "Iteration 859, loss = 2.82339141\n",
      "Iteration 860, loss = 2.82331745\n",
      "Iteration 861, loss = 2.82318394\n",
      "Iteration 862, loss = 2.82306957\n",
      "Iteration 863, loss = 2.82300014\n",
      "Iteration 864, loss = 2.82290932\n",
      "Iteration 865, loss = 2.82282052\n",
      "Iteration 866, loss = 2.82271514\n",
      "Iteration 867, loss = 2.82261629\n",
      "Iteration 868, loss = 2.82247710\n",
      "Iteration 869, loss = 2.82238158\n",
      "Iteration 870, loss = 2.82230271\n",
      "Iteration 871, loss = 2.82220925\n",
      "Iteration 872, loss = 2.82207976\n",
      "Iteration 873, loss = 2.82200662\n",
      "Iteration 874, loss = 2.82189017\n",
      "Iteration 875, loss = 2.82180473\n",
      "Iteration 876, loss = 2.82170641\n",
      "Iteration 877, loss = 2.82159550\n",
      "Iteration 878, loss = 2.82152191\n",
      "Iteration 879, loss = 2.82138287\n",
      "Iteration 880, loss = 2.82126905\n",
      "Iteration 881, loss = 2.82121441\n",
      "Iteration 882, loss = 2.82111233\n",
      "Iteration 883, loss = 2.82098571\n",
      "Iteration 884, loss = 2.82090857\n",
      "Iteration 885, loss = 2.82079584\n",
      "Iteration 886, loss = 2.82070784\n",
      "Iteration 887, loss = 2.82061318\n",
      "Iteration 888, loss = 2.82049393\n",
      "Iteration 889, loss = 2.82041054\n",
      "Iteration 890, loss = 2.82030955\n",
      "Iteration 891, loss = 2.82021776\n",
      "Iteration 892, loss = 2.82014104\n",
      "Iteration 893, loss = 2.82002161\n",
      "Iteration 894, loss = 2.81992197\n",
      "Iteration 895, loss = 2.81983286\n",
      "Iteration 896, loss = 2.81978301\n",
      "Iteration 897, loss = 2.81964647\n",
      "Iteration 898, loss = 2.81957244\n",
      "Iteration 899, loss = 2.81944014\n",
      "Iteration 900, loss = 2.81936142\n",
      "Iteration 901, loss = 2.81925187\n",
      "Iteration 902, loss = 2.81917319\n",
      "Iteration 903, loss = 2.81909901\n",
      "Iteration 904, loss = 2.81897190\n",
      "Iteration 905, loss = 2.81885763\n",
      "Iteration 906, loss = 2.81875954\n",
      "Iteration 907, loss = 2.81869208\n",
      "Iteration 908, loss = 2.81856786\n",
      "Iteration 909, loss = 2.81849080\n",
      "Iteration 910, loss = 2.81837077\n",
      "Iteration 911, loss = 2.81830511\n",
      "Iteration 912, loss = 2.81821294\n",
      "Iteration 913, loss = 2.81812758\n",
      "Iteration 914, loss = 2.81800332\n",
      "Iteration 915, loss = 2.81790645\n",
      "Iteration 916, loss = 2.81781738\n",
      "Iteration 917, loss = 2.81775693\n",
      "Iteration 918, loss = 2.81764789\n",
      "Iteration 919, loss = 2.81754723\n",
      "Iteration 920, loss = 2.81745820\n",
      "Iteration 921, loss = 2.81735997\n",
      "Iteration 922, loss = 2.81728229\n",
      "Iteration 923, loss = 2.81717876\n",
      "Iteration 924, loss = 2.81707515\n",
      "Iteration 925, loss = 2.81700078\n",
      "Iteration 926, loss = 2.81686743\n",
      "Iteration 927, loss = 2.81679994\n",
      "Iteration 928, loss = 2.81667861\n",
      "Iteration 929, loss = 2.81658654\n",
      "Iteration 930, loss = 2.81652888\n",
      "Iteration 931, loss = 2.81645410\n",
      "Iteration 932, loss = 2.81630211\n",
      "Iteration 933, loss = 2.81622949\n",
      "Iteration 934, loss = 2.81613353\n",
      "Iteration 935, loss = 2.81607055\n",
      "Iteration 936, loss = 2.81592422\n",
      "Iteration 937, loss = 2.81586639\n",
      "Iteration 938, loss = 2.81575847\n",
      "Iteration 939, loss = 2.81566537\n",
      "Iteration 940, loss = 2.81557251\n",
      "Iteration 941, loss = 2.81550073\n",
      "Iteration 942, loss = 2.81540304\n",
      "Iteration 943, loss = 2.81533108\n",
      "Iteration 944, loss = 2.81522867\n",
      "Iteration 945, loss = 2.81513476\n",
      "Iteration 946, loss = 2.81503460\n",
      "Iteration 947, loss = 2.81495272\n",
      "Iteration 948, loss = 2.81485267\n",
      "Iteration 949, loss = 2.81479511\n",
      "Iteration 950, loss = 2.81467786\n",
      "Iteration 951, loss = 2.81454296\n",
      "Iteration 952, loss = 2.81447675\n",
      "Iteration 953, loss = 2.81439201\n",
      "Iteration 954, loss = 2.81432202\n",
      "Iteration 955, loss = 2.81419009\n",
      "Iteration 956, loss = 2.81411022\n",
      "Iteration 957, loss = 2.81407326\n",
      "Iteration 958, loss = 2.81393097\n",
      "Iteration 959, loss = 2.81383837\n",
      "Iteration 960, loss = 2.81375560\n",
      "Iteration 961, loss = 2.81368013\n",
      "Iteration 962, loss = 2.81355984\n",
      "Iteration 963, loss = 2.81347783\n",
      "Iteration 964, loss = 2.81337856\n",
      "Iteration 965, loss = 2.81328432\n",
      "Iteration 966, loss = 2.81319309\n",
      "Iteration 967, loss = 2.81313538\n",
      "Iteration 968, loss = 2.81303818\n",
      "Iteration 969, loss = 2.81292678\n",
      "Iteration 970, loss = 2.81284078\n",
      "Iteration 971, loss = 2.81273210\n",
      "Iteration 972, loss = 2.81267596\n",
      "Iteration 973, loss = 2.81258254\n",
      "Iteration 974, loss = 2.81247225\n",
      "Iteration 975, loss = 2.81242468\n",
      "Iteration 976, loss = 2.81231543\n",
      "Iteration 977, loss = 2.81222865\n",
      "Iteration 978, loss = 2.81211908\n",
      "Iteration 979, loss = 2.81206193\n",
      "Iteration 980, loss = 2.81195264\n",
      "Iteration 981, loss = 2.81188158\n",
      "Iteration 982, loss = 2.81180794\n",
      "Iteration 983, loss = 2.81171703\n",
      "Iteration 984, loss = 2.81161377\n",
      "Iteration 985, loss = 2.81151667\n",
      "Iteration 986, loss = 2.81142745\n",
      "Iteration 987, loss = 2.81133960\n",
      "Iteration 988, loss = 2.81126915\n",
      "Iteration 989, loss = 2.81117658\n",
      "Iteration 990, loss = 2.81111152\n",
      "Iteration 991, loss = 2.81101715\n",
      "Iteration 992, loss = 2.81092734\n",
      "Iteration 993, loss = 2.81082715\n",
      "Iteration 994, loss = 2.81074128\n",
      "Iteration 995, loss = 2.81065174\n",
      "Iteration 996, loss = 2.81055092\n",
      "Iteration 997, loss = 2.81048526\n",
      "Iteration 998, loss = 2.81040923\n",
      "Iteration 999, loss = 2.81030287\n",
      "Iteration 1000, loss = 2.81023049\n",
      "Iteration 1001, loss = 2.81012226\n",
      "Iteration 1002, loss = 2.81005238\n",
      "Iteration 1003, loss = 2.80994005\n",
      "Iteration 1004, loss = 2.80987372\n",
      "Iteration 1005, loss = 2.80978790\n",
      "Iteration 1006, loss = 2.80972691\n",
      "Iteration 1007, loss = 2.80962991\n",
      "Iteration 1008, loss = 2.80951906\n",
      "Iteration 1009, loss = 2.80946921\n",
      "Iteration 1010, loss = 2.80935070\n",
      "Iteration 1011, loss = 2.80928545\n",
      "Iteration 1012, loss = 2.80916585\n",
      "Iteration 1013, loss = 2.80908549\n",
      "Iteration 1014, loss = 2.80900525\n",
      "Iteration 1015, loss = 2.80890825\n",
      "Iteration 1016, loss = 2.80882480\n",
      "Iteration 1017, loss = 2.80877354\n",
      "Iteration 1018, loss = 2.80869620\n",
      "Iteration 1019, loss = 2.80857005\n",
      "Iteration 1020, loss = 2.80849072\n",
      "Iteration 1021, loss = 2.80841708\n",
      "Iteration 1022, loss = 2.80831243\n",
      "Iteration 1023, loss = 2.80822234\n",
      "Iteration 1024, loss = 2.80816606\n",
      "Iteration 1025, loss = 2.80806973\n",
      "Iteration 1026, loss = 2.80797644\n",
      "Iteration 1027, loss = 2.80789958\n",
      "Iteration 1028, loss = 2.80784998\n",
      "Iteration 1029, loss = 2.80771875\n",
      "Iteration 1030, loss = 2.80767613\n",
      "Iteration 1031, loss = 2.80755569\n",
      "Iteration 1032, loss = 2.80749700\n",
      "Iteration 1033, loss = 2.80738951\n",
      "Iteration 1034, loss = 2.80731052\n",
      "Iteration 1035, loss = 2.80721908\n",
      "Iteration 1036, loss = 2.80711781\n",
      "Iteration 1037, loss = 2.80706740\n",
      "Iteration 1038, loss = 2.80696245\n",
      "Iteration 1039, loss = 2.80690780\n",
      "Iteration 1040, loss = 2.80683551\n",
      "Iteration 1041, loss = 2.80673235\n",
      "Iteration 1042, loss = 2.80664534\n",
      "Iteration 1043, loss = 2.80656487\n",
      "Iteration 1044, loss = 2.80646524\n",
      "Iteration 1045, loss = 2.80639646\n",
      "Iteration 1046, loss = 2.80629558\n",
      "Iteration 1047, loss = 2.80622483\n",
      "Iteration 1048, loss = 2.80614265\n",
      "Iteration 1049, loss = 2.80604371\n",
      "Iteration 1050, loss = 2.80598460\n",
      "Iteration 1051, loss = 2.80590166\n",
      "Iteration 1052, loss = 2.80578927\n",
      "Iteration 1053, loss = 2.80575654\n",
      "Iteration 1054, loss = 2.80561033\n",
      "Iteration 1055, loss = 2.80558466\n",
      "Iteration 1056, loss = 2.80547334\n",
      "Iteration 1057, loss = 2.80544128\n",
      "Iteration 1058, loss = 2.80532986\n",
      "Iteration 1059, loss = 2.80522258\n",
      "Iteration 1060, loss = 2.80514137\n",
      "Iteration 1061, loss = 2.80507091\n",
      "Iteration 1062, loss = 2.80498773\n",
      "Iteration 1063, loss = 2.80490752\n",
      "Iteration 1064, loss = 2.80482178\n",
      "Iteration 1065, loss = 2.80474791\n",
      "Iteration 1066, loss = 2.80466664\n",
      "Iteration 1067, loss = 2.80457265\n",
      "Iteration 1068, loss = 2.80448335\n",
      "Iteration 1069, loss = 2.80442482\n",
      "Iteration 1070, loss = 2.80435439\n",
      "Iteration 1071, loss = 2.80426613\n",
      "Iteration 1072, loss = 2.80419599\n",
      "Iteration 1073, loss = 2.80411699\n",
      "Iteration 1074, loss = 2.80399958\n",
      "Iteration 1075, loss = 2.80397005\n",
      "Iteration 1076, loss = 2.80385444\n",
      "Iteration 1077, loss = 2.80376641\n",
      "Iteration 1078, loss = 2.80369222\n",
      "Iteration 1079, loss = 2.80359654\n",
      "Iteration 1080, loss = 2.80356912\n",
      "Iteration 1081, loss = 2.80349233\n",
      "Iteration 1082, loss = 2.80333864\n",
      "Iteration 1083, loss = 2.80330670\n",
      "Iteration 1084, loss = 2.80318793\n",
      "Iteration 1085, loss = 2.80316023\n",
      "Iteration 1086, loss = 2.80308793\n",
      "Iteration 1087, loss = 2.80296430\n",
      "Iteration 1088, loss = 2.80290783\n",
      "Iteration 1089, loss = 2.80279689\n",
      "Iteration 1090, loss = 2.80272897\n",
      "Iteration 1091, loss = 2.80266336\n",
      "Iteration 1092, loss = 2.80256635\n",
      "Iteration 1093, loss = 2.80247765\n",
      "Iteration 1094, loss = 2.80237694\n",
      "Iteration 1095, loss = 2.80231409\n",
      "Iteration 1096, loss = 2.80222980\n",
      "Iteration 1097, loss = 2.80215591\n",
      "Iteration 1098, loss = 2.80209473\n",
      "Iteration 1099, loss = 2.80201707\n",
      "Iteration 1100, loss = 2.80196522\n",
      "Iteration 1101, loss = 2.80184912\n",
      "Iteration 1102, loss = 2.80180647\n",
      "Iteration 1103, loss = 2.80172669\n",
      "Iteration 1104, loss = 2.80162016\n",
      "Iteration 1105, loss = 2.80152092\n",
      "Iteration 1106, loss = 2.80145156\n",
      "Iteration 1107, loss = 2.80139182\n",
      "Iteration 1108, loss = 2.80131320\n",
      "Iteration 1109, loss = 2.80124346\n",
      "Iteration 1110, loss = 2.80114892\n",
      "Iteration 1111, loss = 2.80108931\n",
      "Iteration 1112, loss = 2.80099336\n",
      "Iteration 1113, loss = 2.80092964\n",
      "Iteration 1114, loss = 2.80086681\n",
      "Iteration 1115, loss = 2.80074980\n",
      "Iteration 1116, loss = 2.80070417\n",
      "Iteration 1117, loss = 2.80060186\n",
      "Iteration 1118, loss = 2.80054557\n",
      "Iteration 1119, loss = 2.80044832\n",
      "Iteration 1120, loss = 2.80038535\n",
      "Iteration 1121, loss = 2.80028599\n",
      "Iteration 1122, loss = 2.80020701\n",
      "Iteration 1123, loss = 2.80014485\n",
      "Iteration 1124, loss = 2.80005736\n",
      "Iteration 1125, loss = 2.79998786\n",
      "Iteration 1126, loss = 2.79991676\n",
      "Iteration 1127, loss = 2.79982359\n",
      "Iteration 1128, loss = 2.79972823\n",
      "Iteration 1129, loss = 2.79971844\n",
      "Iteration 1130, loss = 2.79959415\n",
      "Iteration 1131, loss = 2.79952215\n",
      "Iteration 1132, loss = 2.79944663\n",
      "Iteration 1133, loss = 2.79935885\n",
      "Iteration 1134, loss = 2.79929406\n",
      "Iteration 1135, loss = 2.79920941\n",
      "Iteration 1136, loss = 2.79915014\n",
      "Iteration 1137, loss = 2.79908013\n",
      "Iteration 1138, loss = 2.79897311\n",
      "Iteration 1139, loss = 2.79890341\n",
      "Iteration 1140, loss = 2.79882394\n",
      "Iteration 1141, loss = 2.79876420\n",
      "Iteration 1142, loss = 2.79866959\n",
      "Iteration 1143, loss = 2.79862832\n",
      "Iteration 1144, loss = 2.79852353\n",
      "Iteration 1145, loss = 2.79843898\n",
      "Iteration 1146, loss = 2.79839910\n",
      "Iteration 1147, loss = 2.79832446\n",
      "Iteration 1148, loss = 2.79822494\n",
      "Iteration 1149, loss = 2.79815516\n",
      "Iteration 1150, loss = 2.79809866\n",
      "Iteration 1151, loss = 2.79802978\n",
      "Iteration 1152, loss = 2.79790349\n",
      "Iteration 1153, loss = 2.79789814\n",
      "Iteration 1154, loss = 2.79777123\n",
      "Iteration 1155, loss = 2.79771011\n",
      "Iteration 1156, loss = 2.79759712\n",
      "Iteration 1157, loss = 2.79756180\n",
      "Iteration 1158, loss = 2.79745741\n",
      "Iteration 1159, loss = 2.79741240\n",
      "Iteration 1160, loss = 2.79730589\n",
      "Iteration 1161, loss = 2.79725683\n",
      "Iteration 1162, loss = 2.79717185\n",
      "Iteration 1163, loss = 2.79708929\n",
      "Iteration 1164, loss = 2.79702223\n",
      "Iteration 1165, loss = 2.79693094\n",
      "Iteration 1166, loss = 2.79687560\n",
      "Iteration 1167, loss = 2.79679968\n",
      "Iteration 1168, loss = 2.79672182\n",
      "Iteration 1169, loss = 2.79663434\n",
      "Iteration 1170, loss = 2.79655708\n",
      "Iteration 1171, loss = 2.79648829\n",
      "Iteration 1172, loss = 2.79643405\n",
      "Iteration 1173, loss = 2.79634986\n",
      "Iteration 1174, loss = 2.79629155\n",
      "Iteration 1175, loss = 2.79619575\n",
      "Iteration 1176, loss = 2.79612715\n",
      "Iteration 1177, loss = 2.79604475\n",
      "Iteration 1178, loss = 2.79596135\n",
      "Iteration 1179, loss = 2.79587594\n",
      "Iteration 1180, loss = 2.79582194\n",
      "Iteration 1181, loss = 2.79578760\n",
      "Iteration 1182, loss = 2.79568230\n",
      "Iteration 1183, loss = 2.79560171\n",
      "Iteration 1184, loss = 2.79555254\n",
      "Iteration 1185, loss = 2.79546315\n",
      "Iteration 1186, loss = 2.79538204\n",
      "Iteration 1187, loss = 2.79530342\n",
      "Iteration 1188, loss = 2.79523090\n",
      "Iteration 1189, loss = 2.79514847\n",
      "Iteration 1190, loss = 2.79511150\n",
      "Iteration 1191, loss = 2.79499358\n",
      "Iteration 1192, loss = 2.79494027\n",
      "Iteration 1193, loss = 2.79485720\n",
      "Iteration 1194, loss = 2.79477230\n",
      "Iteration 1195, loss = 2.79470732\n",
      "Iteration 1196, loss = 2.79465552\n",
      "Iteration 1197, loss = 2.79458607\n",
      "Iteration 1198, loss = 2.79448826\n",
      "Iteration 1199, loss = 2.79440555\n",
      "Iteration 1200, loss = 2.79434459\n",
      "Iteration 1201, loss = 2.79427026\n",
      "Iteration 1202, loss = 2.79421522\n",
      "Iteration 1203, loss = 2.79411078\n",
      "Iteration 1204, loss = 2.79404063\n",
      "Iteration 1205, loss = 2.79397048\n",
      "Iteration 1206, loss = 2.79391403\n",
      "Iteration 1207, loss = 2.79386287\n",
      "Iteration 1208, loss = 2.79379512\n",
      "Iteration 1209, loss = 2.79368947\n",
      "Iteration 1210, loss = 2.79363727\n",
      "Iteration 1211, loss = 2.79352629\n",
      "Iteration 1212, loss = 2.79347076\n",
      "Iteration 1213, loss = 2.79340119\n",
      "Iteration 1214, loss = 2.79332618\n",
      "Iteration 1215, loss = 2.79325466\n",
      "Iteration 1216, loss = 2.79319791\n",
      "Iteration 1217, loss = 2.79310370\n",
      "Iteration 1218, loss = 2.79302732\n",
      "Iteration 1219, loss = 2.79295039\n",
      "Iteration 1220, loss = 2.79288203\n",
      "Iteration 1221, loss = 2.79285126\n",
      "Iteration 1222, loss = 2.79274558\n",
      "Iteration 1223, loss = 2.79268573\n",
      "Iteration 1224, loss = 2.79259876\n",
      "Iteration 1225, loss = 2.79251792\n",
      "Iteration 1226, loss = 2.79247247\n",
      "Iteration 1227, loss = 2.79237617\n",
      "Iteration 1228, loss = 2.79231549\n",
      "Iteration 1229, loss = 2.79223740\n",
      "Iteration 1230, loss = 2.79216618\n",
      "Iteration 1231, loss = 2.79212893\n",
      "Iteration 1232, loss = 2.79202923\n",
      "Iteration 1233, loss = 2.79195173\n",
      "Iteration 1234, loss = 2.79186771\n",
      "Iteration 1235, loss = 2.79181470\n",
      "Iteration 1236, loss = 2.79173385\n",
      "Iteration 1237, loss = 2.79167115\n",
      "Iteration 1238, loss = 2.79160572\n",
      "Iteration 1239, loss = 2.79159822\n",
      "Iteration 1240, loss = 2.79146418\n",
      "Iteration 1241, loss = 2.79138697\n",
      "Iteration 1242, loss = 2.79129215\n",
      "Iteration 1243, loss = 2.79122607\n",
      "Iteration 1244, loss = 2.79114547\n",
      "Iteration 1245, loss = 2.79107248\n",
      "Iteration 1246, loss = 2.79100761\n",
      "Iteration 1247, loss = 2.79095499\n",
      "Iteration 1248, loss = 2.79089110\n",
      "Iteration 1249, loss = 2.79082433\n",
      "Iteration 1250, loss = 2.79073144\n",
      "Iteration 1251, loss = 2.79069146\n",
      "Iteration 1252, loss = 2.79057329\n",
      "Iteration 1253, loss = 2.79051470\n",
      "Iteration 1254, loss = 2.79045131\n",
      "Iteration 1255, loss = 2.79038485\n",
      "Iteration 1256, loss = 2.79033367\n",
      "Iteration 1257, loss = 2.79027679\n",
      "Iteration 1258, loss = 2.79014262\n",
      "Iteration 1259, loss = 2.79012667\n",
      "Iteration 1260, loss = 2.79002025\n",
      "Iteration 1261, loss = 2.78992486\n",
      "Iteration 1262, loss = 2.78988826\n",
      "Iteration 1263, loss = 2.78982648\n",
      "Iteration 1264, loss = 2.78974174\n",
      "Iteration 1265, loss = 2.78967523\n",
      "Iteration 1266, loss = 2.78960633\n",
      "Iteration 1267, loss = 2.78953204\n",
      "Iteration 1268, loss = 2.78946586\n",
      "Iteration 1269, loss = 2.78939158\n",
      "Iteration 1270, loss = 2.78935497\n",
      "Iteration 1271, loss = 2.78924478\n",
      "Iteration 1272, loss = 2.78918689\n",
      "Iteration 1273, loss = 2.78912624\n",
      "Iteration 1274, loss = 2.78906073\n",
      "Iteration 1275, loss = 2.78899948\n",
      "Iteration 1276, loss = 2.78892211\n",
      "Iteration 1277, loss = 2.78885178\n",
      "Iteration 1278, loss = 2.78880465\n",
      "Iteration 1279, loss = 2.78871933\n",
      "Iteration 1280, loss = 2.78864690\n",
      "Iteration 1281, loss = 2.78856870\n",
      "Iteration 1282, loss = 2.78852132\n",
      "Iteration 1283, loss = 2.78840399\n",
      "Iteration 1284, loss = 2.78839210\n",
      "Iteration 1285, loss = 2.78828882\n",
      "Iteration 1286, loss = 2.78821394\n",
      "Iteration 1287, loss = 2.78818479\n",
      "Iteration 1288, loss = 2.78808383\n",
      "Iteration 1289, loss = 2.78800120\n",
      "Iteration 1290, loss = 2.78792894\n",
      "Iteration 1291, loss = 2.78787399\n",
      "Iteration 1292, loss = 2.78782783\n",
      "Iteration 1293, loss = 2.78774175\n",
      "Iteration 1294, loss = 2.78765531\n",
      "Iteration 1295, loss = 2.78761544\n",
      "Iteration 1296, loss = 2.78754083\n",
      "Iteration 1297, loss = 2.78750213\n",
      "Iteration 1298, loss = 2.78740607\n",
      "Iteration 1299, loss = 2.78733358\n",
      "Iteration 1300, loss = 2.78726216\n",
      "Iteration 1301, loss = 2.78717788\n",
      "Iteration 1302, loss = 2.78711695\n",
      "Iteration 1303, loss = 2.78705099\n",
      "Iteration 1304, loss = 2.78698490\n",
      "Iteration 1305, loss = 2.78694048\n",
      "Iteration 1306, loss = 2.78684201\n",
      "Iteration 1307, loss = 2.78681313\n",
      "Iteration 1308, loss = 2.78673456\n",
      "Iteration 1309, loss = 2.78661913\n",
      "Iteration 1310, loss = 2.78658711\n",
      "Iteration 1311, loss = 2.78653919\n",
      "Iteration 1312, loss = 2.78642467\n",
      "Iteration 1313, loss = 2.78637235\n",
      "Iteration 1314, loss = 2.78630970\n",
      "Iteration 1315, loss = 2.78624149\n",
      "Iteration 1316, loss = 2.78616434\n",
      "Iteration 1317, loss = 2.78610447\n",
      "Iteration 1318, loss = 2.78605043\n",
      "Iteration 1319, loss = 2.78597006\n",
      "Iteration 1320, loss = 2.78588257\n",
      "Iteration 1321, loss = 2.78580459\n",
      "Iteration 1322, loss = 2.78574654\n",
      "Iteration 1323, loss = 2.78569798\n",
      "Iteration 1324, loss = 2.78560875\n",
      "Iteration 1325, loss = 2.78556887\n",
      "Iteration 1326, loss = 2.78551287\n",
      "Iteration 1327, loss = 2.78542262\n",
      "Iteration 1328, loss = 2.78535526\n",
      "Iteration 1329, loss = 2.78528454\n",
      "Iteration 1330, loss = 2.78527105\n",
      "Iteration 1331, loss = 2.78515986\n",
      "Iteration 1332, loss = 2.78511184\n",
      "Iteration 1333, loss = 2.78500433\n",
      "Iteration 1334, loss = 2.78492486\n",
      "Iteration 1335, loss = 2.78491812\n",
      "Iteration 1336, loss = 2.78481697\n",
      "Iteration 1337, loss = 2.78476658\n",
      "Iteration 1338, loss = 2.78469965\n",
      "Iteration 1339, loss = 2.78460436\n",
      "Iteration 1340, loss = 2.78454571\n",
      "Iteration 1341, loss = 2.78446661\n",
      "Iteration 1342, loss = 2.78441571\n",
      "Iteration 1343, loss = 2.78435046\n",
      "Iteration 1344, loss = 2.78430021\n",
      "Iteration 1345, loss = 2.78419068\n",
      "Iteration 1346, loss = 2.78415116\n",
      "Iteration 1347, loss = 2.78410483\n",
      "Iteration 1348, loss = 2.78401471\n",
      "Iteration 1349, loss = 2.78393519\n",
      "Iteration 1350, loss = 2.78386563\n",
      "Iteration 1351, loss = 2.78379280\n",
      "Iteration 1352, loss = 2.78377017\n",
      "Iteration 1353, loss = 2.78368359\n",
      "Iteration 1354, loss = 2.78359557\n",
      "Iteration 1355, loss = 2.78358536\n",
      "Iteration 1356, loss = 2.78348371\n",
      "Iteration 1357, loss = 2.78344095\n",
      "Iteration 1358, loss = 2.78336196\n",
      "Iteration 1359, loss = 2.78328543\n",
      "Iteration 1360, loss = 2.78322690\n",
      "Iteration 1361, loss = 2.78316031\n",
      "Iteration 1362, loss = 2.78307933\n",
      "Iteration 1363, loss = 2.78302061\n",
      "Iteration 1364, loss = 2.78294954\n",
      "Iteration 1365, loss = 2.78289575\n",
      "Iteration 1366, loss = 2.78289009\n",
      "Iteration 1367, loss = 2.78278403\n",
      "Iteration 1368, loss = 2.78269115\n",
      "Iteration 1369, loss = 2.78262258\n",
      "Iteration 1370, loss = 2.78258543\n",
      "Iteration 1371, loss = 2.78251699\n",
      "Iteration 1372, loss = 2.78242871\n",
      "Iteration 1373, loss = 2.78237656\n",
      "Iteration 1374, loss = 2.78229807\n",
      "Iteration 1375, loss = 2.78221205\n",
      "Iteration 1376, loss = 2.78219169\n",
      "Iteration 1377, loss = 2.78215469\n",
      "Iteration 1378, loss = 2.78204247\n",
      "Iteration 1379, loss = 2.78196837\n",
      "Iteration 1380, loss = 2.78192311\n",
      "Iteration 1381, loss = 2.78187858\n",
      "Iteration 1382, loss = 2.78177656\n",
      "Iteration 1383, loss = 2.78171376\n",
      "Iteration 1384, loss = 2.78163611\n",
      "Iteration 1385, loss = 2.78158890\n",
      "Iteration 1386, loss = 2.78155186\n",
      "Iteration 1387, loss = 2.78149643\n",
      "Iteration 1388, loss = 2.78141588\n",
      "Iteration 1389, loss = 2.78134449\n",
      "Iteration 1390, loss = 2.78127795\n",
      "Iteration 1391, loss = 2.78122176\n",
      "Iteration 1392, loss = 2.78112555\n",
      "Iteration 1393, loss = 2.78107536\n",
      "Iteration 1394, loss = 2.78101997\n",
      "Iteration 1395, loss = 2.78091586\n",
      "Iteration 1396, loss = 2.78086629\n",
      "Iteration 1397, loss = 2.78078307\n",
      "Iteration 1398, loss = 2.78073062\n",
      "Iteration 1399, loss = 2.78067296\n",
      "Iteration 1400, loss = 2.78061445\n",
      "Iteration 1401, loss = 2.78054955\n",
      "Iteration 1402, loss = 2.78045935\n",
      "Iteration 1403, loss = 2.78040691\n",
      "Iteration 1404, loss = 2.78034780\n",
      "Iteration 1405, loss = 2.78028882\n",
      "Iteration 1406, loss = 2.78021475\n",
      "Iteration 1407, loss = 2.78014856\n",
      "Iteration 1408, loss = 2.78008182\n",
      "Iteration 1409, loss = 2.78002157\n",
      "Iteration 1410, loss = 2.77995432\n",
      "Iteration 1411, loss = 2.77988634\n",
      "Iteration 1412, loss = 2.77984340\n",
      "Iteration 1413, loss = 2.77979424\n",
      "Iteration 1414, loss = 2.77971509\n",
      "Iteration 1415, loss = 2.77965468\n",
      "Iteration 1416, loss = 2.77960446\n",
      "Iteration 1417, loss = 2.77949583\n",
      "Iteration 1418, loss = 2.77945441\n",
      "Iteration 1419, loss = 2.77940027\n",
      "Iteration 1420, loss = 2.77932133\n",
      "Iteration 1421, loss = 2.77925051\n",
      "Iteration 1422, loss = 2.77919792\n",
      "Iteration 1423, loss = 2.77912365\n",
      "Iteration 1424, loss = 2.77906463\n",
      "Iteration 1425, loss = 2.77901834\n",
      "Iteration 1426, loss = 2.77890661\n",
      "Iteration 1427, loss = 2.77885115\n",
      "Iteration 1428, loss = 2.77881217\n",
      "Iteration 1429, loss = 2.77875175\n",
      "Iteration 1430, loss = 2.77870279\n",
      "Iteration 1431, loss = 2.77860661\n",
      "Iteration 1432, loss = 2.77856463\n",
      "Iteration 1433, loss = 2.77848485\n",
      "Iteration 1434, loss = 2.77839569\n",
      "Iteration 1435, loss = 2.77837884\n",
      "Iteration 1436, loss = 2.77831987\n",
      "Iteration 1437, loss = 2.77823263\n",
      "Iteration 1438, loss = 2.77818928\n",
      "Iteration 1439, loss = 2.77810339\n",
      "Iteration 1440, loss = 2.77804464\n",
      "Iteration 1441, loss = 2.77800849\n",
      "Iteration 1442, loss = 2.77789065\n",
      "Iteration 1443, loss = 2.77783908\n",
      "Iteration 1444, loss = 2.77777487\n",
      "Iteration 1445, loss = 2.77775885\n",
      "Iteration 1446, loss = 2.77766999\n",
      "Iteration 1447, loss = 2.77759614\n",
      "Iteration 1448, loss = 2.77756073\n",
      "Iteration 1449, loss = 2.77746624\n",
      "Iteration 1450, loss = 2.77740341\n",
      "Iteration 1451, loss = 2.77735812\n",
      "Iteration 1452, loss = 2.77732044\n",
      "Iteration 1453, loss = 2.77725145\n",
      "Iteration 1454, loss = 2.77720303\n",
      "Iteration 1455, loss = 2.77710440\n",
      "Iteration 1456, loss = 2.77704616\n",
      "Iteration 1457, loss = 2.77698765\n",
      "Iteration 1458, loss = 2.77689481\n",
      "Iteration 1459, loss = 2.77686138\n",
      "Iteration 1460, loss = 2.77681381\n",
      "Iteration 1461, loss = 2.77673930\n",
      "Iteration 1462, loss = 2.77667631\n",
      "Iteration 1463, loss = 2.77659528\n",
      "Iteration 1464, loss = 2.77653613\n",
      "Iteration 1465, loss = 2.77646283\n",
      "Iteration 1466, loss = 2.77640975\n",
      "Iteration 1467, loss = 2.77634155\n",
      "Iteration 1468, loss = 2.77627801\n",
      "Iteration 1469, loss = 2.77624628\n",
      "Iteration 1470, loss = 2.77615158\n",
      "Iteration 1471, loss = 2.77613783\n",
      "Iteration 1472, loss = 2.77602406\n",
      "Iteration 1473, loss = 2.77600519\n",
      "Iteration 1474, loss = 2.77591079\n",
      "Iteration 1475, loss = 2.77584637\n",
      "Iteration 1476, loss = 2.77580205\n",
      "Iteration 1477, loss = 2.77573853\n",
      "Iteration 1478, loss = 2.77564339\n",
      "Iteration 1479, loss = 2.77559857\n",
      "Iteration 1480, loss = 2.77555276\n",
      "Iteration 1481, loss = 2.77552058\n",
      "Iteration 1482, loss = 2.77544046\n",
      "Iteration 1483, loss = 2.77536040\n",
      "Iteration 1484, loss = 2.77527968\n",
      "Iteration 1485, loss = 2.77522399\n",
      "Iteration 1486, loss = 2.77516883\n",
      "Iteration 1487, loss = 2.77510668\n",
      "Iteration 1488, loss = 2.77503632\n",
      "Iteration 1489, loss = 2.77499723\n",
      "Iteration 1490, loss = 2.77493487\n",
      "Iteration 1491, loss = 2.77486677\n",
      "Iteration 1492, loss = 2.77481368\n",
      "Iteration 1493, loss = 2.77476737\n",
      "Iteration 1494, loss = 2.77467545\n",
      "Iteration 1495, loss = 2.77462155\n",
      "Iteration 1496, loss = 2.77458147\n",
      "Iteration 1497, loss = 2.77446898\n",
      "Iteration 1498, loss = 2.77443242\n",
      "Iteration 1499, loss = 2.77438393\n",
      "Iteration 1500, loss = 2.77433561\n",
      "Iteration 1501, loss = 2.77425879\n",
      "Iteration 1502, loss = 2.77417387\n",
      "Iteration 1503, loss = 2.77415492\n",
      "Iteration 1504, loss = 2.77407465\n",
      "Iteration 1505, loss = 2.77403641\n",
      "Iteration 1506, loss = 2.77398547\n",
      "Iteration 1507, loss = 2.77390709\n",
      "Iteration 1508, loss = 2.77383720\n",
      "Iteration 1509, loss = 2.77375996\n",
      "Iteration 1510, loss = 2.77370637\n",
      "Iteration 1511, loss = 2.77366073\n",
      "Iteration 1512, loss = 2.77360911\n",
      "Iteration 1513, loss = 2.77352155\n",
      "Iteration 1514, loss = 2.77346368\n",
      "Iteration 1515, loss = 2.77339283\n",
      "Iteration 1516, loss = 2.77333297\n",
      "Iteration 1517, loss = 2.77331327\n",
      "Iteration 1518, loss = 2.77319378\n",
      "Iteration 1519, loss = 2.77314298\n",
      "Iteration 1520, loss = 2.77308002\n",
      "Iteration 1521, loss = 2.77301932\n",
      "Iteration 1522, loss = 2.77299949\n",
      "Iteration 1523, loss = 2.77291408\n",
      "Iteration 1524, loss = 2.77288602\n",
      "Iteration 1525, loss = 2.77281183\n",
      "Iteration 1526, loss = 2.77274198\n",
      "Iteration 1527, loss = 2.77266478\n",
      "Iteration 1528, loss = 2.77262487\n",
      "Iteration 1529, loss = 2.77256083\n",
      "Iteration 1530, loss = 2.77250123\n",
      "Iteration 1531, loss = 2.77243830\n",
      "Iteration 1532, loss = 2.77240567\n",
      "Iteration 1533, loss = 2.77231010\n",
      "Iteration 1534, loss = 2.77225531\n",
      "Iteration 1535, loss = 2.77222044\n",
      "Iteration 1536, loss = 2.77210939\n",
      "Iteration 1537, loss = 2.77206541\n",
      "Iteration 1538, loss = 2.77200783\n",
      "Iteration 1539, loss = 2.77197939\n",
      "Iteration 1540, loss = 2.77190414\n",
      "Iteration 1541, loss = 2.77184639\n",
      "Iteration 1542, loss = 2.77179960\n",
      "Iteration 1543, loss = 2.77169652\n",
      "Iteration 1544, loss = 2.77167924\n",
      "Iteration 1545, loss = 2.77160337\n",
      "Iteration 1546, loss = 2.77151727\n",
      "Iteration 1547, loss = 2.77149422\n",
      "Iteration 1548, loss = 2.77140070\n",
      "Iteration 1549, loss = 2.77136330\n",
      "Iteration 1550, loss = 2.77129302\n",
      "Iteration 1551, loss = 2.77125937\n",
      "Iteration 1552, loss = 2.77117877\n",
      "Iteration 1553, loss = 2.77113043\n",
      "Iteration 1554, loss = 2.77103873\n",
      "Iteration 1555, loss = 2.77098035\n",
      "Iteration 1556, loss = 2.77093251\n",
      "Iteration 1557, loss = 2.77085540\n",
      "Iteration 1558, loss = 2.77083131\n",
      "Iteration 1559, loss = 2.77075172\n",
      "Iteration 1560, loss = 2.77070952\n",
      "Iteration 1561, loss = 2.77062981\n",
      "Iteration 1562, loss = 2.77057224\n",
      "Iteration 1563, loss = 2.77052552\n",
      "Iteration 1564, loss = 2.77049711\n",
      "Iteration 1565, loss = 2.77041359\n",
      "Iteration 1566, loss = 2.77032792\n",
      "Iteration 1567, loss = 2.77027629\n",
      "Iteration 1568, loss = 2.77026040\n",
      "Iteration 1569, loss = 2.77012855\n",
      "Iteration 1570, loss = 2.77016529\n",
      "Iteration 1571, loss = 2.77004529\n",
      "Iteration 1572, loss = 2.76999289\n",
      "Iteration 1573, loss = 2.77000987\n",
      "Iteration 1574, loss = 2.76985728\n",
      "Iteration 1575, loss = 2.76981524\n",
      "Iteration 1576, loss = 2.76976782\n",
      "Iteration 1577, loss = 2.76970019\n",
      "Iteration 1578, loss = 2.76965160\n",
      "Iteration 1579, loss = 2.76956686\n",
      "Iteration 1580, loss = 2.76951884\n",
      "Iteration 1581, loss = 2.76946324\n",
      "Iteration 1582, loss = 2.76942250\n",
      "Iteration 1583, loss = 2.76935712\n",
      "Iteration 1584, loss = 2.76929453\n",
      "Iteration 1585, loss = 2.76922462\n",
      "Iteration 1586, loss = 2.76917933\n",
      "Iteration 1587, loss = 2.76910219\n",
      "Iteration 1588, loss = 2.76904061\n",
      "Iteration 1589, loss = 2.76898382\n",
      "Iteration 1590, loss = 2.76893425\n",
      "Iteration 1591, loss = 2.76888475\n",
      "Iteration 1592, loss = 2.76882363\n",
      "Iteration 1593, loss = 2.76872548\n",
      "Iteration 1594, loss = 2.76869575\n",
      "Iteration 1595, loss = 2.76863773\n",
      "Iteration 1596, loss = 2.76856443\n",
      "Iteration 1597, loss = 2.76852300\n",
      "Iteration 1598, loss = 2.76844570\n",
      "Iteration 1599, loss = 2.76842926\n",
      "Iteration 1600, loss = 2.76835114\n",
      "Iteration 1601, loss = 2.76833018\n",
      "Iteration 1602, loss = 2.76823128\n",
      "Iteration 1603, loss = 2.76816601\n",
      "Iteration 1604, loss = 2.76813051\n",
      "Iteration 1605, loss = 2.76804295\n",
      "Iteration 1606, loss = 2.76802453\n",
      "Iteration 1607, loss = 2.76795732\n",
      "Iteration 1608, loss = 2.76789686\n",
      "Iteration 1609, loss = 2.76781813\n",
      "Iteration 1610, loss = 2.76773669\n",
      "Iteration 1611, loss = 2.76766751\n",
      "Iteration 1612, loss = 2.76767213\n",
      "Iteration 1613, loss = 2.76757176\n",
      "Iteration 1614, loss = 2.76754132\n",
      "Iteration 1615, loss = 2.76745768\n",
      "Iteration 1616, loss = 2.76738372\n",
      "Iteration 1617, loss = 2.76735556\n",
      "Iteration 1618, loss = 2.76725706\n",
      "Iteration 1619, loss = 2.76724114\n",
      "Iteration 1620, loss = 2.76717239\n",
      "Iteration 1621, loss = 2.76714034\n",
      "Iteration 1622, loss = 2.76705515\n",
      "Iteration 1623, loss = 2.76698026\n",
      "Iteration 1624, loss = 2.76695541\n",
      "Iteration 1625, loss = 2.76688697\n",
      "Iteration 1626, loss = 2.76681566\n",
      "Iteration 1627, loss = 2.76677866\n",
      "Iteration 1628, loss = 2.76673981\n",
      "Iteration 1629, loss = 2.76663504\n",
      "Iteration 1630, loss = 2.76659813\n",
      "Iteration 1631, loss = 2.76656447\n",
      "Iteration 1632, loss = 2.76648525\n",
      "Iteration 1633, loss = 2.76642993\n",
      "Iteration 1634, loss = 2.76632931\n",
      "Iteration 1635, loss = 2.76632803\n",
      "Iteration 1636, loss = 2.76626387\n",
      "Iteration 1637, loss = 2.76623139\n",
      "Iteration 1638, loss = 2.76618911\n",
      "Iteration 1639, loss = 2.76608518\n",
      "Iteration 1640, loss = 2.76602967\n",
      "Iteration 1641, loss = 2.76597476\n",
      "Iteration 1642, loss = 2.76590346\n",
      "Iteration 1643, loss = 2.76584954\n",
      "Iteration 1644, loss = 2.76584156\n",
      "Iteration 1645, loss = 2.76572178\n",
      "Iteration 1646, loss = 2.76565079\n",
      "Iteration 1647, loss = 2.76564289\n",
      "Iteration 1648, loss = 2.76556539\n",
      "Iteration 1649, loss = 2.76551233\n",
      "Iteration 1650, loss = 2.76545473\n",
      "Iteration 1651, loss = 2.76541142\n",
      "Iteration 1652, loss = 2.76533563\n",
      "Iteration 1653, loss = 2.76527750\n",
      "Iteration 1654, loss = 2.76521444\n",
      "Iteration 1655, loss = 2.76514419\n",
      "Iteration 1656, loss = 2.76508301\n",
      "Iteration 1657, loss = 2.76507175\n",
      "Iteration 1658, loss = 2.76500653\n",
      "Iteration 1659, loss = 2.76496456\n",
      "Iteration 1660, loss = 2.76488396\n",
      "Iteration 1661, loss = 2.76480546\n",
      "Iteration 1662, loss = 2.76473459\n",
      "Iteration 1663, loss = 2.76470169\n",
      "Iteration 1664, loss = 2.76468350\n",
      "Iteration 1665, loss = 2.76459891\n",
      "Iteration 1666, loss = 2.76450661\n",
      "Iteration 1667, loss = 2.76448867\n",
      "Iteration 1668, loss = 2.76439735\n",
      "Iteration 1669, loss = 2.76435969\n",
      "Iteration 1670, loss = 2.76429203\n",
      "Iteration 1671, loss = 2.76424208\n",
      "Iteration 1672, loss = 2.76415961\n",
      "Iteration 1673, loss = 2.76414543\n",
      "Iteration 1674, loss = 2.76405652\n",
      "Iteration 1675, loss = 2.76400264\n",
      "Iteration 1676, loss = 2.76395765\n",
      "Iteration 1677, loss = 2.76391987\n",
      "Iteration 1678, loss = 2.76383787\n",
      "Iteration 1679, loss = 2.76381808\n",
      "Iteration 1680, loss = 2.76373532\n",
      "Iteration 1681, loss = 2.76367568\n",
      "Iteration 1682, loss = 2.76362942\n",
      "Iteration 1683, loss = 2.76357154\n",
      "Iteration 1684, loss = 2.76354505\n",
      "Iteration 1685, loss = 2.76346063\n",
      "Iteration 1686, loss = 2.76338991\n",
      "Iteration 1687, loss = 2.76334420\n",
      "Iteration 1688, loss = 2.76329575\n",
      "Iteration 1689, loss = 2.76325471\n",
      "Iteration 1690, loss = 2.76316491\n",
      "Iteration 1691, loss = 2.76309765\n",
      "Iteration 1692, loss = 2.76309377\n",
      "Iteration 1693, loss = 2.76302133\n",
      "Iteration 1694, loss = 2.76295741\n",
      "Iteration 1695, loss = 2.76288380\n",
      "Iteration 1696, loss = 2.76283634\n",
      "Iteration 1697, loss = 2.76278914\n",
      "Iteration 1698, loss = 2.76270806\n",
      "Iteration 1699, loss = 2.76267272\n",
      "Iteration 1700, loss = 2.76263104\n",
      "Iteration 1701, loss = 2.76254537\n",
      "Iteration 1702, loss = 2.76250114\n",
      "Iteration 1703, loss = 2.76243532\n",
      "Iteration 1704, loss = 2.76238970\n",
      "Iteration 1705, loss = 2.76233215\n",
      "Iteration 1706, loss = 2.76224607\n",
      "Iteration 1707, loss = 2.76221546\n",
      "Iteration 1708, loss = 2.76216768\n",
      "Iteration 1709, loss = 2.76211800\n",
      "Iteration 1710, loss = 2.76201398\n",
      "Iteration 1711, loss = 2.76201961\n",
      "Iteration 1712, loss = 2.76193901\n",
      "Iteration 1713, loss = 2.76187562\n",
      "Iteration 1714, loss = 2.76182196\n",
      "Iteration 1715, loss = 2.76177102\n",
      "Iteration 1716, loss = 2.76179391\n",
      "Iteration 1717, loss = 2.76168465\n",
      "Iteration 1718, loss = 2.76159676\n",
      "Iteration 1719, loss = 2.76153841\n",
      "Iteration 1720, loss = 2.76150369\n",
      "Iteration 1721, loss = 2.76142694\n",
      "Iteration 1722, loss = 2.76135189\n",
      "Iteration 1723, loss = 2.76132491\n",
      "Iteration 1724, loss = 2.76124848\n",
      "Iteration 1725, loss = 2.76121484\n",
      "Iteration 1726, loss = 2.76116166\n",
      "Iteration 1727, loss = 2.76112466\n",
      "Iteration 1728, loss = 2.76104242\n",
      "Iteration 1729, loss = 2.76096643\n",
      "Iteration 1730, loss = 2.76093689\n",
      "Iteration 1731, loss = 2.76088339\n",
      "Iteration 1732, loss = 2.76082816\n",
      "Iteration 1733, loss = 2.76078402\n",
      "Iteration 1734, loss = 2.76071398\n",
      "Iteration 1735, loss = 2.76066376\n",
      "Iteration 1736, loss = 2.76062279\n",
      "Iteration 1737, loss = 2.76055620\n",
      "Iteration 1738, loss = 2.76047297\n",
      "Iteration 1739, loss = 2.76042836\n",
      "Iteration 1740, loss = 2.76038476\n",
      "Iteration 1741, loss = 2.76030400\n",
      "Iteration 1742, loss = 2.76026941\n",
      "Iteration 1743, loss = 2.76023310\n",
      "Iteration 1744, loss = 2.76015364\n",
      "Iteration 1745, loss = 2.76012461\n",
      "Iteration 1746, loss = 2.76002867\n",
      "Iteration 1747, loss = 2.75998570\n",
      "Iteration 1748, loss = 2.75992246\n",
      "Iteration 1749, loss = 2.75988539\n",
      "Iteration 1750, loss = 2.75983620\n",
      "Iteration 1751, loss = 2.75976506\n",
      "Iteration 1752, loss = 2.75973301\n",
      "Iteration 1753, loss = 2.75966573\n",
      "Iteration 1754, loss = 2.75961232\n",
      "Iteration 1755, loss = 2.75955311\n",
      "Iteration 1756, loss = 2.75953229\n",
      "Iteration 1757, loss = 2.75944677\n",
      "Iteration 1758, loss = 2.75937417\n",
      "Iteration 1759, loss = 2.75935026\n",
      "Iteration 1760, loss = 2.75929843\n",
      "Iteration 1761, loss = 2.75922284\n",
      "Iteration 1762, loss = 2.75918095\n",
      "Iteration 1763, loss = 2.75913372\n",
      "Iteration 1764, loss = 2.75907146\n",
      "Iteration 1765, loss = 2.75901500\n",
      "Iteration 1766, loss = 2.75895380\n",
      "Iteration 1767, loss = 2.75888782\n",
      "Iteration 1768, loss = 2.75883610\n",
      "Iteration 1769, loss = 2.75879403\n",
      "Iteration 1770, loss = 2.75874147\n",
      "Iteration 1771, loss = 2.75867788\n",
      "Iteration 1772, loss = 2.75865501\n",
      "Iteration 1773, loss = 2.75857619\n",
      "Iteration 1774, loss = 2.75850975\n",
      "Iteration 1775, loss = 2.75847133\n",
      "Iteration 1776, loss = 2.75839662\n",
      "Iteration 1777, loss = 2.75840693\n",
      "Iteration 1778, loss = 2.75827762\n",
      "Iteration 1779, loss = 2.75828102\n",
      "Iteration 1780, loss = 2.75816316\n",
      "Iteration 1781, loss = 2.75811104\n",
      "Iteration 1782, loss = 2.75809368\n",
      "Iteration 1783, loss = 2.75808038\n",
      "Iteration 1784, loss = 2.75796959\n",
      "Iteration 1785, loss = 2.75792677\n",
      "Iteration 1786, loss = 2.75786130\n",
      "Iteration 1787, loss = 2.75784659\n",
      "Iteration 1788, loss = 2.75775826\n",
      "Iteration 1789, loss = 2.75773358\n",
      "Iteration 1790, loss = 2.75768048\n",
      "Iteration 1791, loss = 2.75760660\n",
      "Iteration 1792, loss = 2.75751964\n",
      "Iteration 1793, loss = 2.75749654\n",
      "Iteration 1794, loss = 2.75742253\n",
      "Iteration 1795, loss = 2.75739544\n",
      "Iteration 1796, loss = 2.75735241\n",
      "Iteration 1797, loss = 2.75727605\n",
      "Iteration 1798, loss = 2.75723616\n",
      "Iteration 1799, loss = 2.75716404\n",
      "Iteration 1800, loss = 2.75712428\n",
      "Iteration 1801, loss = 2.75704636\n",
      "Iteration 1802, loss = 2.75698954\n",
      "Iteration 1803, loss = 2.75694337\n",
      "Iteration 1804, loss = 2.75691123\n",
      "Iteration 1805, loss = 2.75685890\n",
      "Iteration 1806, loss = 2.75678948\n",
      "Iteration 1807, loss = 2.75674334\n",
      "Iteration 1808, loss = 2.75670192\n",
      "Iteration 1809, loss = 2.75665755\n",
      "Iteration 1810, loss = 2.75658215\n",
      "Iteration 1811, loss = 2.75650705\n",
      "Iteration 1812, loss = 2.75646055\n",
      "Iteration 1813, loss = 2.75639922\n",
      "Iteration 1814, loss = 2.75634388\n",
      "Iteration 1815, loss = 2.75631319\n",
      "Iteration 1816, loss = 2.75622325\n",
      "Iteration 1817, loss = 2.75617831\n",
      "Iteration 1818, loss = 2.75614272\n",
      "Iteration 1819, loss = 2.75609996\n",
      "Iteration 1820, loss = 2.75602988\n",
      "Iteration 1821, loss = 2.75598825\n",
      "Iteration 1822, loss = 2.75597602\n",
      "Iteration 1823, loss = 2.75587112\n",
      "Iteration 1824, loss = 2.75579774\n",
      "Iteration 1825, loss = 2.75574104\n",
      "Iteration 1826, loss = 2.75573272\n",
      "Iteration 1827, loss = 2.75566023\n",
      "Iteration 1828, loss = 2.75560423\n",
      "Iteration 1829, loss = 2.75554931\n",
      "Iteration 1830, loss = 2.75549880\n",
      "Iteration 1831, loss = 2.75545355\n",
      "Iteration 1832, loss = 2.75540504\n",
      "Iteration 1833, loss = 2.75534611\n",
      "Iteration 1834, loss = 2.75535269\n",
      "Iteration 1835, loss = 2.75522865\n",
      "Iteration 1836, loss = 2.75515466\n",
      "Iteration 1837, loss = 2.75512903\n",
      "Iteration 1838, loss = 2.75506407\n",
      "Iteration 1839, loss = 2.75500160\n",
      "Iteration 1840, loss = 2.75499417\n",
      "Iteration 1841, loss = 2.75494811\n",
      "Iteration 1842, loss = 2.75489745\n",
      "Iteration 1843, loss = 2.75480638\n",
      "Iteration 1844, loss = 2.75475266\n",
      "Iteration 1845, loss = 2.75471681\n",
      "Iteration 1846, loss = 2.75465462\n",
      "Iteration 1847, loss = 2.75465098\n",
      "Iteration 1848, loss = 2.75456030\n",
      "Iteration 1849, loss = 2.75453907\n",
      "Iteration 1850, loss = 2.75444941\n",
      "Iteration 1851, loss = 2.75442754\n",
      "Iteration 1852, loss = 2.75438995\n",
      "Iteration 1853, loss = 2.75433893\n",
      "Iteration 1854, loss = 2.75424027\n",
      "Iteration 1855, loss = 2.75415919\n",
      "Iteration 1856, loss = 2.75410565\n",
      "Iteration 1857, loss = 2.75405648\n",
      "Iteration 1858, loss = 2.75403265\n",
      "Iteration 1859, loss = 2.75398505\n",
      "Iteration 1860, loss = 2.75391091\n",
      "Iteration 1861, loss = 2.75386022\n",
      "Iteration 1862, loss = 2.75384933\n",
      "Iteration 1863, loss = 2.75375865\n",
      "Iteration 1864, loss = 2.75371678\n",
      "Iteration 1865, loss = 2.75369069\n",
      "Iteration 1866, loss = 2.75367380\n",
      "Iteration 1867, loss = 2.75355500\n",
      "Iteration 1868, loss = 2.75348701\n",
      "Iteration 1869, loss = 2.75344396\n",
      "Iteration 1870, loss = 2.75337066\n",
      "Iteration 1871, loss = 2.75333056\n",
      "Iteration 1872, loss = 2.75328639\n",
      "Iteration 1873, loss = 2.75324382\n",
      "Iteration 1874, loss = 2.75321040\n",
      "Iteration 1875, loss = 2.75311382\n",
      "Iteration 1876, loss = 2.75305731\n",
      "Iteration 1877, loss = 2.75301362\n",
      "Iteration 1878, loss = 2.75296178\n",
      "Iteration 1879, loss = 2.75290709\n",
      "Iteration 1880, loss = 2.75287496\n",
      "Iteration 1881, loss = 2.75280660\n",
      "Iteration 1882, loss = 2.75276093\n",
      "Iteration 1883, loss = 2.75271220\n",
      "Iteration 1884, loss = 2.75266034\n",
      "Iteration 1885, loss = 2.75261484\n",
      "Iteration 1886, loss = 2.75256653\n",
      "Iteration 1887, loss = 2.75253053\n",
      "Iteration 1888, loss = 2.75248435\n",
      "Iteration 1889, loss = 2.75238810\n",
      "Iteration 1890, loss = 2.75236242\n",
      "Iteration 1891, loss = 2.75230435\n",
      "Iteration 1892, loss = 2.75225448\n",
      "Iteration 1893, loss = 2.75221065\n",
      "Iteration 1894, loss = 2.75214560\n",
      "Iteration 1895, loss = 2.75213101\n",
      "Iteration 1896, loss = 2.75202909\n",
      "Iteration 1897, loss = 2.75199891\n",
      "Iteration 1898, loss = 2.75193502\n",
      "Iteration 1899, loss = 2.75193479\n",
      "Iteration 1900, loss = 2.75181437\n",
      "Iteration 1901, loss = 2.75176110\n",
      "Iteration 1902, loss = 2.75172800\n",
      "Iteration 1903, loss = 2.75166821\n",
      "Iteration 1904, loss = 2.75162381\n",
      "Iteration 1905, loss = 2.75157622\n",
      "Iteration 1906, loss = 2.75151580\n",
      "Iteration 1907, loss = 2.75144749\n",
      "Iteration 1908, loss = 2.75140443\n",
      "Iteration 1909, loss = 2.75135817\n",
      "Iteration 1910, loss = 2.75129918\n",
      "Iteration 1911, loss = 2.75126751\n",
      "Iteration 1912, loss = 2.75119997\n",
      "Iteration 1913, loss = 2.75118293\n",
      "Iteration 1914, loss = 2.75113410\n",
      "Iteration 1915, loss = 2.75107543\n",
      "Iteration 1916, loss = 2.75103908\n",
      "Iteration 1917, loss = 2.75094204\n",
      "Iteration 1918, loss = 2.75092395\n",
      "Iteration 1919, loss = 2.75084237\n",
      "Iteration 1920, loss = 2.75080831\n",
      "Iteration 1921, loss = 2.75071503\n",
      "Iteration 1922, loss = 2.75071716\n",
      "Iteration 1923, loss = 2.75063888\n",
      "Iteration 1924, loss = 2.75060737\n",
      "Iteration 1925, loss = 2.75053130\n",
      "Iteration 1926, loss = 2.75050703\n",
      "Iteration 1927, loss = 2.75044508\n",
      "Iteration 1928, loss = 2.75041318\n",
      "Iteration 1929, loss = 2.75038386\n",
      "Iteration 1930, loss = 2.75029525\n",
      "Iteration 1931, loss = 2.75024224\n",
      "Iteration 1932, loss = 2.75019180\n",
      "Iteration 1933, loss = 2.75013534\n",
      "Iteration 1934, loss = 2.75007990\n",
      "Iteration 1935, loss = 2.75002158\n",
      "Iteration 1936, loss = 2.74997104\n",
      "Iteration 1937, loss = 2.74993966\n",
      "Iteration 1938, loss = 2.74986602\n",
      "Iteration 1939, loss = 2.74981713\n",
      "Iteration 1940, loss = 2.74977083\n",
      "Iteration 1941, loss = 2.74973292\n",
      "Iteration 1942, loss = 2.74968672\n",
      "Iteration 1943, loss = 2.74964135\n",
      "Iteration 1944, loss = 2.74958703\n",
      "Iteration 1945, loss = 2.74953568\n",
      "Iteration 1946, loss = 2.74947660\n",
      "Iteration 1947, loss = 2.74943841\n",
      "Iteration 1948, loss = 2.74938613\n",
      "Iteration 1949, loss = 2.74935729\n",
      "Iteration 1950, loss = 2.74926672\n",
      "Iteration 1951, loss = 2.74925056\n",
      "Iteration 1952, loss = 2.74920526\n",
      "Iteration 1953, loss = 2.74913230\n",
      "Iteration 1954, loss = 2.74908248\n",
      "Iteration 1955, loss = 2.74903597\n",
      "Iteration 1956, loss = 2.74897224\n",
      "Iteration 1957, loss = 2.74894647\n",
      "Iteration 1958, loss = 2.74886491\n",
      "Iteration 1959, loss = 2.74882539\n",
      "Iteration 1960, loss = 2.74878723\n",
      "Iteration 1961, loss = 2.74875138\n",
      "Iteration 1962, loss = 2.74866985\n",
      "Iteration 1963, loss = 2.74862589\n",
      "Iteration 1964, loss = 2.74856503\n",
      "Iteration 1965, loss = 2.74850169\n",
      "Iteration 1966, loss = 2.74847390\n",
      "Iteration 1967, loss = 2.74843949\n",
      "Iteration 1968, loss = 2.74842912\n",
      "Iteration 1969, loss = 2.74831564\n",
      "Iteration 1970, loss = 2.74827602\n",
      "Iteration 1971, loss = 2.74821043\n",
      "Iteration 1972, loss = 2.74816681\n",
      "Iteration 1973, loss = 2.74813110\n",
      "Iteration 1974, loss = 2.74807298\n",
      "Iteration 1975, loss = 2.74801609\n",
      "Iteration 1976, loss = 2.74798265\n",
      "Iteration 1977, loss = 2.74793505\n",
      "Iteration 1978, loss = 2.74787855\n",
      "Iteration 1979, loss = 2.74783712\n",
      "Iteration 1980, loss = 2.74776127\n",
      "Iteration 1981, loss = 2.74773843\n",
      "Iteration 1982, loss = 2.74767896\n",
      "Iteration 1983, loss = 2.74761803\n",
      "Iteration 1984, loss = 2.74755456\n",
      "Iteration 1985, loss = 2.74752243\n",
      "Iteration 1986, loss = 2.74750193\n",
      "Iteration 1987, loss = 2.74747586\n",
      "Iteration 1988, loss = 2.74743046\n",
      "Iteration 1989, loss = 2.74733492\n",
      "Iteration 1990, loss = 2.74725473\n",
      "Iteration 1991, loss = 2.74721587\n",
      "Iteration 1992, loss = 2.74716988\n",
      "Iteration 1993, loss = 2.74712874\n",
      "Iteration 1994, loss = 2.74707749\n",
      "Iteration 1995, loss = 2.74700749\n",
      "Iteration 1996, loss = 2.74696735\n",
      "Iteration 1997, loss = 2.74694139\n",
      "Iteration 1998, loss = 2.74690891\n",
      "Iteration 1999, loss = 2.74683490\n",
      "Iteration 2000, loss = 2.74678310\n",
      "Iteration 2001, loss = 2.74674422\n",
      "Iteration 2002, loss = 2.74667790\n",
      "Iteration 2003, loss = 2.74662398\n",
      "Iteration 2004, loss = 2.74660349\n",
      "Iteration 2005, loss = 2.74654293\n",
      "Iteration 2006, loss = 2.74650387\n",
      "Iteration 2007, loss = 2.74645608\n",
      "Iteration 2008, loss = 2.74639973\n",
      "Iteration 2009, loss = 2.74636209\n",
      "Iteration 2010, loss = 2.74630487\n",
      "Iteration 2011, loss = 2.74624823\n",
      "Iteration 2012, loss = 2.74622011\n",
      "Iteration 2013, loss = 2.74614988\n",
      "Iteration 2014, loss = 2.74610619\n",
      "Iteration 2015, loss = 2.74604671\n",
      "Iteration 2016, loss = 2.74596176\n",
      "Iteration 2017, loss = 2.74596599\n",
      "Iteration 2018, loss = 2.74589201\n",
      "Iteration 2019, loss = 2.74584455\n",
      "Iteration 2020, loss = 2.74578886\n",
      "Iteration 2021, loss = 2.74576098\n",
      "Iteration 2022, loss = 2.74569023\n",
      "Iteration 2023, loss = 2.74564575\n",
      "Iteration 2024, loss = 2.74560745\n",
      "Iteration 2025, loss = 2.74556465\n",
      "Iteration 2026, loss = 2.74550526\n",
      "Iteration 2027, loss = 2.74548242\n",
      "Iteration 2028, loss = 2.74542049\n",
      "Iteration 2029, loss = 2.74534266\n",
      "Iteration 2030, loss = 2.74533930\n",
      "Iteration 2031, loss = 2.74527018\n",
      "Iteration 2032, loss = 2.74522363\n",
      "Iteration 2033, loss = 2.74515058\n",
      "Iteration 2034, loss = 2.74511331\n",
      "Iteration 2035, loss = 2.74506551\n",
      "Iteration 2036, loss = 2.74500421\n",
      "Iteration 2037, loss = 2.74497138\n",
      "Iteration 2038, loss = 2.74494688\n",
      "Iteration 2039, loss = 2.74487683\n",
      "Iteration 2040, loss = 2.74478784\n",
      "Iteration 2041, loss = 2.74473489\n",
      "Iteration 2042, loss = 2.74473098\n",
      "Iteration 2043, loss = 2.74465603\n",
      "Iteration 2044, loss = 2.74464785\n",
      "Iteration 2045, loss = 2.74456235\n",
      "Iteration 2046, loss = 2.74450134\n",
      "Iteration 2047, loss = 2.74449659\n",
      "Iteration 2048, loss = 2.74446651\n",
      "Iteration 2049, loss = 2.74438734\n",
      "Iteration 2050, loss = 2.74434264\n",
      "Iteration 2051, loss = 2.74427602\n",
      "Iteration 2052, loss = 2.74428590\n",
      "Iteration 2053, loss = 2.74416945\n",
      "Iteration 2054, loss = 2.74414500\n",
      "Iteration 2055, loss = 2.74407422\n",
      "Iteration 2056, loss = 2.74402693\n",
      "Iteration 2057, loss = 2.74398116\n",
      "Iteration 2058, loss = 2.74395518\n",
      "Iteration 2059, loss = 2.74389717\n",
      "Iteration 2060, loss = 2.74387086\n",
      "Iteration 2061, loss = 2.74382078\n",
      "Iteration 2062, loss = 2.74372047\n",
      "Iteration 2063, loss = 2.74370346\n",
      "Iteration 2064, loss = 2.74365531\n",
      "Iteration 2065, loss = 2.74361437\n",
      "Iteration 2066, loss = 2.74352768\n",
      "Iteration 2067, loss = 2.74354369\n",
      "Iteration 2068, loss = 2.74344991\n",
      "Iteration 2069, loss = 2.74344077\n",
      "Iteration 2070, loss = 2.74336761\n",
      "Iteration 2071, loss = 2.74331228\n",
      "Iteration 2072, loss = 2.74323994\n",
      "Iteration 2073, loss = 2.74324559\n",
      "Iteration 2074, loss = 2.74316407\n",
      "Iteration 2075, loss = 2.74314409\n",
      "Iteration 2076, loss = 2.74304559\n",
      "Iteration 2077, loss = 2.74301216\n",
      "Iteration 2078, loss = 2.74296255\n",
      "Iteration 2079, loss = 2.74296854\n",
      "Iteration 2080, loss = 2.74291417\n",
      "Iteration 2081, loss = 2.74286452\n",
      "Iteration 2082, loss = 2.74284149\n",
      "Iteration 2083, loss = 2.74275225\n",
      "Iteration 2084, loss = 2.74266131\n",
      "Iteration 2085, loss = 2.74268721\n",
      "Iteration 2086, loss = 2.74260218\n",
      "Iteration 2087, loss = 2.74253658\n",
      "Iteration 2088, loss = 2.74249759\n",
      "Iteration 2089, loss = 2.74244392\n",
      "Iteration 2090, loss = 2.74239538\n",
      "Iteration 2091, loss = 2.74238538\n",
      "Iteration 2092, loss = 2.74236907\n",
      "Iteration 2093, loss = 2.74226297\n",
      "Iteration 2094, loss = 2.74224666\n",
      "Iteration 2095, loss = 2.74217588\n",
      "Iteration 2096, loss = 2.74213232\n",
      "Iteration 2097, loss = 2.74205555\n",
      "Iteration 2098, loss = 2.74207622\n",
      "Iteration 2099, loss = 2.74197658\n",
      "Iteration 2100, loss = 2.74195309\n",
      "Iteration 2101, loss = 2.74188861\n",
      "Iteration 2102, loss = 2.74184212\n",
      "Iteration 2103, loss = 2.74178919\n",
      "Iteration 2104, loss = 2.74179808\n",
      "Iteration 2105, loss = 2.74171222\n",
      "Iteration 2106, loss = 2.74164026\n",
      "Iteration 2107, loss = 2.74159126\n",
      "Iteration 2108, loss = 2.74154581\n",
      "Iteration 2109, loss = 2.74146793\n",
      "Iteration 2110, loss = 2.74144661\n",
      "Iteration 2111, loss = 2.74140551\n",
      "Iteration 2112, loss = 2.74135582\n",
      "Iteration 2113, loss = 2.74131371\n",
      "Iteration 2114, loss = 2.74127035\n",
      "Iteration 2115, loss = 2.74121774\n",
      "Iteration 2116, loss = 2.74114146\n",
      "Iteration 2117, loss = 2.74113729\n",
      "Iteration 2118, loss = 2.74109084\n",
      "Iteration 2119, loss = 2.74101488\n",
      "Iteration 2120, loss = 2.74098862\n",
      "Iteration 2121, loss = 2.74094606\n",
      "Iteration 2122, loss = 2.74088050\n",
      "Iteration 2123, loss = 2.74086385\n",
      "Iteration 2124, loss = 2.74078570\n",
      "Iteration 2125, loss = 2.74075210\n",
      "Iteration 2126, loss = 2.74076625\n",
      "Iteration 2127, loss = 2.74062925\n",
      "Iteration 2128, loss = 2.74059682\n",
      "Iteration 2129, loss = 2.74053857\n",
      "Iteration 2130, loss = 2.74052200\n",
      "Iteration 2131, loss = 2.74047285\n",
      "Iteration 2132, loss = 2.74043616\n",
      "Iteration 2133, loss = 2.74038338\n",
      "Iteration 2134, loss = 2.74032196\n",
      "Iteration 2135, loss = 2.74029728\n",
      "Iteration 2136, loss = 2.74034978\n",
      "Iteration 2137, loss = 2.74017153\n",
      "Iteration 2138, loss = 2.74012908\n",
      "Iteration 2139, loss = 2.74010261\n",
      "Iteration 2140, loss = 2.74009128\n",
      "Iteration 2141, loss = 2.73996607\n",
      "Iteration 2142, loss = 2.73995139\n",
      "Iteration 2143, loss = 2.73992395\n",
      "Iteration 2144, loss = 2.73987749\n",
      "Iteration 2145, loss = 2.73984113\n",
      "Iteration 2146, loss = 2.73976815\n",
      "Iteration 2147, loss = 2.73972047\n",
      "Iteration 2148, loss = 2.73966402\n",
      "Iteration 2149, loss = 2.73962943\n",
      "Iteration 2150, loss = 2.73959057\n",
      "Iteration 2151, loss = 2.73958016\n",
      "Iteration 2152, loss = 2.73949883\n",
      "Iteration 2153, loss = 2.73948298\n",
      "Iteration 2154, loss = 2.73936994\n",
      "Iteration 2155, loss = 2.73936726\n",
      "Iteration 2156, loss = 2.73933447\n",
      "Iteration 2157, loss = 2.73925124\n",
      "Iteration 2158, loss = 2.73922757\n",
      "Iteration 2159, loss = 2.73917872\n",
      "Iteration 2160, loss = 2.73910451\n",
      "Iteration 2161, loss = 2.73904677\n",
      "Iteration 2162, loss = 2.73900338\n",
      "Iteration 2163, loss = 2.73898739\n",
      "Iteration 2164, loss = 2.73893736\n",
      "Iteration 2165, loss = 2.73887503\n",
      "Iteration 2166, loss = 2.73880318\n",
      "Iteration 2167, loss = 2.73880096\n",
      "Iteration 2168, loss = 2.73872318\n",
      "Iteration 2169, loss = 2.73868966\n",
      "Iteration 2170, loss = 2.73864660\n",
      "Iteration 2171, loss = 2.73859356\n",
      "Iteration 2172, loss = 2.73856589\n",
      "Iteration 2173, loss = 2.73851108\n",
      "Iteration 2174, loss = 2.73842978\n",
      "Iteration 2175, loss = 2.73842377\n",
      "Iteration 2176, loss = 2.73837278\n",
      "Iteration 2177, loss = 2.73831122\n",
      "Iteration 2178, loss = 2.73828806\n",
      "Iteration 2179, loss = 2.73822375\n",
      "Iteration 2180, loss = 2.73819287\n",
      "Iteration 2181, loss = 2.73815130\n",
      "Iteration 2182, loss = 2.73806687\n",
      "Iteration 2183, loss = 2.73804595\n",
      "Iteration 2184, loss = 2.73798859\n",
      "Iteration 2185, loss = 2.73797368\n",
      "Iteration 2186, loss = 2.73787829\n",
      "Iteration 2187, loss = 2.73788656\n",
      "Iteration 2188, loss = 2.73782058\n",
      "Iteration 2189, loss = 2.73779678\n",
      "Iteration 2190, loss = 2.73771652\n",
      "Iteration 2191, loss = 2.73771099\n",
      "Iteration 2192, loss = 2.73761786\n",
      "Iteration 2193, loss = 2.73758034\n",
      "Iteration 2194, loss = 2.73752368\n",
      "Iteration 2195, loss = 2.73750970\n",
      "Iteration 2196, loss = 2.73745057\n",
      "Iteration 2197, loss = 2.73738329\n",
      "Iteration 2198, loss = 2.73735380\n",
      "Iteration 2199, loss = 2.73730572\n",
      "Iteration 2200, loss = 2.73726452\n",
      "Iteration 2201, loss = 2.73727260\n",
      "Iteration 2202, loss = 2.73717817\n",
      "Iteration 2203, loss = 2.73714563\n",
      "Iteration 2204, loss = 2.73709939\n",
      "Iteration 2205, loss = 2.73705838\n",
      "Iteration 2206, loss = 2.73700853\n",
      "Iteration 2207, loss = 2.73690973\n",
      "Iteration 2208, loss = 2.73689901\n",
      "Iteration 2209, loss = 2.73687547\n",
      "Iteration 2210, loss = 2.73685112\n",
      "Iteration 2211, loss = 2.73676470\n",
      "Iteration 2212, loss = 2.73671330\n",
      "Iteration 2213, loss = 2.73668792\n",
      "Iteration 2214, loss = 2.73662271\n",
      "Iteration 2215, loss = 2.73658263\n",
      "Iteration 2216, loss = 2.73655203\n",
      "Iteration 2217, loss = 2.73647394\n",
      "Iteration 2218, loss = 2.73645858\n",
      "Iteration 2219, loss = 2.73638699\n",
      "Iteration 2220, loss = 2.73636384\n",
      "Iteration 2221, loss = 2.73632901\n",
      "Iteration 2222, loss = 2.73628680\n",
      "Iteration 2223, loss = 2.73622689\n",
      "Iteration 2224, loss = 2.73621701\n",
      "Iteration 2225, loss = 2.73610896\n",
      "Iteration 2226, loss = 2.73610050\n",
      "Iteration 2227, loss = 2.73603272\n",
      "Iteration 2228, loss = 2.73600257\n",
      "Iteration 2229, loss = 2.73596639\n",
      "Iteration 2230, loss = 2.73590650\n",
      "Iteration 2231, loss = 2.73587210\n",
      "Iteration 2232, loss = 2.73587164\n",
      "Iteration 2233, loss = 2.73579852\n",
      "Iteration 2234, loss = 2.73575385\n",
      "Iteration 2235, loss = 2.73568167\n",
      "Iteration 2236, loss = 2.73562499\n",
      "Iteration 2237, loss = 2.73560417\n",
      "Iteration 2238, loss = 2.73563401\n",
      "Iteration 2239, loss = 2.73550149\n",
      "Iteration 2240, loss = 2.73548755\n",
      "Iteration 2241, loss = 2.73539685\n",
      "Iteration 2242, loss = 2.73534129\n",
      "Iteration 2243, loss = 2.73533746\n",
      "Iteration 2244, loss = 2.73528378\n",
      "Iteration 2245, loss = 2.73523408\n",
      "Iteration 2246, loss = 2.73522050\n",
      "Iteration 2247, loss = 2.73515714\n",
      "Iteration 2248, loss = 2.73508106\n",
      "Iteration 2249, loss = 2.73507340\n",
      "Iteration 2250, loss = 2.73503302\n",
      "Iteration 2251, loss = 2.73493725\n",
      "Iteration 2252, loss = 2.73491337\n",
      "Iteration 2253, loss = 2.73489628\n",
      "Iteration 2254, loss = 2.73485527\n",
      "Iteration 2255, loss = 2.73478505\n",
      "Iteration 2256, loss = 2.73473965\n",
      "Iteration 2257, loss = 2.73468127\n",
      "Iteration 2258, loss = 2.73463300\n",
      "Iteration 2259, loss = 2.73460526\n",
      "Iteration 2260, loss = 2.73454154\n",
      "Iteration 2261, loss = 2.73452182\n",
      "Iteration 2262, loss = 2.73454754\n",
      "Iteration 2263, loss = 2.73444007\n",
      "Iteration 2264, loss = 2.73437849\n",
      "Iteration 2265, loss = 2.73435466\n",
      "Iteration 2266, loss = 2.73429957\n",
      "Iteration 2267, loss = 2.73424144\n",
      "Iteration 2268, loss = 2.73420905\n",
      "Iteration 2269, loss = 2.73413971\n",
      "Iteration 2270, loss = 2.73410205\n",
      "Iteration 2271, loss = 2.73411635\n",
      "Iteration 2272, loss = 2.73401537\n",
      "Iteration 2273, loss = 2.73397322\n",
      "Iteration 2274, loss = 2.73392280\n",
      "Iteration 2275, loss = 2.73388337\n",
      "Iteration 2276, loss = 2.73386648\n",
      "Iteration 2277, loss = 2.73377601\n",
      "Iteration 2278, loss = 2.73375505\n",
      "Iteration 2279, loss = 2.73371272\n",
      "Iteration 2280, loss = 2.73369648\n",
      "Iteration 2281, loss = 2.73359818\n",
      "Iteration 2282, loss = 2.73359510\n",
      "Iteration 2283, loss = 2.73352797\n",
      "Iteration 2284, loss = 2.73348774\n",
      "Iteration 2285, loss = 2.73344187\n",
      "Iteration 2286, loss = 2.73340555\n",
      "Iteration 2287, loss = 2.73335805\n",
      "Iteration 2288, loss = 2.73330639\n",
      "Iteration 2289, loss = 2.73327479\n",
      "Iteration 2290, loss = 2.73326506\n",
      "Iteration 2291, loss = 2.73321326\n",
      "Iteration 2292, loss = 2.73313415\n",
      "Iteration 2293, loss = 2.73311389\n",
      "Iteration 2294, loss = 2.73306713\n",
      "Iteration 2295, loss = 2.73302333\n",
      "Iteration 2296, loss = 2.73298578\n",
      "Iteration 2297, loss = 2.73297679\n",
      "Iteration 2298, loss = 2.73291940\n",
      "Iteration 2299, loss = 2.73285782\n",
      "Iteration 2300, loss = 2.73278545\n",
      "Iteration 2301, loss = 2.73285630\n",
      "Iteration 2302, loss = 2.73276654\n",
      "Iteration 2303, loss = 2.73265890\n",
      "Iteration 2304, loss = 2.73265049\n",
      "Iteration 2305, loss = 2.73261149\n",
      "Iteration 2306, loss = 2.73253361\n",
      "Iteration 2307, loss = 2.73249532\n",
      "Iteration 2308, loss = 2.73250347\n",
      "Iteration 2309, loss = 2.73244642\n",
      "Iteration 2310, loss = 2.73238457\n",
      "Iteration 2311, loss = 2.73231807\n",
      "Iteration 2312, loss = 2.73225206\n",
      "Iteration 2313, loss = 2.73224697\n",
      "Iteration 2314, loss = 2.73218922\n",
      "Iteration 2315, loss = 2.73212945\n",
      "Iteration 2316, loss = 2.73209977\n",
      "Iteration 2317, loss = 2.73206029\n",
      "Iteration 2318, loss = 2.73201229\n",
      "Iteration 2319, loss = 2.73196303\n",
      "Iteration 2320, loss = 2.73193455\n",
      "Iteration 2321, loss = 2.73187927\n",
      "Iteration 2322, loss = 2.73187016\n",
      "Iteration 2323, loss = 2.73185426\n",
      "Iteration 2324, loss = 2.73177669\n",
      "Iteration 2325, loss = 2.73172808\n",
      "Iteration 2326, loss = 2.73164723\n",
      "Iteration 2327, loss = 2.73161393\n",
      "Iteration 2328, loss = 2.73158423\n",
      "Iteration 2329, loss = 2.73154729\n",
      "Iteration 2330, loss = 2.73149522\n",
      "Iteration 2331, loss = 2.73142130\n",
      "Iteration 2332, loss = 2.73140800\n",
      "Iteration 2333, loss = 2.73134399\n",
      "Iteration 2334, loss = 2.73132080\n",
      "Iteration 2335, loss = 2.73126083\n",
      "Iteration 2336, loss = 2.73121970\n",
      "Iteration 2337, loss = 2.73120390\n",
      "Iteration 2338, loss = 2.73114033\n",
      "Iteration 2339, loss = 2.73114139\n",
      "Iteration 2340, loss = 2.73105429\n",
      "Iteration 2341, loss = 2.73102691\n",
      "Iteration 2342, loss = 2.73099587\n",
      "Iteration 2343, loss = 2.73093678\n",
      "Iteration 2344, loss = 2.73088509\n",
      "Iteration 2345, loss = 2.73083504\n",
      "Iteration 2346, loss = 2.73079785\n",
      "Iteration 2347, loss = 2.73076868\n",
      "Iteration 2348, loss = 2.73072110\n",
      "Iteration 2349, loss = 2.73066189\n",
      "Iteration 2350, loss = 2.73060909\n",
      "Iteration 2351, loss = 2.73061100\n",
      "Iteration 2352, loss = 2.73059683\n",
      "Iteration 2353, loss = 2.73049445\n",
      "Iteration 2354, loss = 2.73043761\n",
      "Iteration 2355, loss = 2.73041801\n",
      "Iteration 2356, loss = 2.73039471\n",
      "Iteration 2357, loss = 2.73035713\n",
      "Iteration 2358, loss = 2.73029073\n",
      "Iteration 2359, loss = 2.73024142\n",
      "Iteration 2360, loss = 2.73022232\n",
      "Iteration 2361, loss = 2.73014484\n",
      "Iteration 2362, loss = 2.73010855\n",
      "Iteration 2363, loss = 2.73005997\n",
      "Iteration 2364, loss = 2.73001485\n",
      "Iteration 2365, loss = 2.72997582\n",
      "Iteration 2366, loss = 2.72994676\n",
      "Iteration 2367, loss = 2.72989398\n",
      "Iteration 2368, loss = 2.72985727\n",
      "Iteration 2369, loss = 2.72984255\n",
      "Iteration 2370, loss = 2.72978031\n",
      "Iteration 2371, loss = 2.72972178\n",
      "Iteration 2372, loss = 2.72967000\n",
      "Iteration 2373, loss = 2.72966843\n",
      "Iteration 2374, loss = 2.72962039\n",
      "Iteration 2375, loss = 2.72960718\n",
      "Iteration 2376, loss = 2.72955161\n",
      "Iteration 2377, loss = 2.72948105\n",
      "Iteration 2378, loss = 2.72947064\n",
      "Iteration 2379, loss = 2.72938275\n",
      "Iteration 2380, loss = 2.72932838\n",
      "Iteration 2381, loss = 2.72927379\n",
      "Iteration 2382, loss = 2.72926113\n",
      "Iteration 2383, loss = 2.72923783\n",
      "Iteration 2384, loss = 2.72918648\n",
      "Iteration 2385, loss = 2.72913741\n",
      "Iteration 2386, loss = 2.72911230\n",
      "Iteration 2387, loss = 2.72907065\n",
      "Iteration 2388, loss = 2.72901199\n",
      "Iteration 2389, loss = 2.72897820\n",
      "Iteration 2390, loss = 2.72895216\n",
      "Iteration 2391, loss = 2.72888927\n",
      "Iteration 2392, loss = 2.72887859\n",
      "Iteration 2393, loss = 2.72876288\n",
      "Iteration 2394, loss = 2.72876493\n",
      "Iteration 2395, loss = 2.72870515\n",
      "Iteration 2396, loss = 2.72867235\n",
      "Iteration 2397, loss = 2.72863567\n",
      "Iteration 2398, loss = 2.72860938\n",
      "Iteration 2399, loss = 2.72854219\n",
      "Iteration 2400, loss = 2.72846013\n",
      "Iteration 2401, loss = 2.72842043\n",
      "Iteration 2402, loss = 2.72842785\n",
      "Iteration 2403, loss = 2.72836760\n",
      "Iteration 2404, loss = 2.72833378\n",
      "Iteration 2405, loss = 2.72828123\n",
      "Iteration 2406, loss = 2.72822261\n",
      "Iteration 2407, loss = 2.72826346\n",
      "Iteration 2408, loss = 2.72814546\n",
      "Iteration 2409, loss = 2.72812108\n",
      "Iteration 2410, loss = 2.72808817\n",
      "Iteration 2411, loss = 2.72803272\n",
      "Iteration 2412, loss = 2.72801579\n",
      "Iteration 2413, loss = 2.72795652\n",
      "Iteration 2414, loss = 2.72791035\n",
      "Iteration 2415, loss = 2.72786274\n",
      "Iteration 2416, loss = 2.72781701\n",
      "Iteration 2417, loss = 2.72779370\n",
      "Iteration 2418, loss = 2.72772441\n",
      "Iteration 2419, loss = 2.72770958\n",
      "Iteration 2420, loss = 2.72765915\n",
      "Iteration 2421, loss = 2.72762344\n",
      "Iteration 2422, loss = 2.72756461\n",
      "Iteration 2423, loss = 2.72754549\n",
      "Iteration 2424, loss = 2.72751452\n",
      "Iteration 2425, loss = 2.72745470\n",
      "Iteration 2426, loss = 2.72743387\n",
      "Iteration 2427, loss = 2.72736743\n",
      "Iteration 2428, loss = 2.72731008\n",
      "Iteration 2429, loss = 2.72731565\n",
      "Iteration 2430, loss = 2.72725455\n",
      "Iteration 2431, loss = 2.72721455\n",
      "Iteration 2432, loss = 2.72720316\n",
      "Iteration 2433, loss = 2.72715357\n",
      "Iteration 2434, loss = 2.72711491\n",
      "Iteration 2435, loss = 2.72703756\n",
      "Iteration 2436, loss = 2.72700047\n",
      "Iteration 2437, loss = 2.72696513\n",
      "Iteration 2438, loss = 2.72692360\n",
      "Iteration 2439, loss = 2.72689134\n",
      "Iteration 2440, loss = 2.72682118\n",
      "Iteration 2441, loss = 2.72680596\n",
      "Iteration 2442, loss = 2.72679169\n",
      "Iteration 2443, loss = 2.72673613\n",
      "Iteration 2444, loss = 2.72664274\n",
      "Iteration 2445, loss = 2.72668231\n",
      "Iteration 2446, loss = 2.72657651\n",
      "Iteration 2447, loss = 2.72654913\n",
      "Iteration 2448, loss = 2.72649200\n",
      "Iteration 2449, loss = 2.72641856\n",
      "Iteration 2450, loss = 2.72640545\n",
      "Iteration 2451, loss = 2.72636242\n",
      "Iteration 2452, loss = 2.72630213\n",
      "Iteration 2453, loss = 2.72631762\n",
      "Iteration 2454, loss = 2.72627909\n",
      "Iteration 2455, loss = 2.72617855\n",
      "Iteration 2456, loss = 2.72615922\n",
      "Iteration 2457, loss = 2.72611210\n",
      "Iteration 2458, loss = 2.72609549\n",
      "Iteration 2459, loss = 2.72604522\n",
      "Iteration 2460, loss = 2.72599684\n",
      "Iteration 2461, loss = 2.72597246\n",
      "Iteration 2462, loss = 2.72595557\n",
      "Iteration 2463, loss = 2.72591372\n",
      "Iteration 2464, loss = 2.72583826\n",
      "Iteration 2465, loss = 2.72578392\n",
      "Iteration 2466, loss = 2.72578002\n",
      "Iteration 2467, loss = 2.72574059\n",
      "Iteration 2468, loss = 2.72565547\n",
      "Iteration 2469, loss = 2.72563019\n",
      "Iteration 2470, loss = 2.72559091\n",
      "Iteration 2471, loss = 2.72557788\n",
      "Iteration 2472, loss = 2.72550349\n",
      "Iteration 2473, loss = 2.72549621\n",
      "Iteration 2474, loss = 2.72545100\n",
      "Iteration 2475, loss = 2.72538457\n",
      "Iteration 2476, loss = 2.72535415\n",
      "Iteration 2477, loss = 2.72529311\n",
      "Iteration 2478, loss = 2.72526700\n",
      "Iteration 2479, loss = 2.72524316\n",
      "Iteration 2480, loss = 2.72520388\n",
      "Iteration 2481, loss = 2.72517839\n",
      "Iteration 2482, loss = 2.72511846\n",
      "Iteration 2483, loss = 2.72507873\n",
      "Iteration 2484, loss = 2.72500599\n",
      "Iteration 2485, loss = 2.72494328\n",
      "Iteration 2486, loss = 2.72492582\n",
      "Iteration 2487, loss = 2.72487442\n",
      "Iteration 2488, loss = 2.72484008\n",
      "Iteration 2489, loss = 2.72480464\n",
      "Iteration 2490, loss = 2.72477589\n",
      "Iteration 2491, loss = 2.72474202\n",
      "Iteration 2492, loss = 2.72470828\n",
      "Iteration 2493, loss = 2.72465499\n",
      "Iteration 2494, loss = 2.72461038\n",
      "Iteration 2495, loss = 2.72459668\n",
      "Iteration 2496, loss = 2.72455732\n",
      "Iteration 2497, loss = 2.72448396\n",
      "Iteration 2498, loss = 2.72448685\n",
      "Iteration 2499, loss = 2.72442536\n",
      "Iteration 2500, loss = 2.72439775\n",
      "Iteration 2501, loss = 2.72431028\n",
      "Iteration 2502, loss = 2.72427312\n",
      "Iteration 2503, loss = 2.72425064\n",
      "Iteration 2504, loss = 2.72418641\n",
      "Iteration 2505, loss = 2.72415108\n",
      "Iteration 2506, loss = 2.72410948\n",
      "Iteration 2507, loss = 2.72409420\n",
      "Iteration 2508, loss = 2.72405739\n",
      "Iteration 2509, loss = 2.72401370\n",
      "Iteration 2510, loss = 2.72396184\n",
      "Iteration 2511, loss = 2.72394087\n",
      "Iteration 2512, loss = 2.72385986\n",
      "Iteration 2513, loss = 2.72384458\n",
      "Iteration 2514, loss = 2.72381688\n",
      "Iteration 2515, loss = 2.72379394\n",
      "Iteration 2516, loss = 2.72371497\n",
      "Iteration 2517, loss = 2.72367588\n",
      "Iteration 2518, loss = 2.72367829\n",
      "Iteration 2519, loss = 2.72360732\n",
      "Iteration 2520, loss = 2.72357891\n",
      "Iteration 2521, loss = 2.72355668\n",
      "Iteration 2522, loss = 2.72349010\n",
      "Iteration 2523, loss = 2.72344346\n",
      "Iteration 2524, loss = 2.72341351\n",
      "Iteration 2525, loss = 2.72333785\n",
      "Iteration 2526, loss = 2.72327731\n",
      "Iteration 2527, loss = 2.72326180\n",
      "Iteration 2528, loss = 2.72322931\n",
      "Iteration 2529, loss = 2.72324622\n",
      "Iteration 2530, loss = 2.72314910\n",
      "Iteration 2531, loss = 2.72314914\n",
      "Iteration 2532, loss = 2.72307621\n",
      "Iteration 2533, loss = 2.72298710\n",
      "Iteration 2534, loss = 2.72301156\n",
      "Iteration 2535, loss = 2.72294334\n",
      "Iteration 2536, loss = 2.72292151\n",
      "Iteration 2537, loss = 2.72286724\n",
      "Iteration 2538, loss = 2.72281564\n",
      "Iteration 2539, loss = 2.72280784\n",
      "Iteration 2540, loss = 2.72273989\n",
      "Iteration 2541, loss = 2.72273599\n",
      "Iteration 2542, loss = 2.72268896\n",
      "Iteration 2543, loss = 2.72264717\n",
      "Iteration 2544, loss = 2.72260028\n",
      "Iteration 2545, loss = 2.72265086\n",
      "Iteration 2546, loss = 2.72252069\n",
      "Iteration 2547, loss = 2.72245847\n",
      "Iteration 2548, loss = 2.72247270\n",
      "Iteration 2549, loss = 2.72236411\n",
      "Iteration 2550, loss = 2.72238574\n",
      "Iteration 2551, loss = 2.72235402\n",
      "Iteration 2552, loss = 2.72231982\n",
      "Iteration 2553, loss = 2.72224601\n",
      "Iteration 2554, loss = 2.72222033\n",
      "Iteration 2555, loss = 2.72217505\n",
      "Iteration 2556, loss = 2.72217898\n",
      "Iteration 2557, loss = 2.72209248\n",
      "Iteration 2558, loss = 2.72208918\n",
      "Iteration 2559, loss = 2.72202049\n",
      "Iteration 2560, loss = 2.72195712\n",
      "Iteration 2561, loss = 2.72193375\n",
      "Iteration 2562, loss = 2.72191400\n",
      "Iteration 2563, loss = 2.72182126\n",
      "Iteration 2564, loss = 2.72179635\n",
      "Iteration 2565, loss = 2.72179091\n",
      "Iteration 2566, loss = 2.72169810\n",
      "Iteration 2567, loss = 2.72169939\n",
      "Iteration 2568, loss = 2.72165666\n",
      "Iteration 2569, loss = 2.72159233\n",
      "Iteration 2570, loss = 2.72156749\n",
      "Iteration 2571, loss = 2.72158515\n",
      "Iteration 2572, loss = 2.72150352\n",
      "Iteration 2573, loss = 2.72143701\n",
      "Iteration 2574, loss = 2.72138991\n",
      "Iteration 2575, loss = 2.72137793\n",
      "Iteration 2576, loss = 2.72133428\n",
      "Iteration 2577, loss = 2.72126636\n",
      "Iteration 2578, loss = 2.72123420\n",
      "Iteration 2579, loss = 2.72121775\n",
      "Iteration 2580, loss = 2.72114126\n",
      "Iteration 2581, loss = 2.72113931\n",
      "Iteration 2582, loss = 2.72111705\n",
      "Iteration 2583, loss = 2.72104665\n",
      "Iteration 2584, loss = 2.72102104\n",
      "Iteration 2585, loss = 2.72099331\n",
      "Iteration 2586, loss = 2.72095893\n",
      "Iteration 2587, loss = 2.72089595\n",
      "Iteration 2588, loss = 2.72091547\n",
      "Iteration 2589, loss = 2.72080856\n",
      "Iteration 2590, loss = 2.72076594\n",
      "Iteration 2591, loss = 2.72072681\n",
      "Iteration 2592, loss = 2.72069195\n",
      "Iteration 2593, loss = 2.72066058\n",
      "Iteration 2594, loss = 2.72066720\n",
      "Iteration 2595, loss = 2.72058587\n",
      "Iteration 2596, loss = 2.72054883\n",
      "Iteration 2597, loss = 2.72050098\n",
      "Iteration 2598, loss = 2.72043459\n",
      "Iteration 2599, loss = 2.72045451\n",
      "Iteration 2600, loss = 2.72036472\n",
      "Iteration 2601, loss = 2.72035924\n",
      "Iteration 2602, loss = 2.72032497\n",
      "Iteration 2603, loss = 2.72024049\n",
      "Iteration 2604, loss = 2.72025637\n",
      "Iteration 2605, loss = 2.72024526\n",
      "Iteration 2606, loss = 2.72015856\n",
      "Iteration 2607, loss = 2.72011274\n",
      "Iteration 2608, loss = 2.72010377\n",
      "Iteration 2609, loss = 2.72005504\n",
      "Iteration 2610, loss = 2.72002088\n",
      "Iteration 2611, loss = 2.71996871\n",
      "Iteration 2612, loss = 2.71991860\n",
      "Iteration 2613, loss = 2.71988505\n",
      "Iteration 2614, loss = 2.71985347\n",
      "Iteration 2615, loss = 2.71979052\n",
      "Iteration 2616, loss = 2.71973332\n",
      "Iteration 2617, loss = 2.71972014\n",
      "Iteration 2618, loss = 2.71969310\n",
      "Iteration 2619, loss = 2.71968382\n",
      "Iteration 2620, loss = 2.71965875\n",
      "Iteration 2621, loss = 2.71956515\n",
      "Iteration 2622, loss = 2.71955359\n",
      "Iteration 2623, loss = 2.71951728\n",
      "Iteration 2624, loss = 2.71946553\n",
      "Iteration 2625, loss = 2.71942993\n",
      "Iteration 2626, loss = 2.71938200\n",
      "Iteration 2627, loss = 2.71933291\n",
      "Iteration 2628, loss = 2.71933000\n",
      "Iteration 2629, loss = 2.71929647\n",
      "Iteration 2630, loss = 2.71924666\n",
      "Iteration 2631, loss = 2.71920066\n",
      "Iteration 2632, loss = 2.71916417\n",
      "Iteration 2633, loss = 2.71909362\n",
      "Iteration 2634, loss = 2.71909730\n",
      "Iteration 2635, loss = 2.71899430\n",
      "Iteration 2636, loss = 2.71901199\n",
      "Iteration 2637, loss = 2.71896936\n",
      "Iteration 2638, loss = 2.71889934\n",
      "Iteration 2639, loss = 2.71888725\n",
      "Iteration 2640, loss = 2.71891692\n",
      "Iteration 2641, loss = 2.71882837\n",
      "Iteration 2642, loss = 2.71877086\n",
      "Iteration 2643, loss = 2.71870853\n",
      "Iteration 2644, loss = 2.71869101\n",
      "Iteration 2645, loss = 2.71864091\n",
      "Iteration 2646, loss = 2.71861458\n",
      "Iteration 2647, loss = 2.71862515\n",
      "Iteration 2648, loss = 2.71854441\n",
      "Iteration 2649, loss = 2.71851791\n",
      "Iteration 2650, loss = 2.71847620\n",
      "Iteration 2651, loss = 2.71844714\n",
      "Iteration 2652, loss = 2.71838404\n",
      "Iteration 2653, loss = 2.71833382\n",
      "Iteration 2654, loss = 2.71831150\n",
      "Iteration 2655, loss = 2.71826187\n",
      "Iteration 2656, loss = 2.71823593\n",
      "Iteration 2657, loss = 2.71820962\n",
      "Iteration 2658, loss = 2.71817870\n",
      "Iteration 2659, loss = 2.71819826\n",
      "Iteration 2660, loss = 2.71809468\n",
      "Iteration 2661, loss = 2.71804940\n",
      "Iteration 2662, loss = 2.71800712\n",
      "Iteration 2663, loss = 2.71796330\n",
      "Iteration 2664, loss = 2.71796023\n",
      "Iteration 2665, loss = 2.71792839\n",
      "Iteration 2666, loss = 2.71786642\n",
      "Iteration 2667, loss = 2.71786680\n",
      "Iteration 2668, loss = 2.71779327\n",
      "Iteration 2669, loss = 2.71771687\n",
      "Iteration 2670, loss = 2.71772484\n",
      "Iteration 2671, loss = 2.71767031\n",
      "Iteration 2672, loss = 2.71762095\n",
      "Iteration 2673, loss = 2.71761331\n",
      "Iteration 2674, loss = 2.71762615\n",
      "Iteration 2675, loss = 2.71755316\n",
      "Iteration 2676, loss = 2.71746876\n",
      "Iteration 2677, loss = 2.71741749\n",
      "Iteration 2678, loss = 2.71739825\n",
      "Iteration 2679, loss = 2.71737927\n",
      "Iteration 2680, loss = 2.71733118\n",
      "Iteration 2681, loss = 2.71732155\n",
      "Iteration 2682, loss = 2.71721569\n",
      "Iteration 2683, loss = 2.71721353\n",
      "Iteration 2684, loss = 2.71715811\n",
      "Iteration 2685, loss = 2.71708886\n",
      "Iteration 2686, loss = 2.71712495\n",
      "Iteration 2687, loss = 2.71704716\n",
      "Iteration 2688, loss = 2.71699992\n",
      "Iteration 2689, loss = 2.71697206\n",
      "Iteration 2690, loss = 2.71694099\n",
      "Iteration 2691, loss = 2.71690854\n",
      "Iteration 2692, loss = 2.71690096\n",
      "Iteration 2693, loss = 2.71683722\n",
      "Iteration 2694, loss = 2.71678225\n",
      "Iteration 2695, loss = 2.71676375\n",
      "Iteration 2696, loss = 2.71672240\n",
      "Iteration 2697, loss = 2.71667154\n",
      "Iteration 2698, loss = 2.71663404\n",
      "Iteration 2699, loss = 2.71661473\n",
      "Iteration 2700, loss = 2.71657708\n",
      "Iteration 2701, loss = 2.71654049\n",
      "Iteration 2702, loss = 2.71648130\n",
      "Iteration 2703, loss = 2.71646194\n",
      "Iteration 2704, loss = 2.71640634\n",
      "Iteration 2705, loss = 2.71636178\n",
      "Iteration 2706, loss = 2.71635453\n",
      "Iteration 2707, loss = 2.71630961\n",
      "Iteration 2708, loss = 2.71626600\n",
      "Iteration 2709, loss = 2.71624553\n",
      "Iteration 2710, loss = 2.71616764\n",
      "Iteration 2711, loss = 2.71616952\n",
      "Iteration 2712, loss = 2.71612021\n",
      "Iteration 2713, loss = 2.71610022\n",
      "Iteration 2714, loss = 2.71604106\n",
      "Iteration 2715, loss = 2.71600665\n",
      "Iteration 2716, loss = 2.71597089\n",
      "Iteration 2717, loss = 2.71593923\n",
      "Iteration 2718, loss = 2.71588446\n",
      "Iteration 2719, loss = 2.71583777\n",
      "Iteration 2720, loss = 2.71579753\n",
      "Iteration 2721, loss = 2.71579176\n",
      "Iteration 2722, loss = 2.71578546\n",
      "Iteration 2723, loss = 2.71572424\n",
      "Iteration 2724, loss = 2.71569396\n",
      "Iteration 2725, loss = 2.71563686\n",
      "Iteration 2726, loss = 2.71559659\n",
      "Iteration 2727, loss = 2.71554025\n",
      "Iteration 2728, loss = 2.71551437\n",
      "Iteration 2729, loss = 2.71550126\n",
      "Iteration 2730, loss = 2.71544794\n",
      "Iteration 2731, loss = 2.71540224\n",
      "Iteration 2732, loss = 2.71536550\n",
      "Iteration 2733, loss = 2.71533197\n",
      "Iteration 2734, loss = 2.71528073\n",
      "Iteration 2735, loss = 2.71527465\n",
      "Iteration 2736, loss = 2.71521774\n",
      "Iteration 2737, loss = 2.71516438\n",
      "Iteration 2738, loss = 2.71515896\n",
      "Iteration 2739, loss = 2.71511452\n",
      "Iteration 2740, loss = 2.71506651\n",
      "Iteration 2741, loss = 2.71508003\n",
      "Iteration 2742, loss = 2.71505880\n",
      "Iteration 2743, loss = 2.71496979\n",
      "Iteration 2744, loss = 2.71492701\n",
      "Iteration 2745, loss = 2.71489872\n",
      "Iteration 2746, loss = 2.71486637\n",
      "Iteration 2747, loss = 2.71482463\n",
      "Iteration 2748, loss = 2.71478811\n",
      "Iteration 2749, loss = 2.71478393\n",
      "Iteration 2750, loss = 2.71477702\n",
      "Iteration 2751, loss = 2.71462952\n",
      "Iteration 2752, loss = 2.71462695\n",
      "Iteration 2753, loss = 2.71457021\n",
      "Iteration 2754, loss = 2.71455983\n",
      "Iteration 2755, loss = 2.71451348\n",
      "Iteration 2756, loss = 2.71448585\n",
      "Iteration 2757, loss = 2.71446571\n",
      "Iteration 2758, loss = 2.71442607\n",
      "Iteration 2759, loss = 2.71435136\n",
      "Iteration 2760, loss = 2.71433203\n",
      "Iteration 2761, loss = 2.71428517\n",
      "Iteration 2762, loss = 2.71423759\n",
      "Iteration 2763, loss = 2.71422204\n",
      "Iteration 2764, loss = 2.71420663\n",
      "Iteration 2765, loss = 2.71414212\n",
      "Iteration 2766, loss = 2.71410810\n",
      "Iteration 2767, loss = 2.71408645\n",
      "Iteration 2768, loss = 2.71404125\n",
      "Iteration 2769, loss = 2.71400544\n",
      "Iteration 2770, loss = 2.71399611\n",
      "Iteration 2771, loss = 2.71392968\n",
      "Iteration 2772, loss = 2.71389850\n",
      "Iteration 2773, loss = 2.71385881\n",
      "Iteration 2774, loss = 2.71379235\n",
      "Iteration 2775, loss = 2.71379418\n",
      "Iteration 2776, loss = 2.71374037\n",
      "Iteration 2777, loss = 2.71371876\n",
      "Iteration 2778, loss = 2.71364914\n",
      "Iteration 2779, loss = 2.71364846\n",
      "Iteration 2780, loss = 2.71362074\n",
      "Iteration 2781, loss = 2.71356935\n",
      "Iteration 2782, loss = 2.71353840\n",
      "Iteration 2783, loss = 2.71347581\n",
      "Iteration 2784, loss = 2.71350785\n",
      "Iteration 2785, loss = 2.71350778\n",
      "Iteration 2786, loss = 2.71342142\n",
      "Iteration 2787, loss = 2.71335587\n",
      "Iteration 2788, loss = 2.71331736\n",
      "Iteration 2789, loss = 2.71327499\n",
      "Iteration 2790, loss = 2.71322587\n",
      "Iteration 2791, loss = 2.71320879\n",
      "Iteration 2792, loss = 2.71324017\n",
      "Iteration 2793, loss = 2.71313506\n",
      "Iteration 2794, loss = 2.71309271\n",
      "Iteration 2795, loss = 2.71308922\n",
      "Iteration 2796, loss = 2.71305420\n",
      "Iteration 2797, loss = 2.71301146\n",
      "Iteration 2798, loss = 2.71294987\n",
      "Iteration 2799, loss = 2.71291812\n",
      "Iteration 2800, loss = 2.71290816\n",
      "Iteration 2801, loss = 2.71285737\n",
      "Iteration 2802, loss = 2.71282772\n",
      "Iteration 2803, loss = 2.71276734\n",
      "Iteration 2804, loss = 2.71273364\n",
      "Iteration 2805, loss = 2.71273385\n",
      "Iteration 2806, loss = 2.71270194\n",
      "Iteration 2807, loss = 2.71264426\n",
      "Iteration 2808, loss = 2.71260895\n",
      "Iteration 2809, loss = 2.71261438\n",
      "Iteration 2810, loss = 2.71258467\n",
      "Iteration 2811, loss = 2.71249134\n",
      "Iteration 2812, loss = 2.71247339\n",
      "Iteration 2813, loss = 2.71242761\n",
      "Iteration 2814, loss = 2.71234625\n",
      "Iteration 2815, loss = 2.71234864\n",
      "Iteration 2816, loss = 2.71231592\n",
      "Iteration 2817, loss = 2.71225670\n",
      "Iteration 2818, loss = 2.71223056\n",
      "Iteration 2819, loss = 2.71220183\n",
      "Iteration 2820, loss = 2.71216631\n",
      "Iteration 2821, loss = 2.71210702\n",
      "Iteration 2822, loss = 2.71213271\n",
      "Iteration 2823, loss = 2.71207510\n",
      "Iteration 2824, loss = 2.71204250\n",
      "Iteration 2825, loss = 2.71203291\n",
      "Iteration 2826, loss = 2.71197989\n",
      "Iteration 2827, loss = 2.71188917\n",
      "Iteration 2828, loss = 2.71186976\n",
      "Iteration 2829, loss = 2.71183329\n",
      "Iteration 2830, loss = 2.71183137\n",
      "Iteration 2831, loss = 2.71175013\n",
      "Iteration 2832, loss = 2.71174244\n",
      "Iteration 2833, loss = 2.71173883\n",
      "Iteration 2834, loss = 2.71169197\n",
      "Iteration 2835, loss = 2.71163384\n",
      "Iteration 2836, loss = 2.71160877\n",
      "Iteration 2837, loss = 2.71156805\n",
      "Iteration 2838, loss = 2.71151190\n",
      "Iteration 2839, loss = 2.71150386\n",
      "Iteration 2840, loss = 2.71144484\n",
      "Iteration 2841, loss = 2.71139893\n",
      "Iteration 2842, loss = 2.71136665\n",
      "Iteration 2843, loss = 2.71135432\n",
      "Iteration 2844, loss = 2.71130537\n",
      "Iteration 2845, loss = 2.71124744\n",
      "Iteration 2846, loss = 2.71125278\n",
      "Iteration 2847, loss = 2.71118649\n",
      "Iteration 2848, loss = 2.71117772\n",
      "Iteration 2849, loss = 2.71111244\n",
      "Iteration 2850, loss = 2.71111627\n",
      "Iteration 2851, loss = 2.71104683\n",
      "Iteration 2852, loss = 2.71102895\n",
      "Iteration 2853, loss = 2.71098434\n",
      "Iteration 2854, loss = 2.71094111\n",
      "Iteration 2855, loss = 2.71088154\n",
      "Iteration 2856, loss = 2.71086014\n",
      "Iteration 2857, loss = 2.71083778\n",
      "Iteration 2858, loss = 2.71082300\n",
      "Iteration 2859, loss = 2.71075608\n",
      "Iteration 2860, loss = 2.71074345\n",
      "Iteration 2861, loss = 2.71071296\n",
      "Iteration 2862, loss = 2.71065379\n",
      "Iteration 2863, loss = 2.71065334\n",
      "Iteration 2864, loss = 2.71070316\n",
      "Iteration 2865, loss = 2.71057368\n",
      "Iteration 2866, loss = 2.71053514\n",
      "Iteration 2867, loss = 2.71055019\n",
      "Iteration 2868, loss = 2.71047976\n",
      "Iteration 2869, loss = 2.71039137\n",
      "Iteration 2870, loss = 2.71040133\n",
      "Iteration 2871, loss = 2.71035193\n",
      "Iteration 2872, loss = 2.71028872\n",
      "Iteration 2873, loss = 2.71030706\n",
      "Iteration 2874, loss = 2.71025286\n",
      "Iteration 2875, loss = 2.71021558\n",
      "Iteration 2876, loss = 2.71016654\n",
      "Iteration 2877, loss = 2.71010936\n",
      "Iteration 2878, loss = 2.71011607\n",
      "Iteration 2879, loss = 2.71004626\n",
      "Iteration 2880, loss = 2.71002620\n",
      "Iteration 2881, loss = 2.70998383\n",
      "Iteration 2882, loss = 2.70993017\n",
      "Iteration 2883, loss = 2.70992763\n",
      "Iteration 2884, loss = 2.70992298\n",
      "Iteration 2885, loss = 2.70985997\n",
      "Iteration 2886, loss = 2.70982292\n",
      "Iteration 2887, loss = 2.70978833\n",
      "Iteration 2888, loss = 2.70972701\n",
      "Iteration 2889, loss = 2.70967511\n",
      "Iteration 2890, loss = 2.70965771\n",
      "Iteration 2891, loss = 2.70965105\n",
      "Iteration 2892, loss = 2.70963317\n",
      "Iteration 2893, loss = 2.70957164\n",
      "Iteration 2894, loss = 2.70955977\n",
      "Iteration 2895, loss = 2.70953676\n",
      "Iteration 2896, loss = 2.70947568\n",
      "Iteration 2897, loss = 2.70941628\n",
      "Iteration 2898, loss = 2.70939346\n",
      "Iteration 2899, loss = 2.70935787\n",
      "Iteration 2900, loss = 2.70928982\n",
      "Iteration 2901, loss = 2.70930477\n",
      "Iteration 2902, loss = 2.70928330\n",
      "Iteration 2903, loss = 2.70926309\n",
      "Iteration 2904, loss = 2.70921264\n",
      "Iteration 2905, loss = 2.70924315\n",
      "Iteration 2906, loss = 2.70912619\n",
      "Iteration 2907, loss = 2.70907600\n",
      "Iteration 2908, loss = 2.70904379\n",
      "Iteration 2909, loss = 2.70904076\n",
      "Iteration 2910, loss = 2.70897190\n",
      "Iteration 2911, loss = 2.70897124\n",
      "Iteration 2912, loss = 2.70892521\n",
      "Iteration 2913, loss = 2.70884914\n",
      "Iteration 2914, loss = 2.70886339\n",
      "Iteration 2915, loss = 2.70881114\n",
      "Iteration 2916, loss = 2.70879196\n",
      "Iteration 2917, loss = 2.70872858\n",
      "Iteration 2918, loss = 2.70869673\n",
      "Iteration 2919, loss = 2.70872025\n",
      "Iteration 2920, loss = 2.70864898\n",
      "Iteration 2921, loss = 2.70865492\n",
      "Iteration 2922, loss = 2.70858613\n",
      "Iteration 2923, loss = 2.70847864\n",
      "Iteration 2924, loss = 2.70847523\n",
      "Iteration 2925, loss = 2.70846214\n",
      "Iteration 2926, loss = 2.70841360\n",
      "Iteration 2927, loss = 2.70840130\n",
      "Iteration 2928, loss = 2.70840337\n",
      "Iteration 2929, loss = 2.70829054\n",
      "Iteration 2930, loss = 2.70824514\n",
      "Iteration 2931, loss = 2.70824663\n",
      "Iteration 2932, loss = 2.70820708\n",
      "Iteration 2933, loss = 2.70816496\n",
      "Iteration 2934, loss = 2.70812096\n",
      "Iteration 2935, loss = 2.70809071\n",
      "Iteration 2936, loss = 2.70821253\n",
      "Iteration 2937, loss = 2.70804928\n",
      "Iteration 2938, loss = 2.70804112\n",
      "Iteration 2939, loss = 2.70796415\n",
      "Iteration 2940, loss = 2.70794175\n",
      "Iteration 2941, loss = 2.70790049\n",
      "Iteration 2942, loss = 2.70787618\n",
      "Iteration 2943, loss = 2.70782830\n",
      "Iteration 2944, loss = 2.70779027\n",
      "Iteration 2945, loss = 2.70775820\n",
      "Iteration 2946, loss = 2.70774086\n",
      "Iteration 2947, loss = 2.70766829\n",
      "Iteration 2948, loss = 2.70765529\n",
      "Iteration 2949, loss = 2.70767860\n",
      "Iteration 2950, loss = 2.70758396\n",
      "Iteration 2951, loss = 2.70755370\n",
      "Iteration 2952, loss = 2.70751530\n",
      "Iteration 2953, loss = 2.70744593\n",
      "Iteration 2954, loss = 2.70745367\n",
      "Iteration 2955, loss = 2.70744261\n",
      "Iteration 2956, loss = 2.70739304\n",
      "Iteration 2957, loss = 2.70734079\n",
      "Iteration 2958, loss = 2.70732009\n",
      "Iteration 2959, loss = 2.70726631\n",
      "Iteration 2960, loss = 2.70730949\n",
      "Iteration 2961, loss = 2.70724834\n",
      "Iteration 2962, loss = 2.70720596\n",
      "Iteration 2963, loss = 2.70719396\n",
      "Iteration 2964, loss = 2.70712111\n",
      "Iteration 2965, loss = 2.70708353\n",
      "Iteration 2966, loss = 2.70706047\n",
      "Iteration 2967, loss = 2.70703941\n",
      "Iteration 2968, loss = 2.70702563\n",
      "Iteration 2969, loss = 2.70698678\n",
      "Iteration 2970, loss = 2.70693909\n",
      "Iteration 2971, loss = 2.70689108\n",
      "Iteration 2972, loss = 2.70685367\n",
      "Iteration 2973, loss = 2.70682635\n",
      "Iteration 2974, loss = 2.70674919\n",
      "Iteration 2975, loss = 2.70674660\n",
      "Iteration 2976, loss = 2.70667958\n",
      "Iteration 2977, loss = 2.70667406\n",
      "Iteration 2978, loss = 2.70663759\n",
      "Iteration 2979, loss = 2.70660153\n",
      "Iteration 2980, loss = 2.70659450\n",
      "Iteration 2981, loss = 2.70654888\n",
      "Iteration 2982, loss = 2.70649035\n",
      "Iteration 2983, loss = 2.70653396\n",
      "Iteration 2984, loss = 2.70640761\n",
      "Iteration 2985, loss = 2.70638478\n",
      "Iteration 2986, loss = 2.70635046\n",
      "Iteration 2987, loss = 2.70639203\n",
      "Iteration 2988, loss = 2.70627963\n",
      "Iteration 2989, loss = 2.70631771\n",
      "Iteration 2990, loss = 2.70626175\n",
      "Iteration 2991, loss = 2.70626189\n",
      "Iteration 2992, loss = 2.70617099\n",
      "Iteration 2993, loss = 2.70613143\n",
      "Iteration 2994, loss = 2.70608629\n",
      "Iteration 2995, loss = 2.70606381\n",
      "Iteration 2996, loss = 2.70600728\n",
      "Iteration 2997, loss = 2.70597353\n",
      "Iteration 2998, loss = 2.70602129\n",
      "Iteration 2999, loss = 2.70596638\n",
      "Iteration 3000, loss = 2.70585430\n",
      "Iteration 3001, loss = 2.70584705\n",
      "Iteration 3002, loss = 2.70585499\n",
      "Iteration 3003, loss = 2.70583160\n",
      "Iteration 3004, loss = 2.70575715\n",
      "Iteration 3005, loss = 2.70571862\n",
      "Iteration 3006, loss = 2.70573876\n",
      "Iteration 3007, loss = 2.70565925\n",
      "Iteration 3008, loss = 2.70559169\n",
      "Iteration 3009, loss = 2.70558202\n",
      "Iteration 3010, loss = 2.70556969\n",
      "Iteration 3011, loss = 2.70550273\n",
      "Iteration 3012, loss = 2.70549160\n",
      "Iteration 3013, loss = 2.70546962\n",
      "Iteration 3014, loss = 2.70546167\n",
      "Iteration 3015, loss = 2.70543875\n",
      "Iteration 3016, loss = 2.70534238\n",
      "Iteration 3017, loss = 2.70533509\n",
      "Iteration 3018, loss = 2.70530327\n",
      "Iteration 3019, loss = 2.70529969\n",
      "Iteration 3020, loss = 2.70523394\n",
      "Iteration 3021, loss = 2.70520785\n",
      "Iteration 3022, loss = 2.70513524\n",
      "Iteration 3023, loss = 2.70515218\n",
      "Iteration 3024, loss = 2.70507194\n",
      "Iteration 3025, loss = 2.70502743\n",
      "Iteration 3026, loss = 2.70505030\n",
      "Iteration 3027, loss = 2.70500747\n",
      "Iteration 3028, loss = 2.70495073\n",
      "Iteration 3029, loss = 2.70490234\n",
      "Iteration 3030, loss = 2.70491352\n",
      "Iteration 3031, loss = 2.70484406\n",
      "Iteration 3032, loss = 2.70485189\n",
      "Iteration 3033, loss = 2.70478714\n",
      "Iteration 3034, loss = 2.70473716\n",
      "Iteration 3035, loss = 2.70474492\n",
      "Iteration 3036, loss = 2.70464109\n",
      "Iteration 3037, loss = 2.70467258\n",
      "Iteration 3038, loss = 2.70460398\n",
      "Iteration 3039, loss = 2.70459706\n",
      "Iteration 3040, loss = 2.70454267\n",
      "Iteration 3041, loss = 2.70451057\n",
      "Iteration 3042, loss = 2.70450266\n",
      "Iteration 3043, loss = 2.70447961\n",
      "Iteration 3044, loss = 2.70443120\n",
      "Iteration 3045, loss = 2.70439784\n",
      "Iteration 3046, loss = 2.70435308\n",
      "Iteration 3047, loss = 2.70430525\n",
      "Iteration 3048, loss = 2.70425273\n",
      "Iteration 3049, loss = 2.70424820\n",
      "Iteration 3050, loss = 2.70425242\n",
      "Iteration 3051, loss = 2.70415922\n",
      "Iteration 3052, loss = 2.70421012\n",
      "Iteration 3053, loss = 2.70411701\n",
      "Iteration 3054, loss = 2.70405827\n",
      "Iteration 3055, loss = 2.70412189\n",
      "Iteration 3056, loss = 2.70403585\n",
      "Iteration 3057, loss = 2.70398339\n",
      "Iteration 3058, loss = 2.70392059\n",
      "Iteration 3059, loss = 2.70391577\n",
      "Iteration 3060, loss = 2.70394421\n",
      "Iteration 3061, loss = 2.70383886\n",
      "Iteration 3062, loss = 2.70380584\n",
      "Iteration 3063, loss = 2.70381038\n",
      "Iteration 3064, loss = 2.70377191\n",
      "Iteration 3065, loss = 2.70369227\n",
      "Iteration 3066, loss = 2.70369930\n",
      "Iteration 3067, loss = 2.70365070\n",
      "Iteration 3068, loss = 2.70359306\n",
      "Iteration 3069, loss = 2.70360288\n",
      "Iteration 3070, loss = 2.70354222\n",
      "Iteration 3071, loss = 2.70351017\n",
      "Iteration 3072, loss = 2.70349349\n",
      "Iteration 3073, loss = 2.70351704\n",
      "Iteration 3074, loss = 2.70342242\n",
      "Iteration 3075, loss = 2.70337407\n",
      "Iteration 3076, loss = 2.70336862\n",
      "Iteration 3077, loss = 2.70333198\n",
      "Iteration 3078, loss = 2.70329364\n",
      "Iteration 3079, loss = 2.70322806\n",
      "Iteration 3080, loss = 2.70325743\n",
      "Iteration 3081, loss = 2.70317744\n",
      "Iteration 3082, loss = 2.70311400\n",
      "Iteration 3083, loss = 2.70315124\n",
      "Iteration 3084, loss = 2.70317891\n",
      "Iteration 3085, loss = 2.70304172\n",
      "Iteration 3086, loss = 2.70301223\n",
      "Iteration 3087, loss = 2.70300139\n",
      "Iteration 3088, loss = 2.70298285\n",
      "Iteration 3089, loss = 2.70294478\n",
      "Iteration 3090, loss = 2.70290256\n",
      "Iteration 3091, loss = 2.70286155\n",
      "Iteration 3092, loss = 2.70287276\n",
      "Iteration 3093, loss = 2.70280690\n",
      "Iteration 3094, loss = 2.70278811\n",
      "Iteration 3095, loss = 2.70272358\n",
      "Iteration 3096, loss = 2.70271379\n",
      "Iteration 3097, loss = 2.70272183\n",
      "Iteration 3098, loss = 2.70261594\n",
      "Iteration 3099, loss = 2.70264979\n",
      "Iteration 3100, loss = 2.70258160\n",
      "Iteration 3101, loss = 2.70255385\n",
      "Iteration 3102, loss = 2.70250116\n",
      "Iteration 3103, loss = 2.70248512\n",
      "Iteration 3104, loss = 2.70243691\n",
      "Iteration 3105, loss = 2.70239651\n",
      "Iteration 3106, loss = 2.70238808\n",
      "Iteration 3107, loss = 2.70236658\n",
      "Iteration 3108, loss = 2.70233752\n",
      "Iteration 3109, loss = 2.70225200\n",
      "Iteration 3110, loss = 2.70225584\n",
      "Iteration 3111, loss = 2.70223954\n",
      "Iteration 3112, loss = 2.70222408\n",
      "Iteration 3113, loss = 2.70214980\n",
      "Iteration 3114, loss = 2.70216968\n",
      "Iteration 3115, loss = 2.70210215\n",
      "Iteration 3116, loss = 2.70205752\n",
      "Iteration 3117, loss = 2.70205879\n",
      "Iteration 3118, loss = 2.70200683\n",
      "Iteration 3119, loss = 2.70193994\n",
      "Iteration 3120, loss = 2.70195990\n",
      "Iteration 3121, loss = 2.70187151\n",
      "Iteration 3122, loss = 2.70188256\n",
      "Iteration 3123, loss = 2.70186558\n",
      "Iteration 3124, loss = 2.70181014\n",
      "Iteration 3125, loss = 2.70181399\n",
      "Iteration 3126, loss = 2.70172617\n",
      "Iteration 3127, loss = 2.70170287\n",
      "Iteration 3128, loss = 2.70165511\n",
      "Iteration 3129, loss = 2.70163708\n",
      "Iteration 3130, loss = 2.70159766\n",
      "Iteration 3131, loss = 2.70158418\n",
      "Iteration 3132, loss = 2.70156516\n",
      "Iteration 3133, loss = 2.70154631\n",
      "Iteration 3134, loss = 2.70146593\n",
      "Iteration 3135, loss = 2.70144366\n",
      "Iteration 3136, loss = 2.70138524\n",
      "Iteration 3137, loss = 2.70138949\n",
      "Iteration 3138, loss = 2.70139507\n",
      "Iteration 3139, loss = 2.70129533\n",
      "Iteration 3140, loss = 2.70129352\n",
      "Iteration 3141, loss = 2.70127090\n",
      "Iteration 3142, loss = 2.70120863\n",
      "Iteration 3143, loss = 2.70118657\n",
      "Iteration 3144, loss = 2.70115893\n",
      "Iteration 3145, loss = 2.70110113\n",
      "Iteration 3146, loss = 2.70106429\n",
      "Iteration 3147, loss = 2.70108075\n",
      "Iteration 3148, loss = 2.70100230\n",
      "Iteration 3149, loss = 2.70100018\n",
      "Iteration 3150, loss = 2.70099112\n",
      "Iteration 3151, loss = 2.70094352\n",
      "Iteration 3152, loss = 2.70091609\n",
      "Iteration 3153, loss = 2.70086495\n",
      "Iteration 3154, loss = 2.70085639\n",
      "Iteration 3155, loss = 2.70078256\n",
      "Iteration 3156, loss = 2.70074915\n",
      "Iteration 3157, loss = 2.70072823\n",
      "Iteration 3158, loss = 2.70069210\n",
      "Iteration 3159, loss = 2.70068411\n",
      "Iteration 3160, loss = 2.70062515\n",
      "Iteration 3161, loss = 2.70065997\n",
      "Iteration 3162, loss = 2.70056393\n",
      "Iteration 3163, loss = 2.70050807\n",
      "Iteration 3164, loss = 2.70051621\n",
      "Iteration 3165, loss = 2.70048025\n",
      "Iteration 3166, loss = 2.70044604\n",
      "Iteration 3167, loss = 2.70044061\n",
      "Iteration 3168, loss = 2.70038326\n",
      "Iteration 3169, loss = 2.70033034\n",
      "Iteration 3170, loss = 2.70034786\n",
      "Iteration 3171, loss = 2.70029254\n",
      "Iteration 3172, loss = 2.70023959\n",
      "Iteration 3173, loss = 2.70020151\n",
      "Iteration 3174, loss = 2.70021749\n",
      "Iteration 3175, loss = 2.70017000\n",
      "Iteration 3176, loss = 2.70013753\n",
      "Iteration 3177, loss = 2.70010443\n",
      "Iteration 3178, loss = 2.70010495\n",
      "Iteration 3179, loss = 2.70001968\n",
      "Iteration 3180, loss = 2.69998432\n",
      "Iteration 3181, loss = 2.70003849\n",
      "Iteration 3182, loss = 2.69994657\n",
      "Iteration 3183, loss = 2.69992598\n",
      "Iteration 3184, loss = 2.69984594\n",
      "Iteration 3185, loss = 2.69985363\n",
      "Iteration 3186, loss = 2.69981853\n",
      "Iteration 3187, loss = 2.69975061\n",
      "Iteration 3188, loss = 2.69973302\n",
      "Iteration 3189, loss = 2.69975884\n",
      "Iteration 3190, loss = 2.69971002\n",
      "Iteration 3191, loss = 2.69971856\n",
      "Iteration 3192, loss = 2.69963673\n",
      "Iteration 3193, loss = 2.69956772\n",
      "Iteration 3194, loss = 2.69959688\n",
      "Iteration 3195, loss = 2.69950917\n",
      "Iteration 3196, loss = 2.69949063\n",
      "Iteration 3197, loss = 2.69947435\n",
      "Iteration 3198, loss = 2.69947406\n",
      "Iteration 3199, loss = 2.69939943\n",
      "Iteration 3200, loss = 2.69937626\n",
      "Iteration 3201, loss = 2.69941085\n",
      "Iteration 3202, loss = 2.69929972\n",
      "Iteration 3203, loss = 2.69927897\n",
      "Iteration 3204, loss = 2.69924708\n",
      "Iteration 3205, loss = 2.69921185\n",
      "Iteration 3206, loss = 2.69917654\n",
      "Iteration 3207, loss = 2.69914661\n",
      "Iteration 3208, loss = 2.69912911\n",
      "Iteration 3209, loss = 2.69911652\n",
      "Iteration 3210, loss = 2.69906728\n",
      "Iteration 3211, loss = 2.69905637\n",
      "Iteration 3212, loss = 2.69900077\n",
      "Iteration 3213, loss = 2.69898101\n",
      "Iteration 3214, loss = 2.69893983\n",
      "Iteration 3215, loss = 2.69893063\n",
      "Iteration 3216, loss = 2.69887898\n",
      "Iteration 3217, loss = 2.69886792\n",
      "Iteration 3218, loss = 2.69881630\n",
      "Iteration 3219, loss = 2.69876531\n",
      "Iteration 3220, loss = 2.69871077\n",
      "Iteration 3221, loss = 2.69870248\n",
      "Iteration 3222, loss = 2.69867221\n",
      "Iteration 3223, loss = 2.69867051\n",
      "Iteration 3224, loss = 2.69867971\n",
      "Iteration 3225, loss = 2.69861065\n",
      "Iteration 3226, loss = 2.69855865\n",
      "Iteration 3227, loss = 2.69849502\n",
      "Iteration 3228, loss = 2.69847174\n",
      "Iteration 3229, loss = 2.69845788\n",
      "Iteration 3230, loss = 2.69843964\n",
      "Iteration 3231, loss = 2.69839759\n",
      "Iteration 3232, loss = 2.69834553\n",
      "Iteration 3233, loss = 2.69835051\n",
      "Iteration 3234, loss = 2.69831570\n",
      "Iteration 3235, loss = 2.69831027\n",
      "Iteration 3236, loss = 2.69827636\n",
      "Iteration 3237, loss = 2.69823580\n",
      "Iteration 3238, loss = 2.69823495\n",
      "Iteration 3239, loss = 2.69817454\n",
      "Iteration 3240, loss = 2.69808652\n",
      "Iteration 3241, loss = 2.69809862\n",
      "Iteration 3242, loss = 2.69806218\n",
      "Iteration 3243, loss = 2.69804365\n",
      "Iteration 3244, loss = 2.69802561\n",
      "Iteration 3245, loss = 2.69796163\n",
      "Iteration 3246, loss = 2.69793630\n",
      "Iteration 3247, loss = 2.69792014\n",
      "Iteration 3248, loss = 2.69789537\n",
      "Iteration 3249, loss = 2.69782731\n",
      "Iteration 3250, loss = 2.69784863\n",
      "Iteration 3251, loss = 2.69777338\n",
      "Iteration 3252, loss = 2.69775750\n",
      "Iteration 3253, loss = 2.69772645\n",
      "Iteration 3254, loss = 2.69768560\n",
      "Iteration 3255, loss = 2.69763664\n",
      "Iteration 3256, loss = 2.69765061\n",
      "Iteration 3257, loss = 2.69760128\n",
      "Iteration 3258, loss = 2.69755594\n",
      "Iteration 3259, loss = 2.69755615\n",
      "Iteration 3260, loss = 2.69749507\n",
      "Iteration 3261, loss = 2.69747203\n",
      "Iteration 3262, loss = 2.69746159\n",
      "Iteration 3263, loss = 2.69741597\n",
      "Iteration 3264, loss = 2.69735684\n",
      "Iteration 3265, loss = 2.69735799\n",
      "Iteration 3266, loss = 2.69729158\n",
      "Iteration 3267, loss = 2.69732925\n",
      "Iteration 3268, loss = 2.69731849\n",
      "Iteration 3269, loss = 2.69723549\n",
      "Iteration 3270, loss = 2.69719306\n",
      "Iteration 3271, loss = 2.69717020\n",
      "Iteration 3272, loss = 2.69711469\n",
      "Iteration 3273, loss = 2.69714410\n",
      "Iteration 3274, loss = 2.69707985\n",
      "Iteration 3275, loss = 2.69702757\n",
      "Iteration 3276, loss = 2.69698888\n",
      "Iteration 3277, loss = 2.69702143\n",
      "Iteration 3278, loss = 2.69693251\n",
      "Iteration 3279, loss = 2.69700162\n",
      "Iteration 3280, loss = 2.69686187\n",
      "Iteration 3281, loss = 2.69685052\n",
      "Iteration 3282, loss = 2.69681010\n",
      "Iteration 3283, loss = 2.69683343\n",
      "Iteration 3284, loss = 2.69675741\n",
      "Iteration 3285, loss = 2.69672492\n",
      "Iteration 3286, loss = 2.69674582\n",
      "Iteration 3287, loss = 2.69666118\n",
      "Iteration 3288, loss = 2.69664572\n",
      "Iteration 3289, loss = 2.69661419\n",
      "Iteration 3290, loss = 2.69657092\n",
      "Iteration 3291, loss = 2.69655908\n",
      "Iteration 3292, loss = 2.69646887\n",
      "Iteration 3293, loss = 2.69652154\n",
      "Iteration 3294, loss = 2.69647614\n",
      "Iteration 3295, loss = 2.69642377\n",
      "Iteration 3296, loss = 2.69640342\n",
      "Iteration 3297, loss = 2.69637605\n",
      "Iteration 3298, loss = 2.69635398\n",
      "Iteration 3299, loss = 2.69630164\n",
      "Iteration 3300, loss = 2.69625585\n",
      "Iteration 3301, loss = 2.69624481\n",
      "Iteration 3302, loss = 2.69619532\n",
      "Iteration 3303, loss = 2.69617343\n",
      "Iteration 3304, loss = 2.69614950\n",
      "Iteration 3305, loss = 2.69611861\n",
      "Iteration 3306, loss = 2.69608817\n",
      "Iteration 3307, loss = 2.69603748\n",
      "Iteration 3308, loss = 2.69601168\n",
      "Iteration 3309, loss = 2.69597584\n",
      "Iteration 3310, loss = 2.69597447\n",
      "Iteration 3311, loss = 2.69603889\n",
      "Iteration 3312, loss = 2.69595618\n",
      "Iteration 3313, loss = 2.69584059\n",
      "Iteration 3314, loss = 2.69583507\n",
      "Iteration 3315, loss = 2.69582598\n",
      "Iteration 3316, loss = 2.69575581\n",
      "Iteration 3317, loss = 2.69573407\n",
      "Iteration 3318, loss = 2.69573511\n",
      "Iteration 3319, loss = 2.69569965\n",
      "Iteration 3320, loss = 2.69565685\n",
      "Iteration 3321, loss = 2.69562415\n",
      "Iteration 3322, loss = 2.69563741\n",
      "Iteration 3323, loss = 2.69558210\n",
      "Iteration 3324, loss = 2.69555258\n",
      "Iteration 3325, loss = 2.69551655\n",
      "Iteration 3326, loss = 2.69552917\n",
      "Iteration 3327, loss = 2.69546500\n",
      "Iteration 3328, loss = 2.69543249\n",
      "Iteration 3329, loss = 2.69536627\n",
      "Iteration 3330, loss = 2.69538930\n",
      "Iteration 3331, loss = 2.69532961\n",
      "Iteration 3332, loss = 2.69527510\n",
      "Iteration 3333, loss = 2.69526028\n",
      "Iteration 3334, loss = 2.69523073\n",
      "Iteration 3335, loss = 2.69518847\n",
      "Iteration 3336, loss = 2.69519727\n",
      "Iteration 3337, loss = 2.69512369\n",
      "Iteration 3338, loss = 2.69514731\n",
      "Iteration 3339, loss = 2.69507374\n",
      "Iteration 3340, loss = 2.69506468\n",
      "Iteration 3341, loss = 2.69505367\n",
      "Iteration 3342, loss = 2.69498847\n",
      "Iteration 3343, loss = 2.69496217\n",
      "Iteration 3344, loss = 2.69499456\n",
      "Iteration 3345, loss = 2.69492221\n",
      "Iteration 3346, loss = 2.69489535\n",
      "Iteration 3347, loss = 2.69483064\n",
      "Iteration 3348, loss = 2.69485422\n",
      "Iteration 3349, loss = 2.69479751\n",
      "Iteration 3350, loss = 2.69472433\n",
      "Iteration 3351, loss = 2.69470199\n",
      "Iteration 3352, loss = 2.69467412\n",
      "Iteration 3353, loss = 2.69463236\n",
      "Iteration 3354, loss = 2.69464926\n",
      "Iteration 3355, loss = 2.69461373\n",
      "Iteration 3356, loss = 2.69458278\n",
      "Iteration 3357, loss = 2.69455312\n",
      "Iteration 3358, loss = 2.69453135\n",
      "Iteration 3359, loss = 2.69450639\n",
      "Iteration 3360, loss = 2.69445611\n",
      "Iteration 3361, loss = 2.69443653\n",
      "Iteration 3362, loss = 2.69440372\n",
      "Iteration 3363, loss = 2.69437882\n",
      "Iteration 3364, loss = 2.69435453\n",
      "Iteration 3365, loss = 2.69431780\n",
      "Iteration 3366, loss = 2.69430559\n",
      "Iteration 3367, loss = 2.69430342\n",
      "Iteration 3368, loss = 2.69421797\n",
      "Iteration 3369, loss = 2.69420304\n",
      "Iteration 3370, loss = 2.69418308\n",
      "Iteration 3371, loss = 2.69413882\n",
      "Iteration 3372, loss = 2.69408502\n",
      "Iteration 3373, loss = 2.69405218\n",
      "Iteration 3374, loss = 2.69406263\n",
      "Iteration 3375, loss = 2.69398628\n",
      "Iteration 3376, loss = 2.69399400\n",
      "Iteration 3377, loss = 2.69394739\n",
      "Iteration 3378, loss = 2.69391750\n",
      "Iteration 3379, loss = 2.69387759\n",
      "Iteration 3380, loss = 2.69386023\n",
      "Iteration 3381, loss = 2.69383638\n",
      "Iteration 3382, loss = 2.69379083\n",
      "Iteration 3383, loss = 2.69384386\n",
      "Iteration 3384, loss = 2.69374306\n",
      "Iteration 3385, loss = 2.69371839\n",
      "Iteration 3386, loss = 2.69373823\n",
      "Iteration 3387, loss = 2.69366990\n",
      "Iteration 3388, loss = 2.69362490\n",
      "Iteration 3389, loss = 2.69361787\n",
      "Iteration 3390, loss = 2.69362048\n",
      "Iteration 3391, loss = 2.69355684\n",
      "Iteration 3392, loss = 2.69350709\n",
      "Iteration 3393, loss = 2.69353544\n",
      "Iteration 3394, loss = 2.69356461\n",
      "Iteration 3395, loss = 2.69340623\n",
      "Iteration 3396, loss = 2.69337298\n",
      "Iteration 3397, loss = 2.69342683\n",
      "Iteration 3398, loss = 2.69338210\n",
      "Iteration 3399, loss = 2.69335436\n",
      "Iteration 3400, loss = 2.69326747\n",
      "Iteration 3401, loss = 2.69323803\n",
      "Iteration 3402, loss = 2.69319207\n",
      "Iteration 3403, loss = 2.69321102\n",
      "Iteration 3404, loss = 2.69315501\n",
      "Iteration 3405, loss = 2.69310175\n",
      "Iteration 3406, loss = 2.69313955\n",
      "Iteration 3407, loss = 2.69307067\n",
      "Iteration 3408, loss = 2.69305128\n",
      "Iteration 3409, loss = 2.69298820\n",
      "Iteration 3410, loss = 2.69302623\n",
      "Iteration 3411, loss = 2.69294182\n",
      "Iteration 3412, loss = 2.69291825\n",
      "Iteration 3413, loss = 2.69286708\n",
      "Iteration 3414, loss = 2.69286974\n",
      "Iteration 3415, loss = 2.69282680\n",
      "Iteration 3416, loss = 2.69278177\n",
      "Iteration 3417, loss = 2.69275658\n",
      "Iteration 3418, loss = 2.69270518\n",
      "Iteration 3419, loss = 2.69270745\n",
      "Iteration 3420, loss = 2.69265865\n",
      "Iteration 3421, loss = 2.69267853\n",
      "Iteration 3422, loss = 2.69259259\n",
      "Iteration 3423, loss = 2.69258920\n",
      "Iteration 3424, loss = 2.69255485\n",
      "Iteration 3425, loss = 2.69255030\n",
      "Iteration 3426, loss = 2.69252633\n",
      "Iteration 3427, loss = 2.69249296\n",
      "Iteration 3428, loss = 2.69245021\n",
      "Iteration 3429, loss = 2.69240674\n",
      "Iteration 3430, loss = 2.69235267\n",
      "Iteration 3431, loss = 2.69238845\n",
      "Iteration 3432, loss = 2.69231471\n",
      "Iteration 3433, loss = 2.69227444\n",
      "Iteration 3434, loss = 2.69228225\n",
      "Iteration 3435, loss = 2.69226113\n",
      "Iteration 3436, loss = 2.69220113\n",
      "Iteration 3437, loss = 2.69217944\n",
      "Iteration 3438, loss = 2.69216538\n",
      "Iteration 3439, loss = 2.69213526\n",
      "Iteration 3440, loss = 2.69207249\n",
      "Iteration 3441, loss = 2.69204867\n",
      "Iteration 3442, loss = 2.69205009\n",
      "Iteration 3443, loss = 2.69202487\n",
      "Iteration 3444, loss = 2.69201211\n",
      "Iteration 3445, loss = 2.69195865\n",
      "Iteration 3446, loss = 2.69190996\n",
      "Iteration 3447, loss = 2.69191830\n",
      "Iteration 3448, loss = 2.69185440\n",
      "Iteration 3449, loss = 2.69181039\n",
      "Iteration 3450, loss = 2.69179181\n",
      "Iteration 3451, loss = 2.69178458\n",
      "Iteration 3452, loss = 2.69174047\n",
      "Iteration 3453, loss = 2.69171569\n",
      "Iteration 3454, loss = 2.69168618\n",
      "Iteration 3455, loss = 2.69165895\n",
      "Iteration 3456, loss = 2.69164354\n",
      "Iteration 3457, loss = 2.69161638\n",
      "Iteration 3458, loss = 2.69157958\n",
      "Iteration 3459, loss = 2.69156101\n",
      "Iteration 3460, loss = 2.69149417\n",
      "Iteration 3461, loss = 2.69155043\n",
      "Iteration 3462, loss = 2.69144839\n",
      "Iteration 3463, loss = 2.69139990\n",
      "Iteration 3464, loss = 2.69138286\n",
      "Iteration 3465, loss = 2.69137492\n",
      "Iteration 3466, loss = 2.69133712\n",
      "Iteration 3467, loss = 2.69128303\n",
      "Iteration 3468, loss = 2.69126841\n",
      "Iteration 3469, loss = 2.69128977\n",
      "Iteration 3470, loss = 2.69126272\n",
      "Iteration 3471, loss = 2.69120021\n",
      "Iteration 3472, loss = 2.69118237\n",
      "Iteration 3473, loss = 2.69110083\n",
      "Iteration 3474, loss = 2.69109080\n",
      "Iteration 3475, loss = 2.69106586\n",
      "Iteration 3476, loss = 2.69106900\n",
      "Iteration 3477, loss = 2.69099894\n",
      "Iteration 3478, loss = 2.69098398\n",
      "Iteration 3479, loss = 2.69104339\n",
      "Iteration 3480, loss = 2.69110038\n",
      "Iteration 3481, loss = 2.69088714\n",
      "Iteration 3482, loss = 2.69086388\n",
      "Iteration 3483, loss = 2.69089097\n",
      "Iteration 3484, loss = 2.69080505\n",
      "Iteration 3485, loss = 2.69078306\n",
      "Iteration 3486, loss = 2.69086996\n",
      "Iteration 3487, loss = 2.69076131\n",
      "Iteration 3488, loss = 2.69068565\n",
      "Iteration 3489, loss = 2.69069193\n",
      "Iteration 3490, loss = 2.69066728\n",
      "Iteration 3491, loss = 2.69060033\n",
      "Iteration 3492, loss = 2.69054217\n",
      "Iteration 3493, loss = 2.69057381\n",
      "Iteration 3494, loss = 2.69068856\n",
      "Iteration 3495, loss = 2.69048739\n",
      "Iteration 3496, loss = 2.69052212\n",
      "Iteration 3497, loss = 2.69046291\n",
      "Iteration 3498, loss = 2.69042855\n",
      "Iteration 3499, loss = 2.69038242\n",
      "Iteration 3500, loss = 2.69035318\n",
      "Iteration 3501, loss = 2.69031570\n",
      "Iteration 3502, loss = 2.69036053\n",
      "Iteration 3503, loss = 2.69027756\n",
      "Iteration 3504, loss = 2.69030637\n",
      "Iteration 3505, loss = 2.69023807\n",
      "Iteration 3506, loss = 2.69018079\n",
      "Iteration 3507, loss = 2.69016786\n",
      "Iteration 3508, loss = 2.69012227\n",
      "Iteration 3509, loss = 2.69009040\n",
      "Iteration 3510, loss = 2.69010218\n",
      "Iteration 3511, loss = 2.69004802\n",
      "Iteration 3512, loss = 2.69007122\n",
      "Iteration 3513, loss = 2.69000506\n",
      "Iteration 3514, loss = 2.68993101\n",
      "Iteration 3515, loss = 2.68991897\n",
      "Iteration 3516, loss = 2.68995223\n",
      "Iteration 3517, loss = 2.68988574\n",
      "Iteration 3518, loss = 2.68982884\n",
      "Iteration 3519, loss = 2.68983366\n",
      "Iteration 3520, loss = 2.68981184\n",
      "Iteration 3521, loss = 2.68975119\n",
      "Iteration 3522, loss = 2.68971117\n",
      "Iteration 3523, loss = 2.68972345\n",
      "Iteration 3524, loss = 2.68969538\n",
      "Iteration 3525, loss = 2.68964481\n",
      "Iteration 3526, loss = 2.68967499\n",
      "Iteration 3527, loss = 2.68958429\n",
      "Iteration 3528, loss = 2.68957588\n",
      "Iteration 3529, loss = 2.68952349\n",
      "Iteration 3530, loss = 2.68950476\n",
      "Iteration 3531, loss = 2.68948490\n",
      "Iteration 3532, loss = 2.68943885\n",
      "Iteration 3533, loss = 2.68941153\n",
      "Iteration 3534, loss = 2.68938519\n",
      "Iteration 3535, loss = 2.68936286\n",
      "Iteration 3536, loss = 2.68935905\n",
      "Iteration 3537, loss = 2.68928739\n",
      "Iteration 3538, loss = 2.68929687\n",
      "Iteration 3539, loss = 2.68925676\n",
      "Iteration 3540, loss = 2.68924796\n",
      "Iteration 3541, loss = 2.68918016\n",
      "Iteration 3542, loss = 2.68914706\n",
      "Iteration 3543, loss = 2.68916304\n",
      "Iteration 3544, loss = 2.68908827\n",
      "Iteration 3545, loss = 2.68909436\n",
      "Iteration 3546, loss = 2.68902917\n",
      "Iteration 3547, loss = 2.68902844\n",
      "Iteration 3548, loss = 2.68901571\n",
      "Iteration 3549, loss = 2.68898104\n",
      "Iteration 3550, loss = 2.68898964\n",
      "Iteration 3551, loss = 2.68892950\n",
      "Iteration 3552, loss = 2.68891667\n",
      "Iteration 3553, loss = 2.68891717\n",
      "Iteration 3554, loss = 2.68883165\n",
      "Iteration 3555, loss = 2.68881051\n",
      "Iteration 3556, loss = 2.68881294\n",
      "Iteration 3557, loss = 2.68872807\n",
      "Iteration 3558, loss = 2.68869959\n",
      "Iteration 3559, loss = 2.68870406\n",
      "Iteration 3560, loss = 2.68870280\n",
      "Iteration 3561, loss = 2.68865614\n",
      "Iteration 3562, loss = 2.68865900\n",
      "Iteration 3563, loss = 2.68862814\n",
      "Iteration 3564, loss = 2.68853618\n",
      "Iteration 3565, loss = 2.68855126\n",
      "Iteration 3566, loss = 2.68851312\n",
      "Iteration 3567, loss = 2.68845158\n",
      "Iteration 3568, loss = 2.68847079\n",
      "Iteration 3569, loss = 2.68844038\n",
      "Iteration 3570, loss = 2.68835168\n",
      "Iteration 3571, loss = 2.68834925\n",
      "Iteration 3572, loss = 2.68832169\n",
      "Iteration 3573, loss = 2.68831636\n",
      "Iteration 3574, loss = 2.68825071\n",
      "Iteration 3575, loss = 2.68828249\n",
      "Iteration 3576, loss = 2.68821328\n",
      "Iteration 3577, loss = 2.68816522\n",
      "Iteration 3578, loss = 2.68819324\n",
      "Iteration 3579, loss = 2.68819441\n",
      "Iteration 3580, loss = 2.68812278\n",
      "Iteration 3581, loss = 2.68807832\n",
      "Iteration 3582, loss = 2.68802885\n",
      "Iteration 3583, loss = 2.68803818\n",
      "Iteration 3584, loss = 2.68799410\n",
      "Iteration 3585, loss = 2.68796705\n",
      "Iteration 3586, loss = 2.68792488\n",
      "Iteration 3587, loss = 2.68789815\n",
      "Iteration 3588, loss = 2.68789641\n",
      "Iteration 3589, loss = 2.68785403\n",
      "Iteration 3590, loss = 2.68779910\n",
      "Iteration 3591, loss = 2.68778921\n",
      "Iteration 3592, loss = 2.68775367\n",
      "Iteration 3593, loss = 2.68774313\n",
      "Iteration 3594, loss = 2.68769982\n",
      "Iteration 3595, loss = 2.68765613\n",
      "Iteration 3596, loss = 2.68767631\n",
      "Iteration 3597, loss = 2.68762069\n",
      "Iteration 3598, loss = 2.68763792\n",
      "Iteration 3599, loss = 2.68755189\n",
      "Iteration 3600, loss = 2.68758762\n",
      "Iteration 3601, loss = 2.68753699\n",
      "Iteration 3602, loss = 2.68749821\n",
      "Iteration 3603, loss = 2.68744148\n",
      "Iteration 3604, loss = 2.68744978\n",
      "Iteration 3605, loss = 2.68739971\n",
      "Iteration 3606, loss = 2.68741852\n",
      "Iteration 3607, loss = 2.68735895\n",
      "Iteration 3608, loss = 2.68733435\n",
      "Iteration 3609, loss = 2.68726435\n",
      "Iteration 3610, loss = 2.68722667\n",
      "Iteration 3611, loss = 2.68722735\n",
      "Iteration 3612, loss = 2.68722541\n",
      "Iteration 3613, loss = 2.68719497\n",
      "Iteration 3614, loss = 2.68723445\n",
      "Iteration 3615, loss = 2.68712857\n",
      "Iteration 3616, loss = 2.68711005\n",
      "Iteration 3617, loss = 2.68706345\n",
      "Iteration 3618, loss = 2.68700974\n",
      "Iteration 3619, loss = 2.68699856\n",
      "Iteration 3620, loss = 2.68695063\n",
      "Iteration 3621, loss = 2.68694910\n",
      "Iteration 3622, loss = 2.68693323\n",
      "Iteration 3623, loss = 2.68692511\n",
      "Iteration 3624, loss = 2.68688619\n",
      "Iteration 3625, loss = 2.68686458\n",
      "Iteration 3626, loss = 2.68685048\n",
      "Iteration 3627, loss = 2.68680760\n",
      "Iteration 3628, loss = 2.68677087\n",
      "Iteration 3629, loss = 2.68674367\n",
      "Iteration 3630, loss = 2.68670963\n",
      "Iteration 3631, loss = 2.68671251\n",
      "Iteration 3632, loss = 2.68663146\n",
      "Iteration 3633, loss = 2.68661335\n",
      "Iteration 3634, loss = 2.68660700\n",
      "Iteration 3635, loss = 2.68655541\n",
      "Iteration 3636, loss = 2.68654627\n",
      "Iteration 3637, loss = 2.68650761\n",
      "Iteration 3638, loss = 2.68655176\n",
      "Iteration 3639, loss = 2.68643977\n",
      "Iteration 3640, loss = 2.68641753\n",
      "Iteration 3641, loss = 2.68640435\n",
      "Iteration 3642, loss = 2.68636003\n",
      "Iteration 3643, loss = 2.68642523\n",
      "Iteration 3644, loss = 2.68633999\n",
      "Iteration 3645, loss = 2.68630639\n",
      "Iteration 3646, loss = 2.68634511\n",
      "Iteration 3647, loss = 2.68619782\n",
      "Iteration 3648, loss = 2.68623296\n",
      "Iteration 3649, loss = 2.68622764\n",
      "Iteration 3650, loss = 2.68613662\n",
      "Iteration 3651, loss = 2.68616817\n",
      "Iteration 3652, loss = 2.68610608\n",
      "Iteration 3653, loss = 2.68606974\n",
      "Iteration 3654, loss = 2.68608167\n",
      "Iteration 3655, loss = 2.68601015\n",
      "Iteration 3656, loss = 2.68598042\n",
      "Iteration 3657, loss = 2.68597181\n",
      "Iteration 3658, loss = 2.68597340\n",
      "Iteration 3659, loss = 2.68592119\n",
      "Iteration 3660, loss = 2.68586297\n",
      "Iteration 3661, loss = 2.68586304\n",
      "Iteration 3662, loss = 2.68584642\n",
      "Iteration 3663, loss = 2.68580324\n",
      "Iteration 3664, loss = 2.68577738\n",
      "Iteration 3665, loss = 2.68575757\n",
      "Iteration 3666, loss = 2.68571530\n",
      "Iteration 3667, loss = 2.68566085\n",
      "Iteration 3668, loss = 2.68565647\n",
      "Iteration 3669, loss = 2.68560323\n",
      "Iteration 3670, loss = 2.68559417\n",
      "Iteration 3671, loss = 2.68560437\n",
      "Iteration 3672, loss = 2.68556429\n",
      "Iteration 3673, loss = 2.68553928\n",
      "Iteration 3674, loss = 2.68550921\n",
      "Iteration 3675, loss = 2.68549431\n",
      "Iteration 3676, loss = 2.68544035\n",
      "Iteration 3677, loss = 2.68542913\n",
      "Iteration 3678, loss = 2.68539919\n",
      "Iteration 3679, loss = 2.68537105\n",
      "Iteration 3680, loss = 2.68535336\n",
      "Iteration 3681, loss = 2.68531159\n",
      "Iteration 3682, loss = 2.68530093\n",
      "Iteration 3683, loss = 2.68524583\n",
      "Iteration 3684, loss = 2.68527081\n",
      "Iteration 3685, loss = 2.68516931\n",
      "Iteration 3686, loss = 2.68520070\n",
      "Iteration 3687, loss = 2.68512782\n",
      "Iteration 3688, loss = 2.68514041\n",
      "Iteration 3689, loss = 2.68507778\n",
      "Iteration 3690, loss = 2.68506208\n",
      "Iteration 3691, loss = 2.68502926\n",
      "Iteration 3692, loss = 2.68504655\n",
      "Iteration 3693, loss = 2.68501534\n",
      "Iteration 3694, loss = 2.68495707\n",
      "Iteration 3695, loss = 2.68493464\n",
      "Iteration 3696, loss = 2.68487777\n",
      "Iteration 3697, loss = 2.68488364\n",
      "Iteration 3698, loss = 2.68485831\n",
      "Iteration 3699, loss = 2.68482528\n",
      "Iteration 3700, loss = 2.68477255\n",
      "Iteration 3701, loss = 2.68477678\n",
      "Iteration 3702, loss = 2.68471412\n",
      "Iteration 3703, loss = 2.68471567\n",
      "Iteration 3704, loss = 2.68475298\n",
      "Iteration 3705, loss = 2.68467915\n",
      "Iteration 3706, loss = 2.68468057\n",
      "Iteration 3707, loss = 2.68462609\n",
      "Iteration 3708, loss = 2.68457608\n",
      "Iteration 3709, loss = 2.68455670\n",
      "Iteration 3710, loss = 2.68450242\n",
      "Iteration 3711, loss = 2.68454356\n",
      "Iteration 3712, loss = 2.68445507\n",
      "Iteration 3713, loss = 2.68445470\n",
      "Iteration 3714, loss = 2.68442428\n",
      "Iteration 3715, loss = 2.68438092\n",
      "Iteration 3716, loss = 2.68436363\n",
      "Iteration 3717, loss = 2.68431362\n",
      "Iteration 3718, loss = 2.68432805\n",
      "Iteration 3719, loss = 2.68427008\n",
      "Iteration 3720, loss = 2.68428425\n",
      "Iteration 3721, loss = 2.68424753\n",
      "Iteration 3722, loss = 2.68425336\n",
      "Iteration 3723, loss = 2.68417773\n",
      "Iteration 3724, loss = 2.68416844\n",
      "Iteration 3725, loss = 2.68415161\n",
      "Iteration 3726, loss = 2.68412157\n",
      "Iteration 3727, loss = 2.68407105\n",
      "Iteration 3728, loss = 2.68415693\n",
      "Iteration 3729, loss = 2.68402779\n",
      "Iteration 3730, loss = 2.68399296\n",
      "Iteration 3731, loss = 2.68395550\n",
      "Iteration 3732, loss = 2.68397944\n",
      "Iteration 3733, loss = 2.68394559\n",
      "Iteration 3734, loss = 2.68390962\n",
      "Iteration 3735, loss = 2.68387235\n",
      "Iteration 3736, loss = 2.68385172\n",
      "Iteration 3737, loss = 2.68381281\n",
      "Iteration 3738, loss = 2.68379252\n",
      "Iteration 3739, loss = 2.68376503\n",
      "Iteration 3740, loss = 2.68370613\n",
      "Iteration 3741, loss = 2.68370122\n",
      "Iteration 3742, loss = 2.68368932\n",
      "Iteration 3743, loss = 2.68362926\n",
      "Iteration 3744, loss = 2.68361054\n",
      "Iteration 3745, loss = 2.68365834\n",
      "Iteration 3746, loss = 2.68354681\n",
      "Iteration 3747, loss = 2.68354409\n",
      "Iteration 3748, loss = 2.68356230\n",
      "Iteration 3749, loss = 2.68353484\n",
      "Iteration 3750, loss = 2.68346334\n",
      "Iteration 3751, loss = 2.68345109\n",
      "Iteration 3752, loss = 2.68346467\n",
      "Iteration 3753, loss = 2.68340573\n",
      "Iteration 3754, loss = 2.68333918\n",
      "Iteration 3755, loss = 2.68334485\n",
      "Iteration 3756, loss = 2.68334485\n",
      "Iteration 3757, loss = 2.68330403\n",
      "Iteration 3758, loss = 2.68328064\n",
      "Iteration 3759, loss = 2.68320507\n",
      "Iteration 3760, loss = 2.68320916\n",
      "Iteration 3761, loss = 2.68316455\n",
      "Iteration 3762, loss = 2.68315657\n",
      "Iteration 3763, loss = 2.68312306\n",
      "Iteration 3764, loss = 2.68313833\n",
      "Iteration 3765, loss = 2.68305371\n",
      "Iteration 3766, loss = 2.68314479\n",
      "Iteration 3767, loss = 2.68308539\n",
      "Iteration 3768, loss = 2.68296076\n",
      "Iteration 3769, loss = 2.68292249\n",
      "Iteration 3770, loss = 2.68295622\n",
      "Iteration 3771, loss = 2.68291264\n",
      "Iteration 3772, loss = 2.68288027\n",
      "Iteration 3773, loss = 2.68284254\n",
      "Iteration 3774, loss = 2.68286460\n",
      "Iteration 3775, loss = 2.68277574\n",
      "Iteration 3776, loss = 2.68276635\n",
      "Iteration 3777, loss = 2.68283631\n",
      "Iteration 3778, loss = 2.68278992\n",
      "Iteration 3779, loss = 2.68271281\n",
      "Iteration 3780, loss = 2.68273306\n",
      "Iteration 3781, loss = 2.68270969\n",
      "Iteration 3782, loss = 2.68262154\n",
      "Iteration 3783, loss = 2.68256805\n",
      "Iteration 3784, loss = 2.68263606\n",
      "Iteration 3785, loss = 2.68261188\n",
      "Iteration 3786, loss = 2.68250727\n",
      "Iteration 3787, loss = 2.68246447\n",
      "Iteration 3788, loss = 2.68245790\n",
      "Iteration 3789, loss = 2.68241104\n",
      "Iteration 3790, loss = 2.68249208\n",
      "Iteration 3791, loss = 2.68240020\n",
      "Iteration 3792, loss = 2.68238257\n",
      "Iteration 3793, loss = 2.68240521\n",
      "Iteration 3794, loss = 2.68228293\n",
      "Iteration 3795, loss = 2.68225577\n",
      "Iteration 3796, loss = 2.68222682\n",
      "Iteration 3797, loss = 2.68221006\n",
      "Iteration 3798, loss = 2.68223918\n",
      "Iteration 3799, loss = 2.68218806\n",
      "Iteration 3800, loss = 2.68214529\n",
      "Iteration 3801, loss = 2.68212107\n",
      "Iteration 3802, loss = 2.68204987\n",
      "Iteration 3803, loss = 2.68208407\n",
      "Iteration 3804, loss = 2.68206343\n",
      "Iteration 3805, loss = 2.68198169\n",
      "Iteration 3806, loss = 2.68200552\n",
      "Iteration 3807, loss = 2.68200113\n",
      "Iteration 3808, loss = 2.68195130\n",
      "Iteration 3809, loss = 2.68192884\n",
      "Iteration 3810, loss = 2.68190034\n",
      "Iteration 3811, loss = 2.68187183\n",
      "Iteration 3812, loss = 2.68180124\n",
      "Iteration 3813, loss = 2.68178170\n",
      "Iteration 3814, loss = 2.68179467\n",
      "Iteration 3815, loss = 2.68174680\n",
      "Iteration 3816, loss = 2.68174517\n",
      "Iteration 3817, loss = 2.68170253\n",
      "Iteration 3818, loss = 2.68167724\n",
      "Iteration 3819, loss = 2.68166581\n",
      "Iteration 3820, loss = 2.68168523\n",
      "Iteration 3821, loss = 2.68159048\n",
      "Iteration 3822, loss = 2.68154988\n",
      "Iteration 3823, loss = 2.68155257\n",
      "Iteration 3824, loss = 2.68151597\n",
      "Iteration 3825, loss = 2.68149295\n",
      "Iteration 3826, loss = 2.68143444\n",
      "Iteration 3827, loss = 2.68140815\n",
      "Iteration 3828, loss = 2.68142622\n",
      "Iteration 3829, loss = 2.68142266\n",
      "Iteration 3830, loss = 2.68136142\n",
      "Iteration 3831, loss = 2.68136117\n",
      "Iteration 3832, loss = 2.68131734\n",
      "Iteration 3833, loss = 2.68128350\n",
      "Iteration 3834, loss = 2.68128935\n",
      "Iteration 3835, loss = 2.68129872\n",
      "Iteration 3836, loss = 2.68124406\n",
      "Iteration 3837, loss = 2.68115692\n",
      "Iteration 3838, loss = 2.68117502\n",
      "Iteration 3839, loss = 2.68116234\n",
      "Iteration 3840, loss = 2.68111365\n",
      "Iteration 3841, loss = 2.68110034\n",
      "Iteration 3842, loss = 2.68112549\n",
      "Iteration 3843, loss = 2.68106602\n",
      "Iteration 3844, loss = 2.68102448\n",
      "Iteration 3845, loss = 2.68102437\n",
      "Iteration 3846, loss = 2.68100272\n",
      "Iteration 3847, loss = 2.68094158\n",
      "Iteration 3848, loss = 2.68090735\n",
      "Iteration 3849, loss = 2.68094018\n",
      "Iteration 3850, loss = 2.68096195\n",
      "Iteration 3851, loss = 2.68088269\n",
      "Iteration 3852, loss = 2.68083484\n",
      "Iteration 3853, loss = 2.68079463\n",
      "Iteration 3854, loss = 2.68075820\n",
      "Iteration 3855, loss = 2.68071425\n",
      "Iteration 3856, loss = 2.68067649\n",
      "Iteration 3857, loss = 2.68064521\n",
      "Iteration 3858, loss = 2.68064308\n",
      "Iteration 3859, loss = 2.68058304\n",
      "Iteration 3860, loss = 2.68059209\n",
      "Iteration 3861, loss = 2.68053407\n",
      "Iteration 3862, loss = 2.68053966\n",
      "Iteration 3863, loss = 2.68052863\n",
      "Iteration 3864, loss = 2.68049151\n",
      "Iteration 3865, loss = 2.68048231\n",
      "Iteration 3866, loss = 2.68046332\n",
      "Iteration 3867, loss = 2.68040724\n",
      "Iteration 3868, loss = 2.68054612\n",
      "Iteration 3869, loss = 2.68034068\n",
      "Iteration 3870, loss = 2.68036175\n",
      "Iteration 3871, loss = 2.68034420\n",
      "Iteration 3872, loss = 2.68028265\n",
      "Iteration 3873, loss = 2.68034977\n",
      "Iteration 3874, loss = 2.68026256\n",
      "Iteration 3875, loss = 2.68022208\n",
      "Iteration 3876, loss = 2.68021680\n",
      "Iteration 3877, loss = 2.68022344\n",
      "Iteration 3878, loss = 2.68010985\n",
      "Iteration 3879, loss = 2.68012459\n",
      "Iteration 3880, loss = 2.68010702\n",
      "Iteration 3881, loss = 2.68007966\n",
      "Iteration 3882, loss = 2.68004646\n",
      "Iteration 3883, loss = 2.68004383\n",
      "Iteration 3884, loss = 2.67995339\n",
      "Iteration 3885, loss = 2.68000628\n",
      "Iteration 3886, loss = 2.67991726\n",
      "Iteration 3887, loss = 2.67992413\n",
      "Iteration 3888, loss = 2.67986377\n",
      "Iteration 3889, loss = 2.67981649\n",
      "Iteration 3890, loss = 2.67985993\n",
      "Iteration 3891, loss = 2.67977184\n",
      "Iteration 3892, loss = 2.67980261\n",
      "Iteration 3893, loss = 2.67978697\n",
      "Iteration 3894, loss = 2.67979548\n",
      "Iteration 3895, loss = 2.67969149\n",
      "Iteration 3896, loss = 2.67968175\n",
      "Iteration 3897, loss = 2.67967651\n",
      "Iteration 3898, loss = 2.67964498\n",
      "Iteration 3899, loss = 2.67961924\n",
      "Iteration 3900, loss = 2.67959072\n",
      "Iteration 3901, loss = 2.67956177\n",
      "Iteration 3902, loss = 2.67953275\n",
      "Iteration 3903, loss = 2.67950649\n",
      "Iteration 3904, loss = 2.67945209\n",
      "Iteration 3905, loss = 2.67946581\n",
      "Iteration 3906, loss = 2.67945655\n",
      "Iteration 3907, loss = 2.67938936\n",
      "Iteration 3908, loss = 2.67935186\n",
      "Iteration 3909, loss = 2.67934311\n",
      "Iteration 3910, loss = 2.67930624\n",
      "Iteration 3911, loss = 2.67938295\n",
      "Iteration 3912, loss = 2.67924061\n",
      "Iteration 3913, loss = 2.67927178\n",
      "Iteration 3914, loss = 2.67918449\n",
      "Iteration 3915, loss = 2.67917995\n",
      "Iteration 3916, loss = 2.67922250\n",
      "Iteration 3917, loss = 2.67914945\n",
      "Iteration 3918, loss = 2.67915194\n",
      "Iteration 3919, loss = 2.67907721\n",
      "Iteration 3920, loss = 2.67911252\n",
      "Iteration 3921, loss = 2.67904910\n",
      "Iteration 3922, loss = 2.67900216\n",
      "Iteration 3923, loss = 2.67901448\n",
      "Iteration 3924, loss = 2.67896755\n",
      "Iteration 3925, loss = 2.67895911\n",
      "Iteration 3926, loss = 2.67893188\n",
      "Iteration 3927, loss = 2.67889538\n",
      "Iteration 3928, loss = 2.67887829\n",
      "Iteration 3929, loss = 2.67883057\n",
      "Iteration 3930, loss = 2.67877583\n",
      "Iteration 3931, loss = 2.67878567\n",
      "Iteration 3932, loss = 2.67876966\n",
      "Iteration 3933, loss = 2.67876667\n",
      "Iteration 3934, loss = 2.67872562\n",
      "Iteration 3935, loss = 2.67865523\n",
      "Iteration 3936, loss = 2.67875151\n",
      "Iteration 3937, loss = 2.67864084\n",
      "Iteration 3938, loss = 2.67864372\n",
      "Iteration 3939, loss = 2.67860017\n",
      "Iteration 3940, loss = 2.67853170\n",
      "Iteration 3941, loss = 2.67852204\n",
      "Iteration 3942, loss = 2.67851962\n",
      "Iteration 3943, loss = 2.67847097\n",
      "Iteration 3944, loss = 2.67849632\n",
      "Iteration 3945, loss = 2.67839024\n",
      "Iteration 3946, loss = 2.67843208\n",
      "Iteration 3947, loss = 2.67837369\n",
      "Iteration 3948, loss = 2.67833572\n",
      "Iteration 3949, loss = 2.67832873\n",
      "Iteration 3950, loss = 2.67832894\n",
      "Iteration 3951, loss = 2.67827132\n",
      "Iteration 3952, loss = 2.67828829\n",
      "Iteration 3953, loss = 2.67828815\n",
      "Iteration 3954, loss = 2.67821642\n",
      "Iteration 3955, loss = 2.67814194\n",
      "Iteration 3956, loss = 2.67815296\n",
      "Iteration 3957, loss = 2.67811448\n",
      "Iteration 3958, loss = 2.67811001\n",
      "Iteration 3959, loss = 2.67814219\n",
      "Iteration 3960, loss = 2.67802106\n",
      "Iteration 3961, loss = 2.67805657\n",
      "Iteration 3962, loss = 2.67799511\n",
      "Iteration 3963, loss = 2.67798193\n",
      "Iteration 3964, loss = 2.67797603\n",
      "Iteration 3965, loss = 2.67792349\n",
      "Iteration 3966, loss = 2.67791398\n",
      "Iteration 3967, loss = 2.67790344\n",
      "Iteration 3968, loss = 2.67784755\n",
      "Iteration 3969, loss = 2.67788235\n",
      "Iteration 3970, loss = 2.67784492\n",
      "Iteration 3971, loss = 2.67785230\n",
      "Iteration 3972, loss = 2.67778223\n",
      "Iteration 3973, loss = 2.67774345\n",
      "Iteration 3974, loss = 2.67769848\n",
      "Iteration 3975, loss = 2.67769435\n",
      "Iteration 3976, loss = 2.67762360\n",
      "Iteration 3977, loss = 2.67763983\n",
      "Iteration 3978, loss = 2.67760078\n",
      "Iteration 3979, loss = 2.67760664\n",
      "Iteration 3980, loss = 2.67756234\n",
      "Iteration 3981, loss = 2.67755590\n",
      "Iteration 3982, loss = 2.67754123\n",
      "Iteration 3983, loss = 2.67744749\n",
      "Iteration 3984, loss = 2.67757448\n",
      "Iteration 3985, loss = 2.67742927\n",
      "Iteration 3986, loss = 2.67738487\n",
      "Iteration 3987, loss = 2.67738833\n",
      "Iteration 3988, loss = 2.67738839\n",
      "Iteration 3989, loss = 2.67731334\n",
      "Iteration 3990, loss = 2.67731300\n",
      "Iteration 3991, loss = 2.67727579\n",
      "Iteration 3992, loss = 2.67725523\n",
      "Iteration 3993, loss = 2.67721734\n",
      "Iteration 3994, loss = 2.67724097\n",
      "Iteration 3995, loss = 2.67718981\n",
      "Iteration 3996, loss = 2.67719930\n",
      "Iteration 3997, loss = 2.67712022\n",
      "Iteration 3998, loss = 2.67709061\n",
      "Iteration 3999, loss = 2.67706034\n",
      "Iteration 4000, loss = 2.67709048\n",
      "Iteration 4001, loss = 2.67702434\n",
      "Iteration 4002, loss = 2.67701716\n",
      "Iteration 4003, loss = 2.67698761\n",
      "Iteration 4004, loss = 2.67695700\n",
      "Iteration 4005, loss = 2.67691490\n",
      "Iteration 4006, loss = 2.67691401\n",
      "Iteration 4007, loss = 2.67688571\n",
      "Iteration 4008, loss = 2.67683806\n",
      "Iteration 4009, loss = 2.67683147\n",
      "Iteration 4010, loss = 2.67679956\n",
      "Iteration 4011, loss = 2.67680230\n",
      "Iteration 4012, loss = 2.67677780\n",
      "Iteration 4013, loss = 2.67679166\n",
      "Iteration 4014, loss = 2.67670860\n",
      "Iteration 4015, loss = 2.67668859\n",
      "Iteration 4016, loss = 2.67665436\n",
      "Iteration 4017, loss = 2.67664876\n",
      "Iteration 4018, loss = 2.67661326\n",
      "Iteration 4019, loss = 2.67657180\n",
      "Iteration 4020, loss = 2.67659643\n",
      "Iteration 4021, loss = 2.67653917\n",
      "Iteration 4022, loss = 2.67653029\n",
      "Iteration 4023, loss = 2.67650113\n",
      "Iteration 4024, loss = 2.67648359\n",
      "Iteration 4025, loss = 2.67643267\n",
      "Iteration 4026, loss = 2.67647163\n",
      "Iteration 4027, loss = 2.67644459\n",
      "Iteration 4028, loss = 2.67642554\n",
      "Iteration 4029, loss = 2.67632487\n",
      "Iteration 4030, loss = 2.67630503\n",
      "Iteration 4031, loss = 2.67628903\n",
      "Iteration 4032, loss = 2.67637713\n",
      "Iteration 4033, loss = 2.67623741\n",
      "Iteration 4034, loss = 2.67621497\n",
      "Iteration 4035, loss = 2.67622456\n",
      "Iteration 4036, loss = 2.67617441\n",
      "Iteration 4037, loss = 2.67613053\n",
      "Iteration 4038, loss = 2.67610524\n",
      "Iteration 4039, loss = 2.67611204\n",
      "Iteration 4040, loss = 2.67607234\n",
      "Iteration 4041, loss = 2.67603353\n",
      "Iteration 4042, loss = 2.67603021\n",
      "Iteration 4043, loss = 2.67598151\n",
      "Iteration 4044, loss = 2.67600634\n",
      "Iteration 4045, loss = 2.67596576\n",
      "Iteration 4046, loss = 2.67594737\n",
      "Iteration 4047, loss = 2.67596103\n",
      "Iteration 4048, loss = 2.67588160\n",
      "Iteration 4049, loss = 2.67584030\n",
      "Iteration 4050, loss = 2.67582377\n",
      "Iteration 4051, loss = 2.67582222\n",
      "Iteration 4052, loss = 2.67577921\n",
      "Iteration 4053, loss = 2.67575706\n",
      "Iteration 4054, loss = 2.67574762\n",
      "Iteration 4055, loss = 2.67573439\n",
      "Iteration 4056, loss = 2.67570630\n",
      "Iteration 4057, loss = 2.67566662\n",
      "Iteration 4058, loss = 2.67564153\n",
      "Iteration 4059, loss = 2.67560570\n",
      "Iteration 4060, loss = 2.67560565\n",
      "Iteration 4061, loss = 2.67555995\n",
      "Iteration 4062, loss = 2.67551613\n",
      "Iteration 4063, loss = 2.67547645\n",
      "Iteration 4064, loss = 2.67549629\n",
      "Iteration 4065, loss = 2.67548915\n",
      "Iteration 4066, loss = 2.67545007\n",
      "Iteration 4067, loss = 2.67541944\n",
      "Iteration 4068, loss = 2.67540537\n",
      "Iteration 4069, loss = 2.67536707\n",
      "Iteration 4070, loss = 2.67533339\n",
      "Iteration 4071, loss = 2.67537475\n",
      "Iteration 4072, loss = 2.67530859\n",
      "Iteration 4073, loss = 2.67527709\n",
      "Iteration 4074, loss = 2.67530036\n",
      "Iteration 4075, loss = 2.67524758\n",
      "Iteration 4076, loss = 2.67518097\n",
      "Iteration 4077, loss = 2.67516940\n",
      "Iteration 4078, loss = 2.67514603\n",
      "Iteration 4079, loss = 2.67515396\n",
      "Iteration 4080, loss = 2.67516441\n",
      "Iteration 4081, loss = 2.67508385\n",
      "Iteration 4082, loss = 2.67504332\n",
      "Iteration 4083, loss = 2.67502534\n",
      "Iteration 4084, loss = 2.67505398\n",
      "Iteration 4085, loss = 2.67500914\n",
      "Iteration 4086, loss = 2.67496728\n",
      "Iteration 4087, loss = 2.67501470\n",
      "Iteration 4088, loss = 2.67495284\n",
      "Iteration 4089, loss = 2.67486690\n",
      "Iteration 4090, loss = 2.67484611\n",
      "Iteration 4091, loss = 2.67484188\n",
      "Iteration 4092, loss = 2.67487245\n",
      "Iteration 4093, loss = 2.67483178\n",
      "Iteration 4094, loss = 2.67476408\n",
      "Iteration 4095, loss = 2.67472068\n",
      "Iteration 4096, loss = 2.67476756\n",
      "Iteration 4097, loss = 2.67472180\n",
      "Iteration 4098, loss = 2.67468122\n",
      "Iteration 4099, loss = 2.67460877\n",
      "Iteration 4100, loss = 2.67461087\n",
      "Iteration 4101, loss = 2.67459135\n",
      "Iteration 4102, loss = 2.67463779\n",
      "Iteration 4103, loss = 2.67459545\n",
      "Iteration 4104, loss = 2.67455106\n",
      "Iteration 4105, loss = 2.67450985\n",
      "Iteration 4106, loss = 2.67451383\n",
      "Iteration 4107, loss = 2.67446665\n",
      "Iteration 4108, loss = 2.67445343\n",
      "Iteration 4109, loss = 2.67442819\n",
      "Iteration 4110, loss = 2.67433692\n",
      "Iteration 4111, loss = 2.67439905\n",
      "Iteration 4112, loss = 2.67437995\n",
      "Iteration 4113, loss = 2.67431385\n",
      "Iteration 4114, loss = 2.67434672\n",
      "Iteration 4115, loss = 2.67423669\n",
      "Iteration 4116, loss = 2.67425399\n",
      "Iteration 4117, loss = 2.67421848\n",
      "Iteration 4118, loss = 2.67422128\n",
      "Iteration 4119, loss = 2.67416676\n",
      "Iteration 4120, loss = 2.67412394\n",
      "Iteration 4121, loss = 2.67412637\n",
      "Iteration 4122, loss = 2.67414912\n",
      "Iteration 4123, loss = 2.67404791\n",
      "Iteration 4124, loss = 2.67405053\n",
      "Iteration 4125, loss = 2.67403251\n",
      "Iteration 4126, loss = 2.67404676\n",
      "Iteration 4127, loss = 2.67397388\n",
      "Iteration 4128, loss = 2.67392013\n",
      "Iteration 4129, loss = 2.67393078\n",
      "Iteration 4130, loss = 2.67384592\n",
      "Iteration 4131, loss = 2.67389422\n",
      "Iteration 4132, loss = 2.67388461\n",
      "Iteration 4133, loss = 2.67386975\n",
      "Iteration 4134, loss = 2.67380298\n",
      "Iteration 4135, loss = 2.67378752\n",
      "Iteration 4136, loss = 2.67375238\n",
      "Iteration 4137, loss = 2.67380737\n",
      "Iteration 4138, loss = 2.67369750\n",
      "Iteration 4139, loss = 2.67369495\n",
      "Iteration 4140, loss = 2.67368721\n",
      "Iteration 4141, loss = 2.67366845\n",
      "Iteration 4142, loss = 2.67362769\n",
      "Iteration 4143, loss = 2.67365299\n",
      "Iteration 4144, loss = 2.67359638\n",
      "Iteration 4145, loss = 2.67356090\n",
      "Iteration 4146, loss = 2.67354087\n",
      "Iteration 4147, loss = 2.67355374\n",
      "Iteration 4148, loss = 2.67345380\n",
      "Iteration 4149, loss = 2.67351157\n",
      "Iteration 4150, loss = 2.67345149\n",
      "Iteration 4151, loss = 2.67345421\n",
      "Iteration 4152, loss = 2.67339153\n",
      "Iteration 4153, loss = 2.67349963\n",
      "Iteration 4154, loss = 2.67336498\n",
      "Iteration 4155, loss = 2.67333861\n",
      "Iteration 4156, loss = 2.67335481\n",
      "Iteration 4157, loss = 2.67328967\n",
      "Iteration 4158, loss = 2.67325550\n",
      "Iteration 4159, loss = 2.67321531\n",
      "Iteration 4160, loss = 2.67318981\n",
      "Iteration 4161, loss = 2.67315162\n",
      "Iteration 4162, loss = 2.67318343\n",
      "Iteration 4163, loss = 2.67314642\n",
      "Iteration 4164, loss = 2.67317500\n",
      "Iteration 4165, loss = 2.67316485\n",
      "Iteration 4166, loss = 2.67304251\n",
      "Iteration 4167, loss = 2.67302255\n",
      "Iteration 4168, loss = 2.67300475\n",
      "Iteration 4169, loss = 2.67304724\n",
      "Iteration 4170, loss = 2.67296285\n",
      "Iteration 4171, loss = 2.67296614\n",
      "Iteration 4172, loss = 2.67294582\n",
      "Iteration 4173, loss = 2.67288711\n",
      "Iteration 4174, loss = 2.67287081\n",
      "Iteration 4175, loss = 2.67285114\n",
      "Iteration 4176, loss = 2.67286071\n",
      "Iteration 4177, loss = 2.67280236\n",
      "Iteration 4178, loss = 2.67280737\n",
      "Iteration 4179, loss = 2.67281089\n",
      "Iteration 4180, loss = 2.67270132\n",
      "Iteration 4181, loss = 2.67274341\n",
      "Iteration 4182, loss = 2.67269602\n",
      "Iteration 4183, loss = 2.67264035\n",
      "Iteration 4184, loss = 2.67267856\n",
      "Iteration 4185, loss = 2.67260737\n",
      "Iteration 4186, loss = 2.67267661\n",
      "Iteration 4187, loss = 2.67253409\n",
      "Iteration 4188, loss = 2.67253860\n",
      "Iteration 4189, loss = 2.67250448\n",
      "Iteration 4190, loss = 2.67251987\n",
      "Iteration 4191, loss = 2.67245256\n",
      "Iteration 4192, loss = 2.67242252\n",
      "Iteration 4193, loss = 2.67245001\n",
      "Iteration 4194, loss = 2.67244248\n",
      "Iteration 4195, loss = 2.67238550\n",
      "Iteration 4196, loss = 2.67237960\n",
      "Iteration 4197, loss = 2.67234101\n",
      "Iteration 4198, loss = 2.67231773\n",
      "Iteration 4199, loss = 2.67226457\n",
      "Iteration 4200, loss = 2.67232086\n",
      "Iteration 4201, loss = 2.67229209\n",
      "Iteration 4202, loss = 2.67223200\n",
      "Iteration 4203, loss = 2.67217487\n",
      "Iteration 4204, loss = 2.67218642\n",
      "Iteration 4205, loss = 2.67214981\n",
      "Iteration 4206, loss = 2.67212366\n",
      "Iteration 4207, loss = 2.67205075\n",
      "Iteration 4208, loss = 2.67207041\n",
      "Iteration 4209, loss = 2.67207711\n",
      "Iteration 4210, loss = 2.67202382\n",
      "Iteration 4211, loss = 2.67198321\n",
      "Iteration 4212, loss = 2.67194797\n",
      "Iteration 4213, loss = 2.67193988\n",
      "Iteration 4214, loss = 2.67190127\n",
      "Iteration 4215, loss = 2.67193428\n",
      "Iteration 4216, loss = 2.67186191\n",
      "Iteration 4217, loss = 2.67192279\n",
      "Iteration 4218, loss = 2.67188326\n",
      "Iteration 4219, loss = 2.67180404\n",
      "Iteration 4220, loss = 2.67181376\n",
      "Iteration 4221, loss = 2.67177612\n",
      "Iteration 4222, loss = 2.67173906\n",
      "Iteration 4223, loss = 2.67170393\n",
      "Iteration 4224, loss = 2.67175709\n",
      "Iteration 4225, loss = 2.67169967\n",
      "Iteration 4226, loss = 2.67163236\n",
      "Iteration 4227, loss = 2.67162352\n",
      "Iteration 4228, loss = 2.67158279\n",
      "Iteration 4229, loss = 2.67170087\n",
      "Iteration 4230, loss = 2.67157009\n",
      "Iteration 4231, loss = 2.67154243\n",
      "Iteration 4232, loss = 2.67159634\n",
      "Iteration 4233, loss = 2.67153119\n",
      "Iteration 4234, loss = 2.67146451\n",
      "Iteration 4235, loss = 2.67144304\n",
      "Iteration 4236, loss = 2.67139698\n",
      "Iteration 4237, loss = 2.67137951\n",
      "Iteration 4238, loss = 2.67136573\n",
      "Iteration 4239, loss = 2.67141438\n",
      "Iteration 4240, loss = 2.67133343\n",
      "Iteration 4241, loss = 2.67130892\n",
      "Iteration 4242, loss = 2.67139813\n",
      "Iteration 4243, loss = 2.67129412\n",
      "Iteration 4244, loss = 2.67127880\n",
      "Iteration 4245, loss = 2.67124210\n",
      "Iteration 4246, loss = 2.67123751\n",
      "Iteration 4247, loss = 2.67115889\n",
      "Iteration 4248, loss = 2.67113046\n",
      "Iteration 4249, loss = 2.67109644\n",
      "Iteration 4250, loss = 2.67111462\n",
      "Iteration 4251, loss = 2.67106501\n",
      "Iteration 4252, loss = 2.67103108\n",
      "Iteration 4253, loss = 2.67108740\n",
      "Iteration 4254, loss = 2.67103716\n",
      "Iteration 4255, loss = 2.67101539\n",
      "Iteration 4256, loss = 2.67094328\n",
      "Iteration 4257, loss = 2.67091487\n",
      "Iteration 4258, loss = 2.67091697\n",
      "Iteration 4259, loss = 2.67093509\n",
      "Iteration 4260, loss = 2.67083062\n",
      "Iteration 4261, loss = 2.67081771\n",
      "Iteration 4262, loss = 2.67086990\n",
      "Iteration 4263, loss = 2.67078998\n",
      "Iteration 4264, loss = 2.67075767\n",
      "Iteration 4265, loss = 2.67075418\n",
      "Iteration 4266, loss = 2.67072684\n",
      "Iteration 4267, loss = 2.67068984\n",
      "Iteration 4268, loss = 2.67067075\n",
      "Iteration 4269, loss = 2.67068762\n",
      "Iteration 4270, loss = 2.67063533\n",
      "Iteration 4271, loss = 2.67062853\n",
      "Iteration 4272, loss = 2.67056993\n",
      "Iteration 4273, loss = 2.67056926\n",
      "Iteration 4274, loss = 2.67052357\n",
      "Iteration 4275, loss = 2.67047585\n",
      "Iteration 4276, loss = 2.67046372\n",
      "Iteration 4277, loss = 2.67047365\n",
      "Iteration 4278, loss = 2.67045729\n",
      "Iteration 4279, loss = 2.67046873\n",
      "Iteration 4280, loss = 2.67040405\n",
      "Iteration 4281, loss = 2.67035251\n",
      "Iteration 4282, loss = 2.67035117\n",
      "Iteration 4283, loss = 2.67038266\n",
      "Iteration 4284, loss = 2.67032620\n",
      "Iteration 4285, loss = 2.67031362\n",
      "Iteration 4286, loss = 2.67023603\n",
      "Iteration 4287, loss = 2.67028053\n",
      "Iteration 4288, loss = 2.67016734\n",
      "Iteration 4289, loss = 2.67017713\n",
      "Iteration 4290, loss = 2.67015057\n",
      "Iteration 4291, loss = 2.67013737\n",
      "Iteration 4292, loss = 2.67010267\n",
      "Iteration 4293, loss = 2.67008734\n",
      "Iteration 4294, loss = 2.67008455\n",
      "Iteration 4295, loss = 2.67005284\n",
      "Iteration 4296, loss = 2.67000023\n",
      "Iteration 4297, loss = 2.66999778\n",
      "Iteration 4298, loss = 2.66999925\n",
      "Iteration 4299, loss = 2.67003782\n",
      "Iteration 4300, loss = 2.66996056\n",
      "Iteration 4301, loss = 2.66992626\n",
      "Iteration 4302, loss = 2.66990338\n",
      "Iteration 4303, loss = 2.66986796\n",
      "Iteration 4304, loss = 2.66984795\n",
      "Iteration 4305, loss = 2.66982830\n",
      "Iteration 4306, loss = 2.66979059\n",
      "Iteration 4307, loss = 2.66976141\n",
      "Iteration 4308, loss = 2.66978734\n",
      "Iteration 4309, loss = 2.66973194\n",
      "Iteration 4310, loss = 2.66972003\n",
      "Iteration 4311, loss = 2.66973329\n",
      "Iteration 4312, loss = 2.66965550\n",
      "Iteration 4313, loss = 2.66959751\n",
      "Iteration 4314, loss = 2.66960945\n",
      "Iteration 4315, loss = 2.66966869\n",
      "Iteration 4316, loss = 2.66956428\n",
      "Iteration 4317, loss = 2.66953734\n",
      "Iteration 4318, loss = 2.66954125\n",
      "Iteration 4319, loss = 2.66963904\n",
      "Iteration 4320, loss = 2.66947275\n",
      "Iteration 4321, loss = 2.66951076\n",
      "Iteration 4322, loss = 2.66942736\n",
      "Iteration 4323, loss = 2.66937402\n",
      "Iteration 4324, loss = 2.66941126\n",
      "Iteration 4325, loss = 2.66938560\n",
      "Iteration 4326, loss = 2.66937010\n",
      "Iteration 4327, loss = 2.66931599\n",
      "Iteration 4328, loss = 2.66928938\n",
      "Iteration 4329, loss = 2.66926448\n",
      "Iteration 4330, loss = 2.66926836\n",
      "Iteration 4331, loss = 2.66928358\n",
      "Iteration 4332, loss = 2.66921792\n",
      "Iteration 4333, loss = 2.66921896\n",
      "Iteration 4334, loss = 2.66916594\n",
      "Iteration 4335, loss = 2.66913934\n",
      "Iteration 4336, loss = 2.66911438\n",
      "Iteration 4337, loss = 2.66909355\n",
      "Iteration 4338, loss = 2.66907951\n",
      "Iteration 4339, loss = 2.66910781\n",
      "Iteration 4340, loss = 2.66898678\n",
      "Iteration 4341, loss = 2.66901675\n",
      "Iteration 4342, loss = 2.66898206\n",
      "Iteration 4343, loss = 2.66895180\n",
      "Iteration 4344, loss = 2.66895521\n",
      "Iteration 4345, loss = 2.66889850\n",
      "Iteration 4346, loss = 2.66894337\n",
      "Iteration 4347, loss = 2.66891644\n",
      "Iteration 4348, loss = 2.66882037\n",
      "Iteration 4349, loss = 2.66882839\n",
      "Iteration 4350, loss = 2.66885626\n",
      "Iteration 4351, loss = 2.66879258\n",
      "Iteration 4352, loss = 2.66873978\n",
      "Iteration 4353, loss = 2.66873702\n",
      "Iteration 4354, loss = 2.66872238\n",
      "Iteration 4355, loss = 2.66870524\n",
      "Iteration 4356, loss = 2.66869364\n",
      "Iteration 4357, loss = 2.66865206\n",
      "Iteration 4358, loss = 2.66869058\n",
      "Iteration 4359, loss = 2.66859657\n",
      "Iteration 4360, loss = 2.66854813\n",
      "Iteration 4361, loss = 2.66859353\n",
      "Iteration 4362, loss = 2.66859752\n",
      "Iteration 4363, loss = 2.66853408\n",
      "Iteration 4364, loss = 2.66846374\n",
      "Iteration 4365, loss = 2.66843348\n",
      "Iteration 4366, loss = 2.66845374\n",
      "Iteration 4367, loss = 2.66841252\n",
      "Iteration 4368, loss = 2.66837366\n",
      "Iteration 4369, loss = 2.66842836\n",
      "Iteration 4370, loss = 2.66837190\n",
      "Iteration 4371, loss = 2.66834257\n",
      "Iteration 4372, loss = 2.66828581\n",
      "Iteration 4373, loss = 2.66831623\n",
      "Iteration 4374, loss = 2.66830970\n",
      "Iteration 4375, loss = 2.66822170\n",
      "Iteration 4376, loss = 2.66820592\n",
      "Iteration 4377, loss = 2.66821993\n",
      "Iteration 4378, loss = 2.66813737\n",
      "Iteration 4379, loss = 2.66819169\n",
      "Iteration 4380, loss = 2.66810174\n",
      "Iteration 4381, loss = 2.66808693\n",
      "Iteration 4382, loss = 2.66808892\n",
      "Iteration 4383, loss = 2.66808048\n",
      "Iteration 4384, loss = 2.66805801\n",
      "Iteration 4385, loss = 2.66798693\n",
      "Iteration 4386, loss = 2.66798173\n",
      "Iteration 4387, loss = 2.66799274\n",
      "Iteration 4388, loss = 2.66795351\n",
      "Iteration 4389, loss = 2.66790842\n",
      "Iteration 4390, loss = 2.66792267\n",
      "Iteration 4391, loss = 2.66791559\n",
      "Iteration 4392, loss = 2.66788055\n",
      "Iteration 4393, loss = 2.66786197\n",
      "Iteration 4394, loss = 2.66784371\n",
      "Iteration 4395, loss = 2.66778386\n",
      "Iteration 4396, loss = 2.66775306\n",
      "Iteration 4397, loss = 2.66776347\n",
      "Iteration 4398, loss = 2.66770162\n",
      "Iteration 4399, loss = 2.66771653\n",
      "Iteration 4400, loss = 2.66769124\n",
      "Iteration 4401, loss = 2.66772474\n",
      "Iteration 4402, loss = 2.66764930\n",
      "Iteration 4403, loss = 2.66760264\n",
      "Iteration 4404, loss = 2.66760065\n",
      "Iteration 4405, loss = 2.66754420\n",
      "Iteration 4406, loss = 2.66759418\n",
      "Iteration 4407, loss = 2.66754065\n",
      "Iteration 4408, loss = 2.66754507\n",
      "Iteration 4409, loss = 2.66746572\n",
      "Iteration 4410, loss = 2.66747473\n",
      "Iteration 4411, loss = 2.66745135\n",
      "Iteration 4412, loss = 2.66741694\n",
      "Iteration 4413, loss = 2.66734286\n",
      "Iteration 4414, loss = 2.66737086\n",
      "Iteration 4415, loss = 2.66739821\n",
      "Iteration 4416, loss = 2.66735888\n",
      "Iteration 4417, loss = 2.66730841\n",
      "Iteration 4418, loss = 2.66727069\n",
      "Iteration 4419, loss = 2.66724824\n",
      "Iteration 4420, loss = 2.66722786\n",
      "Iteration 4421, loss = 2.66722410\n",
      "Iteration 4422, loss = 2.66720060\n",
      "Iteration 4423, loss = 2.66715521\n",
      "Iteration 4424, loss = 2.66716723\n",
      "Iteration 4425, loss = 2.66709610\n",
      "Iteration 4426, loss = 2.66710630\n",
      "Iteration 4427, loss = 2.66714253\n",
      "Iteration 4428, loss = 2.66708182\n",
      "Iteration 4429, loss = 2.66707318\n",
      "Iteration 4430, loss = 2.66700283\n",
      "Iteration 4431, loss = 2.66697853\n",
      "Iteration 4432, loss = 2.66697575\n",
      "Iteration 4433, loss = 2.66699308\n",
      "Iteration 4434, loss = 2.66690442\n",
      "Iteration 4435, loss = 2.66689420\n",
      "Iteration 4436, loss = 2.66687874\n",
      "Iteration 4437, loss = 2.66691137\n",
      "Iteration 4438, loss = 2.66682543\n",
      "Iteration 4439, loss = 2.66686949\n",
      "Iteration 4440, loss = 2.66678302\n",
      "Iteration 4441, loss = 2.66678100\n",
      "Iteration 4442, loss = 2.66670141\n",
      "Iteration 4443, loss = 2.66675058\n",
      "Iteration 4444, loss = 2.66671814\n",
      "Iteration 4445, loss = 2.66671097\n",
      "Iteration 4446, loss = 2.66666167\n",
      "Iteration 4447, loss = 2.66660118\n",
      "Iteration 4448, loss = 2.66663036\n",
      "Iteration 4449, loss = 2.66664024\n",
      "Iteration 4450, loss = 2.66655515\n",
      "Iteration 4451, loss = 2.66655741\n",
      "Iteration 4452, loss = 2.66652790\n",
      "Iteration 4453, loss = 2.66649987\n",
      "Iteration 4454, loss = 2.66646122\n",
      "Iteration 4455, loss = 2.66645362\n",
      "Iteration 4456, loss = 2.66644963\n",
      "Iteration 4457, loss = 2.66641780\n",
      "Iteration 4458, loss = 2.66641807\n",
      "Iteration 4459, loss = 2.66637490\n",
      "Iteration 4460, loss = 2.66640591\n",
      "Iteration 4461, loss = 2.66635266\n",
      "Iteration 4462, loss = 2.66627953\n",
      "Iteration 4463, loss = 2.66634829\n",
      "Iteration 4464, loss = 2.66626850\n",
      "Iteration 4465, loss = 2.66622981\n",
      "Iteration 4466, loss = 2.66622502\n",
      "Iteration 4467, loss = 2.66622082\n",
      "Iteration 4468, loss = 2.66615373\n",
      "Iteration 4469, loss = 2.66613744\n",
      "Iteration 4470, loss = 2.66611968\n",
      "Iteration 4471, loss = 2.66612711\n",
      "Iteration 4472, loss = 2.66609965\n",
      "Iteration 4473, loss = 2.66607191\n",
      "Iteration 4474, loss = 2.66605260\n",
      "Iteration 4475, loss = 2.66601632\n",
      "Iteration 4476, loss = 2.66597570\n",
      "Iteration 4477, loss = 2.66595888\n",
      "Iteration 4478, loss = 2.66602065\n",
      "Iteration 4479, loss = 2.66592338\n",
      "Iteration 4480, loss = 2.66588264\n",
      "Iteration 4481, loss = 2.66594048\n",
      "Iteration 4482, loss = 2.66588398\n",
      "Iteration 4483, loss = 2.66580740\n",
      "Iteration 4484, loss = 2.66585815\n",
      "Iteration 4485, loss = 2.66584787\n",
      "Iteration 4486, loss = 2.66579200\n",
      "Iteration 4487, loss = 2.66577026\n",
      "Iteration 4488, loss = 2.66571394\n",
      "Iteration 4489, loss = 2.66572745\n",
      "Iteration 4490, loss = 2.66570927\n",
      "Iteration 4491, loss = 2.66567421\n",
      "Iteration 4492, loss = 2.66563228\n",
      "Iteration 4493, loss = 2.66568064\n",
      "Iteration 4494, loss = 2.66561537\n",
      "Iteration 4495, loss = 2.66560272\n",
      "Iteration 4496, loss = 2.66561531\n",
      "Iteration 4497, loss = 2.66560378\n",
      "Iteration 4498, loss = 2.66551116\n",
      "Iteration 4499, loss = 2.66557588\n",
      "Iteration 4500, loss = 2.66545683\n",
      "Iteration 4501, loss = 2.66544915\n",
      "Iteration 4502, loss = 2.66543226\n",
      "Iteration 4503, loss = 2.66540656\n",
      "Iteration 4504, loss = 2.66542193\n",
      "Iteration 4505, loss = 2.66536378\n",
      "Iteration 4506, loss = 2.66537821\n",
      "Iteration 4507, loss = 2.66533128\n",
      "Iteration 4508, loss = 2.66531800\n",
      "Iteration 4509, loss = 2.66526834\n",
      "Iteration 4510, loss = 2.66526813\n",
      "Iteration 4511, loss = 2.66526262\n",
      "Iteration 4512, loss = 2.66522778\n",
      "Iteration 4513, loss = 2.66521414\n",
      "Iteration 4514, loss = 2.66516418\n",
      "Iteration 4515, loss = 2.66515098\n",
      "Iteration 4516, loss = 2.66511206\n",
      "Iteration 4517, loss = 2.66513280\n",
      "Iteration 4518, loss = 2.66513527\n",
      "Iteration 4519, loss = 2.66511847\n",
      "Iteration 4520, loss = 2.66505602\n",
      "Iteration 4521, loss = 2.66505569\n",
      "Iteration 4522, loss = 2.66500099\n",
      "Iteration 4523, loss = 2.66495968\n",
      "Iteration 4524, loss = 2.66496484\n",
      "Iteration 4525, loss = 2.66492729\n",
      "Iteration 4526, loss = 2.66491818\n",
      "Iteration 4527, loss = 2.66491051\n",
      "Iteration 4528, loss = 2.66487070\n",
      "Iteration 4529, loss = 2.66491896\n",
      "Iteration 4530, loss = 2.66484246\n",
      "Iteration 4531, loss = 2.66480280\n",
      "Iteration 4532, loss = 2.66485217\n",
      "Iteration 4533, loss = 2.66478128\n",
      "Iteration 4534, loss = 2.66480051\n",
      "Iteration 4535, loss = 2.66471729\n",
      "Iteration 4536, loss = 2.66479271\n",
      "Iteration 4537, loss = 2.66470569\n",
      "Iteration 4538, loss = 2.66462002\n",
      "Iteration 4539, loss = 2.66459817\n",
      "Iteration 4540, loss = 2.66457150\n",
      "Iteration 4541, loss = 2.66457707\n",
      "Iteration 4542, loss = 2.66455867\n",
      "Iteration 4543, loss = 2.66454337\n",
      "Iteration 4544, loss = 2.66453817\n",
      "Iteration 4545, loss = 2.66449612\n",
      "Iteration 4546, loss = 2.66446847\n",
      "Iteration 4547, loss = 2.66443715\n",
      "Iteration 4548, loss = 2.66443716\n",
      "Iteration 4549, loss = 2.66444288\n",
      "Iteration 4550, loss = 2.66440885\n",
      "Iteration 4551, loss = 2.66435766\n",
      "Iteration 4552, loss = 2.66433280\n",
      "Iteration 4553, loss = 2.66436456\n",
      "Iteration 4554, loss = 2.66431758\n",
      "Iteration 4555, loss = 2.66430507\n",
      "Iteration 4556, loss = 2.66423695\n",
      "Iteration 4557, loss = 2.66422668\n",
      "Iteration 4558, loss = 2.66428465\n",
      "Iteration 4559, loss = 2.66419803\n",
      "Iteration 4560, loss = 2.66419849\n",
      "Iteration 4561, loss = 2.66412550\n",
      "Iteration 4562, loss = 2.66412525\n",
      "Iteration 4563, loss = 2.66410945\n",
      "Iteration 4564, loss = 2.66407661\n",
      "Iteration 4565, loss = 2.66415100\n",
      "Iteration 4566, loss = 2.66403944\n",
      "Iteration 4567, loss = 2.66401645\n",
      "Iteration 4568, loss = 2.66401408\n",
      "Iteration 4569, loss = 2.66396214\n",
      "Iteration 4570, loss = 2.66400952\n",
      "Iteration 4571, loss = 2.66408805\n",
      "Iteration 4572, loss = 2.66387826\n",
      "Iteration 4573, loss = 2.66386108\n",
      "Iteration 4574, loss = 2.66386102\n",
      "Iteration 4575, loss = 2.66389989\n",
      "Iteration 4576, loss = 2.66387497\n",
      "Iteration 4577, loss = 2.66379325\n",
      "Iteration 4578, loss = 2.66381192\n",
      "Iteration 4579, loss = 2.66379995\n",
      "Iteration 4580, loss = 2.66381411\n",
      "Iteration 4581, loss = 2.66381998\n",
      "Iteration 4582, loss = 2.66375091\n",
      "Iteration 4583, loss = 2.66369814\n",
      "Iteration 4584, loss = 2.66366421\n",
      "Iteration 4585, loss = 2.66364701\n",
      "Iteration 4586, loss = 2.66358143\n",
      "Iteration 4587, loss = 2.66362455\n",
      "Iteration 4588, loss = 2.66357556\n",
      "Iteration 4589, loss = 2.66355484\n",
      "Iteration 4590, loss = 2.66364122\n",
      "Iteration 4591, loss = 2.66344001\n",
      "Iteration 4592, loss = 2.66348902\n",
      "Iteration 4593, loss = 2.66349915\n",
      "Iteration 4594, loss = 2.66346224\n",
      "Iteration 4595, loss = 2.66345701\n",
      "Iteration 4596, loss = 2.66341734\n",
      "Iteration 4597, loss = 2.66335819\n",
      "Iteration 4598, loss = 2.66337348\n",
      "Iteration 4599, loss = 2.66333001\n",
      "Iteration 4600, loss = 2.66334967\n",
      "Iteration 4601, loss = 2.66334780\n",
      "Iteration 4602, loss = 2.66326811\n",
      "Iteration 4603, loss = 2.66324153\n",
      "Iteration 4604, loss = 2.66326599\n",
      "Iteration 4605, loss = 2.66323233\n",
      "Iteration 4606, loss = 2.66322132\n",
      "Iteration 4607, loss = 2.66321152\n",
      "Iteration 4608, loss = 2.66319891\n",
      "Iteration 4609, loss = 2.66310063\n",
      "Iteration 4610, loss = 2.66308533\n",
      "Iteration 4611, loss = 2.66309782\n",
      "Iteration 4612, loss = 2.66305397\n",
      "Iteration 4613, loss = 2.66305191\n",
      "Iteration 4614, loss = 2.66298352\n",
      "Iteration 4615, loss = 2.66300335\n",
      "Iteration 4616, loss = 2.66293937\n",
      "Iteration 4617, loss = 2.66292741\n",
      "Iteration 4618, loss = 2.66292652\n",
      "Iteration 4619, loss = 2.66290323\n",
      "Iteration 4620, loss = 2.66289224\n",
      "Iteration 4621, loss = 2.66290399\n",
      "Iteration 4622, loss = 2.66288400\n",
      "Iteration 4623, loss = 2.66284227\n",
      "Iteration 4624, loss = 2.66280452\n",
      "Iteration 4625, loss = 2.66283184\n",
      "Iteration 4626, loss = 2.66280264\n",
      "Iteration 4627, loss = 2.66281279\n",
      "Iteration 4628, loss = 2.66276224\n",
      "Iteration 4629, loss = 2.66270852\n",
      "Iteration 4630, loss = 2.66268862\n",
      "Iteration 4631, loss = 2.66269679\n",
      "Iteration 4632, loss = 2.66265691\n",
      "Iteration 4633, loss = 2.66259603\n",
      "Iteration 4634, loss = 2.66263521\n",
      "Iteration 4635, loss = 2.66258472\n",
      "Iteration 4636, loss = 2.66262183\n",
      "Iteration 4637, loss = 2.66250960\n",
      "Iteration 4638, loss = 2.66250018\n",
      "Iteration 4639, loss = 2.66253249\n",
      "Iteration 4640, loss = 2.66249369\n",
      "Iteration 4641, loss = 2.66246408\n",
      "Iteration 4642, loss = 2.66244179\n",
      "Iteration 4643, loss = 2.66243408\n",
      "Iteration 4644, loss = 2.66237046\n",
      "Iteration 4645, loss = 2.66232437\n",
      "Iteration 4646, loss = 2.66236934\n",
      "Iteration 4647, loss = 2.66231087\n",
      "Iteration 4648, loss = 2.66231121\n",
      "Iteration 4649, loss = 2.66231954\n",
      "Iteration 4650, loss = 2.66232800\n",
      "Iteration 4651, loss = 2.66227495\n",
      "Iteration 4652, loss = 2.66226016\n",
      "Iteration 4653, loss = 2.66217962\n",
      "Iteration 4654, loss = 2.66219653\n",
      "Iteration 4655, loss = 2.66220206\n",
      "Iteration 4656, loss = 2.66209107\n",
      "Iteration 4657, loss = 2.66213857\n",
      "Iteration 4658, loss = 2.66208349\n",
      "Iteration 4659, loss = 2.66208238\n",
      "Iteration 4660, loss = 2.66206751\n",
      "Iteration 4661, loss = 2.66208669\n",
      "Iteration 4662, loss = 2.66198540\n",
      "Iteration 4663, loss = 2.66199380\n",
      "Iteration 4664, loss = 2.66200612\n",
      "Iteration 4665, loss = 2.66194649\n",
      "Iteration 4666, loss = 2.66192028\n",
      "Iteration 4667, loss = 2.66188232\n",
      "Iteration 4668, loss = 2.66192981\n",
      "Iteration 4669, loss = 2.66187587\n",
      "Iteration 4670, loss = 2.66181900\n",
      "Iteration 4671, loss = 2.66177692\n",
      "Iteration 4672, loss = 2.66184321\n",
      "Iteration 4673, loss = 2.66175335\n",
      "Iteration 4674, loss = 2.66177686\n",
      "Iteration 4675, loss = 2.66176188\n",
      "Iteration 4676, loss = 2.66169261\n",
      "Iteration 4677, loss = 2.66168962\n",
      "Iteration 4678, loss = 2.66164506\n",
      "Iteration 4679, loss = 2.66167867\n",
      "Iteration 4680, loss = 2.66161495\n",
      "Iteration 4681, loss = 2.66166233\n",
      "Iteration 4682, loss = 2.66162268\n",
      "Iteration 4683, loss = 2.66158743\n",
      "Iteration 4684, loss = 2.66154601\n",
      "Iteration 4685, loss = 2.66151348\n",
      "Iteration 4686, loss = 2.66151250\n",
      "Iteration 4687, loss = 2.66156291\n",
      "Iteration 4688, loss = 2.66146163\n",
      "Iteration 4689, loss = 2.66148656\n",
      "Iteration 4690, loss = 2.66145431\n",
      "Iteration 4691, loss = 2.66140851\n",
      "Iteration 4692, loss = 2.66142652\n",
      "Iteration 4693, loss = 2.66138048\n",
      "Iteration 4694, loss = 2.66136838\n",
      "Iteration 4695, loss = 2.66137547\n",
      "Iteration 4696, loss = 2.66132009\n",
      "Iteration 4697, loss = 2.66129460\n",
      "Iteration 4698, loss = 2.66127891\n",
      "Iteration 4699, loss = 2.66120917\n",
      "Iteration 4700, loss = 2.66127863\n",
      "Iteration 4701, loss = 2.66120536\n",
      "Iteration 4702, loss = 2.66116447\n",
      "Iteration 4703, loss = 2.66111239\n",
      "Iteration 4704, loss = 2.66113722\n",
      "Iteration 4705, loss = 2.66110475\n",
      "Iteration 4706, loss = 2.66110694\n",
      "Iteration 4707, loss = 2.66109120\n",
      "Iteration 4708, loss = 2.66115969\n",
      "Iteration 4709, loss = 2.66099461\n",
      "Iteration 4710, loss = 2.66099192\n",
      "Iteration 4711, loss = 2.66098776\n",
      "Iteration 4712, loss = 2.66094201\n",
      "Iteration 4713, loss = 2.66099386\n",
      "Iteration 4714, loss = 2.66092393\n",
      "Iteration 4715, loss = 2.66091031\n",
      "Iteration 4716, loss = 2.66093939\n",
      "Iteration 4717, loss = 2.66092702\n",
      "Iteration 4718, loss = 2.66081172\n",
      "Iteration 4719, loss = 2.66082482\n",
      "Iteration 4720, loss = 2.66082351\n",
      "Iteration 4721, loss = 2.66083960\n",
      "Iteration 4722, loss = 2.66075954\n",
      "Iteration 4723, loss = 2.66075524\n",
      "Iteration 4724, loss = 2.66073670\n",
      "Iteration 4725, loss = 2.66070682\n",
      "Iteration 4726, loss = 2.66066732\n",
      "Iteration 4727, loss = 2.66061962\n",
      "Iteration 4728, loss = 2.66063544\n",
      "Iteration 4729, loss = 2.66059701\n",
      "Iteration 4730, loss = 2.66059611\n",
      "Iteration 4731, loss = 2.66059795\n",
      "Iteration 4732, loss = 2.66053436\n",
      "Iteration 4733, loss = 2.66052217\n",
      "Iteration 4734, loss = 2.66051531\n",
      "Iteration 4735, loss = 2.66047253\n",
      "Iteration 4736, loss = 2.66049245\n",
      "Iteration 4737, loss = 2.66044744\n",
      "Iteration 4738, loss = 2.66041723\n",
      "Iteration 4739, loss = 2.66042601\n",
      "Iteration 4740, loss = 2.66042315\n",
      "Iteration 4741, loss = 2.66036280\n",
      "Iteration 4742, loss = 2.66031948\n",
      "Iteration 4743, loss = 2.66032294\n",
      "Iteration 4744, loss = 2.66032702\n",
      "Iteration 4745, loss = 2.66030812\n",
      "Iteration 4746, loss = 2.66030325\n",
      "Iteration 4747, loss = 2.66022868\n",
      "Iteration 4748, loss = 2.66022068\n",
      "Iteration 4749, loss = 2.66032584\n",
      "Iteration 4750, loss = 2.66024746\n",
      "Iteration 4751, loss = 2.66013545\n",
      "Iteration 4752, loss = 2.66016760\n",
      "Iteration 4753, loss = 2.66016115\n",
      "Iteration 4754, loss = 2.66010933\n",
      "Iteration 4755, loss = 2.66008068\n",
      "Iteration 4756, loss = 2.66010306\n",
      "Iteration 4757, loss = 2.66005102\n",
      "Iteration 4758, loss = 2.65999933\n",
      "Iteration 4759, loss = 2.65998224\n",
      "Iteration 4760, loss = 2.66001541\n",
      "Iteration 4761, loss = 2.65993417\n",
      "Iteration 4762, loss = 2.65990585\n",
      "Iteration 4763, loss = 2.65992336\n",
      "Iteration 4764, loss = 2.65990747\n",
      "Iteration 4765, loss = 2.65988387\n",
      "Iteration 4766, loss = 2.65980785\n",
      "Iteration 4767, loss = 2.65980604\n",
      "Iteration 4768, loss = 2.65979912\n",
      "Iteration 4769, loss = 2.65974782\n",
      "Iteration 4770, loss = 2.65974620\n",
      "Iteration 4771, loss = 2.65971439\n",
      "Iteration 4772, loss = 2.65968856\n",
      "Iteration 4773, loss = 2.65971028\n",
      "Iteration 4774, loss = 2.65975554\n",
      "Iteration 4775, loss = 2.65966842\n",
      "Iteration 4776, loss = 2.65968434\n",
      "Iteration 4777, loss = 2.65965806\n",
      "Iteration 4778, loss = 2.65962591\n",
      "Iteration 4779, loss = 2.65957026\n",
      "Iteration 4780, loss = 2.65957569\n",
      "Iteration 4781, loss = 2.65953831\n",
      "Iteration 4782, loss = 2.65953511\n",
      "Iteration 4783, loss = 2.65957929\n",
      "Iteration 4784, loss = 2.65944607\n",
      "Iteration 4785, loss = 2.65945019\n",
      "Iteration 4786, loss = 2.65950018\n",
      "Iteration 4787, loss = 2.65942211\n",
      "Iteration 4788, loss = 2.65942863\n",
      "Iteration 4789, loss = 2.65941397\n",
      "Iteration 4790, loss = 2.65935671\n",
      "Iteration 4791, loss = 2.65931160\n",
      "Iteration 4792, loss = 2.65939393\n",
      "Iteration 4793, loss = 2.65926049\n",
      "Iteration 4794, loss = 2.65931204\n",
      "Iteration 4795, loss = 2.65922753\n",
      "Iteration 4796, loss = 2.65925177\n",
      "Iteration 4797, loss = 2.65920863\n",
      "Iteration 4798, loss = 2.65917301\n",
      "Iteration 4799, loss = 2.65914368\n",
      "Iteration 4800, loss = 2.65916014\n",
      "Iteration 4801, loss = 2.65911448\n",
      "Iteration 4802, loss = 2.65908528\n",
      "Iteration 4803, loss = 2.65912199\n",
      "Iteration 4804, loss = 2.65904358\n",
      "Iteration 4805, loss = 2.65908740\n",
      "Iteration 4806, loss = 2.65899444\n",
      "Iteration 4807, loss = 2.65898689\n",
      "Iteration 4808, loss = 2.65895028\n",
      "Iteration 4809, loss = 2.65901746\n",
      "Iteration 4810, loss = 2.65893202\n",
      "Iteration 4811, loss = 2.65894438\n",
      "Iteration 4812, loss = 2.65898089\n",
      "Iteration 4813, loss = 2.65894691\n",
      "Iteration 4814, loss = 2.65887596\n",
      "Iteration 4815, loss = 2.65886077\n",
      "Iteration 4816, loss = 2.65883729\n",
      "Iteration 4817, loss = 2.65877766\n",
      "Iteration 4818, loss = 2.65880432\n",
      "Iteration 4819, loss = 2.65879741\n",
      "Iteration 4820, loss = 2.65880751\n",
      "Iteration 4821, loss = 2.65873337\n",
      "Iteration 4822, loss = 2.65868559\n",
      "Iteration 4823, loss = 2.65867975\n",
      "Iteration 4824, loss = 2.65866385\n",
      "Iteration 4825, loss = 2.65862830\n",
      "Iteration 4826, loss = 2.65874173\n",
      "Iteration 4827, loss = 2.65861299\n",
      "Iteration 4828, loss = 2.65859006\n",
      "Iteration 4829, loss = 2.65859035\n",
      "Iteration 4830, loss = 2.65853329\n",
      "Iteration 4831, loss = 2.65853590\n",
      "Iteration 4832, loss = 2.65848653\n",
      "Iteration 4833, loss = 2.65849232\n",
      "Iteration 4834, loss = 2.65846297\n",
      "Iteration 4835, loss = 2.65844729\n",
      "Iteration 4836, loss = 2.65844282\n",
      "Iteration 4837, loss = 2.65845021\n",
      "Iteration 4838, loss = 2.65838384\n",
      "Iteration 4839, loss = 2.65838855\n",
      "Iteration 4840, loss = 2.65838708\n",
      "Iteration 4841, loss = 2.65834919\n",
      "Iteration 4842, loss = 2.65830910\n",
      "Iteration 4843, loss = 2.65828046\n",
      "Iteration 4844, loss = 2.65824507\n",
      "Iteration 4845, loss = 2.65825293\n",
      "Iteration 4846, loss = 2.65819968\n",
      "Iteration 4847, loss = 2.65820995\n",
      "Iteration 4848, loss = 2.65815327\n",
      "Iteration 4849, loss = 2.65818376\n",
      "Iteration 4850, loss = 2.65817570\n",
      "Iteration 4851, loss = 2.65812002\n",
      "Iteration 4852, loss = 2.65811726\n",
      "Iteration 4853, loss = 2.65810739\n",
      "Iteration 4854, loss = 2.65804347\n",
      "Iteration 4855, loss = 2.65808229\n",
      "Iteration 4856, loss = 2.65805553\n",
      "Iteration 4857, loss = 2.65796143\n",
      "Iteration 4858, loss = 2.65796917\n",
      "Iteration 4859, loss = 2.65789972\n",
      "Iteration 4860, loss = 2.65792906\n",
      "Iteration 4861, loss = 2.65791527\n",
      "Iteration 4862, loss = 2.65789626\n",
      "Iteration 4863, loss = 2.65785994\n",
      "Iteration 4864, loss = 2.65785416\n",
      "Iteration 4865, loss = 2.65787098\n",
      "Iteration 4866, loss = 2.65784797\n",
      "Iteration 4867, loss = 2.65777246\n",
      "Iteration 4868, loss = 2.65780300\n",
      "Iteration 4869, loss = 2.65775384\n",
      "Iteration 4870, loss = 2.65773862\n",
      "Iteration 4871, loss = 2.65771331\n",
      "Iteration 4872, loss = 2.65768903\n",
      "Iteration 4873, loss = 2.65766319\n",
      "Iteration 4874, loss = 2.65767186\n",
      "Iteration 4875, loss = 2.65761632\n",
      "Iteration 4876, loss = 2.65767477\n",
      "Iteration 4877, loss = 2.65759236\n",
      "Iteration 4878, loss = 2.65753623\n",
      "Iteration 4879, loss = 2.65754567\n",
      "Iteration 4880, loss = 2.65753712\n",
      "Iteration 4881, loss = 2.65750555\n",
      "Iteration 4882, loss = 2.65757291\n",
      "Iteration 4883, loss = 2.65744864\n",
      "Iteration 4884, loss = 2.65746010\n",
      "Iteration 4885, loss = 2.65749730\n",
      "Iteration 4886, loss = 2.65743492\n",
      "Iteration 4887, loss = 2.65738930\n",
      "Iteration 4888, loss = 2.65738329\n",
      "Iteration 4889, loss = 2.65735941\n",
      "Iteration 4890, loss = 2.65735369\n",
      "Iteration 4891, loss = 2.65732464\n",
      "Iteration 4892, loss = 2.65733053\n",
      "Iteration 4893, loss = 2.65726584\n",
      "Iteration 4894, loss = 2.65724214\n",
      "Iteration 4895, loss = 2.65721744\n",
      "Iteration 4896, loss = 2.65717704\n",
      "Iteration 4897, loss = 2.65722191\n",
      "Iteration 4898, loss = 2.65720985\n",
      "Iteration 4899, loss = 2.65715544\n",
      "Iteration 4900, loss = 2.65716453\n",
      "Iteration 4901, loss = 2.65708945\n",
      "Iteration 4902, loss = 2.65710612\n",
      "Iteration 4903, loss = 2.65708469\n",
      "Iteration 4904, loss = 2.65703917\n",
      "Iteration 4905, loss = 2.65702734\n",
      "Iteration 4906, loss = 2.65705367\n",
      "Iteration 4907, loss = 2.65697804\n",
      "Iteration 4908, loss = 2.65699934\n",
      "Iteration 4909, loss = 2.65693806\n",
      "Iteration 4910, loss = 2.65699237\n",
      "Iteration 4911, loss = 2.65700253\n",
      "Iteration 4912, loss = 2.65684288\n",
      "Iteration 4913, loss = 2.65691016\n",
      "Iteration 4914, loss = 2.65695792\n",
      "Iteration 4915, loss = 2.65686722\n",
      "Iteration 4916, loss = 2.65687163\n",
      "Iteration 4917, loss = 2.65691168\n",
      "Iteration 4918, loss = 2.65681443\n",
      "Iteration 4919, loss = 2.65679816\n",
      "Iteration 4920, loss = 2.65678396\n",
      "Iteration 4921, loss = 2.65668581\n",
      "Iteration 4922, loss = 2.65667663\n",
      "Iteration 4923, loss = 2.65671220\n",
      "Iteration 4924, loss = 2.65665042\n",
      "Iteration 4925, loss = 2.65663099\n",
      "Iteration 4926, loss = 2.65659328\n",
      "Iteration 4927, loss = 2.65659094\n",
      "Iteration 4928, loss = 2.65660834\n",
      "Iteration 4929, loss = 2.65658149\n",
      "Iteration 4930, loss = 2.65661031\n",
      "Iteration 4931, loss = 2.65652382\n",
      "Iteration 4932, loss = 2.65652466\n",
      "Iteration 4933, loss = 2.65647591\n",
      "Iteration 4934, loss = 2.65645544\n",
      "Iteration 4935, loss = 2.65644666\n",
      "Iteration 4936, loss = 2.65647799\n",
      "Iteration 4937, loss = 2.65643893\n",
      "Iteration 4938, loss = 2.65642075\n",
      "Iteration 4939, loss = 2.65643713\n",
      "Iteration 4940, loss = 2.65634210\n",
      "Iteration 4941, loss = 2.65627065\n",
      "Iteration 4942, loss = 2.65630745\n",
      "Iteration 4943, loss = 2.65629185\n",
      "Iteration 4944, loss = 2.65631471\n",
      "Iteration 4945, loss = 2.65624614\n",
      "Iteration 4946, loss = 2.65620570\n",
      "Iteration 4947, loss = 2.65621292\n",
      "Iteration 4948, loss = 2.65618102\n",
      "Iteration 4949, loss = 2.65618695\n",
      "Iteration 4950, loss = 2.65613409\n",
      "Iteration 4951, loss = 2.65612848\n",
      "Iteration 4952, loss = 2.65611853\n",
      "Iteration 4953, loss = 2.65609036\n",
      "Iteration 4954, loss = 2.65606697\n",
      "Iteration 4955, loss = 2.65603446\n",
      "Iteration 4956, loss = 2.65606559\n",
      "Iteration 4957, loss = 2.65602703\n",
      "Iteration 4958, loss = 2.65597539\n",
      "Iteration 4959, loss = 2.65594796\n",
      "Iteration 4960, loss = 2.65594169\n",
      "Iteration 4961, loss = 2.65591644\n",
      "Iteration 4962, loss = 2.65595613\n",
      "Iteration 4963, loss = 2.65587655\n",
      "Iteration 4964, loss = 2.65584182\n",
      "Iteration 4965, loss = 2.65588825\n",
      "Iteration 4966, loss = 2.65587813\n",
      "Iteration 4967, loss = 2.65578261\n",
      "Iteration 4968, loss = 2.65582664\n",
      "Iteration 4969, loss = 2.65584872\n",
      "Iteration 4970, loss = 2.65575936\n",
      "Iteration 4971, loss = 2.65573861\n",
      "Iteration 4972, loss = 2.65571328\n",
      "Iteration 4973, loss = 2.65574786\n",
      "Iteration 4974, loss = 2.65565271\n",
      "Iteration 4975, loss = 2.65563825\n",
      "Iteration 4976, loss = 2.65560335\n",
      "Iteration 4977, loss = 2.65562117\n",
      "Iteration 4978, loss = 2.65560956\n",
      "Iteration 4979, loss = 2.65556282\n",
      "Iteration 4980, loss = 2.65559436\n",
      "Iteration 4981, loss = 2.65553499\n",
      "Iteration 4982, loss = 2.65550720\n",
      "Iteration 4983, loss = 2.65545279\n",
      "Iteration 4984, loss = 2.65547330\n",
      "Iteration 4985, loss = 2.65550147\n",
      "Iteration 4986, loss = 2.65554593\n",
      "Iteration 4987, loss = 2.65539203\n",
      "Iteration 4988, loss = 2.65537419\n",
      "Iteration 4989, loss = 2.65535676\n",
      "Iteration 4990, loss = 2.65531973\n",
      "Iteration 4991, loss = 2.65531873\n",
      "Iteration 4992, loss = 2.65533912\n",
      "Iteration 4993, loss = 2.65530000\n",
      "Iteration 4994, loss = 2.65528924\n",
      "Iteration 4995, loss = 2.65527018\n",
      "Iteration 4996, loss = 2.65528937\n",
      "Iteration 4997, loss = 2.65516819\n",
      "Iteration 4998, loss = 2.65518413\n",
      "Iteration 4999, loss = 2.65515259\n",
      "Iteration 5000, loss = 2.65511453\n",
      "Iteration 5001, loss = 2.65513531\n",
      "Iteration 5002, loss = 2.65513781\n",
      "Iteration 5003, loss = 2.65512893\n",
      "Iteration 5004, loss = 2.65508337\n",
      "Iteration 5005, loss = 2.65504641\n",
      "Iteration 5006, loss = 2.65502738\n",
      "Iteration 5007, loss = 2.65505578\n",
      "Iteration 5008, loss = 2.65501685\n",
      "Iteration 5009, loss = 2.65496778\n",
      "Iteration 5010, loss = 2.65494645\n",
      "Iteration 5011, loss = 2.65495213\n",
      "Iteration 5012, loss = 2.65496637\n",
      "Iteration 5013, loss = 2.65493533\n",
      "Iteration 5014, loss = 2.65484769\n",
      "Iteration 5015, loss = 2.65486055\n",
      "Iteration 5016, loss = 2.65484680\n",
      "Iteration 5017, loss = 2.65484418\n",
      "Iteration 5018, loss = 2.65480460\n",
      "Iteration 5019, loss = 2.65482312\n",
      "Iteration 5020, loss = 2.65480160\n",
      "Iteration 5021, loss = 2.65477653\n",
      "Iteration 5022, loss = 2.65477709\n",
      "Iteration 5023, loss = 2.65475068\n",
      "Iteration 5024, loss = 2.65468173\n",
      "Iteration 5025, loss = 2.65468913\n",
      "Iteration 5026, loss = 2.65463488\n",
      "Iteration 5027, loss = 2.65461605\n",
      "Iteration 5028, loss = 2.65462699\n",
      "Iteration 5029, loss = 2.65463211\n",
      "Iteration 5030, loss = 2.65461923\n",
      "Iteration 5031, loss = 2.65453051\n",
      "Iteration 5032, loss = 2.65451307\n",
      "Iteration 5033, loss = 2.65451601\n",
      "Iteration 5034, loss = 2.65446688\n",
      "Iteration 5035, loss = 2.65446320\n",
      "Iteration 5036, loss = 2.65453596\n",
      "Iteration 5037, loss = 2.65451650\n",
      "Iteration 5038, loss = 2.65439772\n",
      "Iteration 5039, loss = 2.65438808\n",
      "Iteration 5040, loss = 2.65439079\n",
      "Iteration 5041, loss = 2.65437330\n",
      "Iteration 5042, loss = 2.65432421\n",
      "Iteration 5043, loss = 2.65429592\n",
      "Iteration 5044, loss = 2.65433508\n",
      "Iteration 5045, loss = 2.65428675\n",
      "Iteration 5046, loss = 2.65427334\n",
      "Iteration 5047, loss = 2.65425032\n",
      "Iteration 5048, loss = 2.65422738\n",
      "Iteration 5049, loss = 2.65421829\n",
      "Iteration 5050, loss = 2.65430535\n",
      "Iteration 5051, loss = 2.65420854\n",
      "Iteration 5052, loss = 2.65417560\n",
      "Iteration 5053, loss = 2.65411371\n",
      "Iteration 5054, loss = 2.65417208\n",
      "Iteration 5055, loss = 2.65411353\n",
      "Iteration 5056, loss = 2.65404161\n",
      "Iteration 5057, loss = 2.65406703\n",
      "Iteration 5058, loss = 2.65400020\n",
      "Iteration 5059, loss = 2.65397542\n",
      "Iteration 5060, loss = 2.65400569\n",
      "Iteration 5061, loss = 2.65399561\n",
      "Iteration 5062, loss = 2.65393462\n",
      "Iteration 5063, loss = 2.65393812\n",
      "Iteration 5064, loss = 2.65393860\n",
      "Iteration 5065, loss = 2.65388190\n",
      "Iteration 5066, loss = 2.65389615\n",
      "Iteration 5067, loss = 2.65395476\n",
      "Iteration 5068, loss = 2.65382731\n",
      "Iteration 5069, loss = 2.65380748\n",
      "Iteration 5070, loss = 2.65376880\n",
      "Iteration 5071, loss = 2.65374673\n",
      "Iteration 5072, loss = 2.65376122\n",
      "Iteration 5073, loss = 2.65376091\n",
      "Iteration 5074, loss = 2.65386023\n",
      "Iteration 5075, loss = 2.65373459\n",
      "Iteration 5076, loss = 2.65372961\n",
      "Iteration 5077, loss = 2.65372660\n",
      "Iteration 5078, loss = 2.65363709\n",
      "Iteration 5079, loss = 2.65365352\n",
      "Iteration 5080, loss = 2.65361886\n",
      "Iteration 5081, loss = 2.65359439\n",
      "Iteration 5082, loss = 2.65352844\n",
      "Iteration 5083, loss = 2.65351271\n",
      "Iteration 5084, loss = 2.65349474\n",
      "Iteration 5085, loss = 2.65353366\n",
      "Iteration 5086, loss = 2.65352325\n",
      "Iteration 5087, loss = 2.65349457\n",
      "Iteration 5088, loss = 2.65352580\n",
      "Iteration 5089, loss = 2.65342608\n",
      "Iteration 5090, loss = 2.65340203\n",
      "Iteration 5091, loss = 2.65342141\n",
      "Iteration 5092, loss = 2.65337432\n",
      "Iteration 5093, loss = 2.65337995\n",
      "Iteration 5094, loss = 2.65332858\n",
      "Iteration 5095, loss = 2.65337729\n",
      "Iteration 5096, loss = 2.65328748\n",
      "Iteration 5097, loss = 2.65326438\n",
      "Iteration 5098, loss = 2.65324513\n",
      "Iteration 5099, loss = 2.65322722\n",
      "Iteration 5100, loss = 2.65322771\n",
      "Iteration 5101, loss = 2.65324596\n",
      "Iteration 5102, loss = 2.65321105\n",
      "Iteration 5103, loss = 2.65317614\n",
      "Iteration 5104, loss = 2.65320909\n",
      "Iteration 5105, loss = 2.65310998\n",
      "Iteration 5106, loss = 2.65311301\n",
      "Iteration 5107, loss = 2.65310199\n",
      "Iteration 5108, loss = 2.65312113\n",
      "Iteration 5109, loss = 2.65304444\n",
      "Iteration 5110, loss = 2.65302189\n",
      "Iteration 5111, loss = 2.65298915\n",
      "Iteration 5112, loss = 2.65300691\n",
      "Iteration 5113, loss = 2.65299657\n",
      "Iteration 5114, loss = 2.65300249\n",
      "Iteration 5115, loss = 2.65292229\n",
      "Iteration 5116, loss = 2.65291920\n",
      "Iteration 5117, loss = 2.65290996\n",
      "Iteration 5118, loss = 2.65289805\n",
      "Iteration 5119, loss = 2.65287215\n",
      "Iteration 5120, loss = 2.65281949\n",
      "Iteration 5121, loss = 2.65287600\n",
      "Iteration 5122, loss = 2.65280871\n",
      "Iteration 5123, loss = 2.65276138\n",
      "Iteration 5124, loss = 2.65272980\n",
      "Iteration 5125, loss = 2.65273844\n",
      "Iteration 5126, loss = 2.65272519\n",
      "Iteration 5127, loss = 2.65268977\n",
      "Iteration 5128, loss = 2.65268322\n",
      "Iteration 5129, loss = 2.65266312\n",
      "Iteration 5130, loss = 2.65263291\n",
      "Iteration 5131, loss = 2.65262669\n",
      "Iteration 5132, loss = 2.65262891\n",
      "Iteration 5133, loss = 2.65253692\n",
      "Iteration 5134, loss = 2.65258324\n",
      "Iteration 5135, loss = 2.65254676\n",
      "Iteration 5136, loss = 2.65260016\n",
      "Iteration 5137, loss = 2.65253986\n",
      "Iteration 5138, loss = 2.65252885\n",
      "Iteration 5139, loss = 2.65256313\n",
      "Iteration 5140, loss = 2.65251898\n",
      "Iteration 5141, loss = 2.65242867\n",
      "Iteration 5142, loss = 2.65235655\n",
      "Iteration 5143, loss = 2.65238350\n",
      "Iteration 5144, loss = 2.65240363\n",
      "Iteration 5145, loss = 2.65235251\n",
      "Iteration 5146, loss = 2.65237295\n",
      "Iteration 5147, loss = 2.65233435\n",
      "Iteration 5148, loss = 2.65231549\n",
      "Iteration 5149, loss = 2.65227915\n",
      "Iteration 5150, loss = 2.65227713\n",
      "Iteration 5151, loss = 2.65222187\n",
      "Iteration 5152, loss = 2.65220007\n",
      "Iteration 5153, loss = 2.65220894\n",
      "Iteration 5154, loss = 2.65219752\n",
      "Iteration 5155, loss = 2.65216267\n",
      "Iteration 5156, loss = 2.65224019\n",
      "Iteration 5157, loss = 2.65216778\n",
      "Iteration 5158, loss = 2.65211882\n",
      "Iteration 5159, loss = 2.65208683\n",
      "Iteration 5160, loss = 2.65212642\n",
      "Iteration 5161, loss = 2.65204480\n",
      "Iteration 5162, loss = 2.65205788\n",
      "Iteration 5163, loss = 2.65199777\n",
      "Iteration 5164, loss = 2.65200973\n",
      "Iteration 5165, loss = 2.65200225\n",
      "Iteration 5166, loss = 2.65198787\n",
      "Iteration 5167, loss = 2.65193964\n",
      "Iteration 5168, loss = 2.65202482\n",
      "Iteration 5169, loss = 2.65202852\n",
      "Iteration 5170, loss = 2.65193745\n",
      "Iteration 5171, loss = 2.65185305\n",
      "Iteration 5172, loss = 2.65190232\n",
      "Iteration 5173, loss = 2.65182065\n",
      "Iteration 5174, loss = 2.65179986\n",
      "Iteration 5175, loss = 2.65177702\n",
      "Iteration 5176, loss = 2.65175286\n",
      "Iteration 5177, loss = 2.65174512\n",
      "Iteration 5178, loss = 2.65172502\n",
      "Iteration 5179, loss = 2.65173609\n",
      "Iteration 5180, loss = 2.65182351\n",
      "Iteration 5181, loss = 2.65178592\n",
      "Iteration 5182, loss = 2.65164775\n",
      "Iteration 5183, loss = 2.65173995\n",
      "Iteration 5184, loss = 2.65165011\n",
      "Iteration 5185, loss = 2.65157502\n",
      "Iteration 5186, loss = 2.65157410\n",
      "Iteration 5187, loss = 2.65163907\n",
      "Iteration 5188, loss = 2.65157360\n",
      "Iteration 5189, loss = 2.65150457\n",
      "Iteration 5190, loss = 2.65153313\n",
      "Iteration 5191, loss = 2.65146874\n",
      "Iteration 5192, loss = 2.65144790\n",
      "Iteration 5193, loss = 2.65143216\n",
      "Iteration 5194, loss = 2.65147153\n",
      "Iteration 5195, loss = 2.65141069\n",
      "Iteration 5196, loss = 2.65140045\n",
      "Iteration 5197, loss = 2.65131028\n",
      "Iteration 5198, loss = 2.65138674\n",
      "Iteration 5199, loss = 2.65135387\n",
      "Iteration 5200, loss = 2.65134022\n",
      "Iteration 5201, loss = 2.65133267\n",
      "Iteration 5202, loss = 2.65133147\n",
      "Iteration 5203, loss = 2.65137512\n",
      "Iteration 5204, loss = 2.65125439\n",
      "Iteration 5205, loss = 2.65122254\n",
      "Iteration 5206, loss = 2.65120281\n",
      "Iteration 5207, loss = 2.65120121\n",
      "Iteration 5208, loss = 2.65120222\n",
      "Iteration 5209, loss = 2.65120952\n",
      "Iteration 5210, loss = 2.65113794\n",
      "Iteration 5211, loss = 2.65113329\n",
      "Iteration 5212, loss = 2.65112669\n",
      "Iteration 5213, loss = 2.65108539\n",
      "Iteration 5214, loss = 2.65103541\n",
      "Iteration 5215, loss = 2.65101704\n",
      "Iteration 5216, loss = 2.65102259\n",
      "Iteration 5217, loss = 2.65098473\n",
      "Iteration 5218, loss = 2.65099151\n",
      "Iteration 5219, loss = 2.65097143\n",
      "Iteration 5220, loss = 2.65101628\n",
      "Iteration 5221, loss = 2.65088094\n",
      "Iteration 5222, loss = 2.65093287\n",
      "Iteration 5223, loss = 2.65089068\n",
      "Iteration 5224, loss = 2.65102421\n",
      "Iteration 5225, loss = 2.65085430\n",
      "Iteration 5226, loss = 2.65084182\n",
      "Iteration 5227, loss = 2.65096790\n",
      "Iteration 5228, loss = 2.65079460\n",
      "Iteration 5229, loss = 2.65076658\n",
      "Iteration 5230, loss = 2.65076192\n",
      "Iteration 5231, loss = 2.65074665\n",
      "Iteration 5232, loss = 2.65069796\n",
      "Iteration 5233, loss = 2.65065847\n",
      "Iteration 5234, loss = 2.65065554\n",
      "Iteration 5235, loss = 2.65077190\n",
      "Iteration 5236, loss = 2.65063074\n",
      "Iteration 5237, loss = 2.65065615\n",
      "Iteration 5238, loss = 2.65065107\n",
      "Iteration 5239, loss = 2.65060572\n",
      "Iteration 5240, loss = 2.65056536\n",
      "Iteration 5241, loss = 2.65055410\n",
      "Iteration 5242, loss = 2.65052658\n",
      "Iteration 5243, loss = 2.65054566\n",
      "Iteration 5244, loss = 2.65051433\n",
      "Iteration 5245, loss = 2.65045726\n",
      "Iteration 5246, loss = 2.65059460\n",
      "Iteration 5247, loss = 2.65048089\n",
      "Iteration 5248, loss = 2.65044854\n",
      "Iteration 5249, loss = 2.65035998\n",
      "Iteration 5250, loss = 2.65038679\n",
      "Iteration 5251, loss = 2.65039315\n",
      "Iteration 5252, loss = 2.65038753\n",
      "Iteration 5253, loss = 2.65032808\n",
      "Iteration 5254, loss = 2.65039514\n",
      "Iteration 5255, loss = 2.65030337\n",
      "Iteration 5256, loss = 2.65025969\n",
      "Iteration 5257, loss = 2.65028816\n",
      "Iteration 5258, loss = 2.65023045\n",
      "Iteration 5259, loss = 2.65029885\n",
      "Iteration 5260, loss = 2.65017987\n",
      "Iteration 5261, loss = 2.65022241\n",
      "Iteration 5262, loss = 2.65023837\n",
      "Iteration 5263, loss = 2.65014797\n",
      "Iteration 5264, loss = 2.65009339\n",
      "Iteration 5265, loss = 2.65009473\n",
      "Iteration 5266, loss = 2.65006467\n",
      "Iteration 5267, loss = 2.65006197\n",
      "Iteration 5268, loss = 2.65009941\n",
      "Iteration 5269, loss = 2.64999735\n",
      "Iteration 5270, loss = 2.64999997\n",
      "Iteration 5271, loss = 2.65000314\n",
      "Iteration 5272, loss = 2.64996881\n",
      "Iteration 5273, loss = 2.64996660\n",
      "Iteration 5274, loss = 2.64993640\n",
      "Iteration 5275, loss = 2.64991229\n",
      "Iteration 5276, loss = 2.64992781\n",
      "Iteration 5277, loss = 2.64988710\n",
      "Iteration 5278, loss = 2.64984196\n",
      "Iteration 5279, loss = 2.64991256\n",
      "Iteration 5280, loss = 2.64984877\n",
      "Iteration 5281, loss = 2.64978731\n",
      "Iteration 5282, loss = 2.64979955\n",
      "Iteration 5283, loss = 2.64975502\n",
      "Iteration 5284, loss = 2.64979253\n",
      "Iteration 5285, loss = 2.64975061\n",
      "Iteration 5286, loss = 2.64972732\n",
      "Iteration 5287, loss = 2.64971896\n",
      "Iteration 5288, loss = 2.64963954\n",
      "Iteration 5289, loss = 2.64967590\n",
      "Iteration 5290, loss = 2.64961797\n",
      "Iteration 5291, loss = 2.64965348\n",
      "Iteration 5292, loss = 2.64961126\n",
      "Iteration 5293, loss = 2.64957506\n",
      "Iteration 5294, loss = 2.64955341\n",
      "Iteration 5295, loss = 2.64957147\n",
      "Iteration 5296, loss = 2.64953708\n",
      "Iteration 5297, loss = 2.64957981\n",
      "Iteration 5298, loss = 2.64948795\n",
      "Iteration 5299, loss = 2.64948422\n",
      "Iteration 5300, loss = 2.64947339\n",
      "Iteration 5301, loss = 2.64943303\n",
      "Iteration 5302, loss = 2.64943472\n",
      "Iteration 5303, loss = 2.64942089\n",
      "Iteration 5304, loss = 2.64943767\n",
      "Iteration 5305, loss = 2.64934056\n",
      "Iteration 5306, loss = 2.64936639\n",
      "Iteration 5307, loss = 2.64934578\n",
      "Iteration 5308, loss = 2.64948331\n",
      "Iteration 5309, loss = 2.64929239\n",
      "Iteration 5310, loss = 2.64925828\n",
      "Iteration 5311, loss = 2.64930594\n",
      "Iteration 5312, loss = 2.64924205\n",
      "Iteration 5313, loss = 2.64930269\n",
      "Iteration 5314, loss = 2.64925875\n",
      "Iteration 5315, loss = 2.64918022\n",
      "Iteration 5316, loss = 2.64913560\n",
      "Iteration 5317, loss = 2.64915850\n",
      "Iteration 5318, loss = 2.64906591\n",
      "Iteration 5319, loss = 2.64913393\n",
      "Iteration 5320, loss = 2.64913282\n",
      "Iteration 5321, loss = 2.64912975\n",
      "Iteration 5322, loss = 2.64911917\n",
      "Iteration 5323, loss = 2.64908841\n",
      "Iteration 5324, loss = 2.64902014\n",
      "Iteration 5325, loss = 2.64900504\n",
      "Iteration 5326, loss = 2.64901533\n",
      "Iteration 5327, loss = 2.64897022\n",
      "Iteration 5328, loss = 2.64891021\n",
      "Iteration 5329, loss = 2.64887611\n",
      "Iteration 5330, loss = 2.64890978\n",
      "Iteration 5331, loss = 2.64885358\n",
      "Iteration 5332, loss = 2.64889762\n",
      "Iteration 5333, loss = 2.64882900\n",
      "Iteration 5334, loss = 2.64880908\n",
      "Iteration 5335, loss = 2.64881314\n",
      "Iteration 5336, loss = 2.64879140\n",
      "Iteration 5337, loss = 2.64883040\n",
      "Iteration 5338, loss = 2.64876036\n",
      "Iteration 5339, loss = 2.64873457\n",
      "Iteration 5340, loss = 2.64871651\n",
      "Iteration 5341, loss = 2.64868648\n",
      "Iteration 5342, loss = 2.64869095\n",
      "Iteration 5343, loss = 2.64876038\n",
      "Iteration 5344, loss = 2.64868152\n",
      "Iteration 5345, loss = 2.64866703\n",
      "Iteration 5346, loss = 2.64862425\n",
      "Iteration 5347, loss = 2.64852470\n",
      "Iteration 5348, loss = 2.64858357\n",
      "Iteration 5349, loss = 2.64858111\n",
      "Iteration 5350, loss = 2.64853944\n",
      "Iteration 5351, loss = 2.64853509\n",
      "Iteration 5352, loss = 2.64862983\n",
      "Iteration 5353, loss = 2.64848782\n",
      "Iteration 5354, loss = 2.64851884\n",
      "Iteration 5355, loss = 2.64845628\n",
      "Iteration 5356, loss = 2.64840197\n",
      "Iteration 5357, loss = 2.64837627\n",
      "Iteration 5358, loss = 2.64843010\n",
      "Iteration 5359, loss = 2.64842920\n",
      "Iteration 5360, loss = 2.64839507\n",
      "Iteration 5361, loss = 2.64836024\n",
      "Iteration 5362, loss = 2.64827278\n",
      "Iteration 5363, loss = 2.64827712\n",
      "Iteration 5364, loss = 2.64826582\n",
      "Iteration 5365, loss = 2.64827119\n",
      "Iteration 5366, loss = 2.64820281\n",
      "Iteration 5367, loss = 2.64820991\n",
      "Iteration 5368, loss = 2.64817460\n",
      "Iteration 5369, loss = 2.64818371\n",
      "Iteration 5370, loss = 2.64818273\n",
      "Iteration 5371, loss = 2.64817461\n",
      "Iteration 5372, loss = 2.64819314\n",
      "Iteration 5373, loss = 2.64811900\n",
      "Iteration 5374, loss = 2.64817182\n",
      "Iteration 5375, loss = 2.64811109\n",
      "Iteration 5376, loss = 2.64803959\n",
      "Iteration 5377, loss = 2.64801777\n",
      "Iteration 5378, loss = 2.64801050\n",
      "Iteration 5379, loss = 2.64799468\n",
      "Iteration 5380, loss = 2.64801634\n",
      "Iteration 5381, loss = 2.64799699\n",
      "Iteration 5382, loss = 2.64793640\n",
      "Iteration 5383, loss = 2.64791466\n",
      "Iteration 5384, loss = 2.64788077\n",
      "Iteration 5385, loss = 2.64789349\n",
      "Iteration 5386, loss = 2.64787171\n",
      "Iteration 5387, loss = 2.64785723\n",
      "Iteration 5388, loss = 2.64784312\n",
      "Iteration 5389, loss = 2.64780533\n",
      "Iteration 5390, loss = 2.64779868\n",
      "Iteration 5391, loss = 2.64779885\n",
      "Iteration 5392, loss = 2.64785481\n",
      "Iteration 5393, loss = 2.64780307\n",
      "Iteration 5394, loss = 2.64772235\n",
      "Iteration 5395, loss = 2.64769090\n",
      "Iteration 5396, loss = 2.64770319\n",
      "Iteration 5397, loss = 2.64767327\n",
      "Iteration 5398, loss = 2.64778769\n",
      "Iteration 5399, loss = 2.64761368\n",
      "Iteration 5400, loss = 2.64765217\n",
      "Iteration 5401, loss = 2.64756488\n",
      "Iteration 5402, loss = 2.64761106\n",
      "Iteration 5403, loss = 2.64758009\n",
      "Iteration 5404, loss = 2.64756786\n",
      "Iteration 5405, loss = 2.64749680\n",
      "Iteration 5406, loss = 2.64752053\n",
      "Iteration 5407, loss = 2.64748733\n",
      "Iteration 5408, loss = 2.64757060\n",
      "Iteration 5409, loss = 2.64746746\n",
      "Iteration 5410, loss = 2.64745922\n",
      "Iteration 5411, loss = 2.64746694\n",
      "Iteration 5412, loss = 2.64744825\n",
      "Iteration 5413, loss = 2.64740691\n",
      "Iteration 5414, loss = 2.64741744\n",
      "Iteration 5415, loss = 2.64734512\n",
      "Iteration 5416, loss = 2.64733831\n",
      "Iteration 5417, loss = 2.64731274\n",
      "Iteration 5418, loss = 2.64732486\n",
      "Iteration 5419, loss = 2.64728699\n",
      "Iteration 5420, loss = 2.64727445\n",
      "Iteration 5421, loss = 2.64722085\n",
      "Iteration 5422, loss = 2.64723487\n",
      "Iteration 5423, loss = 2.64718305\n",
      "Iteration 5424, loss = 2.64720894\n",
      "Iteration 5425, loss = 2.64715075\n",
      "Iteration 5426, loss = 2.64721341\n",
      "Iteration 5427, loss = 2.64729962\n",
      "Iteration 5428, loss = 2.64716367\n",
      "Iteration 5429, loss = 2.64710096\n",
      "Iteration 5430, loss = 2.64709985\n",
      "Iteration 5431, loss = 2.64704411\n",
      "Iteration 5432, loss = 2.64700418\n",
      "Iteration 5433, loss = 2.64706071\n",
      "Iteration 5434, loss = 2.64705892\n",
      "Iteration 5435, loss = 2.64698113\n",
      "Iteration 5436, loss = 2.64692498\n",
      "Iteration 5437, loss = 2.64696791\n",
      "Iteration 5438, loss = 2.64690685\n",
      "Iteration 5439, loss = 2.64694996\n",
      "Iteration 5440, loss = 2.64690807\n",
      "Iteration 5441, loss = 2.64691349\n",
      "Iteration 5442, loss = 2.64685683\n",
      "Iteration 5443, loss = 2.64684447\n",
      "Iteration 5444, loss = 2.64684649\n",
      "Iteration 5445, loss = 2.64687419\n",
      "Iteration 5446, loss = 2.64680702\n",
      "Iteration 5447, loss = 2.64672939\n",
      "Iteration 5448, loss = 2.64674365\n",
      "Iteration 5449, loss = 2.64671547\n",
      "Iteration 5450, loss = 2.64676504\n",
      "Iteration 5451, loss = 2.64670603\n",
      "Iteration 5452, loss = 2.64670647\n",
      "Iteration 5453, loss = 2.64668691\n",
      "Iteration 5454, loss = 2.64691611\n",
      "Iteration 5455, loss = 2.64685084\n",
      "Iteration 5456, loss = 2.64662801\n",
      "Iteration 5457, loss = 2.64658915\n",
      "Iteration 5458, loss = 2.64663591\n",
      "Iteration 5459, loss = 2.64653403\n",
      "Iteration 5460, loss = 2.64653414\n",
      "Iteration 5461, loss = 2.64655351\n",
      "Iteration 5462, loss = 2.64650059\n",
      "Iteration 5463, loss = 2.64645390\n",
      "Iteration 5464, loss = 2.64646000\n",
      "Iteration 5465, loss = 2.64646225\n",
      "Iteration 5466, loss = 2.64641801\n",
      "Iteration 5467, loss = 2.64645525\n",
      "Iteration 5468, loss = 2.64642543\n",
      "Iteration 5469, loss = 2.64639230\n",
      "Iteration 5470, loss = 2.64638155\n",
      "Iteration 5471, loss = 2.64632648\n",
      "Iteration 5472, loss = 2.64633998\n",
      "Iteration 5473, loss = 2.64637444\n",
      "Iteration 5474, loss = 2.64630113\n",
      "Iteration 5475, loss = 2.64633632\n",
      "Iteration 5476, loss = 2.64623027\n",
      "Iteration 5477, loss = 2.64622781\n",
      "Iteration 5478, loss = 2.64621673\n",
      "Iteration 5479, loss = 2.64618967\n",
      "Iteration 5480, loss = 2.64621039\n",
      "Iteration 5481, loss = 2.64619911\n",
      "Iteration 5482, loss = 2.64619533\n",
      "Iteration 5483, loss = 2.64613344\n",
      "Iteration 5484, loss = 2.64612092\n",
      "Iteration 5485, loss = 2.64607172\n",
      "Iteration 5486, loss = 2.64607502\n",
      "Iteration 5487, loss = 2.64605928\n",
      "Iteration 5488, loss = 2.64602872\n",
      "Iteration 5489, loss = 2.64602643\n",
      "Iteration 5490, loss = 2.64604295\n",
      "Iteration 5491, loss = 2.64599486\n",
      "Iteration 5492, loss = 2.64594577\n",
      "Iteration 5493, loss = 2.64593608\n",
      "Iteration 5494, loss = 2.64593859\n",
      "Iteration 5495, loss = 2.64595275\n",
      "Iteration 5496, loss = 2.64591041\n",
      "Iteration 5497, loss = 2.64583351\n",
      "Iteration 5498, loss = 2.64586869\n",
      "Iteration 5499, loss = 2.64579796\n",
      "Iteration 5500, loss = 2.64580645\n",
      "Iteration 5501, loss = 2.64578340\n",
      "Iteration 5502, loss = 2.64581103\n",
      "Iteration 5503, loss = 2.64573416\n",
      "Iteration 5504, loss = 2.64571527\n",
      "Iteration 5505, loss = 2.64574007\n",
      "Iteration 5506, loss = 2.64568627\n",
      "Iteration 5507, loss = 2.64570274\n",
      "Iteration 5508, loss = 2.64564827\n",
      "Iteration 5509, loss = 2.64566486\n",
      "Iteration 5510, loss = 2.64563920\n",
      "Iteration 5511, loss = 2.64566231\n",
      "Iteration 5512, loss = 2.64559800\n",
      "Iteration 5513, loss = 2.64562543\n",
      "Iteration 5514, loss = 2.64565590\n",
      "Iteration 5515, loss = 2.64562204\n",
      "Iteration 5516, loss = 2.64555232\n",
      "Iteration 5517, loss = 2.64549944\n",
      "Iteration 5518, loss = 2.64548554\n",
      "Iteration 5519, loss = 2.64546341\n",
      "Iteration 5520, loss = 2.64551519\n",
      "Iteration 5521, loss = 2.64546728\n",
      "Iteration 5522, loss = 2.64542224\n",
      "Iteration 5523, loss = 2.64544458\n",
      "Iteration 5524, loss = 2.64548411\n",
      "Iteration 5525, loss = 2.64540095\n",
      "Iteration 5526, loss = 2.64535610\n",
      "Iteration 5527, loss = 2.64539873\n",
      "Iteration 5528, loss = 2.64532013\n",
      "Iteration 5529, loss = 2.64530926\n",
      "Iteration 5530, loss = 2.64531072\n",
      "Iteration 5531, loss = 2.64527547\n",
      "Iteration 5532, loss = 2.64525924\n",
      "Iteration 5533, loss = 2.64522056\n",
      "Iteration 5534, loss = 2.64521682\n",
      "Iteration 5535, loss = 2.64519330\n",
      "Iteration 5536, loss = 2.64520131\n",
      "Iteration 5537, loss = 2.64526168\n",
      "Iteration 5538, loss = 2.64525800\n",
      "Iteration 5539, loss = 2.64514603\n",
      "Iteration 5540, loss = 2.64515012\n",
      "Iteration 5541, loss = 2.64510079\n",
      "Iteration 5542, loss = 2.64505047\n",
      "Iteration 5543, loss = 2.64509991\n",
      "Iteration 5544, loss = 2.64512064\n",
      "Iteration 5545, loss = 2.64506971\n",
      "Iteration 5546, loss = 2.64500305\n",
      "Iteration 5547, loss = 2.64503715\n",
      "Iteration 5548, loss = 2.64502684\n",
      "Iteration 5549, loss = 2.64505907\n",
      "Iteration 5550, loss = 2.64495111\n",
      "Iteration 5551, loss = 2.64482194\n",
      "Iteration 5552, loss = 2.64489862\n",
      "Iteration 5553, loss = 2.64489839\n",
      "Iteration 5554, loss = 2.64486439\n",
      "Iteration 5555, loss = 2.64488638\n",
      "Iteration 5556, loss = 2.64482628\n",
      "Iteration 5557, loss = 2.64483232\n",
      "Iteration 5558, loss = 2.64478485\n",
      "Iteration 5559, loss = 2.64479568\n",
      "Iteration 5560, loss = 2.64477891\n",
      "Iteration 5561, loss = 2.64480099\n",
      "Iteration 5562, loss = 2.64469921\n",
      "Iteration 5563, loss = 2.64474151\n",
      "Iteration 5564, loss = 2.64466363\n",
      "Iteration 5565, loss = 2.64468409\n",
      "Iteration 5566, loss = 2.64464939\n",
      "Iteration 5567, loss = 2.64459787\n",
      "Iteration 5568, loss = 2.64463230\n",
      "Iteration 5569, loss = 2.64463071\n",
      "Iteration 5570, loss = 2.64459516\n",
      "Iteration 5571, loss = 2.64454079\n",
      "Iteration 5572, loss = 2.64455502\n",
      "Iteration 5573, loss = 2.64449202\n",
      "Iteration 5574, loss = 2.64450435\n",
      "Iteration 5575, loss = 2.64449160\n",
      "Iteration 5576, loss = 2.64453031\n",
      "Iteration 5577, loss = 2.64447856\n",
      "Iteration 5578, loss = 2.64447745\n",
      "Iteration 5579, loss = 2.64447987\n",
      "Iteration 5580, loss = 2.64441289\n",
      "Iteration 5581, loss = 2.64444641\n",
      "Iteration 5582, loss = 2.64433032\n",
      "Iteration 5583, loss = 2.64440167\n",
      "Iteration 5584, loss = 2.64443163\n",
      "Iteration 5585, loss = 2.64429539\n",
      "Iteration 5586, loss = 2.64428795\n",
      "Iteration 5587, loss = 2.64428509\n",
      "Iteration 5588, loss = 2.64426540\n",
      "Iteration 5589, loss = 2.64419611\n",
      "Iteration 5590, loss = 2.64421358\n",
      "Iteration 5591, loss = 2.64417214\n",
      "Iteration 5592, loss = 2.64417343\n",
      "Iteration 5593, loss = 2.64416435\n",
      "Iteration 5594, loss = 2.64419519\n",
      "Iteration 5595, loss = 2.64410729\n",
      "Iteration 5596, loss = 2.64410819\n",
      "Iteration 5597, loss = 2.64409387\n",
      "Iteration 5598, loss = 2.64413206\n",
      "Iteration 5599, loss = 2.64404310\n",
      "Iteration 5600, loss = 2.64404206\n",
      "Iteration 5601, loss = 2.64405277\n",
      "Iteration 5602, loss = 2.64407335\n",
      "Iteration 5603, loss = 2.64399048\n",
      "Iteration 5604, loss = 2.64403103\n",
      "Iteration 5605, loss = 2.64395995\n",
      "Iteration 5606, loss = 2.64393545\n",
      "Iteration 5607, loss = 2.64393031\n",
      "Iteration 5608, loss = 2.64390767\n",
      "Iteration 5609, loss = 2.64386451\n",
      "Iteration 5610, loss = 2.64392971\n",
      "Iteration 5611, loss = 2.64383844\n",
      "Iteration 5612, loss = 2.64384495\n",
      "Iteration 5613, loss = 2.64383233\n",
      "Iteration 5614, loss = 2.64380862\n",
      "Iteration 5615, loss = 2.64378942\n",
      "Iteration 5616, loss = 2.64378023\n",
      "Iteration 5617, loss = 2.64382362\n",
      "Iteration 5618, loss = 2.64373219\n",
      "Iteration 5619, loss = 2.64368807\n",
      "Iteration 5620, loss = 2.64366626\n",
      "Iteration 5621, loss = 2.64364711\n",
      "Iteration 5622, loss = 2.64365961\n",
      "Iteration 5623, loss = 2.64363209\n",
      "Iteration 5624, loss = 2.64361427\n",
      "Iteration 5625, loss = 2.64359181\n",
      "Iteration 5626, loss = 2.64363214\n",
      "Iteration 5627, loss = 2.64356209\n",
      "Iteration 5628, loss = 2.64359963\n",
      "Iteration 5629, loss = 2.64355669\n",
      "Iteration 5630, loss = 2.64351910\n",
      "Iteration 5631, loss = 2.64351404\n",
      "Iteration 5632, loss = 2.64351458\n",
      "Iteration 5633, loss = 2.64348852\n",
      "Iteration 5634, loss = 2.64351315\n",
      "Iteration 5635, loss = 2.64349840\n",
      "Iteration 5636, loss = 2.64343513\n",
      "Iteration 5637, loss = 2.64344984\n",
      "Iteration 5638, loss = 2.64338154\n",
      "Iteration 5639, loss = 2.64339108\n",
      "Iteration 5640, loss = 2.64337774\n",
      "Iteration 5641, loss = 2.64334355\n",
      "Iteration 5642, loss = 2.64331861\n",
      "Iteration 5643, loss = 2.64332964\n",
      "Iteration 5644, loss = 2.64324139\n",
      "Iteration 5645, loss = 2.64326790\n",
      "Iteration 5646, loss = 2.64323138\n",
      "Iteration 5647, loss = 2.64321644\n",
      "Iteration 5648, loss = 2.64322737\n",
      "Iteration 5649, loss = 2.64318152\n",
      "Iteration 5650, loss = 2.64317417\n",
      "Iteration 5651, loss = 2.64316912\n",
      "Iteration 5652, loss = 2.64313517\n",
      "Iteration 5653, loss = 2.64315552\n",
      "Iteration 5654, loss = 2.64311377\n",
      "Iteration 5655, loss = 2.64309607\n",
      "Iteration 5656, loss = 2.64305238\n",
      "Iteration 5657, loss = 2.64307809\n",
      "Iteration 5658, loss = 2.64304869\n",
      "Iteration 5659, loss = 2.64303098\n",
      "Iteration 5660, loss = 2.64301222\n",
      "Iteration 5661, loss = 2.64293512\n",
      "Iteration 5662, loss = 2.64298303\n",
      "Iteration 5663, loss = 2.64292432\n",
      "Iteration 5664, loss = 2.64290380\n",
      "Iteration 5665, loss = 2.64289060\n",
      "Iteration 5666, loss = 2.64290243\n",
      "Iteration 5667, loss = 2.64290752\n",
      "Iteration 5668, loss = 2.64288934\n",
      "Iteration 5669, loss = 2.64282826\n",
      "Iteration 5670, loss = 2.64285205\n",
      "Iteration 5671, loss = 2.64280898\n",
      "Iteration 5672, loss = 2.64277962\n",
      "Iteration 5673, loss = 2.64279573\n",
      "Iteration 5674, loss = 2.64272888\n",
      "Iteration 5675, loss = 2.64272449\n",
      "Iteration 5676, loss = 2.64272593\n",
      "Iteration 5677, loss = 2.64264754\n",
      "Iteration 5678, loss = 2.64270893\n",
      "Iteration 5679, loss = 2.64267837\n",
      "Iteration 5680, loss = 2.64264025\n",
      "Iteration 5681, loss = 2.64262494\n",
      "Iteration 5682, loss = 2.64267020\n",
      "Iteration 5683, loss = 2.64281277\n",
      "Iteration 5684, loss = 2.64258094\n",
      "Iteration 5685, loss = 2.64257529\n",
      "Iteration 5686, loss = 2.64261322\n",
      "Iteration 5687, loss = 2.64256669\n",
      "Iteration 5688, loss = 2.64252027\n",
      "Iteration 5689, loss = 2.64248298\n",
      "Iteration 5690, loss = 2.64248496\n",
      "Iteration 5691, loss = 2.64244466\n",
      "Iteration 5692, loss = 2.64250783\n",
      "Iteration 5693, loss = 2.64251705\n",
      "Iteration 5694, loss = 2.64247249\n",
      "Iteration 5695, loss = 2.64237802\n",
      "Iteration 5696, loss = 2.64235207\n",
      "Iteration 5697, loss = 2.64236680\n",
      "Iteration 5698, loss = 2.64233849\n",
      "Iteration 5699, loss = 2.64234940\n",
      "Iteration 5700, loss = 2.64231812\n",
      "Iteration 5701, loss = 2.64226927\n",
      "Iteration 5702, loss = 2.64226409\n",
      "Iteration 5703, loss = 2.64226320\n",
      "Iteration 5704, loss = 2.64227650\n",
      "Iteration 5705, loss = 2.64220640\n",
      "Iteration 5706, loss = 2.64218097\n",
      "Iteration 5707, loss = 2.64215424\n",
      "Iteration 5708, loss = 2.64216242\n",
      "Iteration 5709, loss = 2.64214715\n",
      "Iteration 5710, loss = 2.64214674\n",
      "Iteration 5711, loss = 2.64207118\n",
      "Iteration 5712, loss = 2.64214347\n",
      "Iteration 5713, loss = 2.64207715\n",
      "Iteration 5714, loss = 2.64210323\n",
      "Iteration 5715, loss = 2.64207732\n",
      "Iteration 5716, loss = 2.64203794\n",
      "Iteration 5717, loss = 2.64207766\n",
      "Iteration 5718, loss = 2.64197214\n",
      "Iteration 5719, loss = 2.64197867\n",
      "Iteration 5720, loss = 2.64197592\n",
      "Iteration 5721, loss = 2.64203354\n",
      "Iteration 5722, loss = 2.64198670\n",
      "Iteration 5723, loss = 2.64192380\n",
      "Iteration 5724, loss = 2.64186771\n",
      "Iteration 5725, loss = 2.64191102\n",
      "Iteration 5726, loss = 2.64191592\n",
      "Iteration 5727, loss = 2.64183855\n",
      "Iteration 5728, loss = 2.64177161\n",
      "Iteration 5729, loss = 2.64179079\n",
      "Iteration 5730, loss = 2.64184297\n",
      "Iteration 5731, loss = 2.64179190\n",
      "Iteration 5732, loss = 2.64172079\n",
      "Iteration 5733, loss = 2.64178190\n",
      "Iteration 5734, loss = 2.64176852\n",
      "Iteration 5735, loss = 2.64174609\n",
      "Iteration 5736, loss = 2.64173051\n",
      "Iteration 5737, loss = 2.64174939\n",
      "Iteration 5738, loss = 2.64164395\n",
      "Iteration 5739, loss = 2.64162232\n",
      "Iteration 5740, loss = 2.64160064\n",
      "Iteration 5741, loss = 2.64163867\n",
      "Iteration 5742, loss = 2.64163457\n",
      "Iteration 5743, loss = 2.64165960\n",
      "Iteration 5744, loss = 2.64154406\n",
      "Iteration 5745, loss = 2.64154979\n",
      "Iteration 5746, loss = 2.64148792\n",
      "Iteration 5747, loss = 2.64147639\n",
      "Iteration 5748, loss = 2.64143876\n",
      "Iteration 5749, loss = 2.64144632\n",
      "Iteration 5750, loss = 2.64143383\n",
      "Iteration 5751, loss = 2.64148166\n",
      "Iteration 5752, loss = 2.64149748\n",
      "Iteration 5753, loss = 2.64143223\n",
      "Iteration 5754, loss = 2.64136673\n",
      "Iteration 5755, loss = 2.64139558\n",
      "Iteration 5756, loss = 2.64140855\n",
      "Iteration 5757, loss = 2.64127176\n",
      "Iteration 5758, loss = 2.64132280\n",
      "Iteration 5759, loss = 2.64128827\n",
      "Iteration 5760, loss = 2.64130285\n",
      "Iteration 5761, loss = 2.64124930\n",
      "Iteration 5762, loss = 2.64131627\n",
      "Iteration 5763, loss = 2.64120367\n",
      "Iteration 5764, loss = 2.64124886\n",
      "Iteration 5765, loss = 2.64123453\n",
      "Iteration 5766, loss = 2.64119425\n",
      "Iteration 5767, loss = 2.64118314\n",
      "Iteration 5768, loss = 2.64115241\n",
      "Iteration 5769, loss = 2.64116113\n",
      "Iteration 5770, loss = 2.64107020\n",
      "Iteration 5771, loss = 2.64108053\n",
      "Iteration 5772, loss = 2.64107269\n",
      "Iteration 5773, loss = 2.64103212\n",
      "Iteration 5774, loss = 2.64102129\n",
      "Iteration 5775, loss = 2.64103705\n",
      "Iteration 5776, loss = 2.64097104\n",
      "Iteration 5777, loss = 2.64101179\n",
      "Iteration 5778, loss = 2.64098096\n",
      "Iteration 5779, loss = 2.64093950\n",
      "Iteration 5780, loss = 2.64093135\n",
      "Iteration 5781, loss = 2.64088891\n",
      "Iteration 5782, loss = 2.64085922\n",
      "Iteration 5783, loss = 2.64087992\n",
      "Iteration 5784, loss = 2.64083942\n",
      "Iteration 5785, loss = 2.64086380\n",
      "Iteration 5786, loss = 2.64083925\n",
      "Iteration 5787, loss = 2.64085577\n",
      "Iteration 5788, loss = 2.64083848\n",
      "Iteration 5789, loss = 2.64076450\n",
      "Iteration 5790, loss = 2.64073860\n",
      "Iteration 5791, loss = 2.64073145\n",
      "Iteration 5792, loss = 2.64071432\n",
      "Iteration 5793, loss = 2.64072822\n",
      "Iteration 5794, loss = 2.64066487\n",
      "Iteration 5795, loss = 2.64069932\n",
      "Iteration 5796, loss = 2.64075973\n",
      "Iteration 5797, loss = 2.64068699\n",
      "Iteration 5798, loss = 2.64074775\n",
      "Iteration 5799, loss = 2.64062626\n",
      "Iteration 5800, loss = 2.64060711\n",
      "Iteration 5801, loss = 2.64053485\n",
      "Iteration 5802, loss = 2.64052266\n",
      "Iteration 5803, loss = 2.64057647\n",
      "Iteration 5804, loss = 2.64049095\n",
      "Iteration 5805, loss = 2.64049089\n",
      "Iteration 5806, loss = 2.64050230\n",
      "Iteration 5807, loss = 2.64059690\n",
      "Iteration 5808, loss = 2.64045581\n",
      "Iteration 5809, loss = 2.64053565\n",
      "Iteration 5810, loss = 2.64045982\n",
      "Iteration 5811, loss = 2.64042111\n",
      "Iteration 5812, loss = 2.64036318\n",
      "Iteration 5813, loss = 2.64040945\n",
      "Iteration 5814, loss = 2.64033248\n",
      "Iteration 5815, loss = 2.64028817\n",
      "Iteration 5816, loss = 2.64031941\n",
      "Iteration 5817, loss = 2.64029544\n",
      "Iteration 5818, loss = 2.64030937\n",
      "Iteration 5819, loss = 2.64023206\n",
      "Iteration 5820, loss = 2.64024919\n",
      "Iteration 5821, loss = 2.64031650\n",
      "Iteration 5822, loss = 2.64023638\n",
      "Iteration 5823, loss = 2.64017189\n",
      "Iteration 5824, loss = 2.64022098\n",
      "Iteration 5825, loss = 2.64017315\n",
      "Iteration 5826, loss = 2.64014402\n",
      "Iteration 5827, loss = 2.64012686\n",
      "Iteration 5828, loss = 2.64016247\n",
      "Iteration 5829, loss = 2.64010726\n",
      "Iteration 5830, loss = 2.64010333\n",
      "Iteration 5831, loss = 2.64002495\n",
      "Iteration 5832, loss = 2.64003781\n",
      "Iteration 5833, loss = 2.64003472\n",
      "Iteration 5834, loss = 2.64002653\n",
      "Iteration 5835, loss = 2.64000289\n",
      "Iteration 5836, loss = 2.63998916\n",
      "Iteration 5837, loss = 2.63993400\n",
      "Iteration 5838, loss = 2.63996106\n",
      "Iteration 5839, loss = 2.63999272\n",
      "Iteration 5840, loss = 2.63994233\n",
      "Iteration 5841, loss = 2.63991940\n",
      "Iteration 5842, loss = 2.63995274\n",
      "Iteration 5843, loss = 2.63986390\n",
      "Iteration 5844, loss = 2.63983382\n",
      "Iteration 5845, loss = 2.63981427\n",
      "Iteration 5846, loss = 2.63989408\n",
      "Iteration 5847, loss = 2.63973434\n",
      "Iteration 5848, loss = 2.63975357\n",
      "Iteration 5849, loss = 2.63975426\n",
      "Iteration 5850, loss = 2.63978072\n",
      "Iteration 5851, loss = 2.63970705\n",
      "Iteration 5852, loss = 2.63973498\n",
      "Iteration 5853, loss = 2.63972767\n",
      "Iteration 5854, loss = 2.63969864\n",
      "Iteration 5855, loss = 2.63964796\n",
      "Iteration 5856, loss = 2.63962280\n",
      "Iteration 5857, loss = 2.63961416\n",
      "Iteration 5858, loss = 2.63966886\n",
      "Iteration 5859, loss = 2.63958130\n",
      "Iteration 5860, loss = 2.63970769\n",
      "Iteration 5861, loss = 2.63955693\n",
      "Iteration 5862, loss = 2.63952335\n",
      "Iteration 5863, loss = 2.63953964\n",
      "Iteration 5864, loss = 2.63953752\n",
      "Iteration 5865, loss = 2.63954624\n",
      "Iteration 5866, loss = 2.63947575\n",
      "Iteration 5867, loss = 2.63948146\n",
      "Iteration 5868, loss = 2.63944721\n",
      "Iteration 5869, loss = 2.63944950\n",
      "Iteration 5870, loss = 2.63942869\n",
      "Iteration 5871, loss = 2.63939703\n",
      "Iteration 5872, loss = 2.63937677\n",
      "Iteration 5873, loss = 2.63935222\n",
      "Iteration 5874, loss = 2.63938750\n",
      "Iteration 5875, loss = 2.63931888\n",
      "Iteration 5876, loss = 2.63933639\n",
      "Iteration 5877, loss = 2.63932058\n",
      "Iteration 5878, loss = 2.63925104\n",
      "Iteration 5879, loss = 2.63927463\n",
      "Iteration 5880, loss = 2.63921688\n",
      "Iteration 5881, loss = 2.63925912\n",
      "Iteration 5882, loss = 2.63922451\n",
      "Iteration 5883, loss = 2.63926214\n",
      "Iteration 5884, loss = 2.63913972\n",
      "Iteration 5885, loss = 2.63909784\n",
      "Iteration 5886, loss = 2.63913671\n",
      "Iteration 5887, loss = 2.63912804\n",
      "Iteration 5888, loss = 2.63909860\n",
      "Iteration 5889, loss = 2.63906226\n",
      "Iteration 5890, loss = 2.63905934\n",
      "Iteration 5891, loss = 2.63907951\n",
      "Iteration 5892, loss = 2.63903031\n",
      "Iteration 5893, loss = 2.63900740\n",
      "Iteration 5894, loss = 2.63902653\n",
      "Iteration 5895, loss = 2.63903580\n",
      "Iteration 5896, loss = 2.63898145\n",
      "Iteration 5897, loss = 2.63897697\n",
      "Iteration 5898, loss = 2.63906234\n",
      "Iteration 5899, loss = 2.63892917\n",
      "Iteration 5900, loss = 2.63892438\n",
      "Iteration 5901, loss = 2.63884376\n",
      "Iteration 5902, loss = 2.63887471\n",
      "Iteration 5903, loss = 2.63885227\n",
      "Iteration 5904, loss = 2.63882584\n",
      "Iteration 5905, loss = 2.63882828\n",
      "Iteration 5906, loss = 2.63879075\n",
      "Iteration 5907, loss = 2.63879971\n",
      "Iteration 5908, loss = 2.63872914\n",
      "Iteration 5909, loss = 2.63874861\n",
      "Iteration 5910, loss = 2.63873005\n",
      "Iteration 5911, loss = 2.63873411\n",
      "Iteration 5912, loss = 2.63867846\n",
      "Iteration 5913, loss = 2.63872583\n",
      "Iteration 5914, loss = 2.63866434\n",
      "Iteration 5915, loss = 2.63867059\n",
      "Iteration 5916, loss = 2.63869151\n",
      "Iteration 5917, loss = 2.63861554\n",
      "Iteration 5918, loss = 2.63860375\n",
      "Iteration 5919, loss = 2.63867529\n",
      "Iteration 5920, loss = 2.63856376\n",
      "Iteration 5921, loss = 2.63850350\n",
      "Iteration 5922, loss = 2.63856735\n",
      "Iteration 5923, loss = 2.63846441\n",
      "Iteration 5924, loss = 2.63845695\n",
      "Iteration 5925, loss = 2.63855477\n",
      "Iteration 5926, loss = 2.63845665\n",
      "Iteration 5927, loss = 2.63844981\n",
      "Iteration 5928, loss = 2.63840202\n",
      "Iteration 5929, loss = 2.63845176\n",
      "Iteration 5930, loss = 2.63838414\n",
      "Iteration 5931, loss = 2.63834237\n",
      "Iteration 5932, loss = 2.63833179\n",
      "Iteration 5933, loss = 2.63831768\n",
      "Iteration 5934, loss = 2.63828684\n",
      "Iteration 5935, loss = 2.63828171\n",
      "Iteration 5936, loss = 2.63826763\n",
      "Iteration 5937, loss = 2.63833866\n",
      "Iteration 5938, loss = 2.63828431\n",
      "Iteration 5939, loss = 2.63826132\n",
      "Iteration 5940, loss = 2.63827594\n",
      "Iteration 5941, loss = 2.63825727\n",
      "Iteration 5942, loss = 2.63822316\n",
      "Iteration 5943, loss = 2.63823171\n",
      "Iteration 5944, loss = 2.63817951\n",
      "Iteration 5945, loss = 2.63815558\n",
      "Iteration 5946, loss = 2.63812931\n",
      "Iteration 5947, loss = 2.63817661\n",
      "Iteration 5948, loss = 2.63809844\n",
      "Iteration 5949, loss = 2.63811027\n",
      "Iteration 5950, loss = 2.63803347\n",
      "Iteration 5951, loss = 2.63808469\n",
      "Iteration 5952, loss = 2.63800434\n",
      "Iteration 5953, loss = 2.63803718\n",
      "Iteration 5954, loss = 2.63806331\n",
      "Iteration 5955, loss = 2.63796993\n",
      "Iteration 5956, loss = 2.63793394\n",
      "Iteration 5957, loss = 2.63797477\n",
      "Iteration 5958, loss = 2.63796351\n",
      "Iteration 5959, loss = 2.63792908\n",
      "Iteration 5960, loss = 2.63791675\n",
      "Iteration 5961, loss = 2.63790922\n",
      "Iteration 5962, loss = 2.63791612\n",
      "Iteration 5963, loss = 2.63788906\n",
      "Iteration 5964, loss = 2.63782743\n",
      "Iteration 5965, loss = 2.63778838\n",
      "Iteration 5966, loss = 2.63785558\n",
      "Iteration 5967, loss = 2.63779720\n",
      "Iteration 5968, loss = 2.63780000\n",
      "Iteration 5969, loss = 2.63778220\n",
      "Iteration 5970, loss = 2.63776954\n",
      "Iteration 5971, loss = 2.63787497\n",
      "Iteration 5972, loss = 2.63765279\n",
      "Iteration 5973, loss = 2.63770124\n",
      "Iteration 5974, loss = 2.63763101\n",
      "Iteration 5975, loss = 2.63769881\n",
      "Iteration 5976, loss = 2.63759088\n",
      "Iteration 5977, loss = 2.63756871\n",
      "Iteration 5978, loss = 2.63762451\n",
      "Iteration 5979, loss = 2.63758691\n",
      "Iteration 5980, loss = 2.63755105\n",
      "Iteration 5981, loss = 2.63753770\n",
      "Iteration 5982, loss = 2.63754104\n",
      "Iteration 5983, loss = 2.63749345\n",
      "Iteration 5984, loss = 2.63750859\n",
      "Iteration 5985, loss = 2.63751910\n",
      "Iteration 5986, loss = 2.63745669\n",
      "Iteration 5987, loss = 2.63741841\n",
      "Iteration 5988, loss = 2.63741975\n",
      "Iteration 5989, loss = 2.63743424\n",
      "Iteration 5990, loss = 2.63747315\n",
      "Iteration 5991, loss = 2.63735891\n",
      "Iteration 5992, loss = 2.63733915\n",
      "Iteration 5993, loss = 2.63734007\n",
      "Iteration 5994, loss = 2.63730867\n",
      "Iteration 5995, loss = 2.63737934\n",
      "Iteration 5996, loss = 2.63742300\n",
      "Iteration 5997, loss = 2.63734448\n",
      "Iteration 5998, loss = 2.63730424\n",
      "Iteration 5999, loss = 2.63733060\n",
      "Iteration 6000, loss = 2.63727987\n",
      "Iteration 6001, loss = 2.63718164\n",
      "Iteration 6002, loss = 2.63724040\n",
      "Iteration 6003, loss = 2.63719957\n",
      "Iteration 6004, loss = 2.63723085\n",
      "Iteration 6005, loss = 2.63713027\n",
      "Iteration 6006, loss = 2.63710709\n",
      "Iteration 6007, loss = 2.63711703\n",
      "Iteration 6008, loss = 2.63723593\n",
      "Iteration 6009, loss = 2.63712764\n",
      "Iteration 6010, loss = 2.63707529\n",
      "Iteration 6011, loss = 2.63707853\n",
      "Iteration 6012, loss = 2.63703503\n",
      "Iteration 6013, loss = 2.63705570\n",
      "Iteration 6014, loss = 2.63698359\n",
      "Iteration 6015, loss = 2.63696925\n",
      "Iteration 6016, loss = 2.63692280\n",
      "Iteration 6017, loss = 2.63695533\n",
      "Iteration 6018, loss = 2.63696542\n",
      "Iteration 6019, loss = 2.63693274\n",
      "Iteration 6020, loss = 2.63688499\n",
      "Iteration 6021, loss = 2.63691146\n",
      "Iteration 6022, loss = 2.63685977\n",
      "Iteration 6023, loss = 2.63678565\n",
      "Iteration 6024, loss = 2.63682585\n",
      "Iteration 6025, loss = 2.63682014\n",
      "Iteration 6026, loss = 2.63681124\n",
      "Iteration 6027, loss = 2.63679245\n",
      "Iteration 6028, loss = 2.63674877\n",
      "Iteration 6029, loss = 2.63676726\n",
      "Iteration 6030, loss = 2.63673267\n",
      "Iteration 6031, loss = 2.63664963\n",
      "Iteration 6032, loss = 2.63674738\n",
      "Iteration 6033, loss = 2.63672761\n",
      "Iteration 6034, loss = 2.63666466\n",
      "Iteration 6035, loss = 2.63666847\n",
      "Iteration 6036, loss = 2.63664128\n",
      "Iteration 6037, loss = 2.63659522\n",
      "Iteration 6038, loss = 2.63659073\n",
      "Iteration 6039, loss = 2.63655872\n",
      "Iteration 6040, loss = 2.63659675\n",
      "Iteration 6041, loss = 2.63659542\n",
      "Iteration 6042, loss = 2.63649956\n",
      "Iteration 6043, loss = 2.63657166\n",
      "Iteration 6044, loss = 2.63649030\n",
      "Iteration 6045, loss = 2.63647203\n",
      "Iteration 6046, loss = 2.63649950\n",
      "Iteration 6047, loss = 2.63647340\n",
      "Iteration 6048, loss = 2.63647072\n",
      "Iteration 6049, loss = 2.63638238\n",
      "Iteration 6050, loss = 2.63638085\n",
      "Iteration 6051, loss = 2.63640032\n",
      "Iteration 6052, loss = 2.63638602\n",
      "Iteration 6053, loss = 2.63633319\n",
      "Iteration 6054, loss = 2.63632113\n",
      "Iteration 6055, loss = 2.63631775\n",
      "Iteration 6056, loss = 2.63639523\n",
      "Iteration 6057, loss = 2.63629936\n",
      "Iteration 6058, loss = 2.63630141\n",
      "Iteration 6059, loss = 2.63624187\n",
      "Iteration 6060, loss = 2.63624624\n",
      "Iteration 6061, loss = 2.63622355\n",
      "Iteration 6062, loss = 2.63621097\n",
      "Iteration 6063, loss = 2.63617668\n",
      "Iteration 6064, loss = 2.63616075\n",
      "Iteration 6065, loss = 2.63615028\n",
      "Iteration 6066, loss = 2.63614764\n",
      "Iteration 6067, loss = 2.63610830\n",
      "Iteration 6068, loss = 2.63613424\n",
      "Iteration 6069, loss = 2.63616035\n",
      "Iteration 6070, loss = 2.63604135\n",
      "Iteration 6071, loss = 2.63611824\n",
      "Iteration 6072, loss = 2.63611442\n",
      "Iteration 6073, loss = 2.63607803\n",
      "Iteration 6074, loss = 2.63595151\n",
      "Iteration 6075, loss = 2.63598335\n",
      "Iteration 6076, loss = 2.63593675\n",
      "Iteration 6077, loss = 2.63597763\n",
      "Iteration 6078, loss = 2.63594005\n",
      "Iteration 6079, loss = 2.63589295\n",
      "Iteration 6080, loss = 2.63593388\n",
      "Iteration 6081, loss = 2.63588319\n",
      "Iteration 6082, loss = 2.63590126\n",
      "Iteration 6083, loss = 2.63583211\n",
      "Iteration 6084, loss = 2.63581550\n",
      "Iteration 6085, loss = 2.63583727\n",
      "Iteration 6086, loss = 2.63584485\n",
      "Iteration 6087, loss = 2.63579590\n",
      "Iteration 6088, loss = 2.63577594\n",
      "Iteration 6089, loss = 2.63579564\n",
      "Iteration 6090, loss = 2.63581468\n",
      "Iteration 6091, loss = 2.63577708\n",
      "Iteration 6092, loss = 2.63571282\n",
      "Iteration 6093, loss = 2.63574257\n",
      "Iteration 6094, loss = 2.63566434\n",
      "Iteration 6095, loss = 2.63566338\n",
      "Iteration 6096, loss = 2.63567366\n",
      "Iteration 6097, loss = 2.63567975\n",
      "Iteration 6098, loss = 2.63559814\n",
      "Iteration 6099, loss = 2.63563098\n",
      "Iteration 6100, loss = 2.63557922\n",
      "Iteration 6101, loss = 2.63565640\n",
      "Iteration 6102, loss = 2.63555967\n",
      "Iteration 6103, loss = 2.63553618\n",
      "Iteration 6104, loss = 2.63555815\n",
      "Iteration 6105, loss = 2.63544794\n",
      "Iteration 6106, loss = 2.63548078\n",
      "Iteration 6107, loss = 2.63544329\n",
      "Iteration 6108, loss = 2.63548013\n",
      "Iteration 6109, loss = 2.63544682\n",
      "Iteration 6110, loss = 2.63542597\n",
      "Iteration 6111, loss = 2.63539855\n",
      "Iteration 6112, loss = 2.63538741\n",
      "Iteration 6113, loss = 2.63534155\n",
      "Iteration 6114, loss = 2.63537689\n",
      "Iteration 6115, loss = 2.63531991\n",
      "Iteration 6116, loss = 2.63529447\n",
      "Iteration 6117, loss = 2.63532386\n",
      "Iteration 6118, loss = 2.63529881\n",
      "Iteration 6119, loss = 2.63524022\n",
      "Iteration 6120, loss = 2.63525375\n",
      "Iteration 6121, loss = 2.63538269\n",
      "Iteration 6122, loss = 2.63520298\n",
      "Iteration 6123, loss = 2.63517582\n",
      "Iteration 6124, loss = 2.63518539\n",
      "Iteration 6125, loss = 2.63518765\n",
      "Iteration 6126, loss = 2.63517118\n",
      "Iteration 6127, loss = 2.63515198\n",
      "Iteration 6128, loss = 2.63511633\n",
      "Iteration 6129, loss = 2.63514439\n",
      "Iteration 6130, loss = 2.63523127\n",
      "Iteration 6131, loss = 2.63529028\n",
      "Iteration 6132, loss = 2.63510690\n",
      "Iteration 6133, loss = 2.63505813\n",
      "Iteration 6134, loss = 2.63500341\n",
      "Iteration 6135, loss = 2.63500470\n",
      "Iteration 6136, loss = 2.63500552\n",
      "Iteration 6137, loss = 2.63498947\n",
      "Iteration 6138, loss = 2.63496148\n",
      "Iteration 6139, loss = 2.63490111\n",
      "Iteration 6140, loss = 2.63494425\n",
      "Iteration 6141, loss = 2.63493779\n",
      "Iteration 6142, loss = 2.63487323\n",
      "Iteration 6143, loss = 2.63486955\n",
      "Iteration 6144, loss = 2.63485089\n",
      "Iteration 6145, loss = 2.63481294\n",
      "Iteration 6146, loss = 2.63481838\n",
      "Iteration 6147, loss = 2.63482206\n",
      "Iteration 6148, loss = 2.63480674\n",
      "Iteration 6149, loss = 2.63479746\n",
      "Iteration 6150, loss = 2.63479455\n",
      "Iteration 6151, loss = 2.63475873\n",
      "Iteration 6152, loss = 2.63471463\n",
      "Iteration 6153, loss = 2.63470768\n",
      "Iteration 6154, loss = 2.63469886\n",
      "Iteration 6155, loss = 2.63471587\n",
      "Iteration 6156, loss = 2.63465556\n",
      "Iteration 6157, loss = 2.63468515\n",
      "Iteration 6158, loss = 2.63465833\n",
      "Iteration 6159, loss = 2.63458470\n",
      "Iteration 6160, loss = 2.63460607\n",
      "Iteration 6161, loss = 2.63459939\n",
      "Iteration 6162, loss = 2.63455299\n",
      "Iteration 6163, loss = 2.63459349\n",
      "Iteration 6164, loss = 2.63451360\n",
      "Iteration 6165, loss = 2.63449188\n",
      "Iteration 6166, loss = 2.63451958\n",
      "Iteration 6167, loss = 2.63447024\n",
      "Iteration 6168, loss = 2.63444358\n",
      "Iteration 6169, loss = 2.63443057\n",
      "Iteration 6170, loss = 2.63445818\n",
      "Iteration 6171, loss = 2.63437009\n",
      "Iteration 6172, loss = 2.63443578\n",
      "Iteration 6173, loss = 2.63441878\n",
      "Iteration 6174, loss = 2.63442408\n",
      "Iteration 6175, loss = 2.63436477\n",
      "Iteration 6176, loss = 2.63446206\n",
      "Iteration 6177, loss = 2.63433083\n",
      "Iteration 6178, loss = 2.63432237\n",
      "Iteration 6179, loss = 2.63434419\n",
      "Iteration 6180, loss = 2.63425024\n",
      "Iteration 6181, loss = 2.63428331\n",
      "Iteration 6182, loss = 2.63430023\n",
      "Iteration 6183, loss = 2.63424420\n",
      "Iteration 6184, loss = 2.63428768\n",
      "Iteration 6185, loss = 2.63418872\n",
      "Iteration 6186, loss = 2.63414548\n",
      "Iteration 6187, loss = 2.63413148\n",
      "Iteration 6188, loss = 2.63413368\n",
      "Iteration 6189, loss = 2.63418046\n",
      "Iteration 6190, loss = 2.63406110\n",
      "Iteration 6191, loss = 2.63415799\n",
      "Iteration 6192, loss = 2.63406488\n",
      "Iteration 6193, loss = 2.63404301\n",
      "Iteration 6194, loss = 2.63404663\n",
      "Iteration 6195, loss = 2.63404976\n",
      "Iteration 6196, loss = 2.63397477\n",
      "Iteration 6197, loss = 2.63407850\n",
      "Iteration 6198, loss = 2.63396749\n",
      "Iteration 6199, loss = 2.63404724\n",
      "Iteration 6200, loss = 2.63394779\n",
      "Iteration 6201, loss = 2.63396555\n",
      "Iteration 6202, loss = 2.63390508\n",
      "Iteration 6203, loss = 2.63392113\n",
      "Iteration 6204, loss = 2.63387887\n",
      "Iteration 6205, loss = 2.63384203\n",
      "Iteration 6206, loss = 2.63384230\n",
      "Iteration 6207, loss = 2.63384302\n",
      "Iteration 6208, loss = 2.63386196\n",
      "Iteration 6209, loss = 2.63386526\n",
      "Iteration 6210, loss = 2.63379351\n",
      "Iteration 6211, loss = 2.63378323\n",
      "Iteration 6212, loss = 2.63377742\n",
      "Iteration 6213, loss = 2.63376705\n",
      "Iteration 6214, loss = 2.63372335\n",
      "Iteration 6215, loss = 2.63377348\n",
      "Iteration 6216, loss = 2.63370287\n",
      "Iteration 6217, loss = 2.63372070\n",
      "Iteration 6218, loss = 2.63366119\n",
      "Iteration 6219, loss = 2.63368217\n",
      "Iteration 6220, loss = 2.63364465\n",
      "Iteration 6221, loss = 2.63364070\n",
      "Iteration 6222, loss = 2.63364295\n",
      "Iteration 6223, loss = 2.63361458\n",
      "Iteration 6224, loss = 2.63356414\n",
      "Iteration 6225, loss = 2.63354497\n",
      "Iteration 6226, loss = 2.63368050\n",
      "Iteration 6227, loss = 2.63369082\n",
      "Iteration 6228, loss = 2.63349931\n",
      "Iteration 6229, loss = 2.63352595\n",
      "Iteration 6230, loss = 2.63347008\n",
      "Iteration 6231, loss = 2.63346641\n",
      "Iteration 6232, loss = 2.63342440\n",
      "Iteration 6233, loss = 2.63342129\n",
      "Iteration 6234, loss = 2.63339552\n",
      "Iteration 6235, loss = 2.63341734\n",
      "Iteration 6236, loss = 2.63343687\n",
      "Iteration 6237, loss = 2.63333073\n",
      "Iteration 6238, loss = 2.63333485\n",
      "Iteration 6239, loss = 2.63335654\n",
      "Iteration 6240, loss = 2.63338905\n",
      "Iteration 6241, loss = 2.63324205\n",
      "Iteration 6242, loss = 2.63328062\n",
      "Iteration 6243, loss = 2.63330936\n",
      "Iteration 6244, loss = 2.63321989\n",
      "Iteration 6245, loss = 2.63318459\n",
      "Iteration 6246, loss = 2.63323907\n",
      "Iteration 6247, loss = 2.63323778\n",
      "Iteration 6248, loss = 2.63319255\n",
      "Iteration 6249, loss = 2.63324078\n",
      "Iteration 6250, loss = 2.63314748\n",
      "Iteration 6251, loss = 2.63312336\n",
      "Iteration 6252, loss = 2.63313033\n",
      "Iteration 6253, loss = 2.63309303\n",
      "Iteration 6254, loss = 2.63325752\n",
      "Iteration 6255, loss = 2.63303257\n",
      "Iteration 6256, loss = 2.63309379\n",
      "Iteration 6257, loss = 2.63304112\n",
      "Iteration 6258, loss = 2.63304920\n",
      "Iteration 6259, loss = 2.63304867\n",
      "Iteration 6260, loss = 2.63301952\n",
      "Iteration 6261, loss = 2.63302418\n",
      "Iteration 6262, loss = 2.63297555\n",
      "Iteration 6263, loss = 2.63296722\n",
      "Iteration 6264, loss = 2.63293946\n",
      "Iteration 6265, loss = 2.63286755\n",
      "Iteration 6266, loss = 2.63288477\n",
      "Iteration 6267, loss = 2.63287899\n",
      "Iteration 6268, loss = 2.63290944\n",
      "Iteration 6269, loss = 2.63287820\n",
      "Iteration 6270, loss = 2.63285296\n",
      "Iteration 6271, loss = 2.63276977\n",
      "Iteration 6272, loss = 2.63274650\n",
      "Iteration 6273, loss = 2.63278456\n",
      "Iteration 6274, loss = 2.63280438\n",
      "Iteration 6275, loss = 2.63273850\n",
      "Iteration 6276, loss = 2.63282229\n",
      "Iteration 6277, loss = 2.63273513\n",
      "Iteration 6278, loss = 2.63272108\n",
      "Iteration 6279, loss = 2.63267133\n",
      "Iteration 6280, loss = 2.63269862\n",
      "Iteration 6281, loss = 2.63265954\n",
      "Iteration 6282, loss = 2.63270208\n",
      "Iteration 6283, loss = 2.63265454\n",
      "Iteration 6284, loss = 2.63261321\n",
      "Iteration 6285, loss = 2.63257312\n",
      "Iteration 6286, loss = 2.63259847\n",
      "Iteration 6287, loss = 2.63254963\n",
      "Iteration 6288, loss = 2.63254349\n",
      "Iteration 6289, loss = 2.63254312\n",
      "Iteration 6290, loss = 2.63247647\n",
      "Iteration 6291, loss = 2.63250498\n",
      "Iteration 6292, loss = 2.63251200\n",
      "Iteration 6293, loss = 2.63248477\n",
      "Iteration 6294, loss = 2.63251447\n",
      "Iteration 6295, loss = 2.63248089\n",
      "Iteration 6296, loss = 2.63241986\n",
      "Iteration 6297, loss = 2.63242461\n",
      "Iteration 6298, loss = 2.63240018\n",
      "Iteration 6299, loss = 2.63245905\n",
      "Iteration 6300, loss = 2.63237309\n",
      "Iteration 6301, loss = 2.63235625\n",
      "Iteration 6302, loss = 2.63240898\n",
      "Iteration 6303, loss = 2.63230066\n",
      "Iteration 6304, loss = 2.63225451\n",
      "Iteration 6305, loss = 2.63226230\n",
      "Iteration 6306, loss = 2.63227030\n",
      "Iteration 6307, loss = 2.63226601\n",
      "Iteration 6308, loss = 2.63221772\n",
      "Iteration 6309, loss = 2.63220696\n",
      "Iteration 6310, loss = 2.63219496\n",
      "Iteration 6311, loss = 2.63219092\n",
      "Iteration 6312, loss = 2.63220268\n",
      "Iteration 6313, loss = 2.63216366\n",
      "Iteration 6314, loss = 2.63218117\n",
      "Iteration 6315, loss = 2.63220628\n",
      "Iteration 6316, loss = 2.63211051\n",
      "Iteration 6317, loss = 2.63211331\n",
      "Iteration 6318, loss = 2.63211582\n",
      "Iteration 6319, loss = 2.63208058\n",
      "Iteration 6320, loss = 2.63199473\n",
      "Iteration 6321, loss = 2.63202149\n",
      "Iteration 6322, loss = 2.63203463\n",
      "Iteration 6323, loss = 2.63195477\n",
      "Iteration 6324, loss = 2.63200271\n",
      "Iteration 6325, loss = 2.63196108\n",
      "Iteration 6326, loss = 2.63196691\n",
      "Iteration 6327, loss = 2.63192479\n",
      "Iteration 6328, loss = 2.63193174\n",
      "Iteration 6329, loss = 2.63193109\n",
      "Iteration 6330, loss = 2.63187151\n",
      "Iteration 6331, loss = 2.63188292\n",
      "Iteration 6332, loss = 2.63186833\n",
      "Iteration 6333, loss = 2.63186276\n",
      "Iteration 6334, loss = 2.63181648\n",
      "Iteration 6335, loss = 2.63184533\n",
      "Iteration 6336, loss = 2.63183373\n",
      "Iteration 6337, loss = 2.63174788\n",
      "Iteration 6338, loss = 2.63174822\n",
      "Iteration 6339, loss = 2.63179599\n",
      "Iteration 6340, loss = 2.63165451\n",
      "Iteration 6341, loss = 2.63167013\n",
      "Iteration 6342, loss = 2.63170462\n",
      "Iteration 6343, loss = 2.63159866\n",
      "Iteration 6344, loss = 2.63168202\n",
      "Iteration 6345, loss = 2.63162536\n",
      "Iteration 6346, loss = 2.63159082\n",
      "Iteration 6347, loss = 2.63165052\n",
      "Iteration 6348, loss = 2.63167655\n",
      "Iteration 6349, loss = 2.63163872\n",
      "Iteration 6350, loss = 2.63155222\n",
      "Iteration 6351, loss = 2.63151180\n",
      "Iteration 6352, loss = 2.63153880\n",
      "Iteration 6353, loss = 2.63153314\n",
      "Iteration 6354, loss = 2.63152924\n",
      "Iteration 6355, loss = 2.63148033\n",
      "Iteration 6356, loss = 2.63141656\n",
      "Iteration 6357, loss = 2.63153823\n",
      "Iteration 6358, loss = 2.63142422\n",
      "Iteration 6359, loss = 2.63140258\n",
      "Iteration 6360, loss = 2.63139531\n",
      "Iteration 6361, loss = 2.63138041\n",
      "Iteration 6362, loss = 2.63135755\n",
      "Iteration 6363, loss = 2.63137139\n",
      "Iteration 6364, loss = 2.63133027\n",
      "Iteration 6365, loss = 2.63146641\n",
      "Iteration 6366, loss = 2.63129572\n",
      "Iteration 6367, loss = 2.63126525\n",
      "Iteration 6368, loss = 2.63123750\n",
      "Iteration 6369, loss = 2.63128115\n",
      "Iteration 6370, loss = 2.63126341\n",
      "Iteration 6371, loss = 2.63121874\n",
      "Iteration 6372, loss = 2.63121711\n",
      "Iteration 6373, loss = 2.63130447\n",
      "Iteration 6374, loss = 2.63122940\n",
      "Iteration 6375, loss = 2.63113829\n",
      "Iteration 6376, loss = 2.63114887\n",
      "Iteration 6377, loss = 2.63116480\n",
      "Iteration 6378, loss = 2.63119532\n",
      "Iteration 6379, loss = 2.63110350\n",
      "Iteration 6380, loss = 2.63107249\n",
      "Iteration 6381, loss = 2.63104122\n",
      "Iteration 6382, loss = 2.63105310\n",
      "Iteration 6383, loss = 2.63109723\n",
      "Iteration 6384, loss = 2.63106010\n",
      "Iteration 6385, loss = 2.63098841\n",
      "Iteration 6386, loss = 2.63100066\n",
      "Iteration 6387, loss = 2.63094716\n",
      "Iteration 6388, loss = 2.63104397\n",
      "Iteration 6389, loss = 2.63095213\n",
      "Iteration 6390, loss = 2.63088239\n",
      "Iteration 6391, loss = 2.63094133\n",
      "Iteration 6392, loss = 2.63085670\n",
      "Iteration 6393, loss = 2.63095319\n",
      "Iteration 6394, loss = 2.63085871\n",
      "Iteration 6395, loss = 2.63098229\n",
      "Iteration 6396, loss = 2.63103991\n",
      "Iteration 6397, loss = 2.63086270\n",
      "Iteration 6398, loss = 2.63084352\n",
      "Iteration 6399, loss = 2.63079899\n",
      "Iteration 6400, loss = 2.63076816\n",
      "Iteration 6401, loss = 2.63073741\n",
      "Iteration 6402, loss = 2.63072877\n",
      "Iteration 6403, loss = 2.63068707\n",
      "Iteration 6404, loss = 2.63065896\n",
      "Iteration 6405, loss = 2.63074361\n",
      "Iteration 6406, loss = 2.63068108\n",
      "Iteration 6407, loss = 2.63059870\n",
      "Iteration 6408, loss = 2.63065143\n",
      "Iteration 6409, loss = 2.63062158\n",
      "Iteration 6410, loss = 2.63058582\n",
      "Iteration 6411, loss = 2.63057940\n",
      "Iteration 6412, loss = 2.63061626\n",
      "Iteration 6413, loss = 2.63051704\n",
      "Iteration 6414, loss = 2.63051107\n",
      "Iteration 6415, loss = 2.63050127\n",
      "Iteration 6416, loss = 2.63051263\n",
      "Iteration 6417, loss = 2.63053171\n",
      "Iteration 6418, loss = 2.63055808\n",
      "Iteration 6419, loss = 2.63047817\n",
      "Iteration 6420, loss = 2.63047310\n",
      "Iteration 6421, loss = 2.63042816\n",
      "Iteration 6422, loss = 2.63038570\n",
      "Iteration 6423, loss = 2.63038623\n",
      "Iteration 6424, loss = 2.63033847\n",
      "Iteration 6425, loss = 2.63033039\n",
      "Iteration 6426, loss = 2.63043575\n",
      "Iteration 6427, loss = 2.63040546\n",
      "Iteration 6428, loss = 2.63030427\n",
      "Iteration 6429, loss = 2.63031065\n",
      "Iteration 6430, loss = 2.63028404\n",
      "Iteration 6431, loss = 2.63023401\n",
      "Iteration 6432, loss = 2.63028735\n",
      "Iteration 6433, loss = 2.63027048\n",
      "Iteration 6434, loss = 2.63037663\n",
      "Iteration 6435, loss = 2.63022200\n",
      "Iteration 6436, loss = 2.63018219\n",
      "Iteration 6437, loss = 2.63018635\n",
      "Iteration 6438, loss = 2.63018089\n",
      "Iteration 6439, loss = 2.63017116\n",
      "Iteration 6440, loss = 2.63011284\n",
      "Iteration 6441, loss = 2.63010663\n",
      "Iteration 6442, loss = 2.63012220\n",
      "Iteration 6443, loss = 2.63007540\n",
      "Iteration 6444, loss = 2.63005569\n",
      "Iteration 6445, loss = 2.63004226\n",
      "Iteration 6446, loss = 2.63015096\n",
      "Iteration 6447, loss = 2.63009398\n",
      "Iteration 6448, loss = 2.63002427\n",
      "Iteration 6449, loss = 2.63002899\n",
      "Iteration 6450, loss = 2.63005149\n",
      "Iteration 6451, loss = 2.62997476\n",
      "Iteration 6452, loss = 2.63002769\n",
      "Iteration 6453, loss = 2.62991768\n",
      "Iteration 6454, loss = 2.62991032\n",
      "Iteration 6455, loss = 2.62990713\n",
      "Iteration 6456, loss = 2.62992226\n",
      "Iteration 6457, loss = 2.62984142\n",
      "Iteration 6458, loss = 2.62986551\n",
      "Iteration 6459, loss = 2.62989036\n",
      "Iteration 6460, loss = 2.62978735\n",
      "Iteration 6461, loss = 2.62979949\n",
      "Iteration 6462, loss = 2.62987065\n",
      "Iteration 6463, loss = 2.62982268\n",
      "Iteration 6464, loss = 2.62976686\n",
      "Iteration 6465, loss = 2.62973978\n",
      "Iteration 6466, loss = 2.62974192\n",
      "Iteration 6467, loss = 2.62975926\n",
      "Iteration 6468, loss = 2.62978077\n",
      "Iteration 6469, loss = 2.62964454\n",
      "Iteration 6470, loss = 2.62964656\n",
      "Iteration 6471, loss = 2.62969542\n",
      "Iteration 6472, loss = 2.62972566\n",
      "Iteration 6473, loss = 2.62962652\n",
      "Iteration 6474, loss = 2.62970109\n",
      "Iteration 6475, loss = 2.62958512\n",
      "Iteration 6476, loss = 2.62958002\n",
      "Iteration 6477, loss = 2.62952591\n",
      "Iteration 6478, loss = 2.62953568\n",
      "Iteration 6479, loss = 2.62960199\n",
      "Iteration 6480, loss = 2.62954979\n",
      "Iteration 6481, loss = 2.62948194\n",
      "Iteration 6482, loss = 2.62963545\n",
      "Iteration 6483, loss = 2.62945943\n",
      "Iteration 6484, loss = 2.62949789\n",
      "Iteration 6485, loss = 2.62952031\n",
      "Iteration 6486, loss = 2.62943840\n",
      "Iteration 6487, loss = 2.62942908\n",
      "Iteration 6488, loss = 2.62940605\n",
      "Iteration 6489, loss = 2.62947227\n",
      "Iteration 6490, loss = 2.62934009\n",
      "Iteration 6491, loss = 2.62931622\n",
      "Iteration 6492, loss = 2.62934884\n",
      "Iteration 6493, loss = 2.62934813\n",
      "Iteration 6494, loss = 2.62938130\n",
      "Iteration 6495, loss = 2.62928147\n",
      "Iteration 6496, loss = 2.62925078\n",
      "Iteration 6497, loss = 2.62926339\n",
      "Iteration 6498, loss = 2.62920782\n",
      "Iteration 6499, loss = 2.62921773\n",
      "Iteration 6500, loss = 2.62935769\n",
      "Iteration 6501, loss = 2.62923862\n",
      "Iteration 6502, loss = 2.62926498\n",
      "Iteration 6503, loss = 2.62914981\n",
      "Iteration 6504, loss = 2.62919684\n",
      "Iteration 6505, loss = 2.62905123\n",
      "Iteration 6506, loss = 2.62909757\n",
      "Iteration 6507, loss = 2.62926227\n",
      "Iteration 6508, loss = 2.62913137\n",
      "Iteration 6509, loss = 2.62907036\n",
      "Iteration 6510, loss = 2.62912969\n",
      "Iteration 6511, loss = 2.62906893\n",
      "Iteration 6512, loss = 2.62905032\n",
      "Iteration 6513, loss = 2.62901014\n",
      "Iteration 6514, loss = 2.62896723\n",
      "Iteration 6515, loss = 2.62901455\n",
      "Iteration 6516, loss = 2.62894077\n",
      "Iteration 6517, loss = 2.62891977\n",
      "Iteration 6518, loss = 2.62887896\n",
      "Iteration 6519, loss = 2.62895028\n",
      "Iteration 6520, loss = 2.62887497\n",
      "Iteration 6521, loss = 2.62891107\n",
      "Iteration 6522, loss = 2.62890465\n",
      "Iteration 6523, loss = 2.62884863\n",
      "Iteration 6524, loss = 2.62880462\n",
      "Iteration 6525, loss = 2.62880026\n",
      "Iteration 6526, loss = 2.62878387\n",
      "Iteration 6527, loss = 2.62874189\n",
      "Iteration 6528, loss = 2.62877834\n",
      "Iteration 6529, loss = 2.62876386\n",
      "Iteration 6530, loss = 2.62873621\n",
      "Iteration 6531, loss = 2.62878593\n",
      "Iteration 6532, loss = 2.62868033\n",
      "Iteration 6533, loss = 2.62867192\n",
      "Iteration 6534, loss = 2.62868109\n",
      "Iteration 6535, loss = 2.62865140\n",
      "Iteration 6536, loss = 2.62864392\n",
      "Iteration 6537, loss = 2.62865378\n",
      "Iteration 6538, loss = 2.62870998\n",
      "Iteration 6539, loss = 2.62860571\n",
      "Iteration 6540, loss = 2.62863148\n",
      "Iteration 6541, loss = 2.62853141\n",
      "Iteration 6542, loss = 2.62862414\n",
      "Iteration 6543, loss = 2.62848584\n",
      "Iteration 6544, loss = 2.62851355\n",
      "Iteration 6545, loss = 2.62857553\n",
      "Iteration 6546, loss = 2.62854516\n",
      "Iteration 6547, loss = 2.62858882\n",
      "Iteration 6548, loss = 2.62845037\n",
      "Iteration 6549, loss = 2.62850250\n",
      "Iteration 6550, loss = 2.62841705\n",
      "Iteration 6551, loss = 2.62844521\n",
      "Iteration 6552, loss = 2.62839731\n",
      "Iteration 6553, loss = 2.62841538\n",
      "Iteration 6554, loss = 2.62835713\n",
      "Iteration 6555, loss = 2.62837706\n",
      "Iteration 6556, loss = 2.62838773\n",
      "Iteration 6557, loss = 2.62833216\n",
      "Iteration 6558, loss = 2.62829014\n",
      "Iteration 6559, loss = 2.62829502\n",
      "Iteration 6560, loss = 2.62830073\n",
      "Iteration 6561, loss = 2.62831848\n",
      "Iteration 6562, loss = 2.62825983\n",
      "Iteration 6563, loss = 2.62826639\n",
      "Iteration 6564, loss = 2.62826901\n",
      "Iteration 6565, loss = 2.62819383\n",
      "Iteration 6566, loss = 2.62816361\n",
      "Iteration 6567, loss = 2.62818569\n",
      "Iteration 6568, loss = 2.62816357\n",
      "Iteration 6569, loss = 2.62815443\n",
      "Iteration 6570, loss = 2.62827329\n",
      "Iteration 6571, loss = 2.62810405\n",
      "Iteration 6572, loss = 2.62807549\n",
      "Iteration 6573, loss = 2.62804438\n",
      "Iteration 6574, loss = 2.62807617\n",
      "Iteration 6575, loss = 2.62807734\n",
      "Iteration 6576, loss = 2.62804853\n",
      "Iteration 6577, loss = 2.62805130\n",
      "Iteration 6578, loss = 2.62807010\n",
      "Iteration 6579, loss = 2.62801417\n",
      "Iteration 6580, loss = 2.62800931\n",
      "Iteration 6581, loss = 2.62795455\n",
      "Iteration 6582, loss = 2.62795159\n",
      "Iteration 6583, loss = 2.62794603\n",
      "Iteration 6584, loss = 2.62819322\n",
      "Iteration 6585, loss = 2.62788898\n",
      "Iteration 6586, loss = 2.62786537\n",
      "Iteration 6587, loss = 2.62779043\n",
      "Iteration 6588, loss = 2.62785745\n",
      "Iteration 6589, loss = 2.62782315\n",
      "Iteration 6590, loss = 2.62794085\n",
      "Iteration 6591, loss = 2.62780817\n",
      "Iteration 6592, loss = 2.62783572\n",
      "Iteration 6593, loss = 2.62782027\n",
      "Iteration 6594, loss = 2.62775349\n",
      "Iteration 6595, loss = 2.62775074\n",
      "Iteration 6596, loss = 2.62784272\n",
      "Iteration 6597, loss = 2.62766182\n",
      "Iteration 6598, loss = 2.62772435\n",
      "Iteration 6599, loss = 2.62772098\n",
      "Iteration 6600, loss = 2.62766182\n",
      "Iteration 6601, loss = 2.62768220\n",
      "Iteration 6602, loss = 2.62762892\n",
      "Iteration 6603, loss = 2.62759535\n",
      "Iteration 6604, loss = 2.62757775\n",
      "Iteration 6605, loss = 2.62763796\n",
      "Iteration 6606, loss = 2.62768503\n",
      "Iteration 6607, loss = 2.62756267\n",
      "Iteration 6608, loss = 2.62754623\n",
      "Iteration 6609, loss = 2.62757539\n",
      "Iteration 6610, loss = 2.62748991\n",
      "Iteration 6611, loss = 2.62748605\n",
      "Iteration 6612, loss = 2.62748045\n",
      "Iteration 6613, loss = 2.62741045\n",
      "Iteration 6614, loss = 2.62751212\n",
      "Iteration 6615, loss = 2.62750128\n",
      "Iteration 6616, loss = 2.62740728\n",
      "Iteration 6617, loss = 2.62738503\n",
      "Iteration 6618, loss = 2.62739286\n",
      "Iteration 6619, loss = 2.62733683\n",
      "Iteration 6620, loss = 2.62734065\n",
      "Iteration 6621, loss = 2.62735733\n",
      "Iteration 6622, loss = 2.62730443\n",
      "Iteration 6623, loss = 2.62727192\n",
      "Iteration 6624, loss = 2.62726102\n",
      "Iteration 6625, loss = 2.62729814\n",
      "Iteration 6626, loss = 2.62726107\n",
      "Iteration 6627, loss = 2.62727898\n",
      "Iteration 6628, loss = 2.62717712\n",
      "Iteration 6629, loss = 2.62726976\n",
      "Iteration 6630, loss = 2.62720457\n",
      "Iteration 6631, loss = 2.62717601\n",
      "Iteration 6632, loss = 2.62716858\n",
      "Iteration 6633, loss = 2.62708637\n",
      "Iteration 6634, loss = 2.62712855\n",
      "Iteration 6635, loss = 2.62720121\n",
      "Iteration 6636, loss = 2.62707973\n",
      "Iteration 6637, loss = 2.62707646\n",
      "Iteration 6638, loss = 2.62704258\n",
      "Iteration 6639, loss = 2.62713499\n",
      "Iteration 6640, loss = 2.62712873\n",
      "Iteration 6641, loss = 2.62699991\n",
      "Iteration 6642, loss = 2.62701841\n",
      "Iteration 6643, loss = 2.62709260\n",
      "Iteration 6644, loss = 2.62704710\n",
      "Iteration 6645, loss = 2.62701229\n",
      "Iteration 6646, loss = 2.62705699\n",
      "Iteration 6647, loss = 2.62703501\n",
      "Iteration 6648, loss = 2.62691062\n",
      "Iteration 6649, loss = 2.62690573\n",
      "Iteration 6650, loss = 2.62692617\n",
      "Iteration 6651, loss = 2.62691667\n",
      "Iteration 6652, loss = 2.62689208\n",
      "Iteration 6653, loss = 2.62690427\n",
      "Iteration 6654, loss = 2.62680725\n",
      "Iteration 6655, loss = 2.62676158\n",
      "Iteration 6656, loss = 2.62683669\n",
      "Iteration 6657, loss = 2.62682338\n",
      "Iteration 6658, loss = 2.62677077\n",
      "Iteration 6659, loss = 2.62675033\n",
      "Iteration 6660, loss = 2.62671019\n",
      "Iteration 6661, loss = 2.62671992\n",
      "Iteration 6662, loss = 2.62674141\n",
      "Iteration 6663, loss = 2.62676403\n",
      "Iteration 6664, loss = 2.62674662\n",
      "Iteration 6665, loss = 2.62667963\n",
      "Iteration 6666, loss = 2.62658503\n",
      "Iteration 6667, loss = 2.62665294\n",
      "Iteration 6668, loss = 2.62662711\n",
      "Iteration 6669, loss = 2.62656958\n",
      "Iteration 6670, loss = 2.62654079\n",
      "Iteration 6671, loss = 2.62659022\n",
      "Iteration 6672, loss = 2.62653065\n",
      "Iteration 6673, loss = 2.62654047\n",
      "Iteration 6674, loss = 2.62654336\n",
      "Iteration 6675, loss = 2.62657504\n",
      "Iteration 6676, loss = 2.62652988\n",
      "Iteration 6677, loss = 2.62647376\n",
      "Iteration 6678, loss = 2.62644126\n",
      "Iteration 6679, loss = 2.62644117\n",
      "Iteration 6680, loss = 2.62643149\n",
      "Iteration 6681, loss = 2.62636667\n",
      "Iteration 6682, loss = 2.62637270\n",
      "Iteration 6683, loss = 2.62636830\n",
      "Iteration 6684, loss = 2.62635431\n",
      "Iteration 6685, loss = 2.62633211\n",
      "Iteration 6686, loss = 2.62651357\n",
      "Iteration 6687, loss = 2.62636950\n",
      "Iteration 6688, loss = 2.62633042\n",
      "Iteration 6689, loss = 2.62624619\n",
      "Iteration 6690, loss = 2.62627859\n",
      "Iteration 6691, loss = 2.62624867\n",
      "Iteration 6692, loss = 2.62622139\n",
      "Iteration 6693, loss = 2.62627273\n",
      "Iteration 6694, loss = 2.62624476\n",
      "Iteration 6695, loss = 2.62620781\n",
      "Iteration 6696, loss = 2.62624700\n",
      "Iteration 6697, loss = 2.62626269\n",
      "Iteration 6698, loss = 2.62624438\n",
      "Iteration 6699, loss = 2.62615427\n",
      "Iteration 6700, loss = 2.62615725\n",
      "Iteration 6701, loss = 2.62607836\n",
      "Iteration 6702, loss = 2.62608900\n",
      "Iteration 6703, loss = 2.62608898\n",
      "Iteration 6704, loss = 2.62606284\n",
      "Iteration 6705, loss = 2.62600530\n",
      "Iteration 6706, loss = 2.62613894\n",
      "Iteration 6707, loss = 2.62601579\n",
      "Iteration 6708, loss = 2.62598213\n",
      "Iteration 6709, loss = 2.62611483\n",
      "Iteration 6710, loss = 2.62599959\n",
      "Iteration 6711, loss = 2.62589274\n",
      "Iteration 6712, loss = 2.62596799\n",
      "Iteration 6713, loss = 2.62592571\n",
      "Iteration 6714, loss = 2.62594818\n",
      "Iteration 6715, loss = 2.62582480\n",
      "Iteration 6716, loss = 2.62587495\n",
      "Iteration 6717, loss = 2.62583265\n",
      "Iteration 6718, loss = 2.62584557\n",
      "Iteration 6719, loss = 2.62585532\n",
      "Iteration 6720, loss = 2.62582489\n",
      "Iteration 6721, loss = 2.62584904\n",
      "Iteration 6722, loss = 2.62576912\n",
      "Iteration 6723, loss = 2.62583853\n",
      "Iteration 6724, loss = 2.62575848\n",
      "Iteration 6725, loss = 2.62570047\n",
      "Iteration 6726, loss = 2.62571513\n",
      "Iteration 6727, loss = 2.62570131\n",
      "Iteration 6728, loss = 2.62567130\n",
      "Iteration 6729, loss = 2.62567400\n",
      "Iteration 6730, loss = 2.62569366\n",
      "Iteration 6731, loss = 2.62561639\n",
      "Iteration 6732, loss = 2.62559941\n",
      "Iteration 6733, loss = 2.62565029\n",
      "Iteration 6734, loss = 2.62566289\n",
      "Iteration 6735, loss = 2.62559197\n",
      "Iteration 6736, loss = 2.62551492\n",
      "Iteration 6737, loss = 2.62556818\n",
      "Iteration 6738, loss = 2.62566321\n",
      "Iteration 6739, loss = 2.62549509\n",
      "Iteration 6740, loss = 2.62554008\n",
      "Iteration 6741, loss = 2.62551697\n",
      "Iteration 6742, loss = 2.62549816\n",
      "Iteration 6743, loss = 2.62553355\n",
      "Iteration 6744, loss = 2.62552081\n",
      "Iteration 6745, loss = 2.62561336\n",
      "Iteration 6746, loss = 2.62541910\n",
      "Iteration 6747, loss = 2.62540693\n",
      "Iteration 6748, loss = 2.62538824\n",
      "Iteration 6749, loss = 2.62537138\n",
      "Iteration 6750, loss = 2.62532575\n",
      "Iteration 6751, loss = 2.62533541\n",
      "Iteration 6752, loss = 2.62529434\n",
      "Iteration 6753, loss = 2.62527618\n",
      "Iteration 6754, loss = 2.62528711\n",
      "Iteration 6755, loss = 2.62526692\n",
      "Iteration 6756, loss = 2.62532128\n",
      "Iteration 6757, loss = 2.62525435\n",
      "Iteration 6758, loss = 2.62526935\n",
      "Iteration 6759, loss = 2.62528016\n",
      "Iteration 6760, loss = 2.62518804\n",
      "Iteration 6761, loss = 2.62519579\n",
      "Iteration 6762, loss = 2.62513457\n",
      "Iteration 6763, loss = 2.62515994\n",
      "Iteration 6764, loss = 2.62514308\n",
      "Iteration 6765, loss = 2.62509771\n",
      "Iteration 6766, loss = 2.62512511\n",
      "Iteration 6767, loss = 2.62511456\n",
      "Iteration 6768, loss = 2.62502223\n",
      "Iteration 6769, loss = 2.62519879\n",
      "Iteration 6770, loss = 2.62509303\n",
      "Iteration 6771, loss = 2.62501069\n",
      "Iteration 6772, loss = 2.62503869\n",
      "Iteration 6773, loss = 2.62497551\n",
      "Iteration 6774, loss = 2.62510175\n",
      "Iteration 6775, loss = 2.62498410\n",
      "Iteration 6776, loss = 2.62496907\n",
      "Iteration 6777, loss = 2.62501770\n",
      "Iteration 6778, loss = 2.62501358\n",
      "Iteration 6779, loss = 2.62491830\n",
      "Iteration 6780, loss = 2.62487999\n",
      "Iteration 6781, loss = 2.62488337\n",
      "Iteration 6782, loss = 2.62495586\n",
      "Iteration 6783, loss = 2.62481423\n",
      "Iteration 6784, loss = 2.62481407\n",
      "Iteration 6785, loss = 2.62484727\n",
      "Iteration 6786, loss = 2.62482687\n",
      "Iteration 6787, loss = 2.62478852\n",
      "Iteration 6788, loss = 2.62482821\n",
      "Iteration 6789, loss = 2.62476639\n",
      "Iteration 6790, loss = 2.62478280\n",
      "Iteration 6791, loss = 2.62474225\n",
      "Iteration 6792, loss = 2.62471580\n",
      "Iteration 6793, loss = 2.62465134\n",
      "Iteration 6794, loss = 2.62469350\n",
      "Iteration 6795, loss = 2.62465817\n",
      "Iteration 6796, loss = 2.62464872\n",
      "Iteration 6797, loss = 2.62458954\n",
      "Iteration 6798, loss = 2.62463717\n",
      "Iteration 6799, loss = 2.62462941\n",
      "Iteration 6800, loss = 2.62462056\n",
      "Iteration 6801, loss = 2.62457113\n",
      "Iteration 6802, loss = 2.62453675\n",
      "Iteration 6803, loss = 2.62468611\n",
      "Iteration 6804, loss = 2.62455298\n",
      "Iteration 6805, loss = 2.62457754\n",
      "Iteration 6806, loss = 2.62449603\n",
      "Iteration 6807, loss = 2.62453481\n",
      "Iteration 6808, loss = 2.62448778\n",
      "Iteration 6809, loss = 2.62449271\n",
      "Iteration 6810, loss = 2.62443478\n",
      "Iteration 6811, loss = 2.62444078\n",
      "Iteration 6812, loss = 2.62454559\n",
      "Iteration 6813, loss = 2.62446013\n",
      "Iteration 6814, loss = 2.62438572\n",
      "Iteration 6815, loss = 2.62437510\n",
      "Iteration 6816, loss = 2.62436774\n",
      "Iteration 6817, loss = 2.62430391\n",
      "Iteration 6818, loss = 2.62429818\n",
      "Iteration 6819, loss = 2.62425958\n",
      "Iteration 6820, loss = 2.62426952\n",
      "Iteration 6821, loss = 2.62430468\n",
      "Iteration 6822, loss = 2.62429248\n",
      "Iteration 6823, loss = 2.62421007\n",
      "Iteration 6824, loss = 2.62418642\n",
      "Iteration 6825, loss = 2.62425329\n",
      "Iteration 6826, loss = 2.62428572\n",
      "Iteration 6827, loss = 2.62422630\n",
      "Iteration 6828, loss = 2.62414496\n",
      "Iteration 6829, loss = 2.62415014\n",
      "Iteration 6830, loss = 2.62416757\n",
      "Iteration 6831, loss = 2.62412787\n",
      "Iteration 6832, loss = 2.62409715\n",
      "Iteration 6833, loss = 2.62406096\n",
      "Iteration 6834, loss = 2.62418188\n",
      "Iteration 6835, loss = 2.62406535\n",
      "Iteration 6836, loss = 2.62408572\n",
      "Iteration 6837, loss = 2.62410202\n",
      "Iteration 6838, loss = 2.62405255\n",
      "Iteration 6839, loss = 2.62398202\n",
      "Iteration 6840, loss = 2.62398149\n",
      "Iteration 6841, loss = 2.62398337\n",
      "Iteration 6842, loss = 2.62396552\n",
      "Iteration 6843, loss = 2.62407849\n",
      "Iteration 6844, loss = 2.62398049\n",
      "Iteration 6845, loss = 2.62394387\n",
      "Iteration 6846, loss = 2.62403137\n",
      "Iteration 6847, loss = 2.62382723\n",
      "Iteration 6848, loss = 2.62399206\n",
      "Iteration 6849, loss = 2.62398339\n",
      "Iteration 6850, loss = 2.62391079\n",
      "Iteration 6851, loss = 2.62380797\n",
      "Iteration 6852, loss = 2.62380374\n",
      "Iteration 6853, loss = 2.62378315\n",
      "Iteration 6854, loss = 2.62376445\n",
      "Iteration 6855, loss = 2.62383184\n",
      "Iteration 6856, loss = 2.62375987\n",
      "Iteration 6857, loss = 2.62388804\n",
      "Iteration 6858, loss = 2.62372212\n",
      "Iteration 6859, loss = 2.62372544\n",
      "Iteration 6860, loss = 2.62379187\n",
      "Iteration 6861, loss = 2.62369351\n",
      "Iteration 6862, loss = 2.62375773\n",
      "Iteration 6863, loss = 2.62368262\n",
      "Iteration 6864, loss = 2.62369466\n",
      "Iteration 6865, loss = 2.62361005\n",
      "Iteration 6866, loss = 2.62360728\n",
      "Iteration 6867, loss = 2.62364660\n",
      "Iteration 6868, loss = 2.62355844\n",
      "Iteration 6869, loss = 2.62356049\n",
      "Iteration 6870, loss = 2.62352302\n",
      "Iteration 6871, loss = 2.62355752\n",
      "Iteration 6872, loss = 2.62354011\n",
      "Iteration 6873, loss = 2.62351546\n",
      "Iteration 6874, loss = 2.62362822\n",
      "Iteration 6875, loss = 2.62347590\n",
      "Iteration 6876, loss = 2.62343129\n",
      "Iteration 6877, loss = 2.62347483\n",
      "Iteration 6878, loss = 2.62341697\n",
      "Iteration 6879, loss = 2.62348379\n",
      "Iteration 6880, loss = 2.62338707\n",
      "Iteration 6881, loss = 2.62337064\n",
      "Iteration 6882, loss = 2.62335746\n",
      "Iteration 6883, loss = 2.62346379\n",
      "Iteration 6884, loss = 2.62342203\n",
      "Iteration 6885, loss = 2.62343870\n",
      "Iteration 6886, loss = 2.62331977\n",
      "Iteration 6887, loss = 2.62327849\n",
      "Iteration 6888, loss = 2.62327597\n",
      "Iteration 6889, loss = 2.62331306\n",
      "Iteration 6890, loss = 2.62323455\n",
      "Iteration 6891, loss = 2.62334371\n",
      "Iteration 6892, loss = 2.62324686\n",
      "Iteration 6893, loss = 2.62323033\n",
      "Iteration 6894, loss = 2.62316139\n",
      "Iteration 6895, loss = 2.62320327\n",
      "Iteration 6896, loss = 2.62315332\n",
      "Iteration 6897, loss = 2.62318496\n",
      "Iteration 6898, loss = 2.62322076\n",
      "Iteration 6899, loss = 2.62315137\n",
      "Iteration 6900, loss = 2.62310529\n",
      "Iteration 6901, loss = 2.62304429\n",
      "Iteration 6902, loss = 2.62310707\n",
      "Iteration 6903, loss = 2.62308881\n",
      "Iteration 6904, loss = 2.62302412\n",
      "Iteration 6905, loss = 2.62300797\n",
      "Iteration 6906, loss = 2.62297473\n",
      "Iteration 6907, loss = 2.62315201\n",
      "Iteration 6908, loss = 2.62303207\n",
      "Iteration 6909, loss = 2.62293793\n",
      "Iteration 6910, loss = 2.62290163\n",
      "Iteration 6911, loss = 2.62290946\n",
      "Iteration 6912, loss = 2.62291458\n",
      "Iteration 6913, loss = 2.62288086\n",
      "Iteration 6914, loss = 2.62289756\n",
      "Iteration 6915, loss = 2.62290810\n",
      "Iteration 6916, loss = 2.62286067\n",
      "Iteration 6917, loss = 2.62284349\n",
      "Iteration 6918, loss = 2.62287932\n",
      "Iteration 6919, loss = 2.62293209\n",
      "Iteration 6920, loss = 2.62274086\n",
      "Iteration 6921, loss = 2.62272256\n",
      "Iteration 6922, loss = 2.62277836\n",
      "Iteration 6923, loss = 2.62275659\n",
      "Iteration 6924, loss = 2.62282894\n",
      "Iteration 6925, loss = 2.62277218\n",
      "Iteration 6926, loss = 2.62266731\n",
      "Iteration 6927, loss = 2.62270483\n",
      "Iteration 6928, loss = 2.62267007\n",
      "Iteration 6929, loss = 2.62265920\n",
      "Iteration 6930, loss = 2.62267191\n",
      "Iteration 6931, loss = 2.62261165\n",
      "Iteration 6932, loss = 2.62262270\n",
      "Iteration 6933, loss = 2.62256333\n",
      "Iteration 6934, loss = 2.62264231\n",
      "Iteration 6935, loss = 2.62256845\n",
      "Iteration 6936, loss = 2.62257846\n",
      "Iteration 6937, loss = 2.62257042\n",
      "Iteration 6938, loss = 2.62256971\n",
      "Iteration 6939, loss = 2.62259580\n",
      "Iteration 6940, loss = 2.62248008\n",
      "Iteration 6941, loss = 2.62246499\n",
      "Iteration 6942, loss = 2.62248713\n",
      "Iteration 6943, loss = 2.62247885\n",
      "Iteration 6944, loss = 2.62247163\n",
      "Iteration 6945, loss = 2.62243014\n",
      "Iteration 6946, loss = 2.62239724\n",
      "Iteration 6947, loss = 2.62243527\n",
      "Iteration 6948, loss = 2.62246460\n",
      "Iteration 6949, loss = 2.62236211\n",
      "Iteration 6950, loss = 2.62232526\n",
      "Iteration 6951, loss = 2.62233010\n",
      "Iteration 6952, loss = 2.62237999\n",
      "Iteration 6953, loss = 2.62229642\n",
      "Iteration 6954, loss = 2.62231227\n",
      "Iteration 6955, loss = 2.62227184\n",
      "Iteration 6956, loss = 2.62222204\n",
      "Iteration 6957, loss = 2.62223591\n",
      "Iteration 6958, loss = 2.62223656\n",
      "Iteration 6959, loss = 2.62227592\n",
      "Iteration 6960, loss = 2.62220033\n",
      "Iteration 6961, loss = 2.62223131\n",
      "Iteration 6962, loss = 2.62215861\n",
      "Iteration 6963, loss = 2.62219468\n",
      "Iteration 6964, loss = 2.62221191\n",
      "Iteration 6965, loss = 2.62213108\n",
      "Iteration 6966, loss = 2.62216937\n",
      "Iteration 6967, loss = 2.62209670\n",
      "Iteration 6968, loss = 2.62206232\n",
      "Iteration 6969, loss = 2.62206558\n",
      "Iteration 6970, loss = 2.62202007\n",
      "Iteration 6971, loss = 2.62204315\n",
      "Iteration 6972, loss = 2.62204633\n",
      "Iteration 6973, loss = 2.62201145\n",
      "Iteration 6974, loss = 2.62200866\n",
      "Iteration 6975, loss = 2.62208038\n",
      "Iteration 6976, loss = 2.62198017\n",
      "Iteration 6977, loss = 2.62196664\n",
      "Iteration 6978, loss = 2.62193642\n",
      "Iteration 6979, loss = 2.62197894\n",
      "Iteration 6980, loss = 2.62189286\n",
      "Iteration 6981, loss = 2.62203909\n",
      "Iteration 6982, loss = 2.62191606\n",
      "Iteration 6983, loss = 2.62186870\n",
      "Iteration 6984, loss = 2.62194970\n",
      "Iteration 6985, loss = 2.62183848\n",
      "Iteration 6986, loss = 2.62180231\n",
      "Iteration 6987, loss = 2.62183599\n",
      "Iteration 6988, loss = 2.62183980\n",
      "Iteration 6989, loss = 2.62178015\n",
      "Iteration 6990, loss = 2.62175456\n",
      "Iteration 6991, loss = 2.62179504\n",
      "Iteration 6992, loss = 2.62178215\n",
      "Iteration 6993, loss = 2.62173290\n",
      "Iteration 6994, loss = 2.62170190\n",
      "Iteration 6995, loss = 2.62170718\n",
      "Iteration 6996, loss = 2.62165614\n",
      "Iteration 6997, loss = 2.62165634\n",
      "Iteration 6998, loss = 2.62168711\n",
      "Iteration 6999, loss = 2.62161668\n",
      "Iteration 7000, loss = 2.62183078\n",
      "Iteration 7001, loss = 2.62170877\n",
      "Iteration 7002, loss = 2.62158116\n",
      "Iteration 7003, loss = 2.62157626\n",
      "Iteration 7004, loss = 2.62166711\n",
      "Iteration 7005, loss = 2.62158783\n",
      "Iteration 7006, loss = 2.62153310\n",
      "Iteration 7007, loss = 2.62153103\n",
      "Iteration 7008, loss = 2.62149822\n",
      "Iteration 7009, loss = 2.62143024\n",
      "Iteration 7010, loss = 2.62148966\n",
      "Iteration 7011, loss = 2.62149315\n",
      "Iteration 7012, loss = 2.62142221\n",
      "Iteration 7013, loss = 2.62142434\n",
      "Iteration 7014, loss = 2.62139887\n",
      "Iteration 7015, loss = 2.62148880\n",
      "Iteration 7016, loss = 2.62143439\n",
      "Iteration 7017, loss = 2.62135445\n",
      "Iteration 7018, loss = 2.62136910\n",
      "Iteration 7019, loss = 2.62134528\n",
      "Iteration 7020, loss = 2.62136218\n",
      "Iteration 7021, loss = 2.62132012\n",
      "Iteration 7022, loss = 2.62126216\n",
      "Iteration 7023, loss = 2.62127270\n",
      "Iteration 7024, loss = 2.62124721\n",
      "Iteration 7025, loss = 2.62126731\n",
      "Iteration 7026, loss = 2.62119546\n",
      "Iteration 7027, loss = 2.62119932\n",
      "Iteration 7028, loss = 2.62118242\n",
      "Iteration 7029, loss = 2.62118495\n",
      "Iteration 7030, loss = 2.62113682\n",
      "Iteration 7031, loss = 2.62121466\n",
      "Iteration 7032, loss = 2.62112628\n",
      "Iteration 7033, loss = 2.62123133\n",
      "Iteration 7034, loss = 2.62110235\n",
      "Iteration 7035, loss = 2.62112639\n",
      "Iteration 7036, loss = 2.62113580\n",
      "Iteration 7037, loss = 2.62113211\n",
      "Iteration 7038, loss = 2.62114071\n",
      "Iteration 7039, loss = 2.62118783\n",
      "Iteration 7040, loss = 2.62099226\n",
      "Iteration 7041, loss = 2.62100241\n",
      "Iteration 7042, loss = 2.62098810\n",
      "Iteration 7043, loss = 2.62102344\n",
      "Iteration 7044, loss = 2.62100800\n",
      "Iteration 7045, loss = 2.62096909\n",
      "Iteration 7046, loss = 2.62093516\n",
      "Iteration 7047, loss = 2.62091188\n",
      "Iteration 7048, loss = 2.62093834\n",
      "Iteration 7049, loss = 2.62089761\n",
      "Iteration 7050, loss = 2.62092325\n",
      "Iteration 7051, loss = 2.62078524\n",
      "Iteration 7052, loss = 2.62092738\n",
      "Iteration 7053, loss = 2.62085381\n",
      "Iteration 7054, loss = 2.62087931\n",
      "Iteration 7055, loss = 2.62082210\n",
      "Iteration 7056, loss = 2.62080562\n",
      "Iteration 7057, loss = 2.62079537\n",
      "Iteration 7058, loss = 2.62084984\n",
      "Iteration 7059, loss = 2.62072854\n",
      "Iteration 7060, loss = 2.62071927\n",
      "Iteration 7061, loss = 2.62079358\n",
      "Iteration 7062, loss = 2.62069178\n",
      "Iteration 7063, loss = 2.62066474\n",
      "Iteration 7064, loss = 2.62070668\n",
      "Iteration 7065, loss = 2.62066085\n",
      "Iteration 7066, loss = 2.62070232\n",
      "Iteration 7067, loss = 2.62065237\n",
      "Iteration 7068, loss = 2.62060195\n",
      "Iteration 7069, loss = 2.62055219\n",
      "Iteration 7070, loss = 2.62056626\n",
      "Iteration 7071, loss = 2.62055366\n",
      "Iteration 7072, loss = 2.62053077\n",
      "Iteration 7073, loss = 2.62054934\n",
      "Iteration 7074, loss = 2.62046385\n",
      "Iteration 7075, loss = 2.62050142\n",
      "Iteration 7076, loss = 2.62054036\n",
      "Iteration 7077, loss = 2.62049740\n",
      "Iteration 7078, loss = 2.62046700\n",
      "Iteration 7079, loss = 2.62045888\n",
      "Iteration 7080, loss = 2.62045912\n",
      "Iteration 7081, loss = 2.62040962\n",
      "Iteration 7082, loss = 2.62038119\n",
      "Iteration 7083, loss = 2.62035932\n",
      "Iteration 7084, loss = 2.62034112\n",
      "Iteration 7085, loss = 2.62034717\n",
      "Iteration 7086, loss = 2.62049531\n",
      "Iteration 7087, loss = 2.62030050\n",
      "Iteration 7088, loss = 2.62049478\n",
      "Iteration 7089, loss = 2.62029764\n",
      "Iteration 7090, loss = 2.62027437\n",
      "Iteration 7091, loss = 2.62031710\n",
      "Iteration 7092, loss = 2.62028255\n",
      "Iteration 7093, loss = 2.62031291\n",
      "Iteration 7094, loss = 2.62040406\n",
      "Iteration 7095, loss = 2.62016126\n",
      "Iteration 7096, loss = 2.62015808\n",
      "Iteration 7097, loss = 2.62022382\n",
      "Iteration 7098, loss = 2.62037171\n",
      "Iteration 7099, loss = 2.62020818\n",
      "Iteration 7100, loss = 2.62014957\n",
      "Iteration 7101, loss = 2.62023030\n",
      "Iteration 7102, loss = 2.62025038\n",
      "Iteration 7103, loss = 2.62009558\n",
      "Iteration 7104, loss = 2.62004987\n",
      "Iteration 7105, loss = 2.62011164\n",
      "Iteration 7106, loss = 2.62002981\n",
      "Iteration 7107, loss = 2.62001605\n",
      "Iteration 7108, loss = 2.62001781\n",
      "Iteration 7109, loss = 2.62001468\n",
      "Iteration 7110, loss = 2.61999423\n",
      "Iteration 7111, loss = 2.61998580\n",
      "Iteration 7112, loss = 2.61994429\n",
      "Iteration 7113, loss = 2.61992995\n",
      "Iteration 7114, loss = 2.61989975\n",
      "Iteration 7115, loss = 2.61988636\n",
      "Iteration 7116, loss = 2.61995607\n",
      "Iteration 7117, loss = 2.61990680\n",
      "Iteration 7118, loss = 2.61983160\n",
      "Iteration 7119, loss = 2.61986128\n",
      "Iteration 7120, loss = 2.61982014\n",
      "Iteration 7121, loss = 2.61986664\n",
      "Iteration 7122, loss = 2.61979073\n",
      "Iteration 7123, loss = 2.61980404\n",
      "Iteration 7124, loss = 2.61993493\n",
      "Iteration 7125, loss = 2.61986182\n",
      "Iteration 7126, loss = 2.61972795\n",
      "Iteration 7127, loss = 2.61976691\n",
      "Iteration 7128, loss = 2.61977984\n",
      "Iteration 7129, loss = 2.61966560\n",
      "Iteration 7130, loss = 2.61973017\n",
      "Iteration 7131, loss = 2.61964574\n",
      "Iteration 7132, loss = 2.61960442\n",
      "Iteration 7133, loss = 2.61969069\n",
      "Iteration 7134, loss = 2.61967627\n",
      "Iteration 7135, loss = 2.61960826\n",
      "Iteration 7136, loss = 2.61967841\n",
      "Iteration 7137, loss = 2.61962287\n",
      "Iteration 7138, loss = 2.61957308\n",
      "Iteration 7139, loss = 2.61957684\n",
      "Iteration 7140, loss = 2.61958115\n",
      "Iteration 7141, loss = 2.61956402\n",
      "Iteration 7142, loss = 2.61962454\n",
      "Iteration 7143, loss = 2.61949372\n",
      "Iteration 7144, loss = 2.61946350\n",
      "Iteration 7145, loss = 2.61947730\n",
      "Iteration 7146, loss = 2.61954116\n",
      "Iteration 7147, loss = 2.61940636\n",
      "Iteration 7148, loss = 2.61943106\n",
      "Iteration 7149, loss = 2.61941333\n",
      "Iteration 7150, loss = 2.61936727\n",
      "Iteration 7151, loss = 2.61937786\n",
      "Iteration 7152, loss = 2.61936728\n",
      "Iteration 7153, loss = 2.61937285\n",
      "Iteration 7154, loss = 2.61935541\n",
      "Iteration 7155, loss = 2.61936899\n",
      "Iteration 7156, loss = 2.61929491\n",
      "Iteration 7157, loss = 2.61929424\n",
      "Iteration 7158, loss = 2.61922933\n",
      "Iteration 7159, loss = 2.61928802\n",
      "Iteration 7160, loss = 2.61927459\n",
      "Iteration 7161, loss = 2.61919861\n",
      "Iteration 7162, loss = 2.61928778\n",
      "Iteration 7163, loss = 2.61926353\n",
      "Iteration 7164, loss = 2.61923619\n",
      "Iteration 7165, loss = 2.61921872\n",
      "Iteration 7166, loss = 2.61914828\n",
      "Iteration 7167, loss = 2.61920745\n",
      "Iteration 7168, loss = 2.61911450\n",
      "Iteration 7169, loss = 2.61909170\n",
      "Iteration 7170, loss = 2.61909101\n",
      "Iteration 7171, loss = 2.61910413\n",
      "Iteration 7172, loss = 2.61916962\n",
      "Iteration 7173, loss = 2.61919850\n",
      "Iteration 7174, loss = 2.61909825\n",
      "Iteration 7175, loss = 2.61907577\n",
      "Iteration 7176, loss = 2.61900430\n",
      "Iteration 7177, loss = 2.61902586\n",
      "Iteration 7178, loss = 2.61901259\n",
      "Iteration 7179, loss = 2.61895793\n",
      "Iteration 7180, loss = 2.61891719\n",
      "Iteration 7181, loss = 2.61892243\n",
      "Iteration 7182, loss = 2.61896278\n",
      "Iteration 7183, loss = 2.61895360\n",
      "Iteration 7184, loss = 2.61890995\n",
      "Iteration 7185, loss = 2.61891939\n",
      "Iteration 7186, loss = 2.61889911\n",
      "Iteration 7187, loss = 2.61888247\n",
      "Iteration 7188, loss = 2.61880169\n",
      "Iteration 7189, loss = 2.61893275\n",
      "Iteration 7190, loss = 2.61889011\n",
      "Iteration 7191, loss = 2.61885322\n",
      "Iteration 7192, loss = 2.61876414\n",
      "Iteration 7193, loss = 2.61874121\n",
      "Iteration 7194, loss = 2.61880952\n",
      "Iteration 7195, loss = 2.61896341\n",
      "Iteration 7196, loss = 2.61872772\n",
      "Iteration 7197, loss = 2.61870167\n",
      "Iteration 7198, loss = 2.61869644\n",
      "Iteration 7199, loss = 2.61875685\n",
      "Iteration 7200, loss = 2.61871660\n",
      "Iteration 7201, loss = 2.61879407\n",
      "Iteration 7202, loss = 2.61867939\n",
      "Iteration 7203, loss = 2.61862233\n",
      "Iteration 7204, loss = 2.61861273\n",
      "Iteration 7205, loss = 2.61864726\n",
      "Iteration 7206, loss = 2.61871121\n",
      "Iteration 7207, loss = 2.61863085\n",
      "Iteration 7208, loss = 2.61870230\n",
      "Iteration 7209, loss = 2.61853117\n",
      "Iteration 7210, loss = 2.61851701\n",
      "Iteration 7211, loss = 2.61850959\n",
      "Iteration 7212, loss = 2.61863437\n",
      "Iteration 7213, loss = 2.61841806\n",
      "Iteration 7214, loss = 2.61857203\n",
      "Iteration 7215, loss = 2.61844045\n",
      "Iteration 7216, loss = 2.61842216\n",
      "Iteration 7217, loss = 2.61846660\n",
      "Iteration 7218, loss = 2.61844073\n",
      "Iteration 7219, loss = 2.61840696\n",
      "Iteration 7220, loss = 2.61836837\n",
      "Iteration 7221, loss = 2.61841426\n",
      "Iteration 7222, loss = 2.61833808\n",
      "Iteration 7223, loss = 2.61832443\n",
      "Iteration 7224, loss = 2.61838229\n",
      "Iteration 7225, loss = 2.61833371\n",
      "Iteration 7226, loss = 2.61830598\n",
      "Iteration 7227, loss = 2.61826235\n",
      "Iteration 7228, loss = 2.61822065\n",
      "Iteration 7229, loss = 2.61833584\n",
      "Iteration 7230, loss = 2.61819741\n",
      "Iteration 7231, loss = 2.61820410\n",
      "Iteration 7232, loss = 2.61824636\n",
      "Iteration 7233, loss = 2.61818013\n",
      "Iteration 7234, loss = 2.61819904\n",
      "Iteration 7235, loss = 2.61824865\n",
      "Iteration 7236, loss = 2.61811186\n",
      "Iteration 7237, loss = 2.61812944\n",
      "Iteration 7238, loss = 2.61815966\n",
      "Iteration 7239, loss = 2.61823338\n",
      "Iteration 7240, loss = 2.61814838\n",
      "Iteration 7241, loss = 2.61816509\n",
      "Iteration 7242, loss = 2.61810825\n",
      "Iteration 7243, loss = 2.61807970\n",
      "Iteration 7244, loss = 2.61803547\n",
      "Iteration 7245, loss = 2.61805633\n",
      "Iteration 7246, loss = 2.61800579\n",
      "Iteration 7247, loss = 2.61806359\n",
      "Iteration 7248, loss = 2.61811104\n",
      "Iteration 7249, loss = 2.61798111\n",
      "Iteration 7250, loss = 2.61793963\n",
      "Iteration 7251, loss = 2.61794171\n",
      "Iteration 7252, loss = 2.61792719\n",
      "Iteration 7253, loss = 2.61788352\n",
      "Iteration 7254, loss = 2.61790425\n",
      "Iteration 7255, loss = 2.61803047\n",
      "Iteration 7256, loss = 2.61792201\n",
      "Iteration 7257, loss = 2.61786137\n",
      "Iteration 7258, loss = 2.61781311\n",
      "Iteration 7259, loss = 2.61781819\n",
      "Iteration 7260, loss = 2.61781076\n",
      "Iteration 7261, loss = 2.61782002\n",
      "Iteration 7262, loss = 2.61779676\n",
      "Iteration 7263, loss = 2.61782240\n",
      "Iteration 7264, loss = 2.61773379\n",
      "Iteration 7265, loss = 2.61774769\n",
      "Iteration 7266, loss = 2.61769936\n",
      "Iteration 7267, loss = 2.61783518\n",
      "Iteration 7268, loss = 2.61767716\n",
      "Iteration 7269, loss = 2.61779248\n",
      "Iteration 7270, loss = 2.61771694\n",
      "Iteration 7271, loss = 2.61766497\n",
      "Iteration 7272, loss = 2.61762475\n",
      "Iteration 7273, loss = 2.61762747\n",
      "Iteration 7274, loss = 2.61762320\n",
      "Iteration 7275, loss = 2.61761039\n",
      "Iteration 7276, loss = 2.61754152\n",
      "Iteration 7277, loss = 2.61760233\n",
      "Iteration 7278, loss = 2.61755591\n",
      "Iteration 7279, loss = 2.61748679\n",
      "Iteration 7280, loss = 2.61752500\n",
      "Iteration 7281, loss = 2.61760891\n",
      "Iteration 7282, loss = 2.61747459\n",
      "Iteration 7283, loss = 2.61748544\n",
      "Iteration 7284, loss = 2.61742956\n",
      "Iteration 7285, loss = 2.61739756\n",
      "Iteration 7286, loss = 2.61746709\n",
      "Iteration 7287, loss = 2.61738076\n",
      "Iteration 7288, loss = 2.61738169\n",
      "Iteration 7289, loss = 2.61738038\n",
      "Iteration 7290, loss = 2.61738386\n",
      "Iteration 7291, loss = 2.61734310\n",
      "Iteration 7292, loss = 2.61735565\n",
      "Iteration 7293, loss = 2.61741183\n",
      "Iteration 7294, loss = 2.61730065\n",
      "Iteration 7295, loss = 2.61736663\n",
      "Iteration 7296, loss = 2.61738558\n",
      "Iteration 7297, loss = 2.61724589\n",
      "Iteration 7298, loss = 2.61728016\n",
      "Iteration 7299, loss = 2.61742364\n",
      "Iteration 7300, loss = 2.61724386\n",
      "Iteration 7301, loss = 2.61717836\n",
      "Iteration 7302, loss = 2.61727142\n",
      "Iteration 7303, loss = 2.61721440\n",
      "Iteration 7304, loss = 2.61715825\n",
      "Iteration 7305, loss = 2.61717456\n",
      "Iteration 7306, loss = 2.61724354\n",
      "Iteration 7307, loss = 2.61732220\n",
      "Iteration 7308, loss = 2.61710099\n",
      "Iteration 7309, loss = 2.61706397\n",
      "Iteration 7310, loss = 2.61710245\n",
      "Iteration 7311, loss = 2.61715867\n",
      "Iteration 7312, loss = 2.61700853\n",
      "Iteration 7313, loss = 2.61703937\n",
      "Iteration 7314, loss = 2.61701534\n",
      "Iteration 7315, loss = 2.61701257\n",
      "Iteration 7316, loss = 2.61696856\n",
      "Iteration 7317, loss = 2.61697352\n",
      "Iteration 7318, loss = 2.61715286\n",
      "Iteration 7319, loss = 2.61709046\n",
      "Iteration 7320, loss = 2.61698210\n",
      "Iteration 7321, loss = 2.61689094\n",
      "Iteration 7322, loss = 2.61712351\n",
      "Iteration 7323, loss = 2.61688790\n",
      "Iteration 7324, loss = 2.61683171\n",
      "Iteration 7325, loss = 2.61690470\n",
      "Iteration 7326, loss = 2.61687688\n",
      "Iteration 7327, loss = 2.61681115\n",
      "Iteration 7328, loss = 2.61684586\n",
      "Iteration 7329, loss = 2.61686715\n",
      "Iteration 7330, loss = 2.61678086\n",
      "Iteration 7331, loss = 2.61683459\n",
      "Iteration 7332, loss = 2.61678714\n",
      "Iteration 7333, loss = 2.61672597\n",
      "Iteration 7334, loss = 2.61673392\n",
      "Iteration 7335, loss = 2.61675145\n",
      "Iteration 7336, loss = 2.61668529\n",
      "Iteration 7337, loss = 2.61668818\n",
      "Iteration 7338, loss = 2.61675891\n",
      "Iteration 7339, loss = 2.61660635\n",
      "Iteration 7340, loss = 2.61660215\n",
      "Iteration 7341, loss = 2.61660111\n",
      "Iteration 7342, loss = 2.61665572\n",
      "Iteration 7343, loss = 2.61670584\n",
      "Iteration 7344, loss = 2.61654936\n",
      "Iteration 7345, loss = 2.61656043\n",
      "Iteration 7346, loss = 2.61660854\n",
      "Iteration 7347, loss = 2.61655266\n",
      "Iteration 7348, loss = 2.61653411\n",
      "Iteration 7349, loss = 2.61655850\n",
      "Iteration 7350, loss = 2.61655151\n",
      "Iteration 7351, loss = 2.61647771\n",
      "Iteration 7352, loss = 2.61663557\n",
      "Iteration 7353, loss = 2.61651208\n",
      "Iteration 7354, loss = 2.61643173\n",
      "Iteration 7355, loss = 2.61645565\n",
      "Iteration 7356, loss = 2.61643341\n",
      "Iteration 7357, loss = 2.61642800\n",
      "Iteration 7358, loss = 2.61640996\n",
      "Iteration 7359, loss = 2.61635566\n",
      "Iteration 7360, loss = 2.61633857\n",
      "Iteration 7361, loss = 2.61636815\n",
      "Iteration 7362, loss = 2.61641003\n",
      "Iteration 7363, loss = 2.61633720\n",
      "Iteration 7364, loss = 2.61628471\n",
      "Iteration 7365, loss = 2.61629677\n",
      "Iteration 7366, loss = 2.61630901\n",
      "Iteration 7367, loss = 2.61622933\n",
      "Iteration 7368, loss = 2.61625088\n",
      "Iteration 7369, loss = 2.61622828\n",
      "Iteration 7370, loss = 2.61619832\n",
      "Iteration 7371, loss = 2.61626832\n",
      "Iteration 7372, loss = 2.61620481\n",
      "Iteration 7373, loss = 2.61624596\n",
      "Iteration 7374, loss = 2.61623134\n",
      "Iteration 7375, loss = 2.61611551\n",
      "Iteration 7376, loss = 2.61613671\n",
      "Iteration 7377, loss = 2.61618125\n",
      "Iteration 7378, loss = 2.61611375\n",
      "Iteration 7379, loss = 2.61613536\n",
      "Iteration 7380, loss = 2.61605755\n",
      "Iteration 7381, loss = 2.61609883\n",
      "Iteration 7382, loss = 2.61602608\n",
      "Iteration 7383, loss = 2.61605915\n",
      "Iteration 7384, loss = 2.61601804\n",
      "Iteration 7385, loss = 2.61603425\n",
      "Iteration 7386, loss = 2.61599695\n",
      "Iteration 7387, loss = 2.61600783\n",
      "Iteration 7388, loss = 2.61610247\n",
      "Iteration 7389, loss = 2.61598850\n",
      "Iteration 7390, loss = 2.61589878\n",
      "Iteration 7391, loss = 2.61597655\n",
      "Iteration 7392, loss = 2.61589699\n",
      "Iteration 7393, loss = 2.61592671\n",
      "Iteration 7394, loss = 2.61586108\n",
      "Iteration 7395, loss = 2.61588604\n",
      "Iteration 7396, loss = 2.61584170\n",
      "Iteration 7397, loss = 2.61590507\n",
      "Iteration 7398, loss = 2.61584627\n",
      "Iteration 7399, loss = 2.61591378\n",
      "Iteration 7400, loss = 2.61583107\n",
      "Iteration 7401, loss = 2.61577389\n",
      "Iteration 7402, loss = 2.61584236\n",
      "Iteration 7403, loss = 2.61591633\n",
      "Iteration 7404, loss = 2.61568720\n",
      "Iteration 7405, loss = 2.61580139\n",
      "Iteration 7406, loss = 2.61569452\n",
      "Iteration 7407, loss = 2.61568272\n",
      "Iteration 7408, loss = 2.61576850\n",
      "Iteration 7409, loss = 2.61564207\n",
      "Iteration 7410, loss = 2.61570117\n",
      "Iteration 7411, loss = 2.61564332\n",
      "Iteration 7412, loss = 2.61566978\n",
      "Iteration 7413, loss = 2.61575309\n",
      "Iteration 7414, loss = 2.61562049\n",
      "Iteration 7415, loss = 2.61553138\n",
      "Iteration 7416, loss = 2.61572495\n",
      "Iteration 7417, loss = 2.61558571\n",
      "Iteration 7418, loss = 2.61550076\n",
      "Iteration 7419, loss = 2.61552889\n",
      "Iteration 7420, loss = 2.61551410\n",
      "Iteration 7421, loss = 2.61554813\n",
      "Iteration 7422, loss = 2.61550505\n",
      "Iteration 7423, loss = 2.61551305\n",
      "Iteration 7424, loss = 2.61542589\n",
      "Iteration 7425, loss = 2.61540781\n",
      "Iteration 7426, loss = 2.61542152\n",
      "Iteration 7427, loss = 2.61537451\n",
      "Iteration 7428, loss = 2.61551452\n",
      "Iteration 7429, loss = 2.61544913\n",
      "Iteration 7430, loss = 2.61536534\n",
      "Iteration 7431, loss = 2.61532595\n",
      "Iteration 7432, loss = 2.61534883\n",
      "Iteration 7433, loss = 2.61530482\n",
      "Iteration 7434, loss = 2.61530333\n",
      "Iteration 7435, loss = 2.61540133\n",
      "Iteration 7436, loss = 2.61525596\n",
      "Iteration 7437, loss = 2.61526516\n",
      "Iteration 7438, loss = 2.61528651\n",
      "Iteration 7439, loss = 2.61526137\n",
      "Iteration 7440, loss = 2.61532754\n",
      "Iteration 7441, loss = 2.61522480\n",
      "Iteration 7442, loss = 2.61523111\n",
      "Iteration 7443, loss = 2.61527876\n",
      "Iteration 7444, loss = 2.61515467\n",
      "Iteration 7445, loss = 2.61512788\n",
      "Iteration 7446, loss = 2.61513534\n",
      "Iteration 7447, loss = 2.61508343\n",
      "Iteration 7448, loss = 2.61510335\n",
      "Iteration 7449, loss = 2.61508382\n",
      "Iteration 7450, loss = 2.61506860\n",
      "Iteration 7451, loss = 2.61505270\n",
      "Iteration 7452, loss = 2.61511987\n",
      "Iteration 7453, loss = 2.61497164\n",
      "Iteration 7454, loss = 2.61501748\n",
      "Iteration 7455, loss = 2.61500779\n",
      "Iteration 7456, loss = 2.61495286\n",
      "Iteration 7457, loss = 2.61499590\n",
      "Iteration 7458, loss = 2.61492424\n",
      "Iteration 7459, loss = 2.61497338\n",
      "Iteration 7460, loss = 2.61497323\n",
      "Iteration 7461, loss = 2.61519092\n",
      "Iteration 7462, loss = 2.61506263\n",
      "Iteration 7463, loss = 2.61492027\n",
      "Iteration 7464, loss = 2.61484554\n",
      "Iteration 7465, loss = 2.61486533\n",
      "Iteration 7466, loss = 2.61487499\n",
      "Iteration 7467, loss = 2.61480816\n",
      "Iteration 7468, loss = 2.61480725\n",
      "Iteration 7469, loss = 2.61480728\n",
      "Iteration 7470, loss = 2.61481112\n",
      "Iteration 7471, loss = 2.61485981\n",
      "Iteration 7472, loss = 2.61472855\n",
      "Iteration 7473, loss = 2.61471388\n",
      "Iteration 7474, loss = 2.61483588\n",
      "Iteration 7475, loss = 2.61476451\n",
      "Iteration 7476, loss = 2.61469833\n",
      "Iteration 7477, loss = 2.61469686\n",
      "Iteration 7478, loss = 2.61466605\n",
      "Iteration 7479, loss = 2.61465376\n",
      "Iteration 7480, loss = 2.61459312\n",
      "Iteration 7481, loss = 2.61469406\n",
      "Iteration 7482, loss = 2.61464394\n",
      "Iteration 7483, loss = 2.61464183\n",
      "Iteration 7484, loss = 2.61449462\n",
      "Iteration 7485, loss = 2.61462397\n",
      "Iteration 7486, loss = 2.61458678\n",
      "Iteration 7487, loss = 2.61447477\n",
      "Iteration 7488, loss = 2.61466285\n",
      "Iteration 7489, loss = 2.61457701\n",
      "Iteration 7490, loss = 2.61452885\n",
      "Iteration 7491, loss = 2.61460354\n",
      "Iteration 7492, loss = 2.61445336\n",
      "Iteration 7493, loss = 2.61439076\n",
      "Iteration 7494, loss = 2.61445112\n",
      "Iteration 7495, loss = 2.61442207\n",
      "Iteration 7496, loss = 2.61444834\n",
      "Iteration 7497, loss = 2.61435892\n",
      "Iteration 7498, loss = 2.61434945\n",
      "Iteration 7499, loss = 2.61442128\n",
      "Iteration 7500, loss = 2.61435800\n",
      "Iteration 7501, loss = 2.61444239\n",
      "Iteration 7502, loss = 2.61438654\n",
      "Iteration 7503, loss = 2.61443986\n",
      "Iteration 7504, loss = 2.61429423\n",
      "Iteration 7505, loss = 2.61426740\n",
      "Iteration 7506, loss = 2.61433124\n",
      "Iteration 7507, loss = 2.61427401\n",
      "Iteration 7508, loss = 2.61429954\n",
      "Iteration 7509, loss = 2.61431595\n",
      "Iteration 7510, loss = 2.61422564\n",
      "Iteration 7511, loss = 2.61424535\n",
      "Iteration 7512, loss = 2.61437284\n",
      "Iteration 7513, loss = 2.61430184\n",
      "Iteration 7514, loss = 2.61423324\n",
      "Iteration 7515, loss = 2.61421352\n",
      "Iteration 7516, loss = 2.61418211\n",
      "Iteration 7517, loss = 2.61412231\n",
      "Iteration 7518, loss = 2.61410774\n",
      "Iteration 7519, loss = 2.61410286\n",
      "Iteration 7520, loss = 2.61413875\n",
      "Iteration 7521, loss = 2.61409484\n",
      "Iteration 7522, loss = 2.61405850\n",
      "Iteration 7523, loss = 2.61405891\n",
      "Iteration 7524, loss = 2.61402698\n",
      "Iteration 7525, loss = 2.61398219\n",
      "Iteration 7526, loss = 2.61408178\n",
      "Iteration 7527, loss = 2.61395957\n",
      "Iteration 7528, loss = 2.61398563\n",
      "Iteration 7529, loss = 2.61404670\n",
      "Iteration 7530, loss = 2.61403065\n",
      "Iteration 7531, loss = 2.61384331\n",
      "Iteration 7532, loss = 2.61386843\n",
      "Iteration 7533, loss = 2.61402325\n",
      "Iteration 7534, loss = 2.61395850\n",
      "Iteration 7535, loss = 2.61381280\n",
      "Iteration 7536, loss = 2.61388987\n",
      "Iteration 7537, loss = 2.61386081\n",
      "Iteration 7538, loss = 2.61385009\n",
      "Iteration 7539, loss = 2.61381903\n",
      "Iteration 7540, loss = 2.61378336\n",
      "Iteration 7541, loss = 2.61377924\n",
      "Iteration 7542, loss = 2.61376094\n",
      "Iteration 7543, loss = 2.61381925\n",
      "Iteration 7544, loss = 2.61373806\n",
      "Iteration 7545, loss = 2.61375179\n",
      "Iteration 7546, loss = 2.61367251\n",
      "Iteration 7547, loss = 2.61374382\n",
      "Iteration 7548, loss = 2.61366563\n",
      "Iteration 7549, loss = 2.61366045\n",
      "Iteration 7550, loss = 2.61365249\n",
      "Iteration 7551, loss = 2.61363894\n",
      "Iteration 7552, loss = 2.61360413\n",
      "Iteration 7553, loss = 2.61364172\n",
      "Iteration 7554, loss = 2.61358668\n",
      "Iteration 7555, loss = 2.61358894\n",
      "Iteration 7556, loss = 2.61358113\n",
      "Iteration 7557, loss = 2.61354124\n",
      "Iteration 7558, loss = 2.61356681\n",
      "Iteration 7559, loss = 2.61370047\n",
      "Iteration 7560, loss = 2.61354468\n",
      "Iteration 7561, loss = 2.61356248\n",
      "Iteration 7562, loss = 2.61355352\n",
      "Iteration 7563, loss = 2.61348819\n",
      "Iteration 7564, loss = 2.61351071\n",
      "Iteration 7565, loss = 2.61345415\n",
      "Iteration 7566, loss = 2.61354133\n",
      "Iteration 7567, loss = 2.61349337\n",
      "Iteration 7568, loss = 2.61342502\n",
      "Iteration 7569, loss = 2.61348869\n",
      "Iteration 7570, loss = 2.61336479\n",
      "Iteration 7571, loss = 2.61334882\n",
      "Iteration 7572, loss = 2.61338829\n",
      "Iteration 7573, loss = 2.61332754\n",
      "Iteration 7574, loss = 2.61332915\n",
      "Iteration 7575, loss = 2.61327892\n",
      "Iteration 7576, loss = 2.61335685\n",
      "Iteration 7577, loss = 2.61333353\n",
      "Iteration 7578, loss = 2.61329370\n",
      "Iteration 7579, loss = 2.61323233\n",
      "Iteration 7580, loss = 2.61324090\n",
      "Iteration 7581, loss = 2.61324814\n",
      "Iteration 7582, loss = 2.61340267\n",
      "Iteration 7583, loss = 2.61321032\n",
      "Iteration 7584, loss = 2.61313345\n",
      "Iteration 7585, loss = 2.61312642\n",
      "Iteration 7586, loss = 2.61317245\n",
      "Iteration 7587, loss = 2.61322888\n",
      "Iteration 7588, loss = 2.61312698\n",
      "Iteration 7589, loss = 2.61306524\n",
      "Iteration 7590, loss = 2.61317817\n",
      "Iteration 7591, loss = 2.61303658\n",
      "Iteration 7592, loss = 2.61307669\n",
      "Iteration 7593, loss = 2.61304854\n",
      "Iteration 7594, loss = 2.61316509\n",
      "Iteration 7595, loss = 2.61297306\n",
      "Iteration 7596, loss = 2.61297659\n",
      "Iteration 7597, loss = 2.61300076\n",
      "Iteration 7598, loss = 2.61315769\n",
      "Iteration 7599, loss = 2.61293873\n",
      "Iteration 7600, loss = 2.61295363\n",
      "Iteration 7601, loss = 2.61300354\n",
      "Iteration 7602, loss = 2.61292972\n",
      "Iteration 7603, loss = 2.61290436\n",
      "Iteration 7604, loss = 2.61290020\n",
      "Iteration 7605, loss = 2.61287199\n",
      "Iteration 7606, loss = 2.61289240\n",
      "Iteration 7607, loss = 2.61288802\n",
      "Iteration 7608, loss = 2.61283366\n",
      "Iteration 7609, loss = 2.61284604\n",
      "Iteration 7610, loss = 2.61281705\n",
      "Iteration 7611, loss = 2.61277843\n",
      "Iteration 7612, loss = 2.61283338\n",
      "Iteration 7613, loss = 2.61279689\n",
      "Iteration 7614, loss = 2.61275164\n",
      "Iteration 7615, loss = 2.61274264\n",
      "Iteration 7616, loss = 2.61274338\n",
      "Iteration 7617, loss = 2.61273279\n",
      "Iteration 7618, loss = 2.61270760\n",
      "Iteration 7619, loss = 2.61265689\n",
      "Iteration 7620, loss = 2.61272874\n",
      "Iteration 7621, loss = 2.61268525\n",
      "Iteration 7622, loss = 2.61266358\n",
      "Iteration 7623, loss = 2.61258766\n",
      "Iteration 7624, loss = 2.61267262\n",
      "Iteration 7625, loss = 2.61262560\n",
      "Iteration 7626, loss = 2.61260663\n",
      "Iteration 7627, loss = 2.61265083\n",
      "Iteration 7628, loss = 2.61252802\n",
      "Iteration 7629, loss = 2.61253287\n",
      "Iteration 7630, loss = 2.61251159\n",
      "Iteration 7631, loss = 2.61248853\n",
      "Iteration 7632, loss = 2.61250783\n",
      "Iteration 7633, loss = 2.61241724\n",
      "Iteration 7634, loss = 2.61254611\n",
      "Iteration 7635, loss = 2.61251590\n",
      "Iteration 7636, loss = 2.61250001\n",
      "Iteration 7637, loss = 2.61241605\n",
      "Iteration 7638, loss = 2.61237809\n",
      "Iteration 7639, loss = 2.61239112\n",
      "Iteration 7640, loss = 2.61237923\n",
      "Iteration 7641, loss = 2.61234416\n",
      "Iteration 7642, loss = 2.61235611\n",
      "Iteration 7643, loss = 2.61246372\n",
      "Iteration 7644, loss = 2.61228623\n",
      "Iteration 7645, loss = 2.61232515\n",
      "Iteration 7646, loss = 2.61228620\n",
      "Iteration 7647, loss = 2.61230753\n",
      "Iteration 7648, loss = 2.61228811\n",
      "Iteration 7649, loss = 2.61226260\n",
      "Iteration 7650, loss = 2.61222380\n",
      "Iteration 7651, loss = 2.61223813\n",
      "Iteration 7652, loss = 2.61220380\n",
      "Iteration 7653, loss = 2.61218655\n",
      "Iteration 7654, loss = 2.61216950\n",
      "Iteration 7655, loss = 2.61221578\n",
      "Iteration 7656, loss = 2.61210606\n",
      "Iteration 7657, loss = 2.61216867\n",
      "Iteration 7658, loss = 2.61212472\n",
      "Iteration 7659, loss = 2.61207607\n",
      "Iteration 7660, loss = 2.61215136\n",
      "Iteration 7661, loss = 2.61208044\n",
      "Iteration 7662, loss = 2.61211613\n",
      "Iteration 7663, loss = 2.61204322\n",
      "Iteration 7664, loss = 2.61203262\n",
      "Iteration 7665, loss = 2.61199775\n",
      "Iteration 7666, loss = 2.61210220\n",
      "Iteration 7667, loss = 2.61199749\n",
      "Iteration 7668, loss = 2.61219856\n",
      "Iteration 7669, loss = 2.61194206\n",
      "Iteration 7670, loss = 2.61199962\n",
      "Iteration 7671, loss = 2.61193352\n",
      "Iteration 7672, loss = 2.61196023\n",
      "Iteration 7673, loss = 2.61190843\n",
      "Iteration 7674, loss = 2.61196399\n",
      "Iteration 7675, loss = 2.61198643\n",
      "Iteration 7676, loss = 2.61193305\n",
      "Iteration 7677, loss = 2.61190670\n",
      "Iteration 7678, loss = 2.61182881\n",
      "Iteration 7679, loss = 2.61185808\n",
      "Iteration 7680, loss = 2.61179007\n",
      "Iteration 7681, loss = 2.61177655\n",
      "Iteration 7682, loss = 2.61177562\n",
      "Iteration 7683, loss = 2.61178222\n",
      "Iteration 7684, loss = 2.61181217\n",
      "Iteration 7685, loss = 2.61176480\n",
      "Iteration 7686, loss = 2.61174945\n",
      "Iteration 7687, loss = 2.61167402\n",
      "Iteration 7688, loss = 2.61173442\n",
      "Iteration 7689, loss = 2.61172998\n",
      "Iteration 7690, loss = 2.61166573\n",
      "Iteration 7691, loss = 2.61165679\n",
      "Iteration 7692, loss = 2.61162173\n",
      "Iteration 7693, loss = 2.61165987\n",
      "Iteration 7694, loss = 2.61162637\n",
      "Iteration 7695, loss = 2.61158851\n",
      "Iteration 7696, loss = 2.61155832\n",
      "Iteration 7697, loss = 2.61156179\n",
      "Iteration 7698, loss = 2.61157442\n",
      "Iteration 7699, loss = 2.61154309\n",
      "Iteration 7700, loss = 2.61154958\n",
      "Iteration 7701, loss = 2.61155305\n",
      "Iteration 7702, loss = 2.61150865\n",
      "Iteration 7703, loss = 2.61153655\n",
      "Iteration 7704, loss = 2.61162434\n",
      "Iteration 7705, loss = 2.61153916\n",
      "Iteration 7706, loss = 2.61147022\n",
      "Iteration 7707, loss = 2.61140493\n",
      "Iteration 7708, loss = 2.61142019\n",
      "Iteration 7709, loss = 2.61143241\n",
      "Iteration 7710, loss = 2.61154356\n",
      "Iteration 7711, loss = 2.61142334\n",
      "Iteration 7712, loss = 2.61137797\n",
      "Iteration 7713, loss = 2.61140378\n",
      "Iteration 7714, loss = 2.61135383\n",
      "Iteration 7715, loss = 2.61137102\n",
      "Iteration 7716, loss = 2.61140755\n",
      "Iteration 7717, loss = 2.61135345\n",
      "Iteration 7718, loss = 2.61128608\n",
      "Iteration 7719, loss = 2.61129360\n",
      "Iteration 7720, loss = 2.61123078\n",
      "Iteration 7721, loss = 2.61126154\n",
      "Iteration 7722, loss = 2.61124502\n",
      "Iteration 7723, loss = 2.61127411\n",
      "Iteration 7724, loss = 2.61122785\n",
      "Iteration 7725, loss = 2.61126473\n",
      "Iteration 7726, loss = 2.61115958\n",
      "Iteration 7727, loss = 2.61113917\n",
      "Iteration 7728, loss = 2.61124587\n",
      "Iteration 7729, loss = 2.61113082\n",
      "Iteration 7730, loss = 2.61115233\n",
      "Iteration 7731, loss = 2.61111940\n",
      "Iteration 7732, loss = 2.61120939\n",
      "Iteration 7733, loss = 2.61114750\n",
      "Iteration 7734, loss = 2.61108609\n",
      "Iteration 7735, loss = 2.61113307\n",
      "Iteration 7736, loss = 2.61102885\n",
      "Iteration 7737, loss = 2.61102722\n",
      "Iteration 7738, loss = 2.61099002\n",
      "Iteration 7739, loss = 2.61105593\n",
      "Iteration 7740, loss = 2.61104057\n",
      "Iteration 7741, loss = 2.61100951\n",
      "Iteration 7742, loss = 2.61101559\n",
      "Iteration 7743, loss = 2.61094093\n",
      "Iteration 7744, loss = 2.61092942\n",
      "Iteration 7745, loss = 2.61091627\n",
      "Iteration 7746, loss = 2.61093763\n",
      "Iteration 7747, loss = 2.61082669\n",
      "Iteration 7748, loss = 2.61087429\n",
      "Iteration 7749, loss = 2.61084185\n",
      "Iteration 7750, loss = 2.61085179\n",
      "Iteration 7751, loss = 2.61082478\n",
      "Iteration 7752, loss = 2.61094387\n",
      "Iteration 7753, loss = 2.61070367\n",
      "Iteration 7754, loss = 2.61089102\n",
      "Iteration 7755, loss = 2.61085501\n",
      "Iteration 7756, loss = 2.61083445\n",
      "Iteration 7757, loss = 2.61080541\n",
      "Iteration 7758, loss = 2.61079317\n",
      "Iteration 7759, loss = 2.61073119\n",
      "Iteration 7760, loss = 2.61068356\n",
      "Iteration 7761, loss = 2.61085187\n",
      "Iteration 7762, loss = 2.61080779\n",
      "Iteration 7763, loss = 2.61078182\n",
      "Iteration 7764, loss = 2.61064374\n",
      "Iteration 7765, loss = 2.61064772\n",
      "Iteration 7766, loss = 2.61057822\n",
      "Iteration 7767, loss = 2.61060838\n",
      "Iteration 7768, loss = 2.61056571\n",
      "Iteration 7769, loss = 2.61057733\n",
      "Iteration 7770, loss = 2.61069071\n",
      "Iteration 7771, loss = 2.61066267\n",
      "Iteration 7772, loss = 2.61055806\n",
      "Iteration 7773, loss = 2.61050325\n",
      "Iteration 7774, loss = 2.61057261\n",
      "Iteration 7775, loss = 2.61057451\n",
      "Iteration 7776, loss = 2.61045972\n",
      "Iteration 7777, loss = 2.61042506\n",
      "Iteration 7778, loss = 2.61045667\n",
      "Iteration 7779, loss = 2.61045866\n",
      "Iteration 7780, loss = 2.61044286\n",
      "Iteration 7781, loss = 2.61044357\n",
      "Iteration 7782, loss = 2.61041465\n",
      "Iteration 7783, loss = 2.61038619\n",
      "Iteration 7784, loss = 2.61031086\n",
      "Iteration 7785, loss = 2.61034301\n",
      "Iteration 7786, loss = 2.61035829\n",
      "Iteration 7787, loss = 2.61033219\n",
      "Iteration 7788, loss = 2.61028020\n",
      "Iteration 7789, loss = 2.61033215\n",
      "Iteration 7790, loss = 2.61028395\n",
      "Iteration 7791, loss = 2.61032727\n",
      "Iteration 7792, loss = 2.61039508\n",
      "Iteration 7793, loss = 2.61026084\n",
      "Iteration 7794, loss = 2.61027287\n",
      "Iteration 7795, loss = 2.61023996\n",
      "Iteration 7796, loss = 2.61021567\n",
      "Iteration 7797, loss = 2.61015818\n",
      "Iteration 7798, loss = 2.61016427\n",
      "Iteration 7799, loss = 2.61020494\n",
      "Iteration 7800, loss = 2.61010000\n",
      "Iteration 7801, loss = 2.61010707\n",
      "Iteration 7802, loss = 2.61017380\n",
      "Iteration 7803, loss = 2.61013605\n",
      "Iteration 7804, loss = 2.61015951\n",
      "Iteration 7805, loss = 2.61009075\n",
      "Iteration 7806, loss = 2.61002316\n",
      "Iteration 7807, loss = 2.61005491\n",
      "Iteration 7808, loss = 2.61022004\n",
      "Iteration 7809, loss = 2.60998243\n",
      "Iteration 7810, loss = 2.61007280\n",
      "Iteration 7811, loss = 2.61003216\n",
      "Iteration 7812, loss = 2.61009101\n",
      "Iteration 7813, loss = 2.61002849\n",
      "Iteration 7814, loss = 2.60994364\n",
      "Iteration 7815, loss = 2.61000901\n",
      "Iteration 7816, loss = 2.61012822\n",
      "Iteration 7817, loss = 2.60992288\n",
      "Iteration 7818, loss = 2.60995515\n",
      "Iteration 7819, loss = 2.60988118\n",
      "Iteration 7820, loss = 2.60985641\n",
      "Iteration 7821, loss = 2.60983740\n",
      "Iteration 7822, loss = 2.60983119\n",
      "Iteration 7823, loss = 2.60981242\n",
      "Iteration 7824, loss = 2.60982519\n",
      "Iteration 7825, loss = 2.60981921\n",
      "Iteration 7826, loss = 2.60986447\n",
      "Iteration 7827, loss = 2.60983112\n",
      "Iteration 7828, loss = 2.60974651\n",
      "Iteration 7829, loss = 2.60972023\n",
      "Iteration 7830, loss = 2.60971764\n",
      "Iteration 7831, loss = 2.60980067\n",
      "Iteration 7832, loss = 2.60969187\n",
      "Iteration 7833, loss = 2.60973373\n",
      "Iteration 7834, loss = 2.60968002\n",
      "Iteration 7835, loss = 2.60963933\n",
      "Iteration 7836, loss = 2.60970956\n",
      "Iteration 7837, loss = 2.60971443\n",
      "Iteration 7838, loss = 2.60967039\n",
      "Iteration 7839, loss = 2.60963606\n",
      "Iteration 7840, loss = 2.60962933\n",
      "Iteration 7841, loss = 2.60955856\n",
      "Iteration 7842, loss = 2.60961280\n",
      "Iteration 7843, loss = 2.60957159\n",
      "Iteration 7844, loss = 2.60949504\n",
      "Iteration 7845, loss = 2.60958410\n",
      "Iteration 7846, loss = 2.60953324\n",
      "Iteration 7847, loss = 2.60953451\n",
      "Iteration 7848, loss = 2.60947940\n",
      "Iteration 7849, loss = 2.60952896\n",
      "Iteration 7850, loss = 2.60946100\n",
      "Iteration 7851, loss = 2.60946596\n",
      "Iteration 7852, loss = 2.60946553\n",
      "Iteration 7853, loss = 2.60943282\n",
      "Iteration 7854, loss = 2.60944143\n",
      "Iteration 7855, loss = 2.60938637\n",
      "Iteration 7856, loss = 2.60937976\n",
      "Iteration 7857, loss = 2.60933525\n",
      "Iteration 7858, loss = 2.60932616\n",
      "Iteration 7859, loss = 2.60944606\n",
      "Iteration 7860, loss = 2.60931049\n",
      "Iteration 7861, loss = 2.60945470\n",
      "Iteration 7862, loss = 2.60937913\n",
      "Iteration 7863, loss = 2.60924844\n",
      "Iteration 7864, loss = 2.60933640\n",
      "Iteration 7865, loss = 2.60957815\n",
      "Iteration 7866, loss = 2.60918989\n",
      "Iteration 7867, loss = 2.60924276\n",
      "Iteration 7868, loss = 2.60931059\n",
      "Iteration 7869, loss = 2.60919857\n",
      "Iteration 7870, loss = 2.60918356\n",
      "Iteration 7871, loss = 2.60913902\n",
      "Iteration 7872, loss = 2.60912589\n",
      "Iteration 7873, loss = 2.60913374\n",
      "Iteration 7874, loss = 2.60919557\n",
      "Iteration 7875, loss = 2.60919407\n",
      "Iteration 7876, loss = 2.60910315\n",
      "Iteration 7877, loss = 2.60907835\n",
      "Iteration 7878, loss = 2.60905543\n",
      "Iteration 7879, loss = 2.60921906\n",
      "Iteration 7880, loss = 2.60911910\n",
      "Iteration 7881, loss = 2.60899795\n",
      "Iteration 7882, loss = 2.60901282\n",
      "Iteration 7883, loss = 2.60901027\n",
      "Iteration 7884, loss = 2.60892417\n",
      "Iteration 7885, loss = 2.60901137\n",
      "Iteration 7886, loss = 2.60902104\n",
      "Iteration 7887, loss = 2.60905236\n",
      "Iteration 7888, loss = 2.60904854\n",
      "Iteration 7889, loss = 2.60881502\n",
      "Iteration 7890, loss = 2.60899974\n",
      "Iteration 7891, loss = 2.60902134\n",
      "Iteration 7892, loss = 2.60889366\n",
      "Iteration 7893, loss = 2.60886813\n",
      "Iteration 7894, loss = 2.60885058\n",
      "Iteration 7895, loss = 2.60886589\n",
      "Iteration 7896, loss = 2.60885866\n",
      "Iteration 7897, loss = 2.60878427\n",
      "Iteration 7898, loss = 2.60885196\n",
      "Iteration 7899, loss = 2.60874794\n",
      "Iteration 7900, loss = 2.60873835\n",
      "Iteration 7901, loss = 2.60875892\n",
      "Iteration 7902, loss = 2.60873277\n",
      "Iteration 7903, loss = 2.60875058\n",
      "Iteration 7904, loss = 2.60868182\n",
      "Iteration 7905, loss = 2.60871601\n",
      "Iteration 7906, loss = 2.60878693\n",
      "Iteration 7907, loss = 2.60872157\n",
      "Iteration 7908, loss = 2.60861914\n",
      "Iteration 7909, loss = 2.60872318\n",
      "Iteration 7910, loss = 2.60862636\n",
      "Iteration 7911, loss = 2.60857810\n",
      "Iteration 7912, loss = 2.60854797\n",
      "Iteration 7913, loss = 2.60877483\n",
      "Iteration 7914, loss = 2.60861382\n",
      "Iteration 7915, loss = 2.60854687\n",
      "Iteration 7916, loss = 2.60860834\n",
      "Iteration 7917, loss = 2.60853234\n",
      "Iteration 7918, loss = 2.60846336\n",
      "Iteration 7919, loss = 2.60851925\n",
      "Iteration 7920, loss = 2.60845675\n",
      "Iteration 7921, loss = 2.60848251\n",
      "Iteration 7922, loss = 2.60843171\n",
      "Iteration 7923, loss = 2.60847296\n",
      "Iteration 7924, loss = 2.60846415\n",
      "Iteration 7925, loss = 2.60846590\n",
      "Iteration 7926, loss = 2.60836159\n",
      "Iteration 7927, loss = 2.60839352\n",
      "Iteration 7928, loss = 2.60844522\n",
      "Iteration 7929, loss = 2.60841664\n",
      "Iteration 7930, loss = 2.60845031\n",
      "Iteration 7931, loss = 2.60834163\n",
      "Iteration 7932, loss = 2.60868162\n",
      "Iteration 7933, loss = 2.60830746\n",
      "Iteration 7934, loss = 2.60830025\n",
      "Iteration 7935, loss = 2.60827472\n",
      "Iteration 7936, loss = 2.60831530\n",
      "Iteration 7937, loss = 2.60822441\n",
      "Iteration 7938, loss = 2.60822853\n",
      "Iteration 7939, loss = 2.60824903\n",
      "Iteration 7940, loss = 2.60820299\n",
      "Iteration 7941, loss = 2.60819314\n",
      "Iteration 7942, loss = 2.60821477\n",
      "Iteration 7943, loss = 2.60817799\n",
      "Iteration 7944, loss = 2.60820766\n",
      "Iteration 7945, loss = 2.60816086\n",
      "Iteration 7946, loss = 2.60816839\n",
      "Iteration 7947, loss = 2.60814897\n",
      "Iteration 7948, loss = 2.60813884\n",
      "Iteration 7949, loss = 2.60812309\n",
      "Iteration 7950, loss = 2.60804618\n",
      "Iteration 7951, loss = 2.60806049\n",
      "Iteration 7952, loss = 2.60813708\n",
      "Iteration 7953, loss = 2.60802586\n",
      "Iteration 7954, loss = 2.60803296\n",
      "Iteration 7955, loss = 2.60795637\n",
      "Iteration 7956, loss = 2.60800372\n",
      "Iteration 7957, loss = 2.60800673\n",
      "Iteration 7958, loss = 2.60800219\n",
      "Iteration 7959, loss = 2.60793676\n",
      "Iteration 7960, loss = 2.60801112\n",
      "Iteration 7961, loss = 2.60791408\n",
      "Iteration 7962, loss = 2.60790051\n",
      "Iteration 7963, loss = 2.60790957\n",
      "Iteration 7964, loss = 2.60784827\n",
      "Iteration 7965, loss = 2.60790577\n",
      "Iteration 7966, loss = 2.60794413\n",
      "Iteration 7967, loss = 2.60778883\n",
      "Iteration 7968, loss = 2.60783132\n",
      "Iteration 7969, loss = 2.60781910\n",
      "Iteration 7970, loss = 2.60786803\n",
      "Iteration 7971, loss = 2.60778629\n",
      "Iteration 7972, loss = 2.60778011\n",
      "Iteration 7973, loss = 2.60783462\n",
      "Iteration 7974, loss = 2.60772013\n",
      "Iteration 7975, loss = 2.60776764\n",
      "Iteration 7976, loss = 2.60770646\n",
      "Iteration 7977, loss = 2.60769251\n",
      "Iteration 7978, loss = 2.60766314\n",
      "Iteration 7979, loss = 2.60762679\n",
      "Iteration 7980, loss = 2.60765696\n",
      "Iteration 7981, loss = 2.60767487\n",
      "Iteration 7982, loss = 2.60772067\n",
      "Iteration 7983, loss = 2.60761331\n",
      "Iteration 7984, loss = 2.60763873\n",
      "Iteration 7985, loss = 2.60767040\n",
      "Iteration 7986, loss = 2.60757220\n",
      "Iteration 7987, loss = 2.60755841\n",
      "Iteration 7988, loss = 2.60757276\n",
      "Iteration 7989, loss = 2.60758973\n",
      "Iteration 7990, loss = 2.60753592\n",
      "Iteration 7991, loss = 2.60751015\n",
      "Iteration 7992, loss = 2.60750572\n",
      "Iteration 7993, loss = 2.60754329\n",
      "Iteration 7994, loss = 2.60751353\n",
      "Iteration 7995, loss = 2.60753738\n",
      "Iteration 7996, loss = 2.60741552\n",
      "Iteration 7997, loss = 2.60739902\n",
      "Iteration 7998, loss = 2.60737939\n",
      "Iteration 7999, loss = 2.60735804\n",
      "Iteration 8000, loss = 2.60756089\n",
      "Iteration 8001, loss = 2.60737932\n",
      "Iteration 8002, loss = 2.60728451\n",
      "Iteration 8003, loss = 2.60736694\n",
      "Iteration 8004, loss = 2.60744055\n",
      "Iteration 8005, loss = 2.60728258\n",
      "Iteration 8006, loss = 2.60737094\n",
      "Iteration 8007, loss = 2.60729742\n",
      "Iteration 8008, loss = 2.60730492\n",
      "Iteration 8009, loss = 2.60724816\n",
      "Iteration 8010, loss = 2.60724096\n",
      "Iteration 8011, loss = 2.60726807\n",
      "Iteration 8012, loss = 2.60721317\n",
      "Iteration 8013, loss = 2.60726318\n",
      "Iteration 8014, loss = 2.60716662\n",
      "Iteration 8015, loss = 2.60715957\n",
      "Iteration 8016, loss = 2.60716293\n",
      "Iteration 8017, loss = 2.60718163\n",
      "Iteration 8018, loss = 2.60709565\n",
      "Iteration 8019, loss = 2.60736930\n",
      "Iteration 8020, loss = 2.60720124\n",
      "Iteration 8021, loss = 2.60718317\n",
      "Iteration 8022, loss = 2.60714822\n",
      "Iteration 8023, loss = 2.60707731\n",
      "Iteration 8024, loss = 2.60708244\n",
      "Iteration 8025, loss = 2.60701061\n",
      "Iteration 8026, loss = 2.60705953\n",
      "Iteration 8027, loss = 2.60703015\n",
      "Iteration 8028, loss = 2.60710514\n",
      "Iteration 8029, loss = 2.60698216\n",
      "Iteration 8030, loss = 2.60697815\n",
      "Iteration 8031, loss = 2.60694602\n",
      "Iteration 8032, loss = 2.60695839\n",
      "Iteration 8033, loss = 2.60699195\n",
      "Iteration 8034, loss = 2.60702021\n",
      "Iteration 8035, loss = 2.60685797\n",
      "Iteration 8036, loss = 2.60688267\n",
      "Iteration 8037, loss = 2.60685187\n",
      "Iteration 8038, loss = 2.60686938\n",
      "Iteration 8039, loss = 2.60692991\n",
      "Iteration 8040, loss = 2.60685440\n",
      "Iteration 8041, loss = 2.60680904\n",
      "Iteration 8042, loss = 2.60679331\n",
      "Iteration 8043, loss = 2.60679629\n",
      "Iteration 8044, loss = 2.60677412\n",
      "Iteration 8045, loss = 2.60680937\n",
      "Iteration 8046, loss = 2.60692789\n",
      "Iteration 8047, loss = 2.60681390\n",
      "Iteration 8048, loss = 2.60678085\n",
      "Iteration 8049, loss = 2.60669572\n",
      "Iteration 8050, loss = 2.60670567\n",
      "Iteration 8051, loss = 2.60668892\n",
      "Iteration 8052, loss = 2.60671721\n",
      "Iteration 8053, loss = 2.60674351\n",
      "Iteration 8054, loss = 2.60661935\n",
      "Iteration 8055, loss = 2.60667232\n",
      "Iteration 8056, loss = 2.60664918\n",
      "Iteration 8057, loss = 2.60669047\n",
      "Iteration 8058, loss = 2.60663481\n",
      "Iteration 8059, loss = 2.60655369\n",
      "Iteration 8060, loss = 2.60653458\n",
      "Iteration 8061, loss = 2.60654181\n",
      "Iteration 8062, loss = 2.60659994\n",
      "Iteration 8063, loss = 2.60663199\n",
      "Iteration 8064, loss = 2.60650206\n",
      "Iteration 8065, loss = 2.60648047\n",
      "Iteration 8066, loss = 2.60648081\n",
      "Iteration 8067, loss = 2.60644682\n",
      "Iteration 8068, loss = 2.60650657\n",
      "Iteration 8069, loss = 2.60639903\n",
      "Iteration 8070, loss = 2.60638387\n",
      "Iteration 8071, loss = 2.60642086\n",
      "Iteration 8072, loss = 2.60643788\n",
      "Iteration 8073, loss = 2.60650358\n",
      "Iteration 8074, loss = 2.60635300\n",
      "Iteration 8075, loss = 2.60635675\n",
      "Iteration 8076, loss = 2.60637939\n",
      "Iteration 8077, loss = 2.60636171\n",
      "Iteration 8078, loss = 2.60636725\n",
      "Iteration 8079, loss = 2.60640104\n",
      "Iteration 8080, loss = 2.60634827\n",
      "Iteration 8081, loss = 2.60629910\n",
      "Iteration 8082, loss = 2.60626555\n",
      "Iteration 8083, loss = 2.60622569\n",
      "Iteration 8084, loss = 2.60633815\n",
      "Iteration 8085, loss = 2.60622214\n",
      "Iteration 8086, loss = 2.60627875\n",
      "Iteration 8087, loss = 2.60623775\n",
      "Iteration 8088, loss = 2.60621889\n",
      "Iteration 8089, loss = 2.60632405\n",
      "Iteration 8090, loss = 2.60620457\n",
      "Iteration 8091, loss = 2.60615903\n",
      "Iteration 8092, loss = 2.60618009\n",
      "Iteration 8093, loss = 2.60618669\n",
      "Iteration 8094, loss = 2.60614193\n",
      "Iteration 8095, loss = 2.60615619\n",
      "Iteration 8096, loss = 2.60612242\n",
      "Iteration 8097, loss = 2.60602673\n",
      "Iteration 8098, loss = 2.60611003\n",
      "Iteration 8099, loss = 2.60607643\n",
      "Iteration 8100, loss = 2.60603628\n",
      "Iteration 8101, loss = 2.60605415\n",
      "Iteration 8102, loss = 2.60599796\n",
      "Iteration 8103, loss = 2.60605672\n",
      "Iteration 8104, loss = 2.60591673\n",
      "Iteration 8105, loss = 2.60596818\n",
      "Iteration 8106, loss = 2.60598991\n",
      "Iteration 8107, loss = 2.60594878\n",
      "Iteration 8108, loss = 2.60590408\n",
      "Iteration 8109, loss = 2.60596614\n",
      "Iteration 8110, loss = 2.60594436\n",
      "Iteration 8111, loss = 2.60586879\n",
      "Iteration 8112, loss = 2.60587034\n",
      "Iteration 8113, loss = 2.60594930\n",
      "Iteration 8114, loss = 2.60582581\n",
      "Iteration 8115, loss = 2.60581206\n",
      "Iteration 8116, loss = 2.60577121\n",
      "Iteration 8117, loss = 2.60578265\n",
      "Iteration 8118, loss = 2.60576276\n",
      "Iteration 8119, loss = 2.60579267\n",
      "Iteration 8120, loss = 2.60583093\n",
      "Iteration 8121, loss = 2.60572980\n",
      "Iteration 8122, loss = 2.60572977\n",
      "Iteration 8123, loss = 2.60576375\n",
      "Iteration 8124, loss = 2.60569678\n",
      "Iteration 8125, loss = 2.60584143\n",
      "Iteration 8126, loss = 2.60554363\n",
      "Iteration 8127, loss = 2.60576396\n",
      "Iteration 8128, loss = 2.60569935\n",
      "Iteration 8129, loss = 2.60569789\n",
      "Iteration 8130, loss = 2.60561124\n",
      "Iteration 8131, loss = 2.60559627\n",
      "Iteration 8132, loss = 2.60558146\n",
      "Iteration 8133, loss = 2.60563748\n",
      "Iteration 8134, loss = 2.60552630\n",
      "Iteration 8135, loss = 2.60551488\n",
      "Iteration 8136, loss = 2.60553386\n",
      "Iteration 8137, loss = 2.60550571\n",
      "Iteration 8138, loss = 2.60547232\n",
      "Iteration 8139, loss = 2.60547750\n",
      "Iteration 8140, loss = 2.60556716\n",
      "Iteration 8141, loss = 2.60542819\n",
      "Iteration 8142, loss = 2.60545998\n",
      "Iteration 8143, loss = 2.60545186\n",
      "Iteration 8144, loss = 2.60541684\n",
      "Iteration 8145, loss = 2.60537726\n",
      "Iteration 8146, loss = 2.60540579\n",
      "Iteration 8147, loss = 2.60541719\n",
      "Iteration 8148, loss = 2.60542232\n",
      "Iteration 8149, loss = 2.60535604\n",
      "Iteration 8150, loss = 2.60535274\n",
      "Iteration 8151, loss = 2.60531465\n",
      "Iteration 8152, loss = 2.60537205\n",
      "Iteration 8153, loss = 2.60529841\n",
      "Iteration 8154, loss = 2.60529527\n",
      "Iteration 8155, loss = 2.60526476\n",
      "Iteration 8156, loss = 2.60532939\n",
      "Iteration 8157, loss = 2.60527091\n",
      "Iteration 8158, loss = 2.60523496\n",
      "Iteration 8159, loss = 2.60530092\n",
      "Iteration 8160, loss = 2.60518684\n",
      "Iteration 8161, loss = 2.60515684\n",
      "Iteration 8162, loss = 2.60518875\n",
      "Iteration 8163, loss = 2.60526650\n",
      "Iteration 8164, loss = 2.60511706\n",
      "Iteration 8165, loss = 2.60516446\n",
      "Iteration 8166, loss = 2.60513945\n",
      "Iteration 8167, loss = 2.60519997\n",
      "Iteration 8168, loss = 2.60511421\n",
      "Iteration 8169, loss = 2.60507271\n",
      "Iteration 8170, loss = 2.60514910\n",
      "Iteration 8171, loss = 2.60500772\n",
      "Iteration 8172, loss = 2.60501564\n",
      "Iteration 8173, loss = 2.60518465\n",
      "Iteration 8174, loss = 2.60504547\n",
      "Iteration 8175, loss = 2.60501422\n",
      "Iteration 8176, loss = 2.60498382\n",
      "Iteration 8177, loss = 2.60492928\n",
      "Iteration 8178, loss = 2.60494649\n",
      "Iteration 8179, loss = 2.60499777\n",
      "Iteration 8180, loss = 2.60486814\n",
      "Iteration 8181, loss = 2.60490808\n",
      "Iteration 8182, loss = 2.60489282\n",
      "Iteration 8183, loss = 2.60488834\n",
      "Iteration 8184, loss = 2.60491381\n",
      "Iteration 8185, loss = 2.60501742\n",
      "Iteration 8186, loss = 2.60485755\n",
      "Iteration 8187, loss = 2.60483121\n",
      "Iteration 8188, loss = 2.60492988\n",
      "Iteration 8189, loss = 2.60489375\n",
      "Iteration 8190, loss = 2.60473377\n",
      "Iteration 8191, loss = 2.60476532\n",
      "Iteration 8192, loss = 2.60482771\n",
      "Iteration 8193, loss = 2.60473546\n",
      "Iteration 8194, loss = 2.60471386\n",
      "Iteration 8195, loss = 2.60471564\n",
      "Iteration 8196, loss = 2.60473809\n",
      "Iteration 8197, loss = 2.60468400\n",
      "Iteration 8198, loss = 2.60469805\n",
      "Iteration 8199, loss = 2.60464301\n",
      "Iteration 8200, loss = 2.60472516\n",
      "Iteration 8201, loss = 2.60464697\n",
      "Iteration 8202, loss = 2.60462779\n",
      "Iteration 8203, loss = 2.60467776\n",
      "Iteration 8204, loss = 2.60468601\n",
      "Iteration 8205, loss = 2.60458276\n",
      "Iteration 8206, loss = 2.60459554\n",
      "Iteration 8207, loss = 2.60450036\n",
      "Iteration 8208, loss = 2.60454265\n",
      "Iteration 8209, loss = 2.60455356\n",
      "Iteration 8210, loss = 2.60463004\n",
      "Iteration 8211, loss = 2.60445747\n",
      "Iteration 8212, loss = 2.60443743\n",
      "Iteration 8213, loss = 2.60444129\n",
      "Iteration 8214, loss = 2.60448089\n",
      "Iteration 8215, loss = 2.60443961\n",
      "Iteration 8216, loss = 2.60441484\n",
      "Iteration 8217, loss = 2.60440948\n",
      "Iteration 8218, loss = 2.60442569\n",
      "Iteration 8219, loss = 2.60446896\n",
      "Iteration 8220, loss = 2.60444536\n",
      "Iteration 8221, loss = 2.60437633\n",
      "Iteration 8222, loss = 2.60435986\n",
      "Iteration 8223, loss = 2.60429246\n",
      "Iteration 8224, loss = 2.60435875\n",
      "Iteration 8225, loss = 2.60429899\n",
      "Iteration 8226, loss = 2.60430304\n",
      "Iteration 8227, loss = 2.60427079\n",
      "Iteration 8228, loss = 2.60427995\n",
      "Iteration 8229, loss = 2.60427445\n",
      "Iteration 8230, loss = 2.60429910\n",
      "Iteration 8231, loss = 2.60420813\n",
      "Iteration 8232, loss = 2.60419389\n",
      "Iteration 8233, loss = 2.60430380\n",
      "Iteration 8234, loss = 2.60429379\n",
      "Iteration 8235, loss = 2.60421769\n",
      "Iteration 8236, loss = 2.60422443\n",
      "Iteration 8237, loss = 2.60421644\n",
      "Iteration 8238, loss = 2.60435193\n",
      "Iteration 8239, loss = 2.60412000\n",
      "Iteration 8240, loss = 2.60425959\n",
      "Iteration 8241, loss = 2.60400126\n",
      "Iteration 8242, loss = 2.60414337\n",
      "Iteration 8243, loss = 2.60409114\n",
      "Iteration 8244, loss = 2.60413712\n",
      "Iteration 8245, loss = 2.60401489\n",
      "Iteration 8246, loss = 2.60404878\n",
      "Iteration 8247, loss = 2.60410440\n",
      "Iteration 8248, loss = 2.60395442\n",
      "Iteration 8249, loss = 2.60424872\n",
      "Iteration 8250, loss = 2.60396056\n",
      "Iteration 8251, loss = 2.60399898\n",
      "Iteration 8252, loss = 2.60399385\n",
      "Iteration 8253, loss = 2.60406315\n",
      "Iteration 8254, loss = 2.60402413\n",
      "Iteration 8255, loss = 2.60390257\n",
      "Iteration 8256, loss = 2.60396757\n",
      "Iteration 8257, loss = 2.60381832\n",
      "Iteration 8258, loss = 2.60392007\n",
      "Iteration 8259, loss = 2.60397616\n",
      "Iteration 8260, loss = 2.60408069\n",
      "Iteration 8261, loss = 2.60388177\n",
      "Iteration 8262, loss = 2.60401775\n",
      "Iteration 8263, loss = 2.60377309\n",
      "Iteration 8264, loss = 2.60385999\n",
      "Iteration 8265, loss = 2.60381843\n",
      "Iteration 8266, loss = 2.60384487\n",
      "Iteration 8267, loss = 2.60372652\n",
      "Iteration 8268, loss = 2.60375091\n",
      "Iteration 8269, loss = 2.60368269\n",
      "Iteration 8270, loss = 2.60375654\n",
      "Iteration 8271, loss = 2.60363250\n",
      "Iteration 8272, loss = 2.60367134\n",
      "Iteration 8273, loss = 2.60370910\n",
      "Iteration 8274, loss = 2.60355435\n",
      "Iteration 8275, loss = 2.60364691\n",
      "Iteration 8276, loss = 2.60376337\n",
      "Iteration 8277, loss = 2.60370148\n",
      "Iteration 8278, loss = 2.60359417\n",
      "Iteration 8279, loss = 2.60374272\n",
      "Iteration 8280, loss = 2.60374938\n",
      "Iteration 8281, loss = 2.60357757\n",
      "Iteration 8282, loss = 2.60353511\n",
      "Iteration 8283, loss = 2.60347159\n",
      "Iteration 8284, loss = 2.60350535\n",
      "Iteration 8285, loss = 2.60350251\n",
      "Iteration 8286, loss = 2.60350921\n",
      "Iteration 8287, loss = 2.60360537\n",
      "Iteration 8288, loss = 2.60350221\n",
      "Iteration 8289, loss = 2.60345649\n",
      "Iteration 8290, loss = 2.60353798\n",
      "Iteration 8291, loss = 2.60360153\n",
      "Iteration 8292, loss = 2.60338212\n",
      "Iteration 8293, loss = 2.60333629\n",
      "Iteration 8294, loss = 2.60337529\n",
      "Iteration 8295, loss = 2.60337933\n",
      "Iteration 8296, loss = 2.60338277\n",
      "Iteration 8297, loss = 2.60329310\n",
      "Iteration 8298, loss = 2.60334666\n",
      "Iteration 8299, loss = 2.60334247\n",
      "Iteration 8300, loss = 2.60332182\n",
      "Iteration 8301, loss = 2.60326416\n",
      "Iteration 8302, loss = 2.60329457\n",
      "Iteration 8303, loss = 2.60330499\n",
      "Iteration 8304, loss = 2.60321158\n",
      "Iteration 8305, loss = 2.60324417\n",
      "Iteration 8306, loss = 2.60318840\n",
      "Iteration 8307, loss = 2.60333660\n",
      "Iteration 8308, loss = 2.60314000\n",
      "Iteration 8309, loss = 2.60321883\n",
      "Iteration 8310, loss = 2.60319292\n",
      "Iteration 8311, loss = 2.60309647\n",
      "Iteration 8312, loss = 2.60318774\n",
      "Iteration 8313, loss = 2.60308971\n",
      "Iteration 8314, loss = 2.60308989\n",
      "Iteration 8315, loss = 2.60303890\n",
      "Iteration 8316, loss = 2.60302784\n",
      "Iteration 8317, loss = 2.60305622\n",
      "Iteration 8318, loss = 2.60307506\n",
      "Iteration 8319, loss = 2.60303416\n",
      "Iteration 8320, loss = 2.60303246\n",
      "Iteration 8321, loss = 2.60307764\n",
      "Iteration 8322, loss = 2.60308263\n",
      "Iteration 8323, loss = 2.60303114\n",
      "Iteration 8324, loss = 2.60300380\n",
      "Iteration 8325, loss = 2.60296467\n",
      "Iteration 8326, loss = 2.60294545\n",
      "Iteration 8327, loss = 2.60294914\n",
      "Iteration 8328, loss = 2.60293320\n",
      "Iteration 8329, loss = 2.60295878\n",
      "Iteration 8330, loss = 2.60284839\n",
      "Iteration 8331, loss = 2.60282750\n",
      "Iteration 8332, loss = 2.60285095\n",
      "Iteration 8333, loss = 2.60290522\n",
      "Iteration 8334, loss = 2.60289379\n",
      "Iteration 8335, loss = 2.60283212\n",
      "Iteration 8336, loss = 2.60285358\n",
      "Iteration 8337, loss = 2.60294776\n",
      "Iteration 8338, loss = 2.60283072\n",
      "Iteration 8339, loss = 2.60279309\n",
      "Iteration 8340, loss = 2.60284813\n",
      "Iteration 8341, loss = 2.60279190\n",
      "Iteration 8342, loss = 2.60275554\n",
      "Iteration 8343, loss = 2.60284117\n",
      "Iteration 8344, loss = 2.60272971\n",
      "Iteration 8345, loss = 2.60267325\n",
      "Iteration 8346, loss = 2.60274113\n",
      "Iteration 8347, loss = 2.60271446\n",
      "Iteration 8348, loss = 2.60265617\n",
      "Iteration 8349, loss = 2.60265156\n",
      "Iteration 8350, loss = 2.60258258\n",
      "Iteration 8351, loss = 2.60259707\n",
      "Iteration 8352, loss = 2.60263032\n",
      "Iteration 8353, loss = 2.60254648\n",
      "Iteration 8354, loss = 2.60258642\n",
      "Iteration 8355, loss = 2.60248356\n",
      "Iteration 8356, loss = 2.60256360\n",
      "Iteration 8357, loss = 2.60256525\n",
      "Iteration 8358, loss = 2.60252396\n",
      "Iteration 8359, loss = 2.60256706\n",
      "Iteration 8360, loss = 2.60250134\n",
      "Iteration 8361, loss = 2.60248255\n",
      "Iteration 8362, loss = 2.60244695\n",
      "Iteration 8363, loss = 2.60244716\n",
      "Iteration 8364, loss = 2.60250326\n",
      "Iteration 8365, loss = 2.60243148\n",
      "Iteration 8366, loss = 2.60241092\n",
      "Iteration 8367, loss = 2.60240894\n",
      "Iteration 8368, loss = 2.60238606\n",
      "Iteration 8369, loss = 2.60235515\n",
      "Iteration 8370, loss = 2.60241177\n",
      "Iteration 8371, loss = 2.60235513\n",
      "Iteration 8372, loss = 2.60234487\n",
      "Iteration 8373, loss = 2.60229898\n",
      "Iteration 8374, loss = 2.60234645\n",
      "Iteration 8375, loss = 2.60241871\n",
      "Iteration 8376, loss = 2.60220832\n",
      "Iteration 8377, loss = 2.60231964\n",
      "Iteration 8378, loss = 2.60232443\n",
      "Iteration 8379, loss = 2.60219210\n",
      "Iteration 8380, loss = 2.60217024\n",
      "Iteration 8381, loss = 2.60227996\n",
      "Iteration 8382, loss = 2.60217005\n",
      "Iteration 8383, loss = 2.60216773\n",
      "Iteration 8384, loss = 2.60212730\n",
      "Iteration 8385, loss = 2.60212716\n",
      "Iteration 8386, loss = 2.60210451\n",
      "Iteration 8387, loss = 2.60213064\n",
      "Iteration 8388, loss = 2.60211447\n",
      "Iteration 8389, loss = 2.60210357\n",
      "Iteration 8390, loss = 2.60212459\n",
      "Iteration 8391, loss = 2.60205424\n",
      "Iteration 8392, loss = 2.60208012\n",
      "Iteration 8393, loss = 2.60197612\n",
      "Iteration 8394, loss = 2.60204671\n",
      "Iteration 8395, loss = 2.60200617\n",
      "Iteration 8396, loss = 2.60210875\n",
      "Iteration 8397, loss = 2.60224486\n",
      "Iteration 8398, loss = 2.60196969\n",
      "Iteration 8399, loss = 2.60192522\n",
      "Iteration 8400, loss = 2.60194137\n",
      "Iteration 8401, loss = 2.60192726\n",
      "Iteration 8402, loss = 2.60189996\n",
      "Iteration 8403, loss = 2.60183390\n",
      "Iteration 8404, loss = 2.60191678\n",
      "Iteration 8405, loss = 2.60187392\n",
      "Iteration 8406, loss = 2.60180349\n",
      "Iteration 8407, loss = 2.60183890\n",
      "Iteration 8408, loss = 2.60183816\n",
      "Iteration 8409, loss = 2.60182667\n",
      "Iteration 8410, loss = 2.60182681\n",
      "Iteration 8411, loss = 2.60194933\n",
      "Iteration 8412, loss = 2.60178921\n",
      "Iteration 8413, loss = 2.60176093\n",
      "Iteration 8414, loss = 2.60172870\n",
      "Iteration 8415, loss = 2.60173517\n",
      "Iteration 8416, loss = 2.60179150\n",
      "Iteration 8417, loss = 2.60177069\n",
      "Iteration 8418, loss = 2.60182386\n",
      "Iteration 8419, loss = 2.60165207\n",
      "Iteration 8420, loss = 2.60168359\n",
      "Iteration 8421, loss = 2.60169787\n",
      "Iteration 8422, loss = 2.60159783\n",
      "Iteration 8423, loss = 2.60167319\n",
      "Iteration 8424, loss = 2.60164254\n",
      "Iteration 8425, loss = 2.60164704\n",
      "Iteration 8426, loss = 2.60159073\n",
      "Iteration 8427, loss = 2.60154930\n",
      "Iteration 8428, loss = 2.60157535\n",
      "Iteration 8429, loss = 2.60157758\n",
      "Iteration 8430, loss = 2.60166037\n",
      "Iteration 8431, loss = 2.60142929\n",
      "Iteration 8432, loss = 2.60150854\n",
      "Iteration 8433, loss = 2.60156464\n",
      "Iteration 8434, loss = 2.60145937\n",
      "Iteration 8435, loss = 2.60151251\n",
      "Iteration 8436, loss = 2.60144473\n",
      "Iteration 8437, loss = 2.60143032\n",
      "Iteration 8438, loss = 2.60144435\n",
      "Iteration 8439, loss = 2.60141541\n",
      "Iteration 8440, loss = 2.60142119\n",
      "Iteration 8441, loss = 2.60137191\n",
      "Iteration 8442, loss = 2.60145445\n",
      "Iteration 8443, loss = 2.60139393\n",
      "Iteration 8444, loss = 2.60146106\n",
      "Iteration 8445, loss = 2.60126178\n",
      "Iteration 8446, loss = 2.60146947\n",
      "Iteration 8447, loss = 2.60138543\n",
      "Iteration 8448, loss = 2.60130294\n",
      "Iteration 8449, loss = 2.60133414\n",
      "Iteration 8450, loss = 2.60134323\n",
      "Iteration 8451, loss = 2.60140698\n",
      "Iteration 8452, loss = 2.60128540\n",
      "Iteration 8453, loss = 2.60123597\n",
      "Iteration 8454, loss = 2.60118814\n",
      "Iteration 8455, loss = 2.60122562\n",
      "Iteration 8456, loss = 2.60121596\n",
      "Iteration 8457, loss = 2.60119132\n",
      "Iteration 8458, loss = 2.60119813\n",
      "Iteration 8459, loss = 2.60125998\n",
      "Iteration 8460, loss = 2.60122745\n",
      "Iteration 8461, loss = 2.60119270\n",
      "Iteration 8462, loss = 2.60146589\n",
      "Iteration 8463, loss = 2.60108967\n",
      "Iteration 8464, loss = 2.60103589\n",
      "Iteration 8465, loss = 2.60112073\n",
      "Iteration 8466, loss = 2.60116628\n",
      "Iteration 8467, loss = 2.60103568\n",
      "Iteration 8468, loss = 2.60102967\n",
      "Iteration 8469, loss = 2.60116897\n",
      "Iteration 8470, loss = 2.60107383\n",
      "Iteration 8471, loss = 2.60101268\n",
      "Iteration 8472, loss = 2.60103102\n",
      "Iteration 8473, loss = 2.60098887\n",
      "Iteration 8474, loss = 2.60090286\n",
      "Iteration 8475, loss = 2.60107823\n",
      "Iteration 8476, loss = 2.60111864\n",
      "Iteration 8477, loss = 2.60097909\n",
      "Iteration 8478, loss = 2.60103838\n",
      "Iteration 8479, loss = 2.60085661\n",
      "Iteration 8480, loss = 2.60086397\n",
      "Iteration 8481, loss = 2.60087295\n",
      "Iteration 8482, loss = 2.60090129\n",
      "Iteration 8483, loss = 2.60084845\n",
      "Iteration 8484, loss = 2.60088673\n",
      "Iteration 8485, loss = 2.60084015\n",
      "Iteration 8486, loss = 2.60080396\n",
      "Iteration 8487, loss = 2.60077876\n",
      "Iteration 8488, loss = 2.60074326\n",
      "Iteration 8489, loss = 2.60083504\n",
      "Iteration 8490, loss = 2.60072722\n",
      "Iteration 8491, loss = 2.60104626\n",
      "Iteration 8492, loss = 2.60104548\n",
      "Iteration 8493, loss = 2.60070156\n",
      "Iteration 8494, loss = 2.60068727\n",
      "Iteration 8495, loss = 2.60064693\n",
      "Iteration 8496, loss = 2.60071206\n",
      "Iteration 8497, loss = 2.60063523\n",
      "Iteration 8498, loss = 2.60066486\n",
      "Iteration 8499, loss = 2.60065315\n",
      "Iteration 8500, loss = 2.60063133\n",
      "Iteration 8501, loss = 2.60059831\n",
      "Iteration 8502, loss = 2.60052500\n",
      "Iteration 8503, loss = 2.60053321\n",
      "Iteration 8504, loss = 2.60057868\n",
      "Iteration 8505, loss = 2.60049319\n",
      "Iteration 8506, loss = 2.60052227\n",
      "Iteration 8507, loss = 2.60050904\n",
      "Iteration 8508, loss = 2.60060682\n",
      "Iteration 8509, loss = 2.60055017\n",
      "Iteration 8510, loss = 2.60047442\n",
      "Iteration 8511, loss = 2.60044099\n",
      "Iteration 8512, loss = 2.60037837\n",
      "Iteration 8513, loss = 2.60045866\n",
      "Iteration 8514, loss = 2.60047253\n",
      "Iteration 8515, loss = 2.60037006\n",
      "Iteration 8516, loss = 2.60048354\n",
      "Iteration 8517, loss = 2.60037286\n",
      "Iteration 8518, loss = 2.60033843\n",
      "Iteration 8519, loss = 2.60046925\n",
      "Iteration 8520, loss = 2.60043415\n",
      "Iteration 8521, loss = 2.60030108\n",
      "Iteration 8522, loss = 2.60031716\n",
      "Iteration 8523, loss = 2.60028687\n",
      "Iteration 8524, loss = 2.60039904\n",
      "Iteration 8525, loss = 2.60040516\n",
      "Iteration 8526, loss = 2.60024934\n",
      "Iteration 8527, loss = 2.60022808\n",
      "Iteration 8528, loss = 2.60017527\n",
      "Iteration 8529, loss = 2.60032840\n",
      "Iteration 8530, loss = 2.60018755\n",
      "Iteration 8531, loss = 2.60021433\n",
      "Iteration 8532, loss = 2.60013933\n",
      "Iteration 8533, loss = 2.60013205\n",
      "Iteration 8534, loss = 2.60015416\n",
      "Iteration 8535, loss = 2.60011483\n",
      "Iteration 8536, loss = 2.60014642\n",
      "Iteration 8537, loss = 2.60025593\n",
      "Iteration 8538, loss = 2.60013384\n",
      "Iteration 8539, loss = 2.60008349\n",
      "Iteration 8540, loss = 2.60003417\n",
      "Iteration 8541, loss = 2.60005201\n",
      "Iteration 8542, loss = 2.60011374\n",
      "Iteration 8543, loss = 2.60004421\n",
      "Iteration 8544, loss = 2.59999628\n",
      "Iteration 8545, loss = 2.60017440\n",
      "Iteration 8546, loss = 2.59998820\n",
      "Iteration 8547, loss = 2.59996787\n",
      "Iteration 8548, loss = 2.59992763\n",
      "Iteration 8549, loss = 2.59992021\n",
      "Iteration 8550, loss = 2.59997256\n",
      "Iteration 8551, loss = 2.59998602\n",
      "Iteration 8552, loss = 2.59993806\n",
      "Iteration 8553, loss = 2.59991577\n",
      "Iteration 8554, loss = 2.59989858\n",
      "Iteration 8555, loss = 2.59984287\n",
      "Iteration 8556, loss = 2.59987561\n",
      "Iteration 8557, loss = 2.60000317\n",
      "Iteration 8558, loss = 2.59987040\n",
      "Iteration 8559, loss = 2.59975794\n",
      "Iteration 8560, loss = 2.59978162\n",
      "Iteration 8561, loss = 2.59974323\n",
      "Iteration 8562, loss = 2.59984257\n",
      "Iteration 8563, loss = 2.59982189\n",
      "Iteration 8564, loss = 2.59977090\n",
      "Iteration 8565, loss = 2.59969072\n",
      "Iteration 8566, loss = 2.59973537\n",
      "Iteration 8567, loss = 2.59977004\n",
      "Iteration 8568, loss = 2.59971165\n",
      "Iteration 8569, loss = 2.59972277\n",
      "Iteration 8570, loss = 2.59968260\n",
      "Iteration 8571, loss = 2.59961091\n",
      "Iteration 8572, loss = 2.59965147\n",
      "Iteration 8573, loss = 2.59964175\n",
      "Iteration 8574, loss = 2.59963729\n",
      "Iteration 8575, loss = 2.59969479\n",
      "Iteration 8576, loss = 2.59948695\n",
      "Iteration 8577, loss = 2.59959842\n",
      "Iteration 8578, loss = 2.59975599\n",
      "Iteration 8579, loss = 2.59957722\n",
      "Iteration 8580, loss = 2.59964600\n",
      "Iteration 8581, loss = 2.59951901\n",
      "Iteration 8582, loss = 2.59951484\n",
      "Iteration 8583, loss = 2.59945891\n",
      "Iteration 8584, loss = 2.59946650\n",
      "Iteration 8585, loss = 2.59952593\n",
      "Iteration 8586, loss = 2.59948165\n",
      "Iteration 8587, loss = 2.59947496\n",
      "Iteration 8588, loss = 2.59937535\n",
      "Iteration 8589, loss = 2.59941180\n",
      "Iteration 8590, loss = 2.59937755\n",
      "Iteration 8591, loss = 2.59936696\n",
      "Iteration 8592, loss = 2.59936737\n",
      "Iteration 8593, loss = 2.59938829\n",
      "Iteration 8594, loss = 2.59936849\n",
      "Iteration 8595, loss = 2.59931986\n",
      "Iteration 8596, loss = 2.59931423\n",
      "Iteration 8597, loss = 2.59931394\n",
      "Iteration 8598, loss = 2.59931407\n",
      "Iteration 8599, loss = 2.59921748\n",
      "Iteration 8600, loss = 2.59924435\n",
      "Iteration 8601, loss = 2.59926388\n",
      "Iteration 8602, loss = 2.59926793\n",
      "Iteration 8603, loss = 2.59924684\n",
      "Iteration 8604, loss = 2.59929523\n",
      "Iteration 8605, loss = 2.59920370\n",
      "Iteration 8606, loss = 2.59919168\n",
      "Iteration 8607, loss = 2.59930882\n",
      "Iteration 8608, loss = 2.59922635\n",
      "Iteration 8609, loss = 2.59914421\n",
      "Iteration 8610, loss = 2.59909802\n",
      "Iteration 8611, loss = 2.59916317\n",
      "Iteration 8612, loss = 2.59911230\n",
      "Iteration 8613, loss = 2.59910724\n",
      "Iteration 8614, loss = 2.59902439\n",
      "Iteration 8615, loss = 2.59913876\n",
      "Iteration 8616, loss = 2.59910205\n",
      "Iteration 8617, loss = 2.59899322\n",
      "Iteration 8618, loss = 2.59923921\n",
      "Iteration 8619, loss = 2.59919676\n",
      "Iteration 8620, loss = 2.59915082\n",
      "Iteration 8621, loss = 2.59897799\n",
      "Iteration 8622, loss = 2.59897356\n",
      "Iteration 8623, loss = 2.59904191\n",
      "Iteration 8624, loss = 2.59891644\n",
      "Iteration 8625, loss = 2.59893563\n",
      "Iteration 8626, loss = 2.59894035\n",
      "Iteration 8627, loss = 2.59903673\n",
      "Iteration 8628, loss = 2.59892086\n",
      "Iteration 8629, loss = 2.59907686\n",
      "Iteration 8630, loss = 2.59893154\n",
      "Iteration 8631, loss = 2.59886865\n",
      "Iteration 8632, loss = 2.59892873\n",
      "Iteration 8633, loss = 2.59887463\n",
      "Iteration 8634, loss = 2.59876934\n",
      "Iteration 8635, loss = 2.59873256\n",
      "Iteration 8636, loss = 2.59884889\n",
      "Iteration 8637, loss = 2.59891196\n",
      "Iteration 8638, loss = 2.59882720\n",
      "Iteration 8639, loss = 2.59871930\n",
      "Iteration 8640, loss = 2.59878967\n",
      "Iteration 8641, loss = 2.59875043\n",
      "Iteration 8642, loss = 2.59865900\n",
      "Iteration 8643, loss = 2.59865847\n",
      "Iteration 8644, loss = 2.59867438\n",
      "Iteration 8645, loss = 2.59865615\n",
      "Iteration 8646, loss = 2.59867754\n",
      "Iteration 8647, loss = 2.59860116\n",
      "Iteration 8648, loss = 2.59858871\n",
      "Iteration 8649, loss = 2.59858694\n",
      "Iteration 8650, loss = 2.59858554\n",
      "Iteration 8651, loss = 2.59865021\n",
      "Iteration 8652, loss = 2.59859320\n",
      "Iteration 8653, loss = 2.59868709\n",
      "Iteration 8654, loss = 2.59851844\n",
      "Iteration 8655, loss = 2.59855298\n",
      "Iteration 8656, loss = 2.59850182\n",
      "Iteration 8657, loss = 2.59854291\n",
      "Iteration 8658, loss = 2.59852069\n",
      "Iteration 8659, loss = 2.59846292\n",
      "Iteration 8660, loss = 2.59848092\n",
      "Iteration 8661, loss = 2.59840538\n",
      "Iteration 8662, loss = 2.59852472\n",
      "Iteration 8663, loss = 2.59856268\n",
      "Iteration 8664, loss = 2.59839609\n",
      "Iteration 8665, loss = 2.59853056\n",
      "Iteration 8666, loss = 2.59854590\n",
      "Iteration 8667, loss = 2.59840082\n",
      "Iteration 8668, loss = 2.59829957\n",
      "Iteration 8669, loss = 2.59839335\n",
      "Iteration 8670, loss = 2.59840156\n",
      "Iteration 8671, loss = 2.59833849\n",
      "Iteration 8672, loss = 2.59827844\n",
      "Iteration 8673, loss = 2.59829939\n",
      "Iteration 8674, loss = 2.59835851\n",
      "Iteration 8675, loss = 2.59825577\n",
      "Iteration 8676, loss = 2.59832157\n",
      "Iteration 8677, loss = 2.59826192\n",
      "Iteration 8678, loss = 2.59822611\n",
      "Iteration 8679, loss = 2.59829213\n",
      "Iteration 8680, loss = 2.59830478\n",
      "Iteration 8681, loss = 2.59818671\n",
      "Iteration 8682, loss = 2.59815614\n",
      "Iteration 8683, loss = 2.59817090\n",
      "Iteration 8684, loss = 2.59818489\n",
      "Iteration 8685, loss = 2.59807931\n",
      "Iteration 8686, loss = 2.59835524\n",
      "Iteration 8687, loss = 2.59815329\n",
      "Iteration 8688, loss = 2.59807450\n",
      "Iteration 8689, loss = 2.59811685\n",
      "Iteration 8690, loss = 2.59812416\n",
      "Iteration 8691, loss = 2.59813066\n",
      "Iteration 8692, loss = 2.59807309\n",
      "Iteration 8693, loss = 2.59797642\n",
      "Iteration 8694, loss = 2.59807241\n",
      "Iteration 8695, loss = 2.59796072\n",
      "Iteration 8696, loss = 2.59808096\n",
      "Iteration 8697, loss = 2.59790847\n",
      "Iteration 8698, loss = 2.59809386\n",
      "Iteration 8699, loss = 2.59799908\n",
      "Iteration 8700, loss = 2.59797815\n",
      "Iteration 8701, loss = 2.59790546\n",
      "Iteration 8702, loss = 2.59808163\n",
      "Iteration 8703, loss = 2.59798362\n",
      "Iteration 8704, loss = 2.59797468\n",
      "Iteration 8705, loss = 2.59782351\n",
      "Iteration 8706, loss = 2.59791002\n",
      "Iteration 8707, loss = 2.59791958\n",
      "Iteration 8708, loss = 2.59783794\n",
      "Iteration 8709, loss = 2.59791357\n",
      "Iteration 8710, loss = 2.59781264\n",
      "Iteration 8711, loss = 2.59791908\n",
      "Iteration 8712, loss = 2.59783865\n",
      "Iteration 8713, loss = 2.59775844\n",
      "Iteration 8714, loss = 2.59786505\n",
      "Iteration 8715, loss = 2.59769375\n",
      "Iteration 8716, loss = 2.59779698\n",
      "Iteration 8717, loss = 2.59777094\n",
      "Iteration 8718, loss = 2.59767582\n",
      "Iteration 8719, loss = 2.59769388\n",
      "Iteration 8720, loss = 2.59767498\n",
      "Iteration 8721, loss = 2.59779853\n",
      "Iteration 8722, loss = 2.59766053\n",
      "Iteration 8723, loss = 2.59760584\n",
      "Iteration 8724, loss = 2.59760086\n",
      "Iteration 8725, loss = 2.59760482\n",
      "Iteration 8726, loss = 2.59776913\n",
      "Iteration 8727, loss = 2.59760241\n",
      "Iteration 8728, loss = 2.59762062\n",
      "Iteration 8729, loss = 2.59761873\n",
      "Iteration 8730, loss = 2.59754181\n",
      "Iteration 8731, loss = 2.59752059\n",
      "Iteration 8732, loss = 2.59748385\n",
      "Iteration 8733, loss = 2.59751676\n",
      "Iteration 8734, loss = 2.59753078\n",
      "Iteration 8735, loss = 2.59762345\n",
      "Iteration 8736, loss = 2.59751730\n",
      "Iteration 8737, loss = 2.59747213\n",
      "Iteration 8738, loss = 2.59744460\n",
      "Iteration 8739, loss = 2.59738846\n",
      "Iteration 8740, loss = 2.59737521\n",
      "Iteration 8741, loss = 2.59749724\n",
      "Iteration 8742, loss = 2.59767086\n",
      "Iteration 8743, loss = 2.59740877\n",
      "Iteration 8744, loss = 2.59745604\n",
      "Iteration 8745, loss = 2.59735931\n",
      "Iteration 8746, loss = 2.59732136\n",
      "Iteration 8747, loss = 2.59732692\n",
      "Iteration 8748, loss = 2.59729761\n",
      "Iteration 8749, loss = 2.59723569\n",
      "Iteration 8750, loss = 2.59740144\n",
      "Iteration 8751, loss = 2.59734629\n",
      "Iteration 8752, loss = 2.59722376\n",
      "Iteration 8753, loss = 2.59721870\n",
      "Iteration 8754, loss = 2.59720805\n",
      "Iteration 8755, loss = 2.59725752\n",
      "Iteration 8756, loss = 2.59727979\n",
      "Iteration 8757, loss = 2.59718268\n",
      "Iteration 8758, loss = 2.59717389\n",
      "Iteration 8759, loss = 2.59712685\n",
      "Iteration 8760, loss = 2.59714470\n",
      "Iteration 8761, loss = 2.59722227\n",
      "Iteration 8762, loss = 2.59712696\n",
      "Iteration 8763, loss = 2.59710819\n",
      "Iteration 8764, loss = 2.59712907\n",
      "Iteration 8765, loss = 2.59710444\n",
      "Iteration 8766, loss = 2.59708933\n",
      "Iteration 8767, loss = 2.59706750\n",
      "Iteration 8768, loss = 2.59712104\n",
      "Iteration 8769, loss = 2.59702319\n",
      "Iteration 8770, loss = 2.59703131\n",
      "Iteration 8771, loss = 2.59693835\n",
      "Iteration 8772, loss = 2.59692004\n",
      "Iteration 8773, loss = 2.59693587\n",
      "Iteration 8774, loss = 2.59697457\n",
      "Iteration 8775, loss = 2.59693668\n",
      "Iteration 8776, loss = 2.59692015\n",
      "Iteration 8777, loss = 2.59688611\n",
      "Iteration 8778, loss = 2.59690218\n",
      "Iteration 8779, loss = 2.59685727\n",
      "Iteration 8780, loss = 2.59683147\n",
      "Iteration 8781, loss = 2.59685742\n",
      "Iteration 8782, loss = 2.59693082\n",
      "Iteration 8783, loss = 2.59687571\n",
      "Iteration 8784, loss = 2.59681046\n",
      "Iteration 8785, loss = 2.59685687\n",
      "Iteration 8786, loss = 2.59681260\n",
      "Iteration 8787, loss = 2.59692047\n",
      "Iteration 8788, loss = 2.59674052\n",
      "Iteration 8789, loss = 2.59680525\n",
      "Iteration 8790, loss = 2.59677683\n",
      "Iteration 8791, loss = 2.59676785\n",
      "Iteration 8792, loss = 2.59670248\n",
      "Iteration 8793, loss = 2.59669175\n",
      "Iteration 8794, loss = 2.59667544\n",
      "Iteration 8795, loss = 2.59672024\n",
      "Iteration 8796, loss = 2.59667594\n",
      "Iteration 8797, loss = 2.59665595\n",
      "Iteration 8798, loss = 2.59664287\n",
      "Iteration 8799, loss = 2.59662751\n",
      "Iteration 8800, loss = 2.59681424\n",
      "Iteration 8801, loss = 2.59674283\n",
      "Iteration 8802, loss = 2.59659124\n",
      "Iteration 8803, loss = 2.59656904\n",
      "Iteration 8804, loss = 2.59653421\n",
      "Iteration 8805, loss = 2.59654728\n",
      "Iteration 8806, loss = 2.59650985\n",
      "Iteration 8807, loss = 2.59655801\n",
      "Iteration 8808, loss = 2.59648113\n",
      "Iteration 8809, loss = 2.59648097\n",
      "Iteration 8810, loss = 2.59654522\n",
      "Iteration 8811, loss = 2.59642559\n",
      "Iteration 8812, loss = 2.59655662\n",
      "Iteration 8813, loss = 2.59644771\n",
      "Iteration 8814, loss = 2.59639148\n",
      "Iteration 8815, loss = 2.59641341\n",
      "Iteration 8816, loss = 2.59660908\n",
      "Iteration 8817, loss = 2.59645025\n",
      "Iteration 8818, loss = 2.59650314\n",
      "Iteration 8819, loss = 2.59639722\n",
      "Iteration 8820, loss = 2.59636443\n",
      "Iteration 8821, loss = 2.59635931\n",
      "Iteration 8822, loss = 2.59635229\n",
      "Iteration 8823, loss = 2.59637016\n",
      "Iteration 8824, loss = 2.59628656\n",
      "Iteration 8825, loss = 2.59631646\n",
      "Iteration 8826, loss = 2.59625079\n",
      "Iteration 8827, loss = 2.59622466\n",
      "Iteration 8828, loss = 2.59619782\n",
      "Iteration 8829, loss = 2.59626368\n",
      "Iteration 8830, loss = 2.59624312\n",
      "Iteration 8831, loss = 2.59617016\n",
      "Iteration 8832, loss = 2.59615691\n",
      "Iteration 8833, loss = 2.59617373\n",
      "Iteration 8834, loss = 2.59614293\n",
      "Iteration 8835, loss = 2.59620675\n",
      "Iteration 8836, loss = 2.59615856\n",
      "Iteration 8837, loss = 2.59611474\n",
      "Iteration 8838, loss = 2.59606061\n",
      "Iteration 8839, loss = 2.59615888\n",
      "Iteration 8840, loss = 2.59610025\n",
      "Iteration 8841, loss = 2.59609423\n",
      "Iteration 8842, loss = 2.59611803\n",
      "Iteration 8843, loss = 2.59611652\n",
      "Iteration 8844, loss = 2.59602318\n",
      "Iteration 8845, loss = 2.59605535\n",
      "Iteration 8846, loss = 2.59605494\n",
      "Iteration 8847, loss = 2.59605984\n",
      "Iteration 8848, loss = 2.59598161\n",
      "Iteration 8849, loss = 2.59591437\n",
      "Iteration 8850, loss = 2.59594478\n",
      "Iteration 8851, loss = 2.59597762\n",
      "Iteration 8852, loss = 2.59593147\n",
      "Iteration 8853, loss = 2.59590705\n",
      "Iteration 8854, loss = 2.59588016\n",
      "Iteration 8855, loss = 2.59589315\n",
      "Iteration 8856, loss = 2.59586634\n",
      "Iteration 8857, loss = 2.59586184\n",
      "Iteration 8858, loss = 2.59579387\n",
      "Iteration 8859, loss = 2.59586770\n",
      "Iteration 8860, loss = 2.59591569\n",
      "Iteration 8861, loss = 2.59580929\n",
      "Iteration 8862, loss = 2.59597479\n",
      "Iteration 8863, loss = 2.59581125\n",
      "Iteration 8864, loss = 2.59597555\n",
      "Iteration 8865, loss = 2.59582590\n",
      "Iteration 8866, loss = 2.59573065\n",
      "Iteration 8867, loss = 2.59573515\n",
      "Iteration 8868, loss = 2.59568839\n",
      "Iteration 8869, loss = 2.59571960\n",
      "Iteration 8870, loss = 2.59574987\n",
      "Iteration 8871, loss = 2.59564831\n",
      "Iteration 8872, loss = 2.59564751\n",
      "Iteration 8873, loss = 2.59567275\n",
      "Iteration 8874, loss = 2.59581389\n",
      "Iteration 8875, loss = 2.59569515\n",
      "Iteration 8876, loss = 2.59558679\n",
      "Iteration 8877, loss = 2.59557514\n",
      "Iteration 8878, loss = 2.59556308\n",
      "Iteration 8879, loss = 2.59559237\n",
      "Iteration 8880, loss = 2.59563381\n",
      "Iteration 8881, loss = 2.59554518\n",
      "Iteration 8882, loss = 2.59556783\n",
      "Iteration 8883, loss = 2.59551211\n",
      "Iteration 8884, loss = 2.59557797\n",
      "Iteration 8885, loss = 2.59545559\n",
      "Iteration 8886, loss = 2.59558308\n",
      "Iteration 8887, loss = 2.59556162\n",
      "Iteration 8888, loss = 2.59547450\n",
      "Iteration 8889, loss = 2.59559144\n",
      "Iteration 8890, loss = 2.59551481\n",
      "Iteration 8891, loss = 2.59550896\n",
      "Iteration 8892, loss = 2.59554323\n",
      "Iteration 8893, loss = 2.59549590\n",
      "Iteration 8894, loss = 2.59544520\n",
      "Iteration 8895, loss = 2.59532143\n",
      "Iteration 8896, loss = 2.59528451\n",
      "Iteration 8897, loss = 2.59542065\n",
      "Iteration 8898, loss = 2.59528043\n",
      "Iteration 8899, loss = 2.59532175\n",
      "Iteration 8900, loss = 2.59530584\n",
      "Iteration 8901, loss = 2.59531914\n",
      "Iteration 8902, loss = 2.59543441\n",
      "Iteration 8903, loss = 2.59530319\n",
      "Iteration 8904, loss = 2.59522932\n",
      "Iteration 8905, loss = 2.59523273\n",
      "Iteration 8906, loss = 2.59522941\n",
      "Iteration 8907, loss = 2.59520173\n",
      "Iteration 8908, loss = 2.59529324\n",
      "Iteration 8909, loss = 2.59534277\n",
      "Iteration 8910, loss = 2.59518315\n",
      "Iteration 8911, loss = 2.59507911\n",
      "Iteration 8912, loss = 2.59505805\n",
      "Iteration 8913, loss = 2.59513704\n",
      "Iteration 8914, loss = 2.59516633\n",
      "Iteration 8915, loss = 2.59521494\n",
      "Iteration 8916, loss = 2.59509830\n",
      "Iteration 8917, loss = 2.59502466\n",
      "Iteration 8918, loss = 2.59516268\n",
      "Iteration 8919, loss = 2.59503745\n",
      "Iteration 8920, loss = 2.59500110\n",
      "Iteration 8921, loss = 2.59500156\n",
      "Iteration 8922, loss = 2.59514545\n",
      "Iteration 8923, loss = 2.59504520\n",
      "Iteration 8924, loss = 2.59490678\n",
      "Iteration 8925, loss = 2.59507858\n",
      "Iteration 8926, loss = 2.59498037\n",
      "Iteration 8927, loss = 2.59495096\n",
      "Iteration 8928, loss = 2.59513185\n",
      "Iteration 8929, loss = 2.59490933\n",
      "Iteration 8930, loss = 2.59494603\n",
      "Iteration 8931, loss = 2.59491388\n",
      "Iteration 8932, loss = 2.59486480\n",
      "Iteration 8933, loss = 2.59485476\n",
      "Iteration 8934, loss = 2.59490492\n",
      "Iteration 8935, loss = 2.59484776\n",
      "Iteration 8936, loss = 2.59486409\n",
      "Iteration 8937, loss = 2.59486460\n",
      "Iteration 8938, loss = 2.59486421\n",
      "Iteration 8939, loss = 2.59484480\n",
      "Iteration 8940, loss = 2.59478876\n",
      "Iteration 8941, loss = 2.59475095\n",
      "Iteration 8942, loss = 2.59469453\n",
      "Iteration 8943, loss = 2.59470845\n",
      "Iteration 8944, loss = 2.59471521\n",
      "Iteration 8945, loss = 2.59479357\n",
      "Iteration 8946, loss = 2.59463520\n",
      "Iteration 8947, loss = 2.59464624\n",
      "Iteration 8948, loss = 2.59475962\n",
      "Iteration 8949, loss = 2.59471538\n",
      "Iteration 8950, loss = 2.59465628\n",
      "Iteration 8951, loss = 2.59462284\n",
      "Iteration 8952, loss = 2.59456474\n",
      "Iteration 8953, loss = 2.59462345\n",
      "Iteration 8954, loss = 2.59452891\n",
      "Iteration 8955, loss = 2.59463504\n",
      "Iteration 8956, loss = 2.59462825\n",
      "Iteration 8957, loss = 2.59455604\n",
      "Iteration 8958, loss = 2.59459784\n",
      "Iteration 8959, loss = 2.59454142\n",
      "Iteration 8960, loss = 2.59455961\n",
      "Iteration 8961, loss = 2.59450843\n",
      "Iteration 8962, loss = 2.59445810\n",
      "Iteration 8963, loss = 2.59448778\n",
      "Iteration 8964, loss = 2.59443339\n",
      "Iteration 8965, loss = 2.59439466\n",
      "Iteration 8966, loss = 2.59442122\n",
      "Iteration 8967, loss = 2.59437398\n",
      "Iteration 8968, loss = 2.59438669\n",
      "Iteration 8969, loss = 2.59436963\n",
      "Iteration 8970, loss = 2.59443022\n",
      "Iteration 8971, loss = 2.59436161\n",
      "Iteration 8972, loss = 2.59431724\n",
      "Iteration 8973, loss = 2.59447657\n",
      "Iteration 8974, loss = 2.59443745\n",
      "Iteration 8975, loss = 2.59429397\n",
      "Iteration 8976, loss = 2.59427239\n",
      "Iteration 8977, loss = 2.59427187\n",
      "Iteration 8978, loss = 2.59430421\n",
      "Iteration 8979, loss = 2.59424512\n",
      "Iteration 8980, loss = 2.59426522\n",
      "Iteration 8981, loss = 2.59423679\n",
      "Iteration 8982, loss = 2.59420929\n",
      "Iteration 8983, loss = 2.59418758\n",
      "Iteration 8984, loss = 2.59414555\n",
      "Iteration 8985, loss = 2.59433830\n",
      "Iteration 8986, loss = 2.59410346\n",
      "Iteration 8987, loss = 2.59408749\n",
      "Iteration 8988, loss = 2.59417313\n",
      "Iteration 8989, loss = 2.59412745\n",
      "Iteration 8990, loss = 2.59406451\n",
      "Iteration 8991, loss = 2.59419993\n",
      "Iteration 8992, loss = 2.59412636\n",
      "Iteration 8993, loss = 2.59404688\n",
      "Iteration 8994, loss = 2.59411664\n",
      "Iteration 8995, loss = 2.59399100\n",
      "Iteration 8996, loss = 2.59424661\n",
      "Iteration 8997, loss = 2.59395630\n",
      "Iteration 8998, loss = 2.59398952\n",
      "Iteration 8999, loss = 2.59413477\n",
      "Iteration 9000, loss = 2.59397183\n",
      "Iteration 9001, loss = 2.59388702\n",
      "Iteration 9002, loss = 2.59393561\n",
      "Iteration 9003, loss = 2.59401501\n",
      "Iteration 9004, loss = 2.59400778\n",
      "Iteration 9005, loss = 2.59392488\n",
      "Iteration 9006, loss = 2.59387262\n",
      "Iteration 9007, loss = 2.59386334\n",
      "Iteration 9008, loss = 2.59390108\n",
      "Iteration 9009, loss = 2.59389165\n",
      "Iteration 9010, loss = 2.59375547\n",
      "Iteration 9011, loss = 2.59383041\n",
      "Iteration 9012, loss = 2.59401901\n",
      "Iteration 9013, loss = 2.59392179\n",
      "Iteration 9014, loss = 2.59376161\n",
      "Iteration 9015, loss = 2.59390273\n",
      "Iteration 9016, loss = 2.59373797\n",
      "Iteration 9017, loss = 2.59377004\n",
      "Iteration 9018, loss = 2.59372028\n",
      "Iteration 9019, loss = 2.59379465\n",
      "Iteration 9020, loss = 2.59392850\n",
      "Iteration 9021, loss = 2.59371928\n",
      "Iteration 9022, loss = 2.59367055\n",
      "Iteration 9023, loss = 2.59375217\n",
      "Iteration 9024, loss = 2.59369503\n",
      "Iteration 9025, loss = 2.59373278\n",
      "Iteration 9026, loss = 2.59358300\n",
      "Iteration 9027, loss = 2.59387164\n",
      "Iteration 9028, loss = 2.59361327\n",
      "Iteration 9029, loss = 2.59360447\n",
      "Iteration 9030, loss = 2.59359956\n",
      "Iteration 9031, loss = 2.59359401\n",
      "Iteration 9032, loss = 2.59363647\n",
      "Iteration 9033, loss = 2.59353219\n",
      "Iteration 9034, loss = 2.59351404\n",
      "Iteration 9035, loss = 2.59358867\n",
      "Iteration 9036, loss = 2.59351371\n",
      "Iteration 9037, loss = 2.59351782\n",
      "Iteration 9038, loss = 2.59347982\n",
      "Iteration 9039, loss = 2.59395114\n",
      "Iteration 9040, loss = 2.59347639\n",
      "Iteration 9041, loss = 2.59339485\n",
      "Iteration 9042, loss = 2.59348190\n",
      "Iteration 9043, loss = 2.59336096\n",
      "Iteration 9044, loss = 2.59339894\n",
      "Iteration 9045, loss = 2.59347096\n",
      "Iteration 9046, loss = 2.59336866\n",
      "Iteration 9047, loss = 2.59332320\n",
      "Iteration 9048, loss = 2.59336435\n",
      "Iteration 9049, loss = 2.59335436\n",
      "Iteration 9050, loss = 2.59332266\n",
      "Iteration 9051, loss = 2.59355599\n",
      "Iteration 9052, loss = 2.59327618\n",
      "Iteration 9053, loss = 2.59325762\n",
      "Iteration 9054, loss = 2.59330467\n",
      "Iteration 9055, loss = 2.59325567\n",
      "Iteration 9056, loss = 2.59331807\n",
      "Iteration 9057, loss = 2.59324782\n",
      "Iteration 9058, loss = 2.59327267\n",
      "Iteration 9059, loss = 2.59316992\n",
      "Iteration 9060, loss = 2.59315772\n",
      "Iteration 9061, loss = 2.59316557\n",
      "Iteration 9062, loss = 2.59315619\n",
      "Iteration 9063, loss = 2.59312420\n",
      "Iteration 9064, loss = 2.59328544\n",
      "Iteration 9065, loss = 2.59317577\n",
      "Iteration 9066, loss = 2.59311779\n",
      "Iteration 9067, loss = 2.59310619\n",
      "Iteration 9068, loss = 2.59307409\n",
      "Iteration 9069, loss = 2.59304814\n",
      "Iteration 9070, loss = 2.59316393\n",
      "Iteration 9071, loss = 2.59304554\n",
      "Iteration 9072, loss = 2.59305887\n",
      "Iteration 9073, loss = 2.59304884\n",
      "Iteration 9074, loss = 2.59303948\n",
      "Iteration 9075, loss = 2.59292887\n",
      "Iteration 9076, loss = 2.59297167\n",
      "Iteration 9077, loss = 2.59298550\n",
      "Iteration 9078, loss = 2.59299317\n",
      "Iteration 9079, loss = 2.59302600\n",
      "Iteration 9080, loss = 2.59289489\n",
      "Iteration 9081, loss = 2.59291738\n",
      "Iteration 9082, loss = 2.59292217\n",
      "Iteration 9083, loss = 2.59288413\n",
      "Iteration 9084, loss = 2.59284375\n",
      "Iteration 9085, loss = 2.59288114\n",
      "Iteration 9086, loss = 2.59306980\n",
      "Iteration 9087, loss = 2.59285457\n",
      "Iteration 9088, loss = 2.59300818\n",
      "Iteration 9089, loss = 2.59286008\n",
      "Iteration 9090, loss = 2.59285209\n",
      "Iteration 9091, loss = 2.59289238\n",
      "Iteration 9092, loss = 2.59271980\n",
      "Iteration 9093, loss = 2.59279951\n",
      "Iteration 9094, loss = 2.59288492\n",
      "Iteration 9095, loss = 2.59277013\n",
      "Iteration 9096, loss = 2.59270966\n",
      "Iteration 9097, loss = 2.59272230\n",
      "Iteration 9098, loss = 2.59264789\n",
      "Iteration 9099, loss = 2.59265127\n",
      "Iteration 9100, loss = 2.59266583\n",
      "Iteration 9101, loss = 2.59267459\n",
      "Iteration 9102, loss = 2.59273108\n",
      "Iteration 9103, loss = 2.59258271\n",
      "Iteration 9104, loss = 2.59262089\n",
      "Iteration 9105, loss = 2.59271656\n",
      "Iteration 9106, loss = 2.59257832\n",
      "Iteration 9107, loss = 2.59258306\n",
      "Iteration 9108, loss = 2.59251890\n",
      "Iteration 9109, loss = 2.59258589\n",
      "Iteration 9110, loss = 2.59250808\n",
      "Iteration 9111, loss = 2.59249058\n",
      "Iteration 9112, loss = 2.59248275\n",
      "Iteration 9113, loss = 2.59249023\n",
      "Iteration 9114, loss = 2.59246891\n",
      "Iteration 9115, loss = 2.59264949\n",
      "Iteration 9116, loss = 2.59240772\n",
      "Iteration 9117, loss = 2.59249014\n",
      "Iteration 9118, loss = 2.59240102\n",
      "Iteration 9119, loss = 2.59247962\n",
      "Iteration 9120, loss = 2.59242667\n",
      "Iteration 9121, loss = 2.59247935\n",
      "Iteration 9122, loss = 2.59242825\n",
      "Iteration 9123, loss = 2.59249754\n",
      "Iteration 9124, loss = 2.59230806\n",
      "Iteration 9125, loss = 2.59236946\n",
      "Iteration 9126, loss = 2.59227809\n",
      "Iteration 9127, loss = 2.59229762\n",
      "Iteration 9128, loss = 2.59252846\n",
      "Iteration 9129, loss = 2.59227740\n",
      "Iteration 9130, loss = 2.59224697\n",
      "Iteration 9131, loss = 2.59230582\n",
      "Iteration 9132, loss = 2.59223618\n",
      "Iteration 9133, loss = 2.59226593\n",
      "Iteration 9134, loss = 2.59230109\n",
      "Iteration 9135, loss = 2.59215176\n",
      "Iteration 9136, loss = 2.59224369\n",
      "Iteration 9137, loss = 2.59217074\n",
      "Iteration 9138, loss = 2.59222343\n",
      "Iteration 9139, loss = 2.59210220\n",
      "Iteration 9140, loss = 2.59214530\n",
      "Iteration 9141, loss = 2.59213379\n",
      "Iteration 9142, loss = 2.59210769\n",
      "Iteration 9143, loss = 2.59209761\n",
      "Iteration 9144, loss = 2.59213529\n",
      "Iteration 9145, loss = 2.59208386\n",
      "Iteration 9146, loss = 2.59201251\n",
      "Iteration 9147, loss = 2.59204631\n",
      "Iteration 9148, loss = 2.59203065\n",
      "Iteration 9149, loss = 2.59194851\n",
      "Iteration 9150, loss = 2.59198189\n",
      "Iteration 9151, loss = 2.59199353\n",
      "Iteration 9152, loss = 2.59197732\n",
      "Iteration 9153, loss = 2.59195774\n",
      "Iteration 9154, loss = 2.59198211\n",
      "Iteration 9155, loss = 2.59218216\n",
      "Iteration 9156, loss = 2.59188190\n",
      "Iteration 9157, loss = 2.59192256\n",
      "Iteration 9158, loss = 2.59191559\n",
      "Iteration 9159, loss = 2.59193454\n",
      "Iteration 9160, loss = 2.59193940\n",
      "Iteration 9161, loss = 2.59190943\n",
      "Iteration 9162, loss = 2.59186931\n",
      "Iteration 9163, loss = 2.59194667\n",
      "Iteration 9164, loss = 2.59187452\n",
      "Iteration 9165, loss = 2.59182708\n",
      "Iteration 9166, loss = 2.59182670\n",
      "Iteration 9167, loss = 2.59177164\n",
      "Iteration 9168, loss = 2.59177812\n",
      "Iteration 9169, loss = 2.59172746\n",
      "Iteration 9170, loss = 2.59181688\n",
      "Iteration 9171, loss = 2.59175684\n",
      "Iteration 9172, loss = 2.59177333\n",
      "Iteration 9173, loss = 2.59167669\n",
      "Iteration 9174, loss = 2.59166239\n",
      "Iteration 9175, loss = 2.59164627\n",
      "Iteration 9176, loss = 2.59164934\n",
      "Iteration 9177, loss = 2.59163836\n",
      "Iteration 9178, loss = 2.59167521\n",
      "Iteration 9179, loss = 2.59166637\n",
      "Iteration 9180, loss = 2.59163251\n",
      "Iteration 9181, loss = 2.59173704\n",
      "Iteration 9182, loss = 2.59155090\n",
      "Iteration 9183, loss = 2.59164828\n",
      "Iteration 9184, loss = 2.59161557\n",
      "Iteration 9185, loss = 2.59154553\n",
      "Iteration 9186, loss = 2.59160048\n",
      "Iteration 9187, loss = 2.59148252\n",
      "Iteration 9188, loss = 2.59147781\n",
      "Iteration 9189, loss = 2.59151913\n",
      "Iteration 9190, loss = 2.59146905\n",
      "Iteration 9191, loss = 2.59198596\n",
      "Iteration 9192, loss = 2.59145542\n",
      "Iteration 9193, loss = 2.59137738\n",
      "Iteration 9194, loss = 2.59146682\n",
      "Iteration 9195, loss = 2.59140927\n",
      "Iteration 9196, loss = 2.59138070\n",
      "Iteration 9197, loss = 2.59138292\n",
      "Iteration 9198, loss = 2.59133100\n",
      "Iteration 9199, loss = 2.59135832\n",
      "Iteration 9200, loss = 2.59139613\n",
      "Iteration 9201, loss = 2.59126437\n",
      "Iteration 9202, loss = 2.59126205\n",
      "Iteration 9203, loss = 2.59138742\n",
      "Iteration 9204, loss = 2.59146237\n",
      "Iteration 9205, loss = 2.59125522\n",
      "Iteration 9206, loss = 2.59122349\n",
      "Iteration 9207, loss = 2.59132494\n",
      "Iteration 9208, loss = 2.59128073\n",
      "Iteration 9209, loss = 2.59132568\n",
      "Iteration 9210, loss = 2.59126167\n",
      "Iteration 9211, loss = 2.59119747\n",
      "Iteration 9212, loss = 2.59124004\n",
      "Iteration 9213, loss = 2.59114974\n",
      "Iteration 9214, loss = 2.59116622\n",
      "Iteration 9215, loss = 2.59123334\n",
      "Iteration 9216, loss = 2.59109395\n",
      "Iteration 9217, loss = 2.59109801\n",
      "Iteration 9218, loss = 2.59107874\n",
      "Iteration 9219, loss = 2.59105750\n",
      "Iteration 9220, loss = 2.59115705\n",
      "Iteration 9221, loss = 2.59107674\n",
      "Iteration 9222, loss = 2.59107554\n",
      "Iteration 9223, loss = 2.59100710\n",
      "Iteration 9224, loss = 2.59122050\n",
      "Iteration 9225, loss = 2.59107965\n",
      "Iteration 9226, loss = 2.59098774\n",
      "Iteration 9227, loss = 2.59101572\n",
      "Iteration 9228, loss = 2.59096706\n",
      "Iteration 9229, loss = 2.59099903\n",
      "Iteration 9230, loss = 2.59094516\n",
      "Iteration 9231, loss = 2.59100667\n",
      "Iteration 9232, loss = 2.59093853\n",
      "Iteration 9233, loss = 2.59089194\n",
      "Iteration 9234, loss = 2.59086705\n",
      "Iteration 9235, loss = 2.59100937\n",
      "Iteration 9236, loss = 2.59103735\n",
      "Iteration 9237, loss = 2.59086853\n",
      "Iteration 9238, loss = 2.59081931\n",
      "Iteration 9239, loss = 2.59075795\n",
      "Iteration 9240, loss = 2.59086329\n",
      "Iteration 9241, loss = 2.59080459\n",
      "Iteration 9242, loss = 2.59075068\n",
      "Iteration 9243, loss = 2.59076686\n",
      "Iteration 9244, loss = 2.59074371\n",
      "Iteration 9245, loss = 2.59075364\n",
      "Iteration 9246, loss = 2.59072283\n",
      "Iteration 9247, loss = 2.59075578\n",
      "Iteration 9248, loss = 2.59074887\n",
      "Iteration 9249, loss = 2.59082773\n",
      "Iteration 9250, loss = 2.59075232\n",
      "Iteration 9251, loss = 2.59073655\n",
      "Iteration 9252, loss = 2.59063607\n",
      "Iteration 9253, loss = 2.59074892\n",
      "Iteration 9254, loss = 2.59063070\n",
      "Iteration 9255, loss = 2.59059512\n",
      "Iteration 9256, loss = 2.59063526\n",
      "Iteration 9257, loss = 2.59061274\n",
      "Iteration 9258, loss = 2.59061463\n",
      "Iteration 9259, loss = 2.59050847\n",
      "Iteration 9260, loss = 2.59054835\n",
      "Iteration 9261, loss = 2.59061630\n",
      "Iteration 9262, loss = 2.59049630\n",
      "Iteration 9263, loss = 2.59057668\n",
      "Iteration 9264, loss = 2.59052535\n",
      "Iteration 9265, loss = 2.59046273\n",
      "Iteration 9266, loss = 2.59044049\n",
      "Iteration 9267, loss = 2.59042856\n",
      "Iteration 9268, loss = 2.59040313\n",
      "Iteration 9269, loss = 2.59043690\n",
      "Iteration 9270, loss = 2.59052113\n",
      "Iteration 9271, loss = 2.59050466\n",
      "Iteration 9272, loss = 2.59054671\n",
      "Iteration 9273, loss = 2.59047187\n",
      "Iteration 9274, loss = 2.59037143\n",
      "Iteration 9275, loss = 2.59051934\n",
      "Iteration 9276, loss = 2.59059534\n",
      "Iteration 9277, loss = 2.59034158\n",
      "Iteration 9278, loss = 2.59028005\n",
      "Iteration 9279, loss = 2.59035609\n",
      "Iteration 9280, loss = 2.59042677\n",
      "Iteration 9281, loss = 2.59033873\n",
      "Iteration 9282, loss = 2.59022202\n",
      "Iteration 9283, loss = 2.59027856\n",
      "Iteration 9284, loss = 2.59042296\n",
      "Iteration 9285, loss = 2.59023294\n",
      "Iteration 9286, loss = 2.59024885\n",
      "Iteration 9287, loss = 2.59023075\n",
      "Iteration 9288, loss = 2.59030526\n",
      "Iteration 9289, loss = 2.59024630\n",
      "Iteration 9290, loss = 2.59021385\n",
      "Iteration 9291, loss = 2.59019294\n",
      "Iteration 9292, loss = 2.59025774\n",
      "Iteration 9293, loss = 2.59012407\n",
      "Iteration 9294, loss = 2.59004938\n",
      "Iteration 9295, loss = 2.59038737\n",
      "Iteration 9296, loss = 2.59011001\n",
      "Iteration 9297, loss = 2.59014430\n",
      "Iteration 9298, loss = 2.59016573\n",
      "Iteration 9299, loss = 2.59006124\n",
      "Iteration 9300, loss = 2.59000485\n",
      "Iteration 9301, loss = 2.59005271\n",
      "Iteration 9302, loss = 2.59005338\n",
      "Iteration 9303, loss = 2.59000459\n",
      "Iteration 9304, loss = 2.58994054\n",
      "Iteration 9305, loss = 2.59008247\n",
      "Iteration 9306, loss = 2.58998655\n",
      "Iteration 9307, loss = 2.58994123\n",
      "Iteration 9308, loss = 2.58992411\n",
      "Iteration 9309, loss = 2.58989494\n",
      "Iteration 9310, loss = 2.58991534\n",
      "Iteration 9311, loss = 2.58996362\n",
      "Iteration 9312, loss = 2.58994756\n",
      "Iteration 9313, loss = 2.58985369\n",
      "Iteration 9314, loss = 2.58980759\n",
      "Iteration 9315, loss = 2.58979677\n",
      "Iteration 9316, loss = 2.58982621\n",
      "Iteration 9317, loss = 2.58979184\n",
      "Iteration 9318, loss = 2.58979434\n",
      "Iteration 9319, loss = 2.58986614\n",
      "Iteration 9320, loss = 2.58975771\n",
      "Iteration 9321, loss = 2.58979680\n",
      "Iteration 9322, loss = 2.58975722\n",
      "Iteration 9323, loss = 2.58968831\n",
      "Iteration 9324, loss = 2.58968306\n",
      "Iteration 9325, loss = 2.58986871\n",
      "Iteration 9326, loss = 2.58972813\n",
      "Iteration 9327, loss = 2.58968354\n",
      "Iteration 9328, loss = 2.58973030\n",
      "Iteration 9329, loss = 2.58963186\n",
      "Iteration 9330, loss = 2.58962631\n",
      "Iteration 9331, loss = 2.58959825\n",
      "Iteration 9332, loss = 2.58971205\n",
      "Iteration 9333, loss = 2.58958073\n",
      "Iteration 9334, loss = 2.58958547\n",
      "Iteration 9335, loss = 2.58964903\n",
      "Iteration 9336, loss = 2.58957164\n",
      "Iteration 9337, loss = 2.58959766\n",
      "Iteration 9338, loss = 2.58945123\n",
      "Iteration 9339, loss = 2.58964608\n",
      "Iteration 9340, loss = 2.58957023\n",
      "Iteration 9341, loss = 2.58952529\n",
      "Iteration 9342, loss = 2.58945909\n",
      "Iteration 9343, loss = 2.58942304\n",
      "Iteration 9344, loss = 2.58949536\n",
      "Iteration 9345, loss = 2.58944563\n",
      "Iteration 9346, loss = 2.58966011\n",
      "Iteration 9347, loss = 2.58947564\n",
      "Iteration 9348, loss = 2.58937798\n",
      "Iteration 9349, loss = 2.58936979\n",
      "Iteration 9350, loss = 2.58940998\n",
      "Iteration 9351, loss = 2.58931025\n",
      "Iteration 9352, loss = 2.58944080\n",
      "Iteration 9353, loss = 2.58933022\n",
      "Iteration 9354, loss = 2.58933492\n",
      "Iteration 9355, loss = 2.58936218\n",
      "Iteration 9356, loss = 2.58926937\n",
      "Iteration 9357, loss = 2.58928850\n",
      "Iteration 9358, loss = 2.58925911\n",
      "Iteration 9359, loss = 2.58941015\n",
      "Iteration 9360, loss = 2.58920037\n",
      "Iteration 9361, loss = 2.58925576\n",
      "Iteration 9362, loss = 2.58928434\n",
      "Iteration 9363, loss = 2.58916787\n",
      "Iteration 9364, loss = 2.58919072\n",
      "Iteration 9365, loss = 2.58918005\n",
      "Iteration 9366, loss = 2.58915220\n",
      "Iteration 9367, loss = 2.58911636\n",
      "Iteration 9368, loss = 2.58922916\n",
      "Iteration 9369, loss = 2.58906120\n",
      "Iteration 9370, loss = 2.58909872\n",
      "Iteration 9371, loss = 2.58916970\n",
      "Iteration 9372, loss = 2.58908238\n",
      "Iteration 9373, loss = 2.58910821\n",
      "Iteration 9374, loss = 2.58909314\n",
      "Iteration 9375, loss = 2.58918497\n",
      "Iteration 9376, loss = 2.58902496\n",
      "Iteration 9377, loss = 2.58929581\n",
      "Iteration 9378, loss = 2.58905293\n",
      "Iteration 9379, loss = 2.58904916\n",
      "Iteration 9380, loss = 2.58896810\n",
      "Iteration 9381, loss = 2.58894735\n",
      "Iteration 9382, loss = 2.58904209\n",
      "Iteration 9383, loss = 2.58894335\n",
      "Iteration 9384, loss = 2.58891851\n",
      "Iteration 9385, loss = 2.58890425\n",
      "Iteration 9386, loss = 2.58892289\n",
      "Iteration 9387, loss = 2.58889036\n",
      "Iteration 9388, loss = 2.58886223\n",
      "Iteration 9389, loss = 2.58892653\n",
      "Iteration 9390, loss = 2.58885515\n",
      "Iteration 9391, loss = 2.58881974\n",
      "Iteration 9392, loss = 2.58882813\n",
      "Iteration 9393, loss = 2.58890409\n",
      "Iteration 9394, loss = 2.58882725\n",
      "Iteration 9395, loss = 2.58878780\n",
      "Iteration 9396, loss = 2.58885642\n",
      "Iteration 9397, loss = 2.58876599\n",
      "Iteration 9398, loss = 2.58898329\n",
      "Iteration 9399, loss = 2.58874275\n",
      "Iteration 9400, loss = 2.58870612\n",
      "Iteration 9401, loss = 2.58867056\n",
      "Iteration 9402, loss = 2.58883048\n",
      "Iteration 9403, loss = 2.58868677\n",
      "Iteration 9404, loss = 2.58861370\n",
      "Iteration 9405, loss = 2.58861615\n",
      "Iteration 9406, loss = 2.58859481\n",
      "Iteration 9407, loss = 2.58859658\n",
      "Iteration 9408, loss = 2.58864610\n",
      "Iteration 9409, loss = 2.58858628\n",
      "Iteration 9410, loss = 2.58858831\n",
      "Iteration 9411, loss = 2.58855021\n",
      "Iteration 9412, loss = 2.58852816\n",
      "Iteration 9413, loss = 2.58861363\n",
      "Iteration 9414, loss = 2.58858581\n",
      "Iteration 9415, loss = 2.58849152\n",
      "Iteration 9416, loss = 2.58849415\n",
      "Iteration 9417, loss = 2.58850158\n",
      "Iteration 9418, loss = 2.58855626\n",
      "Iteration 9419, loss = 2.58851680\n",
      "Iteration 9420, loss = 2.58848583\n",
      "Iteration 9421, loss = 2.58837360\n",
      "Iteration 9422, loss = 2.58841040\n",
      "Iteration 9423, loss = 2.58842383\n",
      "Iteration 9424, loss = 2.58844360\n",
      "Iteration 9425, loss = 2.58848223\n",
      "Iteration 9426, loss = 2.58839167\n",
      "Iteration 9427, loss = 2.58835709\n",
      "Iteration 9428, loss = 2.58838477\n",
      "Iteration 9429, loss = 2.58839297\n",
      "Iteration 9430, loss = 2.58846174\n",
      "Iteration 9431, loss = 2.58835423\n",
      "Iteration 9432, loss = 2.58828544\n",
      "Iteration 9433, loss = 2.58834021\n",
      "Iteration 9434, loss = 2.58828251\n",
      "Iteration 9435, loss = 2.58826867\n",
      "Iteration 9436, loss = 2.58824092\n",
      "Iteration 9437, loss = 2.58822837\n",
      "Iteration 9438, loss = 2.58835513\n",
      "Iteration 9439, loss = 2.58841923\n",
      "Iteration 9440, loss = 2.58820616\n",
      "Iteration 9441, loss = 2.58817958\n",
      "Iteration 9442, loss = 2.58822386\n",
      "Iteration 9443, loss = 2.58812544\n",
      "Iteration 9444, loss = 2.58813150\n",
      "Iteration 9445, loss = 2.58817784\n",
      "Iteration 9446, loss = 2.58810026\n",
      "Iteration 9447, loss = 2.58813174\n",
      "Iteration 9448, loss = 2.58802942\n",
      "Iteration 9449, loss = 2.58815470\n",
      "Iteration 9450, loss = 2.58823838\n",
      "Iteration 9451, loss = 2.58803374\n",
      "Iteration 9452, loss = 2.58810685\n",
      "Iteration 9453, loss = 2.58820141\n",
      "Iteration 9454, loss = 2.58804121\n",
      "Iteration 9455, loss = 2.58805378\n",
      "Iteration 9456, loss = 2.58796823\n",
      "Iteration 9457, loss = 2.58799648\n",
      "Iteration 9458, loss = 2.58806686\n",
      "Iteration 9459, loss = 2.58787270\n",
      "Iteration 9460, loss = 2.58802742\n",
      "Iteration 9461, loss = 2.58800682\n",
      "Iteration 9462, loss = 2.58797577\n",
      "Iteration 9463, loss = 2.58787383\n",
      "Iteration 9464, loss = 2.58788311\n",
      "Iteration 9465, loss = 2.58801784\n",
      "Iteration 9466, loss = 2.58787488\n",
      "Iteration 9467, loss = 2.58786765\n",
      "Iteration 9468, loss = 2.58789348\n",
      "Iteration 9469, loss = 2.58790589\n",
      "Iteration 9470, loss = 2.58785534\n",
      "Iteration 9471, loss = 2.58778467\n",
      "Iteration 9472, loss = 2.58782442\n",
      "Iteration 9473, loss = 2.58777225\n",
      "Iteration 9474, loss = 2.58776076\n",
      "Iteration 9475, loss = 2.58773741\n",
      "Iteration 9476, loss = 2.58769359\n",
      "Iteration 9477, loss = 2.58778322\n",
      "Iteration 9478, loss = 2.58772091\n",
      "Iteration 9479, loss = 2.58773007\n",
      "Iteration 9480, loss = 2.58761269\n",
      "Iteration 9481, loss = 2.58765314\n",
      "Iteration 9482, loss = 2.58792229\n",
      "Iteration 9483, loss = 2.58762198\n",
      "Iteration 9484, loss = 2.58759840\n",
      "Iteration 9485, loss = 2.58768821\n",
      "Iteration 9486, loss = 2.58772280\n",
      "Iteration 9487, loss = 2.58761577\n",
      "Iteration 9488, loss = 2.58769482\n",
      "Iteration 9489, loss = 2.58756033\n",
      "Iteration 9490, loss = 2.58754957\n",
      "Iteration 9491, loss = 2.58748019\n",
      "Iteration 9492, loss = 2.58750276\n",
      "Iteration 9493, loss = 2.58761782\n",
      "Iteration 9494, loss = 2.58750018\n",
      "Iteration 9495, loss = 2.58753168\n",
      "Iteration 9496, loss = 2.58744456\n",
      "Iteration 9497, loss = 2.58742583\n",
      "Iteration 9498, loss = 2.58744111\n",
      "Iteration 9499, loss = 2.58744184\n",
      "Iteration 9500, loss = 2.58737452\n",
      "Iteration 9501, loss = 2.58743607\n",
      "Iteration 9502, loss = 2.58740796\n",
      "Iteration 9503, loss = 2.58743642\n",
      "Iteration 9504, loss = 2.58738150\n",
      "Iteration 9505, loss = 2.58746315\n",
      "Iteration 9506, loss = 2.58729107\n",
      "Iteration 9507, loss = 2.58738741\n",
      "Iteration 9508, loss = 2.58743211\n",
      "Iteration 9509, loss = 2.58728938\n",
      "Iteration 9510, loss = 2.58727879\n",
      "Iteration 9511, loss = 2.58734138\n",
      "Iteration 9512, loss = 2.58737600\n",
      "Iteration 9513, loss = 2.58731708\n",
      "Iteration 9514, loss = 2.58733438\n",
      "Iteration 9515, loss = 2.58731983\n",
      "Iteration 9516, loss = 2.58727949\n",
      "Iteration 9517, loss = 2.58725570\n",
      "Iteration 9518, loss = 2.58719306\n",
      "Iteration 9519, loss = 2.58714157\n",
      "Iteration 9520, loss = 2.58715176\n",
      "Iteration 9521, loss = 2.58716722\n",
      "Iteration 9522, loss = 2.58763749\n",
      "Iteration 9523, loss = 2.58712341\n",
      "Iteration 9524, loss = 2.58701861\n",
      "Iteration 9525, loss = 2.58709793\n",
      "Iteration 9526, loss = 2.58708295\n",
      "Iteration 9527, loss = 2.58715341\n",
      "Iteration 9528, loss = 2.58705646\n",
      "Iteration 9529, loss = 2.58724100\n",
      "Iteration 9530, loss = 2.58703421\n",
      "Iteration 9531, loss = 2.58707246\n",
      "Iteration 9532, loss = 2.58705119\n",
      "Iteration 9533, loss = 2.58694616\n",
      "Iteration 9534, loss = 2.58700290\n",
      "Iteration 9535, loss = 2.58694766\n",
      "Iteration 9536, loss = 2.58690852\n",
      "Iteration 9537, loss = 2.58693217\n",
      "Iteration 9538, loss = 2.58689525\n",
      "Iteration 9539, loss = 2.58686948\n",
      "Iteration 9540, loss = 2.58727939\n",
      "Iteration 9541, loss = 2.58700644\n",
      "Iteration 9542, loss = 2.58721528\n",
      "Iteration 9543, loss = 2.58695365\n",
      "Iteration 9544, loss = 2.58684303\n",
      "Iteration 9545, loss = 2.58688954\n",
      "Iteration 9546, loss = 2.58714260\n",
      "Iteration 9547, loss = 2.58691433\n",
      "Iteration 9548, loss = 2.58675429\n",
      "Iteration 9549, loss = 2.58684563\n",
      "Iteration 9550, loss = 2.58680684\n",
      "Iteration 9551, loss = 2.58691894\n",
      "Iteration 9552, loss = 2.58673296\n",
      "Iteration 9553, loss = 2.58672312\n",
      "Iteration 9554, loss = 2.58672512\n",
      "Iteration 9555, loss = 2.58669248\n",
      "Iteration 9556, loss = 2.58679232\n",
      "Iteration 9557, loss = 2.58659413\n",
      "Iteration 9558, loss = 2.58661977\n",
      "Iteration 9559, loss = 2.58680751\n",
      "Iteration 9560, loss = 2.58674045\n",
      "Iteration 9561, loss = 2.58666529\n",
      "Iteration 9562, loss = 2.58661062\n",
      "Iteration 9563, loss = 2.58666473\n",
      "Iteration 9564, loss = 2.58660195\n",
      "Iteration 9565, loss = 2.58667902\n",
      "Iteration 9566, loss = 2.58653361\n",
      "Iteration 9567, loss = 2.58651294\n",
      "Iteration 9568, loss = 2.58660290\n",
      "Iteration 9569, loss = 2.58653804\n",
      "Iteration 9570, loss = 2.58658604\n",
      "Iteration 9571, loss = 2.58654153\n",
      "Iteration 9572, loss = 2.58655484\n",
      "Iteration 9573, loss = 2.58651903\n",
      "Iteration 9574, loss = 2.58642439\n",
      "Iteration 9575, loss = 2.58647250\n",
      "Iteration 9576, loss = 2.58640896\n",
      "Iteration 9577, loss = 2.58648481\n",
      "Iteration 9578, loss = 2.58639879\n",
      "Iteration 9579, loss = 2.58632666\n",
      "Iteration 9580, loss = 2.58634308\n",
      "Iteration 9581, loss = 2.58635821\n",
      "Iteration 9582, loss = 2.58640606\n",
      "Iteration 9583, loss = 2.58634152\n",
      "Iteration 9584, loss = 2.58630565\n",
      "Iteration 9585, loss = 2.58642443\n",
      "Iteration 9586, loss = 2.58641778\n",
      "Iteration 9587, loss = 2.58630950\n",
      "Iteration 9588, loss = 2.58627837\n",
      "Iteration 9589, loss = 2.58648513\n",
      "Iteration 9590, loss = 2.58624995\n",
      "Iteration 9591, loss = 2.58624821\n",
      "Iteration 9592, loss = 2.58638390\n",
      "Iteration 9593, loss = 2.58620661\n",
      "Iteration 9594, loss = 2.58624385\n",
      "Iteration 9595, loss = 2.58625779\n",
      "Iteration 9596, loss = 2.58617696\n",
      "Iteration 9597, loss = 2.58610916\n",
      "Iteration 9598, loss = 2.58612589\n",
      "Iteration 9599, loss = 2.58610294\n",
      "Iteration 9600, loss = 2.58608408\n",
      "Iteration 9601, loss = 2.58610771\n",
      "Iteration 9602, loss = 2.58609218\n",
      "Iteration 9603, loss = 2.58608592\n",
      "Iteration 9604, loss = 2.58612268\n",
      "Iteration 9605, loss = 2.58603138\n",
      "Iteration 9606, loss = 2.58602547\n",
      "Iteration 9607, loss = 2.58604683\n",
      "Iteration 9608, loss = 2.58597962\n",
      "Iteration 9609, loss = 2.58609728\n",
      "Iteration 9610, loss = 2.58596776\n",
      "Iteration 9611, loss = 2.58602300\n",
      "Iteration 9612, loss = 2.58594702\n",
      "Iteration 9613, loss = 2.58591112\n",
      "Iteration 9614, loss = 2.58585678\n",
      "Iteration 9615, loss = 2.58600324\n",
      "Iteration 9616, loss = 2.58592933\n",
      "Iteration 9617, loss = 2.58584173\n",
      "Iteration 9618, loss = 2.58594551\n",
      "Iteration 9619, loss = 2.58585762\n",
      "Iteration 9620, loss = 2.58584762\n",
      "Iteration 9621, loss = 2.58588702\n",
      "Iteration 9622, loss = 2.58593556\n",
      "Iteration 9623, loss = 2.58571912\n",
      "Iteration 9624, loss = 2.58581716\n",
      "Iteration 9625, loss = 2.58595712\n",
      "Iteration 9626, loss = 2.58577220\n",
      "Iteration 9627, loss = 2.58583437\n",
      "Iteration 9628, loss = 2.58569989\n",
      "Iteration 9629, loss = 2.58578377\n",
      "Iteration 9630, loss = 2.58575821\n",
      "Iteration 9631, loss = 2.58571124\n",
      "Iteration 9632, loss = 2.58576166\n",
      "Iteration 9633, loss = 2.58571902\n",
      "Iteration 9634, loss = 2.58575653\n",
      "Iteration 9635, loss = 2.58563658\n",
      "Iteration 9636, loss = 2.58570979\n",
      "Iteration 9637, loss = 2.58565166\n",
      "Iteration 9638, loss = 2.58570125\n",
      "Iteration 9639, loss = 2.58577143\n",
      "Iteration 9640, loss = 2.58569983\n",
      "Iteration 9641, loss = 2.58558304\n",
      "Iteration 9642, loss = 2.58564125\n",
      "Iteration 9643, loss = 2.58560345\n",
      "Iteration 9644, loss = 2.58566111\n",
      "Iteration 9645, loss = 2.58563272\n",
      "Iteration 9646, loss = 2.58558794\n",
      "Iteration 9647, loss = 2.58552002\n",
      "Iteration 9648, loss = 2.58551724\n",
      "Iteration 9649, loss = 2.58552627\n",
      "Iteration 9650, loss = 2.58539888\n",
      "Iteration 9651, loss = 2.58550977\n",
      "Iteration 9652, loss = 2.58547644\n",
      "Iteration 9653, loss = 2.58543276\n",
      "Iteration 9654, loss = 2.58547899\n",
      "Iteration 9655, loss = 2.58542146\n",
      "Iteration 9656, loss = 2.58549246\n",
      "Iteration 9657, loss = 2.58534851\n",
      "Iteration 9658, loss = 2.58531282\n",
      "Iteration 9659, loss = 2.58538970\n",
      "Iteration 9660, loss = 2.58540717\n",
      "Iteration 9661, loss = 2.58536294\n",
      "Iteration 9662, loss = 2.58535863\n",
      "Iteration 9663, loss = 2.58530337\n",
      "Iteration 9664, loss = 2.58537824\n",
      "Iteration 9665, loss = 2.58531301\n",
      "Iteration 9666, loss = 2.58526071\n",
      "Iteration 9667, loss = 2.58538363\n",
      "Iteration 9668, loss = 2.58521941\n",
      "Iteration 9669, loss = 2.58520649\n",
      "Iteration 9670, loss = 2.58519038\n",
      "Iteration 9671, loss = 2.58531144\n",
      "Iteration 9672, loss = 2.58515759\n",
      "Iteration 9673, loss = 2.58511112\n",
      "Iteration 9674, loss = 2.58510240\n",
      "Iteration 9675, loss = 2.58511771\n",
      "Iteration 9676, loss = 2.58515273\n",
      "Iteration 9677, loss = 2.58522447\n",
      "Iteration 9678, loss = 2.58521608\n",
      "Iteration 9679, loss = 2.58522496\n",
      "Iteration 9680, loss = 2.58503778\n",
      "Iteration 9681, loss = 2.58505806\n",
      "Iteration 9682, loss = 2.58510190\n",
      "Iteration 9683, loss = 2.58506022\n",
      "Iteration 9684, loss = 2.58500313\n",
      "Iteration 9685, loss = 2.58499405\n",
      "Iteration 9686, loss = 2.58499262\n",
      "Iteration 9687, loss = 2.58498799\n",
      "Iteration 9688, loss = 2.58498660\n",
      "Iteration 9689, loss = 2.58503987\n",
      "Iteration 9690, loss = 2.58497192\n",
      "Iteration 9691, loss = 2.58494155\n",
      "Iteration 9692, loss = 2.58495582\n",
      "Iteration 9693, loss = 2.58488549\n",
      "Iteration 9694, loss = 2.58496568\n",
      "Iteration 9695, loss = 2.58496167\n",
      "Iteration 9696, loss = 2.58484173\n",
      "Iteration 9697, loss = 2.58485736\n",
      "Iteration 9698, loss = 2.58489874\n",
      "Iteration 9699, loss = 2.58482003\n",
      "Iteration 9700, loss = 2.58485264\n",
      "Iteration 9701, loss = 2.58486314\n",
      "Iteration 9702, loss = 2.58491651\n",
      "Iteration 9703, loss = 2.58492784\n",
      "Iteration 9704, loss = 2.58473551\n",
      "Iteration 9705, loss = 2.58474251\n",
      "Iteration 9706, loss = 2.58472440\n",
      "Iteration 9707, loss = 2.58475749\n",
      "Iteration 9708, loss = 2.58471965\n",
      "Iteration 9709, loss = 2.58475058\n",
      "Iteration 9710, loss = 2.58477163\n",
      "Iteration 9711, loss = 2.58476358\n",
      "Iteration 9712, loss = 2.58482187\n",
      "Iteration 9713, loss = 2.58459862\n",
      "Iteration 9714, loss = 2.58470912\n",
      "Iteration 9715, loss = 2.58480317\n",
      "Iteration 9716, loss = 2.58467686\n",
      "Iteration 9717, loss = 2.58463243\n",
      "Iteration 9718, loss = 2.58457748\n",
      "Iteration 9719, loss = 2.58458469\n",
      "Iteration 9720, loss = 2.58454324\n",
      "Iteration 9721, loss = 2.58460065\n",
      "Iteration 9722, loss = 2.58456506\n",
      "Iteration 9723, loss = 2.58451439\n",
      "Iteration 9724, loss = 2.58447845\n",
      "Iteration 9725, loss = 2.58454958\n",
      "Iteration 9726, loss = 2.58444265\n",
      "Iteration 9727, loss = 2.58456307\n",
      "Iteration 9728, loss = 2.58456368\n",
      "Iteration 9729, loss = 2.58446589\n",
      "Iteration 9730, loss = 2.58453583\n",
      "Iteration 9731, loss = 2.58451526\n",
      "Iteration 9732, loss = 2.58445531\n",
      "Iteration 9733, loss = 2.58450012\n",
      "Iteration 9734, loss = 2.58428712\n",
      "Iteration 9735, loss = 2.58437869\n",
      "Iteration 9736, loss = 2.58437813\n",
      "Iteration 9737, loss = 2.58442933\n",
      "Iteration 9738, loss = 2.58440685\n",
      "Iteration 9739, loss = 2.58434452\n",
      "Iteration 9740, loss = 2.58428620\n",
      "Iteration 9741, loss = 2.58425581\n",
      "Iteration 9742, loss = 2.58425944\n",
      "Iteration 9743, loss = 2.58426493\n",
      "Iteration 9744, loss = 2.58421513\n",
      "Iteration 9745, loss = 2.58416810\n",
      "Iteration 9746, loss = 2.58426715\n",
      "Iteration 9747, loss = 2.58432164\n",
      "Iteration 9748, loss = 2.58432830\n",
      "Iteration 9749, loss = 2.58421797\n",
      "Iteration 9750, loss = 2.58415891\n",
      "Iteration 9751, loss = 2.58413938\n",
      "Iteration 9752, loss = 2.58422148\n",
      "Iteration 9753, loss = 2.58416940\n",
      "Iteration 9754, loss = 2.58418792\n",
      "Iteration 9755, loss = 2.58404882\n",
      "Iteration 9756, loss = 2.58418851\n",
      "Iteration 9757, loss = 2.58413477\n",
      "Iteration 9758, loss = 2.58413525\n",
      "Iteration 9759, loss = 2.58399862\n",
      "Iteration 9760, loss = 2.58401041\n",
      "Iteration 9761, loss = 2.58402460\n",
      "Iteration 9762, loss = 2.58405720\n",
      "Iteration 9763, loss = 2.58396590\n",
      "Iteration 9764, loss = 2.58398346\n",
      "Iteration 9765, loss = 2.58399754\n",
      "Iteration 9766, loss = 2.58399358\n",
      "Iteration 9767, loss = 2.58395856\n",
      "Iteration 9768, loss = 2.58391619\n",
      "Iteration 9769, loss = 2.58395047\n",
      "Iteration 9770, loss = 2.58389704\n",
      "Iteration 9771, loss = 2.58390121\n",
      "Iteration 9772, loss = 2.58388639\n",
      "Iteration 9773, loss = 2.58385218\n",
      "Iteration 9774, loss = 2.58385298\n",
      "Iteration 9775, loss = 2.58397178\n",
      "Iteration 9776, loss = 2.58386296\n",
      "Iteration 9777, loss = 2.58385573\n",
      "Iteration 9778, loss = 2.58378887\n",
      "Iteration 9779, loss = 2.58380850\n",
      "Iteration 9780, loss = 2.58381132\n",
      "Iteration 9781, loss = 2.58381045\n",
      "Iteration 9782, loss = 2.58368333\n",
      "Iteration 9783, loss = 2.58376644\n",
      "Iteration 9784, loss = 2.58377472\n",
      "Iteration 9785, loss = 2.58372114\n",
      "Iteration 9786, loss = 2.58375943\n",
      "Iteration 9787, loss = 2.58410251\n",
      "Iteration 9788, loss = 2.58385687\n",
      "Iteration 9789, loss = 2.58369091\n",
      "Iteration 9790, loss = 2.58362849\n",
      "Iteration 9791, loss = 2.58361708\n",
      "Iteration 9792, loss = 2.58380571\n",
      "Iteration 9793, loss = 2.58355942\n",
      "Iteration 9794, loss = 2.58363299\n",
      "Iteration 9795, loss = 2.58364363\n",
      "Iteration 9796, loss = 2.58364495\n",
      "Iteration 9797, loss = 2.58353237\n",
      "Iteration 9798, loss = 2.58372715\n",
      "Iteration 9799, loss = 2.58350499\n",
      "Iteration 9800, loss = 2.58360578\n",
      "Iteration 9801, loss = 2.58351859\n",
      "Iteration 9802, loss = 2.58345273\n",
      "Iteration 9803, loss = 2.58346051\n",
      "Iteration 9804, loss = 2.58347835\n",
      "Iteration 9805, loss = 2.58339903\n",
      "Iteration 9806, loss = 2.58349896\n",
      "Iteration 9807, loss = 2.58347251\n",
      "Iteration 9808, loss = 2.58349829\n",
      "Iteration 9809, loss = 2.58337358\n",
      "Iteration 9810, loss = 2.58352131\n",
      "Iteration 9811, loss = 2.58336382\n",
      "Iteration 9812, loss = 2.58339577\n",
      "Iteration 9813, loss = 2.58327958\n",
      "Iteration 9814, loss = 2.58331805\n",
      "Iteration 9815, loss = 2.58334316\n",
      "Iteration 9816, loss = 2.58330228\n",
      "Iteration 9817, loss = 2.58330231\n",
      "Iteration 9818, loss = 2.58324065\n",
      "Iteration 9819, loss = 2.58327141\n",
      "Iteration 9820, loss = 2.58326598\n",
      "Iteration 9821, loss = 2.58331507\n",
      "Iteration 9822, loss = 2.58327957\n",
      "Iteration 9823, loss = 2.58326411\n",
      "Iteration 9824, loss = 2.58316769\n",
      "Iteration 9825, loss = 2.58317566\n",
      "Iteration 9826, loss = 2.58317653\n",
      "Iteration 9827, loss = 2.58322733\n",
      "Iteration 9828, loss = 2.58322173\n",
      "Iteration 9829, loss = 2.58320562\n",
      "Iteration 9830, loss = 2.58325408\n",
      "Iteration 9831, loss = 2.58315667\n",
      "Iteration 9832, loss = 2.58314131\n",
      "Iteration 9833, loss = 2.58322747\n",
      "Iteration 9834, loss = 2.58303417\n",
      "Iteration 9835, loss = 2.58313714\n",
      "Iteration 9836, loss = 2.58312314\n",
      "Iteration 9837, loss = 2.58310600\n",
      "Iteration 9838, loss = 2.58305388\n",
      "Iteration 9839, loss = 2.58299510\n",
      "Iteration 9840, loss = 2.58301294\n",
      "Iteration 9841, loss = 2.58301451\n",
      "Iteration 9842, loss = 2.58299433\n",
      "Iteration 9843, loss = 2.58307521\n",
      "Iteration 9844, loss = 2.58290184\n",
      "Iteration 9845, loss = 2.58299103\n",
      "Iteration 9846, loss = 2.58291633\n",
      "Iteration 9847, loss = 2.58289157\n",
      "Iteration 9848, loss = 2.58300725\n",
      "Iteration 9849, loss = 2.58292782\n",
      "Iteration 9850, loss = 2.58289976\n",
      "Iteration 9851, loss = 2.58287927\n",
      "Iteration 9852, loss = 2.58300259\n",
      "Iteration 9853, loss = 2.58287650\n",
      "Iteration 9854, loss = 2.58271674\n",
      "Iteration 9855, loss = 2.58297281\n",
      "Iteration 9856, loss = 2.58285972\n",
      "Iteration 9857, loss = 2.58276118\n",
      "Iteration 9858, loss = 2.58279381\n",
      "Iteration 9859, loss = 2.58274858\n",
      "Iteration 9860, loss = 2.58292019\n",
      "Iteration 9861, loss = 2.58276716\n",
      "Iteration 9862, loss = 2.58273463\n",
      "Iteration 9863, loss = 2.58265093\n",
      "Iteration 9864, loss = 2.58268547\n",
      "Iteration 9865, loss = 2.58265433\n",
      "Iteration 9866, loss = 2.58266097\n",
      "Iteration 9867, loss = 2.58266865\n",
      "Iteration 9868, loss = 2.58273442\n",
      "Iteration 9869, loss = 2.58277070\n",
      "Iteration 9870, loss = 2.58259024\n",
      "Iteration 9871, loss = 2.58256459\n",
      "Iteration 9872, loss = 2.58260405\n",
      "Iteration 9873, loss = 2.58259569\n",
      "Iteration 9874, loss = 2.58267573\n",
      "Iteration 9875, loss = 2.58258837\n",
      "Iteration 9876, loss = 2.58260872\n",
      "Iteration 9877, loss = 2.58249575\n",
      "Iteration 9878, loss = 2.58251423\n",
      "Iteration 9879, loss = 2.58259627\n",
      "Iteration 9880, loss = 2.58247255\n",
      "Iteration 9881, loss = 2.58246406\n",
      "Iteration 9882, loss = 2.58243264\n",
      "Iteration 9883, loss = 2.58250837\n",
      "Iteration 9884, loss = 2.58244713\n",
      "Iteration 9885, loss = 2.58245127\n",
      "Iteration 9886, loss = 2.58240843\n",
      "Iteration 9887, loss = 2.58234555\n",
      "Iteration 9888, loss = 2.58247984\n",
      "Iteration 9889, loss = 2.58249823\n",
      "Iteration 9890, loss = 2.58241176\n",
      "Iteration 9891, loss = 2.58237400\n",
      "Iteration 9892, loss = 2.58233064\n",
      "Iteration 9893, loss = 2.58237316\n",
      "Iteration 9894, loss = 2.58226456\n",
      "Iteration 9895, loss = 2.58227920\n",
      "Iteration 9896, loss = 2.58228611\n",
      "Iteration 9897, loss = 2.58232290\n",
      "Iteration 9898, loss = 2.58221421\n",
      "Iteration 9899, loss = 2.58228444\n",
      "Iteration 9900, loss = 2.58237546\n",
      "Iteration 9901, loss = 2.58220360\n",
      "Iteration 9902, loss = 2.58229188\n",
      "Iteration 9903, loss = 2.58223395\n",
      "Iteration 9904, loss = 2.58219799\n",
      "Iteration 9905, loss = 2.58217339\n",
      "Iteration 9906, loss = 2.58208992\n",
      "Iteration 9907, loss = 2.58223323\n",
      "Iteration 9908, loss = 2.58209933\n",
      "Iteration 9909, loss = 2.58204667\n",
      "Iteration 9910, loss = 2.58210052\n",
      "Iteration 9911, loss = 2.58210768\n",
      "Iteration 9912, loss = 2.58207296\n",
      "Iteration 9913, loss = 2.58197748\n",
      "Iteration 9914, loss = 2.58209690\n",
      "Iteration 9915, loss = 2.58223583\n",
      "Iteration 9916, loss = 2.58198469\n",
      "Iteration 9917, loss = 2.58201262\n",
      "Iteration 9918, loss = 2.58202994\n",
      "Iteration 9919, loss = 2.58194307\n",
      "Iteration 9920, loss = 2.58192888\n",
      "Iteration 9921, loss = 2.58190224\n",
      "Iteration 9922, loss = 2.58189336\n",
      "Iteration 9923, loss = 2.58198988\n",
      "Iteration 9924, loss = 2.58199183\n",
      "Iteration 9925, loss = 2.58190335\n",
      "Iteration 9926, loss = 2.58185361\n",
      "Iteration 9927, loss = 2.58193123\n",
      "Iteration 9928, loss = 2.58179975\n",
      "Iteration 9929, loss = 2.58184095\n",
      "Iteration 9930, loss = 2.58184865\n",
      "Iteration 9931, loss = 2.58184647\n",
      "Iteration 9932, loss = 2.58178809\n",
      "Iteration 9933, loss = 2.58182935\n",
      "Iteration 9934, loss = 2.58177404\n",
      "Iteration 9935, loss = 2.58179052\n",
      "Iteration 9936, loss = 2.58170857\n",
      "Iteration 9937, loss = 2.58177748\n",
      "Iteration 9938, loss = 2.58169775\n",
      "Iteration 9939, loss = 2.58170597\n",
      "Iteration 9940, loss = 2.58167844\n",
      "Iteration 9941, loss = 2.58169153\n",
      "Iteration 9942, loss = 2.58164004\n",
      "Iteration 9943, loss = 2.58170284\n",
      "Iteration 9944, loss = 2.58177466\n",
      "Iteration 9945, loss = 2.58158829\n",
      "Iteration 9946, loss = 2.58165608\n",
      "Iteration 9947, loss = 2.58157104\n",
      "Iteration 9948, loss = 2.58173592\n",
      "Iteration 9949, loss = 2.58156154\n",
      "Iteration 9950, loss = 2.58161568\n",
      "Iteration 9951, loss = 2.58151575\n",
      "Iteration 9952, loss = 2.58155118\n",
      "Iteration 9953, loss = 2.58156105\n",
      "Iteration 9954, loss = 2.58152031\n",
      "Iteration 9955, loss = 2.58147979\n",
      "Iteration 9956, loss = 2.58151473\n",
      "Iteration 9957, loss = 2.58147032\n",
      "Iteration 9958, loss = 2.58161818\n",
      "Iteration 9959, loss = 2.58149107\n",
      "Iteration 9960, loss = 2.58144770\n",
      "Iteration 9961, loss = 2.58144544\n",
      "Iteration 9962, loss = 2.58144282\n",
      "Iteration 9963, loss = 2.58153377\n",
      "Iteration 9964, loss = 2.58135839\n",
      "Iteration 9965, loss = 2.58138364\n",
      "Iteration 9966, loss = 2.58144060\n",
      "Iteration 9967, loss = 2.58130066\n",
      "Iteration 9968, loss = 2.58134771\n",
      "Iteration 9969, loss = 2.58138435\n",
      "Iteration 9970, loss = 2.58125755\n",
      "Iteration 9971, loss = 2.58132134\n",
      "Iteration 9972, loss = 2.58135927\n",
      "Iteration 9973, loss = 2.58129878\n",
      "Iteration 9974, loss = 2.58129817\n",
      "Iteration 9975, loss = 2.58130062\n",
      "Iteration 9976, loss = 2.58120542\n",
      "Iteration 9977, loss = 2.58117311\n",
      "Iteration 9978, loss = 2.58121196\n",
      "Iteration 9979, loss = 2.58115125\n",
      "Iteration 9980, loss = 2.58120663\n",
      "Iteration 9981, loss = 2.58117961\n",
      "Iteration 9982, loss = 2.58129222\n",
      "Iteration 9983, loss = 2.58118577\n",
      "Iteration 9984, loss = 2.58116226\n",
      "Iteration 9985, loss = 2.58117322\n",
      "Iteration 9986, loss = 2.58111414\n",
      "Iteration 9987, loss = 2.58112358\n",
      "Iteration 9988, loss = 2.58102926\n",
      "Iteration 9989, loss = 2.58111386\n",
      "Iteration 9990, loss = 2.58104358\n",
      "Iteration 9991, loss = 2.58104796\n",
      "Iteration 9992, loss = 2.58103671\n",
      "Iteration 9993, loss = 2.58096149\n",
      "Iteration 9994, loss = 2.58104254\n",
      "Iteration 9995, loss = 2.58091202\n",
      "Iteration 9996, loss = 2.58092018\n",
      "Iteration 9997, loss = 2.58094927\n",
      "Iteration 9998, loss = 2.58091318\n",
      "Iteration 9999, loss = 2.58092249\n",
      "Iteration 10000, loss = 2.58088713\n",
      "Iteration 10001, loss = 2.58102063\n",
      "Iteration 10002, loss = 2.58097561\n",
      "Iteration 10003, loss = 2.58087936\n",
      "Iteration 10004, loss = 2.58082693\n",
      "Iteration 10005, loss = 2.58082819\n",
      "Iteration 10006, loss = 2.58102976\n",
      "Iteration 10007, loss = 2.58080245\n",
      "Iteration 10008, loss = 2.58086537\n",
      "Iteration 10009, loss = 2.58078422\n",
      "Iteration 10010, loss = 2.58079875\n",
      "Iteration 10011, loss = 2.58075042\n",
      "Iteration 10012, loss = 2.58084536\n",
      "Iteration 10013, loss = 2.58072329\n",
      "Iteration 10014, loss = 2.58076416\n",
      "Iteration 10015, loss = 2.58070470\n",
      "Iteration 10016, loss = 2.58071917\n",
      "Iteration 10017, loss = 2.58068978\n",
      "Iteration 10018, loss = 2.58071222\n",
      "Iteration 10019, loss = 2.58083350\n",
      "Iteration 10020, loss = 2.58059636\n",
      "Iteration 10021, loss = 2.58065154\n",
      "Iteration 10022, loss = 2.58065291\n",
      "Iteration 10023, loss = 2.58056623\n",
      "Iteration 10024, loss = 2.58061174\n",
      "Iteration 10025, loss = 2.58060890\n",
      "Iteration 10026, loss = 2.58050964\n",
      "Iteration 10027, loss = 2.58058045\n",
      "Iteration 10028, loss = 2.58059273\n",
      "Iteration 10029, loss = 2.58066717\n",
      "Iteration 10030, loss = 2.58054080\n",
      "Iteration 10031, loss = 2.58053335\n",
      "Iteration 10032, loss = 2.58062155\n",
      "Iteration 10033, loss = 2.58053957\n",
      "Iteration 10034, loss = 2.58051330\n",
      "Iteration 10035, loss = 2.58044022\n",
      "Iteration 10036, loss = 2.58048964\n",
      "Iteration 10037, loss = 2.58043664\n",
      "Iteration 10038, loss = 2.58040844\n",
      "Iteration 10039, loss = 2.58041256\n",
      "Iteration 10040, loss = 2.58043878\n",
      "Iteration 10041, loss = 2.58052296\n",
      "Iteration 10042, loss = 2.58032820\n",
      "Iteration 10043, loss = 2.58052135\n",
      "Iteration 10044, loss = 2.58048307\n",
      "Iteration 10045, loss = 2.58032303\n",
      "Iteration 10046, loss = 2.58032225\n",
      "Iteration 10047, loss = 2.58030417\n",
      "Iteration 10048, loss = 2.58037764\n",
      "Iteration 10049, loss = 2.58032379\n",
      "Iteration 10050, loss = 2.58020675\n",
      "Iteration 10051, loss = 2.58022747\n",
      "Iteration 10052, loss = 2.58024654\n",
      "Iteration 10053, loss = 2.58022085\n",
      "Iteration 10054, loss = 2.58028404\n",
      "Iteration 10055, loss = 2.58022573\n",
      "Iteration 10056, loss = 2.58032764\n",
      "Iteration 10057, loss = 2.58036362\n",
      "Iteration 10058, loss = 2.58013994\n",
      "Iteration 10059, loss = 2.58018456\n",
      "Iteration 10060, loss = 2.58018399\n",
      "Iteration 10061, loss = 2.58018882\n",
      "Iteration 10062, loss = 2.58009221\n",
      "Iteration 10063, loss = 2.58008928\n",
      "Iteration 10064, loss = 2.58001527\n",
      "Iteration 10065, loss = 2.58017977\n",
      "Iteration 10066, loss = 2.58010049\n",
      "Iteration 10067, loss = 2.58001066\n",
      "Iteration 10068, loss = 2.58005636\n",
      "Iteration 10069, loss = 2.58005767\n",
      "Iteration 10070, loss = 2.58007847\n",
      "Iteration 10071, loss = 2.57996111\n",
      "Iteration 10072, loss = 2.57994176\n",
      "Iteration 10073, loss = 2.57997279\n",
      "Iteration 10074, loss = 2.57996986\n",
      "Iteration 10075, loss = 2.57989753\n",
      "Iteration 10076, loss = 2.58015889\n",
      "Iteration 10077, loss = 2.57998972\n",
      "Iteration 10078, loss = 2.57986204\n",
      "Iteration 10079, loss = 2.57992477\n",
      "Iteration 10080, loss = 2.57984252\n",
      "Iteration 10081, loss = 2.57986965\n",
      "Iteration 10082, loss = 2.58000923\n",
      "Iteration 10083, loss = 2.58001476\n",
      "Iteration 10084, loss = 2.57987700\n",
      "Iteration 10085, loss = 2.57980902\n",
      "Iteration 10086, loss = 2.57978291\n",
      "Iteration 10087, loss = 2.57978281\n",
      "Iteration 10088, loss = 2.57978832\n",
      "Iteration 10089, loss = 2.57976727\n",
      "Iteration 10090, loss = 2.57981409\n",
      "Iteration 10091, loss = 2.57972085\n",
      "Iteration 10092, loss = 2.57982308\n",
      "Iteration 10093, loss = 2.57974588\n",
      "Iteration 10094, loss = 2.57971088\n",
      "Iteration 10095, loss = 2.57965664\n",
      "Iteration 10096, loss = 2.57964083\n",
      "Iteration 10097, loss = 2.57964642\n",
      "Iteration 10098, loss = 2.57983726\n",
      "Iteration 10099, loss = 2.57960719\n",
      "Iteration 10100, loss = 2.57959488\n",
      "Iteration 10101, loss = 2.57961561\n",
      "Iteration 10102, loss = 2.57965828\n",
      "Iteration 10103, loss = 2.57959942\n",
      "Iteration 10104, loss = 2.57956600\n",
      "Iteration 10105, loss = 2.57962689\n",
      "Iteration 10106, loss = 2.57961718\n",
      "Iteration 10107, loss = 2.57961375\n",
      "Iteration 10108, loss = 2.57955274\n",
      "Iteration 10109, loss = 2.57952283\n",
      "Iteration 10110, loss = 2.57954579\n",
      "Iteration 10111, loss = 2.57951491\n",
      "Iteration 10112, loss = 2.57955482\n",
      "Iteration 10113, loss = 2.57951329\n",
      "Iteration 10114, loss = 2.57946565\n",
      "Iteration 10115, loss = 2.57941716\n",
      "Iteration 10116, loss = 2.57938058\n",
      "Iteration 10117, loss = 2.57936999\n",
      "Iteration 10118, loss = 2.57934872\n",
      "Iteration 10119, loss = 2.57941939\n",
      "Iteration 10120, loss = 2.57935278\n",
      "Iteration 10121, loss = 2.57933050\n",
      "Iteration 10122, loss = 2.57933217\n",
      "Iteration 10123, loss = 2.57933459\n",
      "Iteration 10124, loss = 2.57929305\n",
      "Iteration 10125, loss = 2.57927729\n",
      "Iteration 10126, loss = 2.57930972\n",
      "Iteration 10127, loss = 2.57925258\n",
      "Iteration 10128, loss = 2.57930074\n",
      "Iteration 10129, loss = 2.57921894\n",
      "Iteration 10130, loss = 2.57923954\n",
      "Iteration 10131, loss = 2.57919601\n",
      "Iteration 10132, loss = 2.57921757\n",
      "Iteration 10133, loss = 2.57918175\n",
      "Iteration 10134, loss = 2.57912304\n",
      "Iteration 10135, loss = 2.57918590\n",
      "Iteration 10136, loss = 2.57912720\n",
      "Iteration 10137, loss = 2.57917733\n",
      "Iteration 10138, loss = 2.57914995\n",
      "Iteration 10139, loss = 2.57915553\n",
      "Iteration 10140, loss = 2.57915431\n",
      "Iteration 10141, loss = 2.57908229\n",
      "Iteration 10142, loss = 2.57918829\n",
      "Iteration 10143, loss = 2.57898543\n",
      "Iteration 10144, loss = 2.57909141\n",
      "Iteration 10145, loss = 2.57909679\n",
      "Iteration 10146, loss = 2.57907218\n",
      "Iteration 10147, loss = 2.57904291\n",
      "Iteration 10148, loss = 2.57896240\n",
      "Iteration 10149, loss = 2.57900313\n",
      "Iteration 10150, loss = 2.57901157\n",
      "Iteration 10151, loss = 2.57899850\n",
      "Iteration 10152, loss = 2.57907267\n",
      "Iteration 10153, loss = 2.57904331\n",
      "Iteration 10154, loss = 2.57898978\n",
      "Iteration 10155, loss = 2.57890295\n",
      "Iteration 10156, loss = 2.57886957\n",
      "Iteration 10157, loss = 2.57887751\n",
      "Iteration 10158, loss = 2.57893758\n",
      "Iteration 10159, loss = 2.57884018\n",
      "Iteration 10160, loss = 2.57889281\n",
      "Iteration 10161, loss = 2.57892200\n",
      "Iteration 10162, loss = 2.57877667\n",
      "Iteration 10163, loss = 2.57874897\n",
      "Iteration 10164, loss = 2.57893404\n",
      "Iteration 10165, loss = 2.57876753\n",
      "Iteration 10166, loss = 2.57876480\n",
      "Iteration 10167, loss = 2.57882268\n",
      "Iteration 10168, loss = 2.57871771\n",
      "Iteration 10169, loss = 2.57874347\n",
      "Iteration 10170, loss = 2.57868458\n",
      "Iteration 10171, loss = 2.57879426\n",
      "Iteration 10172, loss = 2.57863962\n",
      "Iteration 10173, loss = 2.57869528\n",
      "Iteration 10174, loss = 2.57863976\n",
      "Iteration 10175, loss = 2.57860137\n",
      "Iteration 10176, loss = 2.57875086\n",
      "Iteration 10177, loss = 2.57863354\n",
      "Iteration 10178, loss = 2.57859614\n",
      "Iteration 10179, loss = 2.57859900\n",
      "Iteration 10180, loss = 2.57859849\n",
      "Iteration 10181, loss = 2.57850427\n",
      "Iteration 10182, loss = 2.57852073\n",
      "Iteration 10183, loss = 2.57861041\n",
      "Iteration 10184, loss = 2.57856406\n",
      "Iteration 10185, loss = 2.57851598\n",
      "Iteration 10186, loss = 2.57857168\n",
      "Iteration 10187, loss = 2.57851614\n",
      "Iteration 10188, loss = 2.57850319\n",
      "Iteration 10189, loss = 2.57847804\n",
      "Iteration 10190, loss = 2.57845666\n",
      "Iteration 10191, loss = 2.57879561\n",
      "Iteration 10192, loss = 2.57865873\n",
      "Iteration 10193, loss = 2.57851459\n",
      "Iteration 10194, loss = 2.57839241\n",
      "Iteration 10195, loss = 2.57841700\n",
      "Iteration 10196, loss = 2.57834617\n",
      "Iteration 10197, loss = 2.57836870\n",
      "Iteration 10198, loss = 2.57839317\n",
      "Iteration 10199, loss = 2.57834810\n",
      "Iteration 10200, loss = 2.57832202\n",
      "Iteration 10201, loss = 2.57828939\n",
      "Iteration 10202, loss = 2.57837799\n",
      "Iteration 10203, loss = 2.57833311\n",
      "Iteration 10204, loss = 2.57824389\n",
      "Iteration 10205, loss = 2.57825416\n",
      "Iteration 10206, loss = 2.57825434\n",
      "Iteration 10207, loss = 2.57822869\n",
      "Iteration 10208, loss = 2.57817501\n",
      "Iteration 10209, loss = 2.57814809\n",
      "Iteration 10210, loss = 2.57815212\n",
      "Iteration 10211, loss = 2.57811898\n",
      "Iteration 10212, loss = 2.57808294\n",
      "Iteration 10213, loss = 2.57816728\n",
      "Iteration 10214, loss = 2.57809678\n",
      "Iteration 10215, loss = 2.57816002\n",
      "Iteration 10216, loss = 2.57829227\n",
      "Iteration 10217, loss = 2.57865346\n",
      "Iteration 10218, loss = 2.57819584\n",
      "Iteration 10219, loss = 2.57802779\n",
      "Iteration 10220, loss = 2.57808658\n",
      "Iteration 10221, loss = 2.57804150\n",
      "Iteration 10222, loss = 2.57807212\n",
      "Iteration 10223, loss = 2.57797406\n",
      "Iteration 10224, loss = 2.57803665\n",
      "Iteration 10225, loss = 2.57806540\n",
      "Iteration 10226, loss = 2.57795424\n",
      "Iteration 10227, loss = 2.57800752\n",
      "Iteration 10228, loss = 2.57801867\n",
      "Iteration 10229, loss = 2.57790293\n",
      "Iteration 10230, loss = 2.57800678\n",
      "Iteration 10231, loss = 2.57810369\n",
      "Iteration 10232, loss = 2.57794090\n",
      "Iteration 10233, loss = 2.57789272\n",
      "Iteration 10234, loss = 2.57798295\n",
      "Iteration 10235, loss = 2.57786638\n",
      "Iteration 10236, loss = 2.57784545\n",
      "Iteration 10237, loss = 2.57777259\n",
      "Iteration 10238, loss = 2.57787602\n",
      "Iteration 10239, loss = 2.57785716\n",
      "Iteration 10240, loss = 2.57783382\n",
      "Iteration 10241, loss = 2.57809339\n",
      "Iteration 10242, loss = 2.57777155\n",
      "Iteration 10243, loss = 2.57777694\n",
      "Iteration 10244, loss = 2.57770324\n",
      "Iteration 10245, loss = 2.57789384\n",
      "Iteration 10246, loss = 2.57778319\n",
      "Iteration 10247, loss = 2.57763773\n",
      "Iteration 10248, loss = 2.57765820\n",
      "Iteration 10249, loss = 2.57774276\n",
      "Iteration 10250, loss = 2.57770227\n",
      "Iteration 10251, loss = 2.57761113\n",
      "Iteration 10252, loss = 2.57756575\n",
      "Iteration 10253, loss = 2.57761280\n",
      "Iteration 10254, loss = 2.57775994\n",
      "Iteration 10255, loss = 2.57758953\n",
      "Iteration 10256, loss = 2.57753710\n",
      "Iteration 10257, loss = 2.57766020\n",
      "Iteration 10258, loss = 2.57762665\n",
      "Iteration 10259, loss = 2.57762896\n",
      "Iteration 10260, loss = 2.57749694\n",
      "Iteration 10261, loss = 2.57755834\n",
      "Iteration 10262, loss = 2.57771740\n",
      "Iteration 10263, loss = 2.57756298\n",
      "Iteration 10264, loss = 2.57750435\n",
      "Iteration 10265, loss = 2.57743847\n",
      "Iteration 10266, loss = 2.57744167\n",
      "Iteration 10267, loss = 2.57742580\n",
      "Iteration 10268, loss = 2.57743447\n",
      "Iteration 10269, loss = 2.57739093\n",
      "Iteration 10270, loss = 2.57744252\n",
      "Iteration 10271, loss = 2.57741669\n",
      "Iteration 10272, loss = 2.57753886\n",
      "Iteration 10273, loss = 2.57742321\n",
      "Iteration 10274, loss = 2.57741095\n",
      "Iteration 10275, loss = 2.57724006\n",
      "Iteration 10276, loss = 2.57732376\n",
      "Iteration 10277, loss = 2.57738325\n",
      "Iteration 10278, loss = 2.57722212\n",
      "Iteration 10279, loss = 2.57730211\n",
      "Iteration 10280, loss = 2.57722866\n",
      "Iteration 10281, loss = 2.57725695\n",
      "Iteration 10282, loss = 2.57731188\n",
      "Iteration 10283, loss = 2.57727184\n",
      "Iteration 10284, loss = 2.57737553\n",
      "Iteration 10285, loss = 2.57717380\n",
      "Iteration 10286, loss = 2.57723665\n",
      "Iteration 10287, loss = 2.57730894\n",
      "Iteration 10288, loss = 2.57714674\n",
      "Iteration 10289, loss = 2.57725950\n",
      "Iteration 10290, loss = 2.57721437\n",
      "Iteration 10291, loss = 2.57714084\n",
      "Iteration 10292, loss = 2.57733141\n",
      "Iteration 10293, loss = 2.57710540\n",
      "Iteration 10294, loss = 2.57703614\n",
      "Iteration 10295, loss = 2.57706465\n",
      "Iteration 10296, loss = 2.57714787\n",
      "Iteration 10297, loss = 2.57709173\n",
      "Iteration 10298, loss = 2.57706958\n",
      "Iteration 10299, loss = 2.57696803\n",
      "Iteration 10300, loss = 2.57705956\n",
      "Iteration 10301, loss = 2.57699918\n",
      "Iteration 10302, loss = 2.57705635\n",
      "Iteration 10303, loss = 2.57691322\n",
      "Iteration 10304, loss = 2.57699747\n",
      "Iteration 10305, loss = 2.57703528\n",
      "Iteration 10306, loss = 2.57695162\n",
      "Iteration 10307, loss = 2.57696176\n",
      "Iteration 10308, loss = 2.57708049\n",
      "Iteration 10309, loss = 2.57687160\n",
      "Iteration 10310, loss = 2.57703872\n",
      "Iteration 10311, loss = 2.57701704\n",
      "Iteration 10312, loss = 2.57702404\n",
      "Iteration 10313, loss = 2.57678633\n",
      "Iteration 10314, loss = 2.57680126\n",
      "Iteration 10315, loss = 2.57676517\n",
      "Iteration 10316, loss = 2.57671767\n",
      "Iteration 10317, loss = 2.57681687\n",
      "Iteration 10318, loss = 2.57673735\n",
      "Iteration 10319, loss = 2.57672706\n",
      "Iteration 10320, loss = 2.57675663\n",
      "Iteration 10321, loss = 2.57671715\n",
      "Iteration 10322, loss = 2.57677529\n",
      "Iteration 10323, loss = 2.57673783\n",
      "Iteration 10324, loss = 2.57667436\n",
      "Iteration 10325, loss = 2.57669192\n",
      "Iteration 10326, loss = 2.57668974\n",
      "Iteration 10327, loss = 2.57684544\n",
      "Iteration 10328, loss = 2.57666724\n",
      "Iteration 10329, loss = 2.57671723\n",
      "Iteration 10330, loss = 2.57660656\n",
      "Iteration 10331, loss = 2.57660698\n",
      "Iteration 10332, loss = 2.57657902\n",
      "Iteration 10333, loss = 2.57654056\n",
      "Iteration 10334, loss = 2.57655196\n",
      "Iteration 10335, loss = 2.57655307\n",
      "Iteration 10336, loss = 2.57654698\n",
      "Iteration 10337, loss = 2.57655199\n",
      "Iteration 10338, loss = 2.57652379\n",
      "Iteration 10339, loss = 2.57645848\n",
      "Iteration 10340, loss = 2.57655860\n",
      "Iteration 10341, loss = 2.57643146\n",
      "Iteration 10342, loss = 2.57641775\n",
      "Iteration 10343, loss = 2.57642136\n",
      "Iteration 10344, loss = 2.57648436\n",
      "Iteration 10345, loss = 2.57646247\n",
      "Iteration 10346, loss = 2.57642279\n",
      "Iteration 10347, loss = 2.57640125\n",
      "Iteration 10348, loss = 2.57635825\n",
      "Iteration 10349, loss = 2.57638684\n",
      "Iteration 10350, loss = 2.57635401\n",
      "Iteration 10351, loss = 2.57633497\n",
      "Iteration 10352, loss = 2.57634998\n",
      "Iteration 10353, loss = 2.57631541\n",
      "Iteration 10354, loss = 2.57636385\n",
      "Iteration 10355, loss = 2.57629464\n",
      "Iteration 10356, loss = 2.57637282\n",
      "Iteration 10357, loss = 2.57630440\n",
      "Iteration 10358, loss = 2.57623052\n",
      "Iteration 10359, loss = 2.57623032\n",
      "Iteration 10360, loss = 2.57619238\n",
      "Iteration 10361, loss = 2.57617280\n",
      "Iteration 10362, loss = 2.57634571\n",
      "Iteration 10363, loss = 2.57623417\n",
      "Iteration 10364, loss = 2.57610671\n",
      "Iteration 10365, loss = 2.57619314\n",
      "Iteration 10366, loss = 2.57621271\n",
      "Iteration 10367, loss = 2.57615176\n",
      "Iteration 10368, loss = 2.57614516\n",
      "Iteration 10369, loss = 2.57631721\n",
      "Iteration 10370, loss = 2.57613324\n",
      "Iteration 10371, loss = 2.57621335\n",
      "Iteration 10372, loss = 2.57612517\n",
      "Iteration 10373, loss = 2.57608294\n",
      "Iteration 10374, loss = 2.57597838\n",
      "Iteration 10375, loss = 2.57620231\n",
      "Iteration 10376, loss = 2.57602418\n",
      "Iteration 10377, loss = 2.57614315\n",
      "Iteration 10378, loss = 2.57610720\n",
      "Iteration 10379, loss = 2.57600712\n",
      "Iteration 10380, loss = 2.57589847\n",
      "Iteration 10381, loss = 2.57603847\n",
      "Iteration 10382, loss = 2.57588764\n",
      "Iteration 10383, loss = 2.57593160\n",
      "Iteration 10384, loss = 2.57586502\n",
      "Iteration 10385, loss = 2.57586037\n",
      "Iteration 10386, loss = 2.57579762\n",
      "Iteration 10387, loss = 2.57595808\n",
      "Iteration 10388, loss = 2.57602814\n",
      "Iteration 10389, loss = 2.57581940\n",
      "Iteration 10390, loss = 2.57584886\n",
      "Iteration 10391, loss = 2.57579373\n",
      "Iteration 10392, loss = 2.57579576\n",
      "Iteration 10393, loss = 2.57587193\n",
      "Iteration 10394, loss = 2.57582921\n",
      "Iteration 10395, loss = 2.57581404\n",
      "Iteration 10396, loss = 2.57569614\n",
      "Iteration 10397, loss = 2.57576162\n",
      "Iteration 10398, loss = 2.57571176\n",
      "Iteration 10399, loss = 2.57569389\n",
      "Iteration 10400, loss = 2.57575088\n",
      "Iteration 10401, loss = 2.57569681\n",
      "Iteration 10402, loss = 2.57571487\n",
      "Iteration 10403, loss = 2.57568231\n",
      "Iteration 10404, loss = 2.57565247\n",
      "Iteration 10405, loss = 2.57569453\n",
      "Iteration 10406, loss = 2.57564920\n",
      "Iteration 10407, loss = 2.57556775\n",
      "Iteration 10408, loss = 2.57563782\n",
      "Iteration 10409, loss = 2.57561779\n",
      "Iteration 10410, loss = 2.57566616\n",
      "Iteration 10411, loss = 2.57549981\n",
      "Iteration 10412, loss = 2.57549082\n",
      "Iteration 10413, loss = 2.57553203\n",
      "Iteration 10414, loss = 2.57556831\n",
      "Iteration 10415, loss = 2.57564821\n",
      "Iteration 10416, loss = 2.57549633\n",
      "Iteration 10417, loss = 2.57544304\n",
      "Iteration 10418, loss = 2.57547727\n",
      "Iteration 10419, loss = 2.57538030\n",
      "Iteration 10420, loss = 2.57540461\n",
      "Iteration 10421, loss = 2.57540379\n",
      "Iteration 10422, loss = 2.57548262\n",
      "Iteration 10423, loss = 2.57542648\n",
      "Iteration 10424, loss = 2.57541870\n",
      "Iteration 10425, loss = 2.57545122\n",
      "Iteration 10426, loss = 2.57537078\n",
      "Iteration 10427, loss = 2.57533275\n",
      "Iteration 10428, loss = 2.57538424\n",
      "Iteration 10429, loss = 2.57570023\n",
      "Iteration 10430, loss = 2.57538634\n",
      "Iteration 10431, loss = 2.57524677\n",
      "Iteration 10432, loss = 2.57530529\n",
      "Iteration 10433, loss = 2.57551702\n",
      "Iteration 10434, loss = 2.57516188\n",
      "Iteration 10435, loss = 2.57533357\n",
      "Iteration 10436, loss = 2.57536223\n",
      "Iteration 10437, loss = 2.57515928\n",
      "Iteration 10438, loss = 2.57519284\n",
      "Iteration 10439, loss = 2.57520557\n",
      "Iteration 10440, loss = 2.57522460\n",
      "Iteration 10441, loss = 2.57515707\n",
      "Iteration 10442, loss = 2.57507773\n",
      "Iteration 10443, loss = 2.57516237\n",
      "Iteration 10444, loss = 2.57507247\n",
      "Iteration 10445, loss = 2.57512844\n",
      "Iteration 10446, loss = 2.57518905\n",
      "Iteration 10447, loss = 2.57507452\n",
      "Iteration 10448, loss = 2.57509386\n",
      "Iteration 10449, loss = 2.57511997\n",
      "Iteration 10450, loss = 2.57523373\n",
      "Iteration 10451, loss = 2.57498050\n",
      "Iteration 10452, loss = 2.57509598\n",
      "Iteration 10453, loss = 2.57502788\n",
      "Iteration 10454, loss = 2.57507615\n",
      "Iteration 10455, loss = 2.57496228\n",
      "Iteration 10456, loss = 2.57506279\n",
      "Iteration 10457, loss = 2.57496648\n",
      "Iteration 10458, loss = 2.57498770\n",
      "Iteration 10459, loss = 2.57495940\n",
      "Iteration 10460, loss = 2.57489898\n",
      "Iteration 10461, loss = 2.57496209\n",
      "Iteration 10462, loss = 2.57488834\n",
      "Iteration 10463, loss = 2.57485702\n",
      "Iteration 10464, loss = 2.57495969\n",
      "Iteration 10465, loss = 2.57496193\n",
      "Iteration 10466, loss = 2.57483236\n",
      "Iteration 10467, loss = 2.57485451\n",
      "Iteration 10468, loss = 2.57502420\n",
      "Iteration 10469, loss = 2.57477353\n",
      "Iteration 10470, loss = 2.57486927\n",
      "Iteration 10471, loss = 2.57488340\n",
      "Iteration 10472, loss = 2.57471174\n",
      "Iteration 10473, loss = 2.57470839\n",
      "Iteration 10474, loss = 2.57498527\n",
      "Iteration 10475, loss = 2.57470453\n",
      "Iteration 10476, loss = 2.57477539\n",
      "Iteration 10477, loss = 2.57471585\n",
      "Iteration 10478, loss = 2.57460433\n",
      "Iteration 10479, loss = 2.57497222\n",
      "Iteration 10480, loss = 2.57455210\n",
      "Iteration 10481, loss = 2.57463093\n",
      "Iteration 10482, loss = 2.57466745\n",
      "Iteration 10483, loss = 2.57449060\n",
      "Iteration 10484, loss = 2.57462269\n",
      "Iteration 10485, loss = 2.57464256\n",
      "Iteration 10486, loss = 2.57455804\n",
      "Iteration 10487, loss = 2.57464248\n",
      "Iteration 10488, loss = 2.57458312\n",
      "Iteration 10489, loss = 2.57455172\n",
      "Iteration 10490, loss = 2.57450422\n",
      "Iteration 10491, loss = 2.57447237\n",
      "Iteration 10492, loss = 2.57457314\n",
      "Iteration 10493, loss = 2.57447347\n",
      "Iteration 10494, loss = 2.57445991\n",
      "Iteration 10495, loss = 2.57449396\n",
      "Iteration 10496, loss = 2.57439809\n",
      "Iteration 10497, loss = 2.57449676\n",
      "Iteration 10498, loss = 2.57438818\n",
      "Iteration 10499, loss = 2.57438899\n",
      "Iteration 10500, loss = 2.57438867\n",
      "Iteration 10501, loss = 2.57433028\n",
      "Iteration 10502, loss = 2.57432621\n",
      "Iteration 10503, loss = 2.57434688\n",
      "Iteration 10504, loss = 2.57434290\n",
      "Iteration 10505, loss = 2.57433884\n",
      "Iteration 10506, loss = 2.57435623\n",
      "Iteration 10507, loss = 2.57430561\n",
      "Iteration 10508, loss = 2.57437531\n",
      "Iteration 10509, loss = 2.57436505\n",
      "Iteration 10510, loss = 2.57427901\n",
      "Iteration 10511, loss = 2.57465997\n",
      "Iteration 10512, loss = 2.57428197\n",
      "Iteration 10513, loss = 2.57430500\n",
      "Iteration 10514, loss = 2.57433836\n",
      "Iteration 10515, loss = 2.57431396\n",
      "Iteration 10516, loss = 2.57418508\n",
      "Iteration 10517, loss = 2.57416078\n",
      "Iteration 10518, loss = 2.57410297\n",
      "Iteration 10519, loss = 2.57413775\n",
      "Iteration 10520, loss = 2.57409516\n",
      "Iteration 10521, loss = 2.57413746\n",
      "Iteration 10522, loss = 2.57411285\n",
      "Iteration 10523, loss = 2.57442129\n",
      "Iteration 10524, loss = 2.57403863\n",
      "Iteration 10525, loss = 2.57410506\n",
      "Iteration 10526, loss = 2.57410099\n",
      "Iteration 10527, loss = 2.57405040\n",
      "Iteration 10528, loss = 2.57412908\n",
      "Iteration 10529, loss = 2.57399981\n",
      "Iteration 10530, loss = 2.57393515\n",
      "Iteration 10531, loss = 2.57399950\n",
      "Iteration 10532, loss = 2.57399055\n",
      "Iteration 10533, loss = 2.57395308\n",
      "Iteration 10534, loss = 2.57393021\n",
      "Iteration 10535, loss = 2.57391214\n",
      "Iteration 10536, loss = 2.57391214\n",
      "Iteration 10537, loss = 2.57386494\n",
      "Iteration 10538, loss = 2.57388174\n",
      "Iteration 10539, loss = 2.57398658\n",
      "Iteration 10540, loss = 2.57388367\n",
      "Iteration 10541, loss = 2.57379068\n",
      "Iteration 10542, loss = 2.57385180\n",
      "Iteration 10543, loss = 2.57393546\n",
      "Iteration 10544, loss = 2.57378027\n",
      "Iteration 10545, loss = 2.57383157\n",
      "Iteration 10546, loss = 2.57381569\n",
      "Iteration 10547, loss = 2.57378063\n",
      "Iteration 10548, loss = 2.57376543\n",
      "Iteration 10549, loss = 2.57382359\n",
      "Iteration 10550, loss = 2.57369118\n",
      "Iteration 10551, loss = 2.57372649\n",
      "Iteration 10552, loss = 2.57364236\n",
      "Iteration 10553, loss = 2.57380256\n",
      "Iteration 10554, loss = 2.57359658\n",
      "Iteration 10555, loss = 2.57367995\n",
      "Iteration 10556, loss = 2.57361864\n",
      "Iteration 10557, loss = 2.57361434\n",
      "Iteration 10558, loss = 2.57365283\n",
      "Iteration 10559, loss = 2.57359139\n",
      "Iteration 10560, loss = 2.57357350\n",
      "Iteration 10561, loss = 2.57360927\n",
      "Iteration 10562, loss = 2.57357788\n",
      "Iteration 10563, loss = 2.57360159\n",
      "Iteration 10564, loss = 2.57366992\n",
      "Iteration 10565, loss = 2.57363294\n",
      "Iteration 10566, loss = 2.57355931\n",
      "Iteration 10567, loss = 2.57353758\n",
      "Iteration 10568, loss = 2.57351525\n",
      "Iteration 10569, loss = 2.57344582\n",
      "Iteration 10570, loss = 2.57347184\n",
      "Iteration 10571, loss = 2.57349729\n",
      "Iteration 10572, loss = 2.57346995\n",
      "Iteration 10573, loss = 2.57344371\n",
      "Iteration 10574, loss = 2.57357247\n",
      "Iteration 10575, loss = 2.57351925\n",
      "Iteration 10576, loss = 2.57340165\n",
      "Iteration 10577, loss = 2.57337945\n",
      "Iteration 10578, loss = 2.57335315\n",
      "Iteration 10579, loss = 2.57332939\n",
      "Iteration 10580, loss = 2.57328501\n",
      "Iteration 10581, loss = 2.57336633\n",
      "Iteration 10582, loss = 2.57335023\n",
      "Iteration 10583, loss = 2.57328528\n",
      "Iteration 10584, loss = 2.57330523\n",
      "Iteration 10585, loss = 2.57326972\n",
      "Iteration 10586, loss = 2.57322596\n",
      "Iteration 10587, loss = 2.57324805\n",
      "Iteration 10588, loss = 2.57323336\n",
      "Iteration 10589, loss = 2.57319948\n",
      "Iteration 10590, loss = 2.57331122\n",
      "Iteration 10591, loss = 2.57321983\n",
      "Iteration 10592, loss = 2.57316189\n",
      "Iteration 10593, loss = 2.57316856\n",
      "Iteration 10594, loss = 2.57325604\n",
      "Iteration 10595, loss = 2.57312258\n",
      "Iteration 10596, loss = 2.57308028\n",
      "Iteration 10597, loss = 2.57312844\n",
      "Iteration 10598, loss = 2.57313396\n",
      "Iteration 10599, loss = 2.57315921\n",
      "Iteration 10600, loss = 2.57316854\n",
      "Iteration 10601, loss = 2.57306345\n",
      "Iteration 10602, loss = 2.57303647\n",
      "Iteration 10603, loss = 2.57305190\n",
      "Iteration 10604, loss = 2.57300553\n",
      "Iteration 10605, loss = 2.57304517\n",
      "Iteration 10606, loss = 2.57302909\n",
      "Iteration 10607, loss = 2.57304885\n",
      "Iteration 10608, loss = 2.57302358\n",
      "Iteration 10609, loss = 2.57303269\n",
      "Iteration 10610, loss = 2.57296032\n",
      "Iteration 10611, loss = 2.57304136\n",
      "Iteration 10612, loss = 2.57286872\n",
      "Iteration 10613, loss = 2.57281884\n",
      "Iteration 10614, loss = 2.57296784\n",
      "Iteration 10615, loss = 2.57290302\n",
      "Iteration 10616, loss = 2.57285964\n",
      "Iteration 10617, loss = 2.57280915\n",
      "Iteration 10618, loss = 2.57286586\n",
      "Iteration 10619, loss = 2.57294378\n",
      "Iteration 10620, loss = 2.57279026\n",
      "Iteration 10621, loss = 2.57277922\n",
      "Iteration 10622, loss = 2.57282814\n",
      "Iteration 10623, loss = 2.57275088\n",
      "Iteration 10624, loss = 2.57282533\n",
      "Iteration 10625, loss = 2.57274264\n",
      "Iteration 10626, loss = 2.57279472\n",
      "Iteration 10627, loss = 2.57268709\n",
      "Iteration 10628, loss = 2.57271440\n",
      "Iteration 10629, loss = 2.57280796\n",
      "Iteration 10630, loss = 2.57274265\n",
      "Iteration 10631, loss = 2.57268324\n",
      "Iteration 10632, loss = 2.57261734\n",
      "Iteration 10633, loss = 2.57259783\n",
      "Iteration 10634, loss = 2.57267978\n",
      "Iteration 10635, loss = 2.57269475\n",
      "Iteration 10636, loss = 2.57258028\n",
      "Iteration 10637, loss = 2.57257001\n",
      "Iteration 10638, loss = 2.57257907\n",
      "Iteration 10639, loss = 2.57252776\n",
      "Iteration 10640, loss = 2.57260123\n",
      "Iteration 10641, loss = 2.57255421\n",
      "Iteration 10642, loss = 2.57264764\n",
      "Iteration 10643, loss = 2.57252084\n",
      "Iteration 10644, loss = 2.57249160\n",
      "Iteration 10645, loss = 2.57268942\n",
      "Iteration 10646, loss = 2.57262822\n",
      "Iteration 10647, loss = 2.57252677\n",
      "Iteration 10648, loss = 2.57244525\n",
      "Iteration 10649, loss = 2.57248058\n",
      "Iteration 10650, loss = 2.57241574\n",
      "Iteration 10651, loss = 2.57243755\n",
      "Iteration 10652, loss = 2.57238727\n",
      "Iteration 10653, loss = 2.57238174\n",
      "Iteration 10654, loss = 2.57229552\n",
      "Iteration 10655, loss = 2.57242058\n",
      "Iteration 10656, loss = 2.57232667\n",
      "Iteration 10657, loss = 2.57231208\n",
      "Iteration 10658, loss = 2.57245895\n",
      "Iteration 10659, loss = 2.57235518\n",
      "Iteration 10660, loss = 2.57229973\n",
      "Iteration 10661, loss = 2.57238714\n",
      "Iteration 10662, loss = 2.57230438\n",
      "Iteration 10663, loss = 2.57226789\n",
      "Iteration 10664, loss = 2.57223969\n",
      "Iteration 10665, loss = 2.57229864\n",
      "Iteration 10666, loss = 2.57253456\n",
      "Iteration 10667, loss = 2.57220965\n",
      "Iteration 10668, loss = 2.57225063\n",
      "Iteration 10669, loss = 2.57216682\n",
      "Iteration 10670, loss = 2.57212892\n",
      "Iteration 10671, loss = 2.57214295\n",
      "Iteration 10672, loss = 2.57211919\n",
      "Iteration 10673, loss = 2.57215343\n",
      "Iteration 10674, loss = 2.57211519\n",
      "Iteration 10675, loss = 2.57207305\n",
      "Iteration 10676, loss = 2.57224674\n",
      "Iteration 10677, loss = 2.57207592\n",
      "Iteration 10678, loss = 2.57203860\n",
      "Iteration 10679, loss = 2.57209631\n",
      "Iteration 10680, loss = 2.57198047\n",
      "Iteration 10681, loss = 2.57208141\n",
      "Iteration 10682, loss = 2.57198881\n",
      "Iteration 10683, loss = 2.57198297\n",
      "Iteration 10684, loss = 2.57198147\n",
      "Iteration 10685, loss = 2.57200342\n",
      "Iteration 10686, loss = 2.57191035\n",
      "Iteration 10687, loss = 2.57192768\n",
      "Iteration 10688, loss = 2.57186729\n",
      "Iteration 10689, loss = 2.57192749\n",
      "Iteration 10690, loss = 2.57200038\n",
      "Iteration 10691, loss = 2.57196183\n",
      "Iteration 10692, loss = 2.57184181\n",
      "Iteration 10693, loss = 2.57182889\n",
      "Iteration 10694, loss = 2.57189613\n",
      "Iteration 10695, loss = 2.57185812\n",
      "Iteration 10696, loss = 2.57193665\n",
      "Iteration 10697, loss = 2.57188124\n",
      "Iteration 10698, loss = 2.57182727\n",
      "Iteration 10699, loss = 2.57177203\n",
      "Iteration 10700, loss = 2.57180620\n",
      "Iteration 10701, loss = 2.57168325\n",
      "Iteration 10702, loss = 2.57207215\n",
      "Iteration 10703, loss = 2.57166921\n",
      "Iteration 10704, loss = 2.57172254\n",
      "Iteration 10705, loss = 2.57173874\n",
      "Iteration 10706, loss = 2.57170311\n",
      "Iteration 10707, loss = 2.57169576\n",
      "Iteration 10708, loss = 2.57167991\n",
      "Iteration 10709, loss = 2.57174889\n",
      "Iteration 10710, loss = 2.57171446\n",
      "Iteration 10711, loss = 2.57166687\n",
      "Iteration 10712, loss = 2.57160508\n",
      "Iteration 10713, loss = 2.57176933\n",
      "Iteration 10714, loss = 2.57185060\n",
      "Iteration 10715, loss = 2.57159592\n",
      "Iteration 10716, loss = 2.57159255\n",
      "Iteration 10717, loss = 2.57155442\n",
      "Iteration 10718, loss = 2.57169907\n",
      "Iteration 10719, loss = 2.57170787\n",
      "Iteration 10720, loss = 2.57174820\n",
      "Iteration 10721, loss = 2.57156639\n",
      "Iteration 10722, loss = 2.57161518\n",
      "Iteration 10723, loss = 2.57153644\n",
      "Iteration 10724, loss = 2.57147411\n",
      "Iteration 10725, loss = 2.57145956\n",
      "Iteration 10726, loss = 2.57147317\n",
      "Iteration 10727, loss = 2.57149147\n",
      "Iteration 10728, loss = 2.57138646\n",
      "Iteration 10729, loss = 2.57141803\n",
      "Iteration 10730, loss = 2.57132658\n",
      "Iteration 10731, loss = 2.57145211\n",
      "Iteration 10732, loss = 2.57135796\n",
      "Iteration 10733, loss = 2.57125973\n",
      "Iteration 10734, loss = 2.57133951\n",
      "Iteration 10735, loss = 2.57127070\n",
      "Iteration 10736, loss = 2.57145462\n",
      "Iteration 10737, loss = 2.57133893\n",
      "Iteration 10738, loss = 2.57146261\n",
      "Iteration 10739, loss = 2.57125551\n",
      "Iteration 10740, loss = 2.57131359\n",
      "Iteration 10741, loss = 2.57124608\n",
      "Iteration 10742, loss = 2.57122170\n",
      "Iteration 10743, loss = 2.57117896\n",
      "Iteration 10744, loss = 2.57114314\n",
      "Iteration 10745, loss = 2.57117096\n",
      "Iteration 10746, loss = 2.57116840\n",
      "Iteration 10747, loss = 2.57108477\n",
      "Iteration 10748, loss = 2.57108069\n",
      "Iteration 10749, loss = 2.57111478\n",
      "Iteration 10750, loss = 2.57114733\n",
      "Iteration 10751, loss = 2.57125732\n",
      "Iteration 10752, loss = 2.57115172\n",
      "Iteration 10753, loss = 2.57116020\n",
      "Iteration 10754, loss = 2.57110404\n",
      "Iteration 10755, loss = 2.57105996\n",
      "Iteration 10756, loss = 2.57106103\n",
      "Iteration 10757, loss = 2.57100103\n",
      "Iteration 10758, loss = 2.57099002\n",
      "Iteration 10759, loss = 2.57099058\n",
      "Iteration 10760, loss = 2.57100568\n",
      "Iteration 10761, loss = 2.57108368\n",
      "Iteration 10762, loss = 2.57083775\n",
      "Iteration 10763, loss = 2.57092909\n",
      "Iteration 10764, loss = 2.57090134\n",
      "Iteration 10765, loss = 2.57083998\n",
      "Iteration 10766, loss = 2.57095257\n",
      "Iteration 10767, loss = 2.57087956\n",
      "Iteration 10768, loss = 2.57090479\n",
      "Iteration 10769, loss = 2.57105111\n",
      "Iteration 10770, loss = 2.57080019\n",
      "Iteration 10771, loss = 2.57086222\n",
      "Iteration 10772, loss = 2.57083634\n",
      "Iteration 10773, loss = 2.57074381\n",
      "Iteration 10774, loss = 2.57079954\n",
      "Iteration 10775, loss = 2.57082374\n",
      "Iteration 10776, loss = 2.57074487\n",
      "Iteration 10777, loss = 2.57078442\n",
      "Iteration 10778, loss = 2.57077999\n",
      "Iteration 10779, loss = 2.57075473\n",
      "Iteration 10780, loss = 2.57070273\n",
      "Iteration 10781, loss = 2.57064929\n",
      "Iteration 10782, loss = 2.57066647\n",
      "Iteration 10783, loss = 2.57076660\n",
      "Iteration 10784, loss = 2.57071711\n",
      "Iteration 10785, loss = 2.57065665\n",
      "Iteration 10786, loss = 2.57057046\n",
      "Iteration 10787, loss = 2.57059357\n",
      "Iteration 10788, loss = 2.57061460\n",
      "Iteration 10789, loss = 2.57066634\n",
      "Iteration 10790, loss = 2.57058506\n",
      "Iteration 10791, loss = 2.57057783\n",
      "Iteration 10792, loss = 2.57060712\n",
      "Iteration 10793, loss = 2.57052301\n",
      "Iteration 10794, loss = 2.57054288\n",
      "Iteration 10795, loss = 2.57048462\n",
      "Iteration 10796, loss = 2.57055128\n",
      "Iteration 10797, loss = 2.57045551\n",
      "Iteration 10798, loss = 2.57058943\n",
      "Iteration 10799, loss = 2.57053232\n",
      "Iteration 10800, loss = 2.57052091\n",
      "Iteration 10801, loss = 2.57040772\n",
      "Iteration 10802, loss = 2.57045856\n",
      "Iteration 10803, loss = 2.57043252\n",
      "Iteration 10804, loss = 2.57037475\n",
      "Iteration 10805, loss = 2.57037747\n",
      "Iteration 10806, loss = 2.57034429\n",
      "Iteration 10807, loss = 2.57050315\n",
      "Iteration 10808, loss = 2.57045002\n",
      "Iteration 10809, loss = 2.57042463\n",
      "Iteration 10810, loss = 2.57032182\n",
      "Iteration 10811, loss = 2.57050799\n",
      "Iteration 10812, loss = 2.57028739\n",
      "Iteration 10813, loss = 2.57021217\n",
      "Iteration 10814, loss = 2.57027042\n",
      "Iteration 10815, loss = 2.57026099\n",
      "Iteration 10816, loss = 2.57042883\n",
      "Iteration 10817, loss = 2.57033449\n",
      "Iteration 10818, loss = 2.57036954\n",
      "Iteration 10819, loss = 2.57021704\n",
      "Iteration 10820, loss = 2.57021114\n",
      "Iteration 10821, loss = 2.57020572\n",
      "Iteration 10822, loss = 2.57009067\n",
      "Iteration 10823, loss = 2.57012488\n",
      "Iteration 10824, loss = 2.57032435\n",
      "Iteration 10825, loss = 2.57017378\n",
      "Iteration 10826, loss = 2.57004537\n",
      "Iteration 10827, loss = 2.57012725\n",
      "Iteration 10828, loss = 2.57008552\n",
      "Iteration 10829, loss = 2.57011101\n",
      "Iteration 10830, loss = 2.57006245\n",
      "Iteration 10831, loss = 2.57007543\n",
      "Iteration 10832, loss = 2.57004505\n",
      "Iteration 10833, loss = 2.57004066\n",
      "Iteration 10834, loss = 2.57007004\n",
      "Iteration 10835, loss = 2.57000443\n",
      "Iteration 10836, loss = 2.57003979\n",
      "Iteration 10837, loss = 2.57012171\n",
      "Iteration 10838, loss = 2.56998247\n",
      "Iteration 10839, loss = 2.57015451\n",
      "Iteration 10840, loss = 2.57001609\n",
      "Iteration 10841, loss = 2.56999880\n",
      "Iteration 10842, loss = 2.57003989\n",
      "Iteration 10843, loss = 2.56988896\n",
      "Iteration 10844, loss = 2.56999486\n",
      "Iteration 10845, loss = 2.56990390\n",
      "Iteration 10846, loss = 2.56983243\n",
      "Iteration 10847, loss = 2.56996889\n",
      "Iteration 10848, loss = 2.56991117\n",
      "Iteration 10849, loss = 2.56979015\n",
      "Iteration 10850, loss = 2.56974381\n",
      "Iteration 10851, loss = 2.56984343\n",
      "Iteration 10852, loss = 2.56970849\n",
      "Iteration 10853, loss = 2.56965450\n",
      "Iteration 10854, loss = 2.56978095\n",
      "Iteration 10855, loss = 2.56985372\n",
      "Iteration 10856, loss = 2.56969165\n",
      "Iteration 10857, loss = 2.56967622\n",
      "Iteration 10858, loss = 2.56987369\n",
      "Iteration 10859, loss = 2.56975749\n",
      "Iteration 10860, loss = 2.56961572\n",
      "Iteration 10861, loss = 2.56961397\n",
      "Iteration 10862, loss = 2.56966797\n",
      "Iteration 10863, loss = 2.56955075\n",
      "Iteration 10864, loss = 2.56958038\n",
      "Iteration 10865, loss = 2.56959165\n",
      "Iteration 10866, loss = 2.56966835\n",
      "Iteration 10867, loss = 2.56952796\n",
      "Iteration 10868, loss = 2.56955463\n",
      "Iteration 10869, loss = 2.56958500\n",
      "Iteration 10870, loss = 2.56955475\n",
      "Iteration 10871, loss = 2.56959789\n",
      "Iteration 10872, loss = 2.56957456\n",
      "Iteration 10873, loss = 2.56947327\n",
      "Iteration 10874, loss = 2.56940042\n",
      "Iteration 10875, loss = 2.56948490\n",
      "Iteration 10876, loss = 2.56949304\n",
      "Iteration 10877, loss = 2.56941953\n",
      "Iteration 10878, loss = 2.56942633\n",
      "Iteration 10879, loss = 2.56943897\n",
      "Iteration 10880, loss = 2.56936807\n",
      "Iteration 10881, loss = 2.56939536\n",
      "Iteration 10882, loss = 2.56940741\n",
      "Iteration 10883, loss = 2.56931836\n",
      "Iteration 10884, loss = 2.56931974\n",
      "Iteration 10885, loss = 2.56931938\n",
      "Iteration 10886, loss = 2.56940196\n",
      "Iteration 10887, loss = 2.56947881\n",
      "Iteration 10888, loss = 2.56934213\n",
      "Iteration 10889, loss = 2.56927128\n",
      "Iteration 10890, loss = 2.56927598\n",
      "Iteration 10891, loss = 2.56923052\n",
      "Iteration 10892, loss = 2.56919427\n",
      "Iteration 10893, loss = 2.56923860\n",
      "Iteration 10894, loss = 2.56922151\n",
      "Iteration 10895, loss = 2.56924400\n",
      "Iteration 10896, loss = 2.56914694\n",
      "Iteration 10897, loss = 2.56938493\n",
      "Iteration 10898, loss = 2.56921375\n",
      "Iteration 10899, loss = 2.56934861\n",
      "Iteration 10900, loss = 2.56920998\n",
      "Iteration 10901, loss = 2.56913803\n",
      "Iteration 10902, loss = 2.56920878\n",
      "Iteration 10903, loss = 2.56911555\n",
      "Iteration 10904, loss = 2.56904187\n",
      "Iteration 10905, loss = 2.56922385\n",
      "Iteration 10906, loss = 2.56898785\n",
      "Iteration 10907, loss = 2.56898917\n",
      "Iteration 10908, loss = 2.56908489\n",
      "Iteration 10909, loss = 2.56901679\n",
      "Iteration 10910, loss = 2.56893142\n",
      "Iteration 10911, loss = 2.56913524\n",
      "Iteration 10912, loss = 2.56903014\n",
      "Iteration 10913, loss = 2.56895977\n",
      "Iteration 10914, loss = 2.56890437\n",
      "Iteration 10915, loss = 2.56894044\n",
      "Iteration 10916, loss = 2.56896734\n",
      "Iteration 10917, loss = 2.56894988\n",
      "Iteration 10918, loss = 2.56882708\n",
      "Iteration 10919, loss = 2.56888537\n",
      "Iteration 10920, loss = 2.56896034\n",
      "Iteration 10921, loss = 2.56889461\n",
      "Iteration 10922, loss = 2.56903693\n",
      "Iteration 10923, loss = 2.56883290\n",
      "Iteration 10924, loss = 2.56893349\n",
      "Iteration 10925, loss = 2.56902707\n",
      "Iteration 10926, loss = 2.56883430\n",
      "Iteration 10927, loss = 2.56890953\n",
      "Iteration 10928, loss = 2.56884492\n",
      "Iteration 10929, loss = 2.56879098\n",
      "Iteration 10930, loss = 2.56876982\n",
      "Iteration 10931, loss = 2.56868087\n",
      "Iteration 10932, loss = 2.56869424\n",
      "Iteration 10933, loss = 2.56878272\n",
      "Iteration 10934, loss = 2.56865413\n",
      "Iteration 10935, loss = 2.56870391\n",
      "Iteration 10936, loss = 2.56883831\n",
      "Iteration 10937, loss = 2.56870492\n",
      "Iteration 10938, loss = 2.56865735\n",
      "Iteration 10939, loss = 2.56864108\n",
      "Iteration 10940, loss = 2.56863311\n",
      "Iteration 10941, loss = 2.56854299\n",
      "Iteration 10942, loss = 2.56855398\n",
      "Iteration 10943, loss = 2.56885405\n",
      "Iteration 10944, loss = 2.56847127\n",
      "Iteration 10945, loss = 2.56856502\n",
      "Iteration 10946, loss = 2.56853491\n",
      "Iteration 10947, loss = 2.56854459\n",
      "Iteration 10948, loss = 2.56858536\n",
      "Iteration 10949, loss = 2.56843420\n",
      "Iteration 10950, loss = 2.56844594\n",
      "Iteration 10951, loss = 2.56846232\n",
      "Iteration 10952, loss = 2.56860427\n",
      "Iteration 10953, loss = 2.56838431\n",
      "Iteration 10954, loss = 2.56845306\n",
      "Iteration 10955, loss = 2.56849494\n",
      "Iteration 10956, loss = 2.56847837\n",
      "Iteration 10957, loss = 2.56843285\n",
      "Iteration 10958, loss = 2.56843010\n",
      "Iteration 10959, loss = 2.56835246\n",
      "Iteration 10960, loss = 2.56844443\n",
      "Iteration 10961, loss = 2.56849511\n",
      "Iteration 10962, loss = 2.56834470\n",
      "Iteration 10963, loss = 2.56842819\n",
      "Iteration 10964, loss = 2.56823803\n",
      "Iteration 10965, loss = 2.56827170\n",
      "Iteration 10966, loss = 2.56825409\n",
      "Iteration 10967, loss = 2.56831504\n",
      "Iteration 10968, loss = 2.56824192\n",
      "Iteration 10969, loss = 2.56823501\n",
      "Iteration 10970, loss = 2.56825053\n",
      "Iteration 10971, loss = 2.56821876\n",
      "Iteration 10972, loss = 2.56819221\n",
      "Iteration 10973, loss = 2.56813509\n",
      "Iteration 10974, loss = 2.56814639\n",
      "Iteration 10975, loss = 2.56816163\n",
      "Iteration 10976, loss = 2.56806389\n",
      "Iteration 10977, loss = 2.56812436\n",
      "Iteration 10978, loss = 2.56808926\n",
      "Iteration 10979, loss = 2.56800256\n",
      "Iteration 10980, loss = 2.56809837\n",
      "Iteration 10981, loss = 2.56803303\n",
      "Iteration 10982, loss = 2.56798123\n",
      "Iteration 10983, loss = 2.56819917\n",
      "Iteration 10984, loss = 2.56802421\n",
      "Iteration 10985, loss = 2.56805933\n",
      "Iteration 10986, loss = 2.56799132\n",
      "Iteration 10987, loss = 2.56804400\n",
      "Iteration 10988, loss = 2.56797557\n",
      "Iteration 10989, loss = 2.56791769\n",
      "Iteration 10990, loss = 2.56786655\n",
      "Iteration 10991, loss = 2.56794347\n",
      "Iteration 10992, loss = 2.56787816\n",
      "Iteration 10993, loss = 2.56797017\n",
      "Iteration 10994, loss = 2.56789454\n",
      "Iteration 10995, loss = 2.56793155\n",
      "Iteration 10996, loss = 2.56802065\n",
      "Iteration 10997, loss = 2.56781477\n",
      "Iteration 10998, loss = 2.56799432\n",
      "Iteration 10999, loss = 2.56782354\n",
      "Iteration 11000, loss = 2.56773235\n",
      "Iteration 11001, loss = 2.56785026\n",
      "Iteration 11002, loss = 2.56780457\n",
      "Iteration 11003, loss = 2.56770576\n",
      "Iteration 11004, loss = 2.56804999\n",
      "Iteration 11005, loss = 2.56776528\n",
      "Iteration 11006, loss = 2.56771970\n",
      "Iteration 11007, loss = 2.56774363\n",
      "Iteration 11008, loss = 2.56767405\n",
      "Iteration 11009, loss = 2.56766816\n",
      "Iteration 11010, loss = 2.56767420\n",
      "Iteration 11011, loss = 2.56774031\n",
      "Iteration 11012, loss = 2.56794597\n",
      "Iteration 11013, loss = 2.56774233\n",
      "Iteration 11014, loss = 2.56768488\n",
      "Iteration 11015, loss = 2.56757172\n",
      "Iteration 11016, loss = 2.56758004\n",
      "Iteration 11017, loss = 2.56762234\n",
      "Iteration 11018, loss = 2.56757962\n",
      "Iteration 11019, loss = 2.56764733\n",
      "Iteration 11020, loss = 2.56760193\n",
      "Iteration 11021, loss = 2.56759081\n",
      "Iteration 11022, loss = 2.56764119\n",
      "Iteration 11023, loss = 2.56755792\n",
      "Iteration 11024, loss = 2.56765433\n",
      "Iteration 11025, loss = 2.56754711\n",
      "Iteration 11026, loss = 2.56769233\n",
      "Iteration 11027, loss = 2.56741134\n",
      "Iteration 11028, loss = 2.56751711\n",
      "Iteration 11029, loss = 2.56740211\n",
      "Iteration 11030, loss = 2.56756681\n",
      "Iteration 11031, loss = 2.56736368\n",
      "Iteration 11032, loss = 2.56756422\n",
      "Iteration 11033, loss = 2.56750734\n",
      "Iteration 11034, loss = 2.56744420\n",
      "Iteration 11035, loss = 2.56737411\n",
      "Iteration 11036, loss = 2.56738391\n",
      "Iteration 11037, loss = 2.56731019\n",
      "Iteration 11038, loss = 2.56736858\n",
      "Iteration 11039, loss = 2.56723664\n",
      "Iteration 11040, loss = 2.56740154\n",
      "Iteration 11041, loss = 2.56731280\n",
      "Iteration 11042, loss = 2.56724457\n",
      "Iteration 11043, loss = 2.56728702\n",
      "Iteration 11044, loss = 2.56751828\n",
      "Iteration 11045, loss = 2.56740432\n",
      "Iteration 11046, loss = 2.56737495\n",
      "Iteration 11047, loss = 2.56715364\n",
      "Iteration 11048, loss = 2.56725477\n",
      "Iteration 11049, loss = 2.56721206\n",
      "Iteration 11050, loss = 2.56708658\n",
      "Iteration 11051, loss = 2.56716938\n",
      "Iteration 11052, loss = 2.56714138\n",
      "Iteration 11053, loss = 2.56707435\n",
      "Iteration 11054, loss = 2.56715867\n",
      "Iteration 11055, loss = 2.56705018\n",
      "Iteration 11056, loss = 2.56704760\n",
      "Iteration 11057, loss = 2.56706527\n",
      "Iteration 11058, loss = 2.56700494\n",
      "Iteration 11059, loss = 2.56700253\n",
      "Iteration 11060, loss = 2.56704321\n",
      "Iteration 11061, loss = 2.56705275\n",
      "Iteration 11062, loss = 2.56708995\n",
      "Iteration 11063, loss = 2.56700562\n",
      "Iteration 11064, loss = 2.56696529\n",
      "Iteration 11065, loss = 2.56697296\n",
      "Iteration 11066, loss = 2.56702473\n",
      "Iteration 11067, loss = 2.56686677\n",
      "Iteration 11068, loss = 2.56691672\n",
      "Iteration 11069, loss = 2.56687840\n",
      "Iteration 11070, loss = 2.56689387\n",
      "Iteration 11071, loss = 2.56684948\n",
      "Iteration 11072, loss = 2.56687740\n",
      "Iteration 11073, loss = 2.56678618\n",
      "Iteration 11074, loss = 2.56703007\n",
      "Iteration 11075, loss = 2.56678397\n",
      "Iteration 11076, loss = 2.56691726\n",
      "Iteration 11077, loss = 2.56696742\n",
      "Iteration 11078, loss = 2.56680894\n",
      "Iteration 11079, loss = 2.56682875\n",
      "Iteration 11080, loss = 2.56674216\n",
      "Iteration 11081, loss = 2.56689337\n",
      "Iteration 11082, loss = 2.56673990\n",
      "Iteration 11083, loss = 2.56679480\n",
      "Iteration 11084, loss = 2.56665218\n",
      "Iteration 11085, loss = 2.56702314\n",
      "Iteration 11086, loss = 2.56677135\n",
      "Iteration 11087, loss = 2.56673019\n",
      "Iteration 11088, loss = 2.56662957\n",
      "Iteration 11089, loss = 2.56658688\n",
      "Iteration 11090, loss = 2.56658964\n",
      "Iteration 11091, loss = 2.56654057\n",
      "Iteration 11092, loss = 2.56655872\n",
      "Iteration 11093, loss = 2.56655422\n",
      "Iteration 11094, loss = 2.56664142\n",
      "Iteration 11095, loss = 2.56655970\n",
      "Iteration 11096, loss = 2.56664340\n",
      "Iteration 11097, loss = 2.56659721\n",
      "Iteration 11098, loss = 2.56661128\n",
      "Iteration 11099, loss = 2.56646555\n",
      "Iteration 11100, loss = 2.56653735\n",
      "Iteration 11101, loss = 2.56651495\n",
      "Iteration 11102, loss = 2.56656895\n",
      "Iteration 11103, loss = 2.56646441\n",
      "Iteration 11104, loss = 2.56641594\n",
      "Iteration 11105, loss = 2.56648010\n",
      "Iteration 11106, loss = 2.56643846\n",
      "Iteration 11107, loss = 2.56643069\n",
      "Iteration 11108, loss = 2.56668512\n",
      "Iteration 11109, loss = 2.56673335\n",
      "Iteration 11110, loss = 2.56633710\n",
      "Iteration 11111, loss = 2.56625895\n",
      "Iteration 11112, loss = 2.56633026\n",
      "Iteration 11113, loss = 2.56646070\n",
      "Iteration 11114, loss = 2.56625154\n",
      "Iteration 11115, loss = 2.56627898\n",
      "Iteration 11116, loss = 2.56628157\n",
      "Iteration 11117, loss = 2.56620781\n",
      "Iteration 11118, loss = 2.56630567\n",
      "Iteration 11119, loss = 2.56627841\n",
      "Iteration 11120, loss = 2.56626054\n",
      "Iteration 11121, loss = 2.56618769\n",
      "Iteration 11122, loss = 2.56622897\n",
      "Iteration 11123, loss = 2.56612737\n",
      "Iteration 11124, loss = 2.56616697\n",
      "Iteration 11125, loss = 2.56614217\n",
      "Iteration 11126, loss = 2.56613108\n",
      "Iteration 11127, loss = 2.56608931\n",
      "Iteration 11128, loss = 2.56623800\n",
      "Iteration 11129, loss = 2.56612004\n",
      "Iteration 11130, loss = 2.56618589\n",
      "Iteration 11131, loss = 2.56627478\n",
      "Iteration 11132, loss = 2.56605354\n",
      "Iteration 11133, loss = 2.56607436\n",
      "Iteration 11134, loss = 2.56610457\n",
      "Iteration 11135, loss = 2.56600715\n",
      "Iteration 11136, loss = 2.56622675\n",
      "Iteration 11137, loss = 2.56609493\n",
      "Iteration 11138, loss = 2.56595779\n",
      "Iteration 11139, loss = 2.56597968\n",
      "Iteration 11140, loss = 2.56593945\n",
      "Iteration 11141, loss = 2.56588597\n",
      "Iteration 11142, loss = 2.56591001\n",
      "Iteration 11143, loss = 2.56587561\n",
      "Iteration 11144, loss = 2.56588912\n",
      "Iteration 11145, loss = 2.56586192\n",
      "Iteration 11146, loss = 2.56584441\n",
      "Iteration 11147, loss = 2.56586936\n",
      "Iteration 11148, loss = 2.56587408\n",
      "Iteration 11149, loss = 2.56582558\n",
      "Iteration 11150, loss = 2.56580541\n",
      "Iteration 11151, loss = 2.56584674\n",
      "Iteration 11152, loss = 2.56579660\n",
      "Iteration 11153, loss = 2.56588562\n",
      "Iteration 11154, loss = 2.56568540\n",
      "Iteration 11155, loss = 2.56606043\n",
      "Iteration 11156, loss = 2.56580090\n",
      "Iteration 11157, loss = 2.56584565\n",
      "Iteration 11158, loss = 2.56568302\n",
      "Iteration 11159, loss = 2.56572894\n",
      "Iteration 11160, loss = 2.56565684\n",
      "Iteration 11161, loss = 2.56573273\n",
      "Iteration 11162, loss = 2.56561936\n",
      "Iteration 11163, loss = 2.56559614\n",
      "Iteration 11164, loss = 2.56565860\n",
      "Iteration 11165, loss = 2.56554399\n",
      "Iteration 11166, loss = 2.56566305\n",
      "Iteration 11167, loss = 2.56556618\n",
      "Iteration 11168, loss = 2.56563574\n",
      "Iteration 11169, loss = 2.56555221\n",
      "Iteration 11170, loss = 2.56556600\n",
      "Iteration 11171, loss = 2.56548927\n",
      "Iteration 11172, loss = 2.56558222\n",
      "Iteration 11173, loss = 2.56557363\n",
      "Iteration 11174, loss = 2.56541687\n",
      "Iteration 11175, loss = 2.56549463\n",
      "Iteration 11176, loss = 2.56556441\n",
      "Iteration 11177, loss = 2.56544534\n",
      "Iteration 11178, loss = 2.56538225\n",
      "Iteration 11179, loss = 2.56539195\n",
      "Iteration 11180, loss = 2.56547346\n",
      "Iteration 11181, loss = 2.56539261\n",
      "Iteration 11182, loss = 2.56548191\n",
      "Iteration 11183, loss = 2.56532561\n",
      "Iteration 11184, loss = 2.56549244\n",
      "Iteration 11185, loss = 2.56531086\n",
      "Iteration 11186, loss = 2.56538031\n",
      "Iteration 11187, loss = 2.56529321\n",
      "Iteration 11188, loss = 2.56523678\n",
      "Iteration 11189, loss = 2.56550054\n",
      "Iteration 11190, loss = 2.56542061\n",
      "Iteration 11191, loss = 2.56526582\n",
      "Iteration 11192, loss = 2.56527648\n",
      "Iteration 11193, loss = 2.56524086\n",
      "Iteration 11194, loss = 2.56524023\n",
      "Iteration 11195, loss = 2.56520685\n",
      "Iteration 11196, loss = 2.56522465\n",
      "Iteration 11197, loss = 2.56516341\n",
      "Iteration 11198, loss = 2.56516910\n",
      "Iteration 11199, loss = 2.56513045\n",
      "Iteration 11200, loss = 2.56507433\n",
      "Iteration 11201, loss = 2.56515347\n",
      "Iteration 11202, loss = 2.56503563\n",
      "Iteration 11203, loss = 2.56505136\n",
      "Iteration 11204, loss = 2.56514134\n",
      "Iteration 11205, loss = 2.56513746\n",
      "Iteration 11206, loss = 2.56507675\n",
      "Iteration 11207, loss = 2.56503209\n",
      "Iteration 11208, loss = 2.56506881\n",
      "Iteration 11209, loss = 2.56509884\n",
      "Iteration 11210, loss = 2.56503458\n",
      "Iteration 11211, loss = 2.56497621\n",
      "Iteration 11212, loss = 2.56516794\n",
      "Iteration 11213, loss = 2.56488421\n",
      "Iteration 11214, loss = 2.56496754\n",
      "Iteration 11215, loss = 2.56497918\n",
      "Iteration 11216, loss = 2.56492099\n",
      "Iteration 11217, loss = 2.56482582\n",
      "Iteration 11218, loss = 2.56501856\n",
      "Iteration 11219, loss = 2.56492913\n",
      "Iteration 11220, loss = 2.56489742\n",
      "Iteration 11221, loss = 2.56491194\n",
      "Iteration 11222, loss = 2.56483371\n",
      "Iteration 11223, loss = 2.56477902\n",
      "Iteration 11224, loss = 2.56477571\n",
      "Iteration 11225, loss = 2.56477970\n",
      "Iteration 11226, loss = 2.56478720\n",
      "Iteration 11227, loss = 2.56477857\n",
      "Iteration 11228, loss = 2.56481878\n",
      "Iteration 11229, loss = 2.56484895\n",
      "Iteration 11230, loss = 2.56468068\n",
      "Iteration 11231, loss = 2.56481803\n",
      "Iteration 11232, loss = 2.56473744\n",
      "Iteration 11233, loss = 2.56474141\n",
      "Iteration 11234, loss = 2.56486848\n",
      "Iteration 11235, loss = 2.56466270\n",
      "Iteration 11236, loss = 2.56481888\n",
      "Iteration 11237, loss = 2.56460886\n",
      "Iteration 11238, loss = 2.56460472\n",
      "Iteration 11239, loss = 2.56461146\n",
      "Iteration 11240, loss = 2.56459485\n",
      "Iteration 11241, loss = 2.56461972\n",
      "Iteration 11242, loss = 2.56459090\n",
      "Iteration 11243, loss = 2.56467984\n",
      "Iteration 11244, loss = 2.56489109\n",
      "Iteration 11245, loss = 2.56450367\n",
      "Iteration 11246, loss = 2.56454613\n",
      "Iteration 11247, loss = 2.56450381\n",
      "Iteration 11248, loss = 2.56455702\n",
      "Iteration 11249, loss = 2.56463901\n",
      "Iteration 11250, loss = 2.56450492\n",
      "Iteration 11251, loss = 2.56472914\n",
      "Iteration 11252, loss = 2.56469086\n",
      "Iteration 11253, loss = 2.56440551\n",
      "Iteration 11254, loss = 2.56440453\n",
      "Iteration 11255, loss = 2.56435231\n",
      "Iteration 11256, loss = 2.56434018\n",
      "Iteration 11257, loss = 2.56437142\n",
      "Iteration 11258, loss = 2.56442849\n",
      "Iteration 11259, loss = 2.56438724\n",
      "Iteration 11260, loss = 2.56430305\n",
      "Iteration 11261, loss = 2.56434826\n",
      "Iteration 11262, loss = 2.56429370\n",
      "Iteration 11263, loss = 2.56436118\n",
      "Iteration 11264, loss = 2.56432262\n",
      "Iteration 11265, loss = 2.56426263\n",
      "Iteration 11266, loss = 2.56420477\n",
      "Iteration 11267, loss = 2.56416165\n",
      "Iteration 11268, loss = 2.56422343\n",
      "Iteration 11269, loss = 2.56439053\n",
      "Iteration 11270, loss = 2.56414825\n",
      "Iteration 11271, loss = 2.56419293\n",
      "Iteration 11272, loss = 2.56416655\n",
      "Iteration 11273, loss = 2.56417596\n",
      "Iteration 11274, loss = 2.56414415\n",
      "Iteration 11275, loss = 2.56416897\n",
      "Iteration 11276, loss = 2.56412580\n",
      "Iteration 11277, loss = 2.56422631\n",
      "Iteration 11278, loss = 2.56416569\n",
      "Iteration 11279, loss = 2.56404078\n",
      "Iteration 11280, loss = 2.56411466\n",
      "Iteration 11281, loss = 2.56415765\n",
      "Iteration 11282, loss = 2.56426641\n",
      "Iteration 11283, loss = 2.56413971\n",
      "Iteration 11284, loss = 2.56403915\n",
      "Iteration 11285, loss = 2.56403656\n",
      "Iteration 11286, loss = 2.56398966\n",
      "Iteration 11287, loss = 2.56397033\n",
      "Iteration 11288, loss = 2.56398920\n",
      "Iteration 11289, loss = 2.56402774\n",
      "Iteration 11290, loss = 2.56388968\n",
      "Iteration 11291, loss = 2.56393242\n",
      "Iteration 11292, loss = 2.56392375\n",
      "Iteration 11293, loss = 2.56398887\n",
      "Iteration 11294, loss = 2.56385562\n",
      "Iteration 11295, loss = 2.56381031\n",
      "Iteration 11296, loss = 2.56379672\n",
      "Iteration 11297, loss = 2.56404170\n",
      "Iteration 11298, loss = 2.56376768\n",
      "Iteration 11299, loss = 2.56386164\n",
      "Iteration 11300, loss = 2.56374384\n",
      "Iteration 11301, loss = 2.56387042\n",
      "Iteration 11302, loss = 2.56379986\n",
      "Iteration 11303, loss = 2.56377439\n",
      "Iteration 11304, loss = 2.56376793\n",
      "Iteration 11305, loss = 2.56384880\n",
      "Iteration 11306, loss = 2.56390867\n",
      "Iteration 11307, loss = 2.56372036\n",
      "Iteration 11308, loss = 2.56376923\n",
      "Iteration 11309, loss = 2.56370766\n",
      "Iteration 11310, loss = 2.56370067\n",
      "Iteration 11311, loss = 2.56359292\n",
      "Iteration 11312, loss = 2.56362082\n",
      "Iteration 11313, loss = 2.56361876\n",
      "Iteration 11314, loss = 2.56357852\n",
      "Iteration 11315, loss = 2.56376530\n",
      "Iteration 11316, loss = 2.56368420\n",
      "Iteration 11317, loss = 2.56359163\n",
      "Iteration 11318, loss = 2.56346097\n",
      "Iteration 11319, loss = 2.56366658\n",
      "Iteration 11320, loss = 2.56357067\n",
      "Iteration 11321, loss = 2.56349187\n",
      "Iteration 11322, loss = 2.56356223\n",
      "Iteration 11323, loss = 2.56351492\n",
      "Iteration 11324, loss = 2.56342840\n",
      "Iteration 11325, loss = 2.56348447\n",
      "Iteration 11326, loss = 2.56340256\n",
      "Iteration 11327, loss = 2.56342301\n",
      "Iteration 11328, loss = 2.56344222\n",
      "Iteration 11329, loss = 2.56375545\n",
      "Iteration 11330, loss = 2.56341496\n",
      "Iteration 11331, loss = 2.56406376\n",
      "Iteration 11332, loss = 2.56331809\n",
      "Iteration 11333, loss = 2.56329099\n",
      "Iteration 11334, loss = 2.56348696\n",
      "Iteration 11335, loss = 2.56353717\n",
      "Iteration 11336, loss = 2.56332408\n",
      "Iteration 11337, loss = 2.56333228\n",
      "Iteration 11338, loss = 2.56353334\n",
      "Iteration 11339, loss = 2.56324383\n",
      "Iteration 11340, loss = 2.56328323\n",
      "Iteration 11341, loss = 2.56340073\n",
      "Iteration 11342, loss = 2.56325314\n",
      "Iteration 11343, loss = 2.56319385\n",
      "Iteration 11344, loss = 2.56316389\n",
      "Iteration 11345, loss = 2.56323043\n",
      "Iteration 11346, loss = 2.56307713\n",
      "Iteration 11347, loss = 2.56317827\n",
      "Iteration 11348, loss = 2.56317845\n",
      "Iteration 11349, loss = 2.56314675\n",
      "Iteration 11350, loss = 2.56313331\n",
      "Iteration 11351, loss = 2.56326122\n",
      "Iteration 11352, loss = 2.56311236\n",
      "Iteration 11353, loss = 2.56325621\n",
      "Iteration 11354, loss = 2.56335212\n",
      "Iteration 11355, loss = 2.56316985\n",
      "Iteration 11356, loss = 2.56311864\n",
      "Iteration 11357, loss = 2.56305384\n",
      "Iteration 11358, loss = 2.56300572\n",
      "Iteration 11359, loss = 2.56307083\n",
      "Iteration 11360, loss = 2.56302864\n",
      "Iteration 11361, loss = 2.56293639\n",
      "Iteration 11362, loss = 2.56302037\n",
      "Iteration 11363, loss = 2.56305955\n",
      "Iteration 11364, loss = 2.56289204\n",
      "Iteration 11365, loss = 2.56315521\n",
      "Iteration 11366, loss = 2.56307697\n",
      "Iteration 11367, loss = 2.56312651\n",
      "Iteration 11368, loss = 2.56293510\n",
      "Iteration 11369, loss = 2.56286625\n",
      "Iteration 11370, loss = 2.56290788\n",
      "Iteration 11371, loss = 2.56283073\n",
      "Iteration 11372, loss = 2.56285604\n",
      "Iteration 11373, loss = 2.56279938\n",
      "Iteration 11374, loss = 2.56286168\n",
      "Iteration 11375, loss = 2.56277543\n",
      "Iteration 11376, loss = 2.56282108\n",
      "Iteration 11377, loss = 2.56277553\n",
      "Iteration 11378, loss = 2.56276759\n",
      "Iteration 11379, loss = 2.56280821\n",
      "Iteration 11380, loss = 2.56264424\n",
      "Iteration 11381, loss = 2.56279121\n",
      "Iteration 11382, loss = 2.56271619\n",
      "Iteration 11383, loss = 2.56283340\n",
      "Iteration 11384, loss = 2.56270091\n",
      "Iteration 11385, loss = 2.56268737\n",
      "Iteration 11386, loss = 2.56276050\n",
      "Iteration 11387, loss = 2.56279060\n",
      "Iteration 11388, loss = 2.56258923\n",
      "Iteration 11389, loss = 2.56257843\n",
      "Iteration 11390, loss = 2.56255371\n",
      "Iteration 11391, loss = 2.56256359\n",
      "Iteration 11392, loss = 2.56264309\n",
      "Iteration 11393, loss = 2.56256684\n",
      "Iteration 11394, loss = 2.56257482\n",
      "Iteration 11395, loss = 2.56255056\n",
      "Iteration 11396, loss = 2.56274182\n",
      "Iteration 11397, loss = 2.56251994\n",
      "Iteration 11398, loss = 2.56272250\n",
      "Iteration 11399, loss = 2.56256347\n",
      "Iteration 11400, loss = 2.56245932\n",
      "Iteration 11401, loss = 2.56238234\n",
      "Iteration 11402, loss = 2.56254717\n",
      "Iteration 11403, loss = 2.56234521\n",
      "Iteration 11404, loss = 2.56237778\n",
      "Iteration 11405, loss = 2.56248558\n",
      "Iteration 11406, loss = 2.56245387\n",
      "Iteration 11407, loss = 2.56237459\n",
      "Iteration 11408, loss = 2.56228235\n",
      "Iteration 11409, loss = 2.56240294\n",
      "Iteration 11410, loss = 2.56245260\n",
      "Iteration 11411, loss = 2.56236597\n",
      "Iteration 11412, loss = 2.56223875\n",
      "Iteration 11413, loss = 2.56227754\n",
      "Iteration 11414, loss = 2.56221342\n",
      "Iteration 11415, loss = 2.56216768\n",
      "Iteration 11416, loss = 2.56220795\n",
      "Iteration 11417, loss = 2.56221357\n",
      "Iteration 11418, loss = 2.56232034\n",
      "Iteration 11419, loss = 2.56224214\n",
      "Iteration 11420, loss = 2.56227189\n",
      "Iteration 11421, loss = 2.56217430\n",
      "Iteration 11422, loss = 2.56212904\n",
      "Iteration 11423, loss = 2.56217142\n",
      "Iteration 11424, loss = 2.56216496\n",
      "Iteration 11425, loss = 2.56209563\n",
      "Iteration 11426, loss = 2.56211222\n",
      "Iteration 11427, loss = 2.56205462\n",
      "Iteration 11428, loss = 2.56210221\n",
      "Iteration 11429, loss = 2.56200581\n",
      "Iteration 11430, loss = 2.56218100\n",
      "Iteration 11431, loss = 2.56214516\n",
      "Iteration 11432, loss = 2.56202310\n",
      "Iteration 11433, loss = 2.56202798\n",
      "Iteration 11434, loss = 2.56197035\n",
      "Iteration 11435, loss = 2.56204314\n",
      "Iteration 11436, loss = 2.56202666\n",
      "Iteration 11437, loss = 2.56188609\n",
      "Iteration 11438, loss = 2.56220812\n",
      "Iteration 11439, loss = 2.56182943\n",
      "Iteration 11440, loss = 2.56188426\n",
      "Iteration 11441, loss = 2.56195650\n",
      "Iteration 11442, loss = 2.56207590\n",
      "Iteration 11443, loss = 2.56181961\n",
      "Iteration 11444, loss = 2.56184992\n",
      "Iteration 11445, loss = 2.56179938\n",
      "Iteration 11446, loss = 2.56191576\n",
      "Iteration 11447, loss = 2.56185188\n",
      "Iteration 11448, loss = 2.56187378\n",
      "Iteration 11449, loss = 2.56186309\n",
      "Iteration 11450, loss = 2.56194921\n",
      "Iteration 11451, loss = 2.56179524\n",
      "Iteration 11452, loss = 2.56172728\n",
      "Iteration 11453, loss = 2.56175156\n",
      "Iteration 11454, loss = 2.56167564\n",
      "Iteration 11455, loss = 2.56187957\n",
      "Iteration 11456, loss = 2.56174670\n",
      "Iteration 11457, loss = 2.56173929\n",
      "Iteration 11458, loss = 2.56169843\n",
      "Iteration 11459, loss = 2.56164908\n",
      "Iteration 11460, loss = 2.56173859\n",
      "Iteration 11461, loss = 2.56151629\n",
      "Iteration 11462, loss = 2.56157443\n",
      "Iteration 11463, loss = 2.56177754\n",
      "Iteration 11464, loss = 2.56167709\n",
      "Iteration 11465, loss = 2.56151684\n",
      "Iteration 11466, loss = 2.56163248\n",
      "Iteration 11467, loss = 2.56157309\n",
      "Iteration 11468, loss = 2.56139546\n",
      "Iteration 11469, loss = 2.56157038\n",
      "Iteration 11470, loss = 2.56148847\n",
      "Iteration 11471, loss = 2.56151157\n",
      "Iteration 11472, loss = 2.56146928\n",
      "Iteration 11473, loss = 2.56147038\n",
      "Iteration 11474, loss = 2.56147160\n",
      "Iteration 11475, loss = 2.56137505\n",
      "Iteration 11476, loss = 2.56146399\n",
      "Iteration 11477, loss = 2.56137592\n",
      "Iteration 11478, loss = 2.56139880\n",
      "Iteration 11479, loss = 2.56132059\n",
      "Iteration 11480, loss = 2.56145351\n",
      "Iteration 11481, loss = 2.56130949\n",
      "Iteration 11482, loss = 2.56141844\n",
      "Iteration 11483, loss = 2.56130877\n",
      "Iteration 11484, loss = 2.56133118\n",
      "Iteration 11485, loss = 2.56131979\n",
      "Iteration 11486, loss = 2.56133441\n",
      "Iteration 11487, loss = 2.56133590\n",
      "Iteration 11488, loss = 2.56123511\n",
      "Iteration 11489, loss = 2.56119147\n",
      "Iteration 11490, loss = 2.56132914\n",
      "Iteration 11491, loss = 2.56128860\n",
      "Iteration 11492, loss = 2.56114390\n",
      "Iteration 11493, loss = 2.56116692\n",
      "Iteration 11494, loss = 2.56119573\n",
      "Iteration 11495, loss = 2.56115460\n",
      "Iteration 11496, loss = 2.56109210\n",
      "Iteration 11497, loss = 2.56112964\n",
      "Iteration 11498, loss = 2.56110994\n",
      "Iteration 11499, loss = 2.56107477\n",
      "Iteration 11500, loss = 2.56119776\n",
      "Iteration 11501, loss = 2.56121007\n",
      "Iteration 11502, loss = 2.56099163\n",
      "Iteration 11503, loss = 2.56099893\n",
      "Iteration 11504, loss = 2.56114560\n",
      "Iteration 11505, loss = 2.56115985\n",
      "Iteration 11506, loss = 2.56110183\n",
      "Iteration 11507, loss = 2.56102373\n",
      "Iteration 11508, loss = 2.56109515\n",
      "Iteration 11509, loss = 2.56105004\n",
      "Iteration 11510, loss = 2.56092010\n",
      "Iteration 11511, loss = 2.56094082\n",
      "Iteration 11512, loss = 2.56091479\n",
      "Iteration 11513, loss = 2.56083701\n",
      "Iteration 11514, loss = 2.56088487\n",
      "Iteration 11515, loss = 2.56087532\n",
      "Iteration 11516, loss = 2.56086722\n",
      "Iteration 11517, loss = 2.56080726\n",
      "Iteration 11518, loss = 2.56093570\n",
      "Iteration 11519, loss = 2.56077961\n",
      "Iteration 11520, loss = 2.56095084\n",
      "Iteration 11521, loss = 2.56078801\n",
      "Iteration 11522, loss = 2.56079068\n",
      "Iteration 11523, loss = 2.56081948\n",
      "Iteration 11524, loss = 2.56094316\n",
      "Iteration 11525, loss = 2.56080501\n",
      "Iteration 11526, loss = 2.56085657\n",
      "Iteration 11527, loss = 2.56071366\n",
      "Iteration 11528, loss = 2.56068067\n",
      "Iteration 11529, loss = 2.56062589\n",
      "Iteration 11530, loss = 2.56067411\n",
      "Iteration 11531, loss = 2.56084300\n",
      "Iteration 11532, loss = 2.56076439\n",
      "Iteration 11533, loss = 2.56062430\n",
      "Iteration 11534, loss = 2.56066375\n",
      "Iteration 11535, loss = 2.56051227\n",
      "Iteration 11536, loss = 2.56071160\n",
      "Iteration 11537, loss = 2.56063741\n",
      "Iteration 11538, loss = 2.56058180\n",
      "Iteration 11539, loss = 2.56052184\n",
      "Iteration 11540, loss = 2.56062496\n",
      "Iteration 11541, loss = 2.56055733\n",
      "Iteration 11542, loss = 2.56049968\n",
      "Iteration 11543, loss = 2.56047040\n",
      "Iteration 11544, loss = 2.56048435\n",
      "Iteration 11545, loss = 2.56050693\n",
      "Iteration 11546, loss = 2.56045438\n",
      "Iteration 11547, loss = 2.56048266\n",
      "Iteration 11548, loss = 2.56043841\n",
      "Iteration 11549, loss = 2.56042955\n",
      "Iteration 11550, loss = 2.56037354\n",
      "Iteration 11551, loss = 2.56037049\n",
      "Iteration 11552, loss = 2.56032241\n",
      "Iteration 11553, loss = 2.56041840\n",
      "Iteration 11554, loss = 2.56043516\n",
      "Iteration 11555, loss = 2.56054849\n",
      "Iteration 11556, loss = 2.56025594\n",
      "Iteration 11557, loss = 2.56034301\n",
      "Iteration 11558, loss = 2.56032040\n",
      "Iteration 11559, loss = 2.56020340\n",
      "Iteration 11560, loss = 2.56027927\n",
      "Iteration 11561, loss = 2.56023602\n",
      "Iteration 11562, loss = 2.56026918\n",
      "Iteration 11563, loss = 2.56025609\n",
      "Iteration 11564, loss = 2.56026092\n",
      "Iteration 11565, loss = 2.56018597\n",
      "Iteration 11566, loss = 2.56037739\n",
      "Iteration 11567, loss = 2.56014423\n",
      "Iteration 11568, loss = 2.56010604\n",
      "Iteration 11569, loss = 2.56019169\n",
      "Iteration 11570, loss = 2.56007363\n",
      "Iteration 11571, loss = 2.56007214\n",
      "Iteration 11572, loss = 2.56019452\n",
      "Iteration 11573, loss = 2.56008814\n",
      "Iteration 11574, loss = 2.56018822\n",
      "Iteration 11575, loss = 2.56012540\n",
      "Iteration 11576, loss = 2.56004356\n",
      "Iteration 11577, loss = 2.56002488\n",
      "Iteration 11578, loss = 2.55999352\n",
      "Iteration 11579, loss = 2.56002878\n",
      "Iteration 11580, loss = 2.56003541\n",
      "Iteration 11581, loss = 2.55999929\n",
      "Iteration 11582, loss = 2.56000753\n",
      "Iteration 11583, loss = 2.55995409\n",
      "Iteration 11584, loss = 2.55994121\n",
      "Iteration 11585, loss = 2.56012194\n",
      "Iteration 11586, loss = 2.56000208\n",
      "Iteration 11587, loss = 2.55990374\n",
      "Iteration 11588, loss = 2.55992367\n",
      "Iteration 11589, loss = 2.55995682\n",
      "Iteration 11590, loss = 2.55994137\n",
      "Iteration 11591, loss = 2.56001937\n",
      "Iteration 11592, loss = 2.55983517\n",
      "Iteration 11593, loss = 2.55981970\n",
      "Iteration 11594, loss = 2.55978238\n",
      "Iteration 11595, loss = 2.55978037\n",
      "Iteration 11596, loss = 2.55980616\n",
      "Iteration 11597, loss = 2.55987909\n",
      "Iteration 11598, loss = 2.55969847\n",
      "Iteration 11599, loss = 2.55965148\n",
      "Iteration 11600, loss = 2.55990444\n",
      "Iteration 11601, loss = 2.55986142\n",
      "Iteration 11602, loss = 2.55971513\n",
      "Iteration 11603, loss = 2.55984651\n",
      "Iteration 11604, loss = 2.55987579\n",
      "Iteration 11605, loss = 2.55998809\n",
      "Iteration 11606, loss = 2.55965395\n",
      "Iteration 11607, loss = 2.55971151\n",
      "Iteration 11608, loss = 2.55968999\n",
      "Iteration 11609, loss = 2.55964397\n",
      "Iteration 11610, loss = 2.55955534\n",
      "Iteration 11611, loss = 2.55963660\n",
      "Iteration 11612, loss = 2.55951737\n",
      "Iteration 11613, loss = 2.55957638\n",
      "Iteration 11614, loss = 2.55952447\n",
      "Iteration 11615, loss = 2.55948667\n",
      "Iteration 11616, loss = 2.55955972\n",
      "Iteration 11617, loss = 2.55951494\n",
      "Iteration 11618, loss = 2.55944087\n",
      "Iteration 11619, loss = 2.55957563\n",
      "Iteration 11620, loss = 2.55948120\n",
      "Iteration 11621, loss = 2.55942230\n",
      "Iteration 11622, loss = 2.55963543\n",
      "Iteration 11623, loss = 2.55946458\n",
      "Iteration 11624, loss = 2.55943936\n",
      "Iteration 11625, loss = 2.55930905\n",
      "Iteration 11626, loss = 2.55934794\n",
      "Iteration 11627, loss = 2.55939803\n",
      "Iteration 11628, loss = 2.55928609\n",
      "Iteration 11629, loss = 2.55940650\n",
      "Iteration 11630, loss = 2.55949781\n",
      "Iteration 11631, loss = 2.55931284\n",
      "Iteration 11632, loss = 2.55932918\n",
      "Iteration 11633, loss = 2.55934431\n",
      "Iteration 11634, loss = 2.55954501\n",
      "Iteration 11635, loss = 2.55919773\n",
      "Iteration 11636, loss = 2.55918017\n",
      "Iteration 11637, loss = 2.55928460\n",
      "Iteration 11638, loss = 2.55926257\n",
      "Iteration 11639, loss = 2.55914614\n",
      "Iteration 11640, loss = 2.55915152\n",
      "Iteration 11641, loss = 2.55919284\n",
      "Iteration 11642, loss = 2.55914182\n",
      "Iteration 11643, loss = 2.55923901\n",
      "Iteration 11644, loss = 2.55909193\n",
      "Iteration 11645, loss = 2.55911899\n",
      "Iteration 11646, loss = 2.55914686\n",
      "Iteration 11647, loss = 2.55907139\n",
      "Iteration 11648, loss = 2.55908894\n",
      "Iteration 11649, loss = 2.55903752\n",
      "Iteration 11650, loss = 2.55906244\n",
      "Iteration 11651, loss = 2.55898326\n",
      "Iteration 11652, loss = 2.55900556\n",
      "Iteration 11653, loss = 2.55899589\n",
      "Iteration 11654, loss = 2.55899963\n",
      "Iteration 11655, loss = 2.55902116\n",
      "Iteration 11656, loss = 2.55893694\n",
      "Iteration 11657, loss = 2.55893704\n",
      "Iteration 11658, loss = 2.55907296\n",
      "Iteration 11659, loss = 2.55893105\n",
      "Iteration 11660, loss = 2.55907228\n",
      "Iteration 11661, loss = 2.55888872\n",
      "Iteration 11662, loss = 2.55890339\n",
      "Iteration 11663, loss = 2.55899054\n",
      "Iteration 11664, loss = 2.55881376\n",
      "Iteration 11665, loss = 2.55881034\n",
      "Iteration 11666, loss = 2.55877359\n",
      "Iteration 11667, loss = 2.55876864\n",
      "Iteration 11668, loss = 2.55886353\n",
      "Iteration 11669, loss = 2.55880349\n",
      "Iteration 11670, loss = 2.55870385\n",
      "Iteration 11671, loss = 2.55877250\n",
      "Iteration 11672, loss = 2.55884626\n",
      "Iteration 11673, loss = 2.55875448\n",
      "Iteration 11674, loss = 2.55875195\n",
      "Iteration 11675, loss = 2.55868620\n",
      "Iteration 11676, loss = 2.55863454\n",
      "Iteration 11677, loss = 2.55865399\n",
      "Iteration 11678, loss = 2.55864226\n",
      "Iteration 11679, loss = 2.55861510\n",
      "Iteration 11680, loss = 2.55867941\n",
      "Iteration 11681, loss = 2.55865747\n",
      "Iteration 11682, loss = 2.55856003\n",
      "Iteration 11683, loss = 2.55867046\n",
      "Iteration 11684, loss = 2.55857588\n",
      "Iteration 11685, loss = 2.55859830\n",
      "Iteration 11686, loss = 2.55855955\n",
      "Iteration 11687, loss = 2.55856820\n",
      "Iteration 11688, loss = 2.55852764\n",
      "Iteration 11689, loss = 2.55845585\n",
      "Iteration 11690, loss = 2.55855586\n",
      "Iteration 11691, loss = 2.55848325\n",
      "Iteration 11692, loss = 2.55877355\n",
      "Iteration 11693, loss = 2.55846066\n",
      "Iteration 11694, loss = 2.55861813\n",
      "Iteration 11695, loss = 2.55853382\n",
      "Iteration 11696, loss = 2.55839664\n",
      "Iteration 11697, loss = 2.55840357\n",
      "Iteration 11698, loss = 2.55845079\n",
      "Iteration 11699, loss = 2.55836942\n",
      "Iteration 11700, loss = 2.55846706\n",
      "Iteration 11701, loss = 2.55838693\n",
      "Iteration 11702, loss = 2.55851294\n",
      "Iteration 11703, loss = 2.55842195\n",
      "Iteration 11704, loss = 2.55833702\n",
      "Iteration 11705, loss = 2.55830233\n",
      "Iteration 11706, loss = 2.55821546\n",
      "Iteration 11707, loss = 2.55828676\n",
      "Iteration 11708, loss = 2.55820814\n",
      "Iteration 11709, loss = 2.55837786\n",
      "Iteration 11710, loss = 2.55833437\n",
      "Iteration 11711, loss = 2.55813404\n",
      "Iteration 11712, loss = 2.55822263\n",
      "Iteration 11713, loss = 2.55823767\n",
      "Iteration 11714, loss = 2.55817673\n",
      "Iteration 11715, loss = 2.55823066\n",
      "Iteration 11716, loss = 2.55816850\n",
      "Iteration 11717, loss = 2.55808784\n",
      "Iteration 11718, loss = 2.55808460\n",
      "Iteration 11719, loss = 2.55809830\n",
      "Iteration 11720, loss = 2.55813056\n",
      "Iteration 11721, loss = 2.55805570\n",
      "Iteration 11722, loss = 2.55830219\n",
      "Iteration 11723, loss = 2.55807347\n",
      "Iteration 11724, loss = 2.55808274\n",
      "Iteration 11725, loss = 2.55800196\n",
      "Iteration 11726, loss = 2.55803122\n",
      "Iteration 11727, loss = 2.55802276\n",
      "Iteration 11728, loss = 2.55797346\n",
      "Iteration 11729, loss = 2.55791985\n",
      "Iteration 11730, loss = 2.55789776\n",
      "Iteration 11731, loss = 2.55805690\n",
      "Iteration 11732, loss = 2.55786466\n",
      "Iteration 11733, loss = 2.55791113\n",
      "Iteration 11734, loss = 2.55791909\n",
      "Iteration 11735, loss = 2.55787317\n",
      "Iteration 11736, loss = 2.55778071\n",
      "Iteration 11737, loss = 2.55788068\n",
      "Iteration 11738, loss = 2.55784631\n",
      "Iteration 11739, loss = 2.55782245\n",
      "Iteration 11740, loss = 2.55781080\n",
      "Iteration 11741, loss = 2.55771885\n",
      "Iteration 11742, loss = 2.55780235\n",
      "Iteration 11743, loss = 2.55771587\n",
      "Iteration 11744, loss = 2.55775767\n",
      "Iteration 11745, loss = 2.55769034\n",
      "Iteration 11746, loss = 2.55774039\n",
      "Iteration 11747, loss = 2.55787991\n",
      "Iteration 11748, loss = 2.55766804\n",
      "Iteration 11749, loss = 2.55802218\n",
      "Iteration 11750, loss = 2.55790125\n",
      "Iteration 11751, loss = 2.55764404\n",
      "Iteration 11752, loss = 2.55772403\n",
      "Iteration 11753, loss = 2.55799226\n",
      "Iteration 11754, loss = 2.55763832\n",
      "Iteration 11755, loss = 2.55765249\n",
      "Iteration 11756, loss = 2.55756146\n",
      "Iteration 11757, loss = 2.55754478\n",
      "Iteration 11758, loss = 2.55775482\n",
      "Iteration 11759, loss = 2.55762700\n",
      "Iteration 11760, loss = 2.55756148\n",
      "Iteration 11761, loss = 2.55745073\n",
      "Iteration 11762, loss = 2.55762975\n",
      "Iteration 11763, loss = 2.55738929\n",
      "Iteration 11764, loss = 2.55746870\n",
      "Iteration 11765, loss = 2.55751782\n",
      "Iteration 11766, loss = 2.55749512\n",
      "Iteration 11767, loss = 2.55739840\n",
      "Iteration 11768, loss = 2.55743081\n",
      "Iteration 11769, loss = 2.55738660\n",
      "Iteration 11770, loss = 2.55742152\n",
      "Iteration 11771, loss = 2.55735382\n",
      "Iteration 11772, loss = 2.55736828\n",
      "Iteration 11773, loss = 2.55727085\n",
      "Iteration 11774, loss = 2.55731045\n",
      "Iteration 11775, loss = 2.55731737\n",
      "Iteration 11776, loss = 2.55725215\n",
      "Iteration 11777, loss = 2.55736614\n",
      "Iteration 11778, loss = 2.55729004\n",
      "Iteration 11779, loss = 2.55722639\n",
      "Iteration 11780, loss = 2.55725327\n",
      "Iteration 11781, loss = 2.55725788\n",
      "Iteration 11782, loss = 2.55727538\n",
      "Iteration 11783, loss = 2.55715095\n",
      "Iteration 11784, loss = 2.55733006\n",
      "Iteration 11785, loss = 2.55713547\n",
      "Iteration 11786, loss = 2.55712223\n",
      "Iteration 11787, loss = 2.55721457\n",
      "Iteration 11788, loss = 2.55720702\n",
      "Iteration 11789, loss = 2.55716354\n",
      "Iteration 11790, loss = 2.55714751\n",
      "Iteration 11791, loss = 2.55705540\n",
      "Iteration 11792, loss = 2.55720638\n",
      "Iteration 11793, loss = 2.55716638\n",
      "Iteration 11794, loss = 2.55704747\n",
      "Iteration 11795, loss = 2.55705484\n",
      "Iteration 11796, loss = 2.55718786\n",
      "Iteration 11797, loss = 2.55699588\n",
      "Iteration 11798, loss = 2.55704742\n",
      "Iteration 11799, loss = 2.55696652\n",
      "Iteration 11800, loss = 2.55691179\n",
      "Iteration 11801, loss = 2.55696547\n",
      "Iteration 11802, loss = 2.55730170\n",
      "Iteration 11803, loss = 2.55725646\n",
      "Iteration 11804, loss = 2.55704368\n",
      "Iteration 11805, loss = 2.55687410\n",
      "Iteration 11806, loss = 2.55695101\n",
      "Iteration 11807, loss = 2.55690693\n",
      "Iteration 11808, loss = 2.55688996\n",
      "Iteration 11809, loss = 2.55700208\n",
      "Iteration 11810, loss = 2.55674806\n",
      "Iteration 11811, loss = 2.55695222\n",
      "Iteration 11812, loss = 2.55684065\n",
      "Iteration 11813, loss = 2.55671656\n",
      "Iteration 11814, loss = 2.55679133\n",
      "Iteration 11815, loss = 2.55676444\n",
      "Iteration 11816, loss = 2.55680319\n",
      "Iteration 11817, loss = 2.55679375\n",
      "Iteration 11818, loss = 2.55679091\n",
      "Iteration 11819, loss = 2.55666379\n",
      "Iteration 11820, loss = 2.55706357\n",
      "Iteration 11821, loss = 2.55686470\n",
      "Iteration 11822, loss = 2.55667511\n",
      "Iteration 11823, loss = 2.55672109\n",
      "Iteration 11824, loss = 2.55663034\n",
      "Iteration 11825, loss = 2.55670024\n",
      "Iteration 11826, loss = 2.55675192\n",
      "Iteration 11827, loss = 2.55686073\n",
      "Iteration 11828, loss = 2.55660109\n",
      "Iteration 11829, loss = 2.55661438\n",
      "Iteration 11830, loss = 2.55652506\n",
      "Iteration 11831, loss = 2.55661564\n",
      "Iteration 11832, loss = 2.55660604\n",
      "Iteration 11833, loss = 2.55652344\n",
      "Iteration 11834, loss = 2.55651693\n",
      "Iteration 11835, loss = 2.55656576\n",
      "Iteration 11836, loss = 2.55650925\n",
      "Iteration 11837, loss = 2.55645354\n",
      "Iteration 11838, loss = 2.55642193\n",
      "Iteration 11839, loss = 2.55647107\n",
      "Iteration 11840, loss = 2.55654648\n",
      "Iteration 11841, loss = 2.55639785\n",
      "Iteration 11842, loss = 2.55646405\n",
      "Iteration 11843, loss = 2.55640158\n",
      "Iteration 11844, loss = 2.55637725\n",
      "Iteration 11845, loss = 2.55631742\n",
      "Iteration 11846, loss = 2.55635588\n",
      "Iteration 11847, loss = 2.55632050\n",
      "Iteration 11848, loss = 2.55641660\n",
      "Iteration 11849, loss = 2.55624491\n",
      "Iteration 11850, loss = 2.55624208\n",
      "Iteration 11851, loss = 2.55651133\n",
      "Iteration 11852, loss = 2.55623462\n",
      "Iteration 11853, loss = 2.55622238\n",
      "Iteration 11854, loss = 2.55632737\n",
      "Iteration 11855, loss = 2.55612024\n",
      "Iteration 11856, loss = 2.55621805\n",
      "Iteration 11857, loss = 2.55620400\n",
      "Iteration 11858, loss = 2.55630961\n",
      "Iteration 11859, loss = 2.55609917\n",
      "Iteration 11860, loss = 2.55614170\n",
      "Iteration 11861, loss = 2.55619778\n",
      "Iteration 11862, loss = 2.55604639\n",
      "Iteration 11863, loss = 2.55611338\n",
      "Iteration 11864, loss = 2.55622192\n",
      "Iteration 11865, loss = 2.55603804\n",
      "Iteration 11866, loss = 2.55601283\n",
      "Iteration 11867, loss = 2.55602366\n",
      "Iteration 11868, loss = 2.55617371\n",
      "Iteration 11869, loss = 2.55604263\n",
      "Iteration 11870, loss = 2.55605688\n",
      "Iteration 11871, loss = 2.55601947\n",
      "Iteration 11872, loss = 2.55598376\n",
      "Iteration 11873, loss = 2.55596329\n",
      "Iteration 11874, loss = 2.55591422\n",
      "Iteration 11875, loss = 2.55592148\n",
      "Iteration 11876, loss = 2.55589847\n",
      "Iteration 11877, loss = 2.55592136\n",
      "Iteration 11878, loss = 2.55583806\n",
      "Iteration 11879, loss = 2.55588914\n",
      "Iteration 11880, loss = 2.55595699\n",
      "Iteration 11881, loss = 2.55591995\n",
      "Iteration 11882, loss = 2.55579279\n",
      "Iteration 11883, loss = 2.55582280\n",
      "Iteration 11884, loss = 2.55594030\n",
      "Iteration 11885, loss = 2.55612412\n",
      "Iteration 11886, loss = 2.55581397\n",
      "Iteration 11887, loss = 2.55594721\n",
      "Iteration 11888, loss = 2.55567856\n",
      "Iteration 11889, loss = 2.55568614\n",
      "Iteration 11890, loss = 2.55574844\n",
      "Iteration 11891, loss = 2.55575854\n",
      "Iteration 11892, loss = 2.55591861\n",
      "Iteration 11893, loss = 2.55586845\n",
      "Iteration 11894, loss = 2.55561353\n",
      "Iteration 11895, loss = 2.55575128\n",
      "Iteration 11896, loss = 2.55571267\n",
      "Iteration 11897, loss = 2.55557921\n",
      "Iteration 11898, loss = 2.55557884\n",
      "Iteration 11899, loss = 2.55584801\n",
      "Iteration 11900, loss = 2.55559417\n",
      "Iteration 11901, loss = 2.55563155\n",
      "Iteration 11902, loss = 2.55558617\n",
      "Iteration 11903, loss = 2.55556909\n",
      "Iteration 11904, loss = 2.55565047\n",
      "Iteration 11905, loss = 2.55556513\n",
      "Iteration 11906, loss = 2.55558323\n",
      "Iteration 11907, loss = 2.55554376\n",
      "Iteration 11908, loss = 2.55553142\n",
      "Iteration 11909, loss = 2.55550711\n",
      "Iteration 11910, loss = 2.55563709\n",
      "Iteration 11911, loss = 2.55543989\n",
      "Iteration 11912, loss = 2.55544347\n",
      "Iteration 11913, loss = 2.55548963\n",
      "Iteration 11914, loss = 2.55553043\n",
      "Iteration 11915, loss = 2.55533595\n",
      "Iteration 11916, loss = 2.55547415\n",
      "Iteration 11917, loss = 2.55525608\n",
      "Iteration 11918, loss = 2.55542153\n",
      "Iteration 11919, loss = 2.55533731\n",
      "Iteration 11920, loss = 2.55534888\n",
      "Iteration 11921, loss = 2.55524488\n",
      "Iteration 11922, loss = 2.55537570\n",
      "Iteration 11923, loss = 2.55527293\n",
      "Iteration 11924, loss = 2.55525945\n",
      "Iteration 11925, loss = 2.55527688\n",
      "Iteration 11926, loss = 2.55519926\n",
      "Iteration 11927, loss = 2.55514573\n",
      "Iteration 11928, loss = 2.55521398\n",
      "Iteration 11929, loss = 2.55517586\n",
      "Iteration 11930, loss = 2.55520089\n",
      "Iteration 11931, loss = 2.55516407\n",
      "Iteration 11932, loss = 2.55511047\n",
      "Iteration 11933, loss = 2.55527794\n",
      "Iteration 11934, loss = 2.55511502\n",
      "Iteration 11935, loss = 2.55517090\n",
      "Iteration 11936, loss = 2.55507907\n",
      "Iteration 11937, loss = 2.55503231\n",
      "Iteration 11938, loss = 2.55503874\n",
      "Iteration 11939, loss = 2.55499663\n",
      "Iteration 11940, loss = 2.55499712\n",
      "Iteration 11941, loss = 2.55501601\n",
      "Iteration 11942, loss = 2.55500321\n",
      "Iteration 11943, loss = 2.55512785\n",
      "Iteration 11944, loss = 2.55503542\n",
      "Iteration 11945, loss = 2.55503070\n",
      "Iteration 11946, loss = 2.55494324\n",
      "Iteration 11947, loss = 2.55494211\n",
      "Iteration 11948, loss = 2.55501684\n",
      "Iteration 11949, loss = 2.55500381\n",
      "Iteration 11950, loss = 2.55491189\n",
      "Iteration 11951, loss = 2.55497218\n",
      "Iteration 11952, loss = 2.55507144\n",
      "Iteration 11953, loss = 2.55491230\n",
      "Iteration 11954, loss = 2.55484033\n",
      "Iteration 11955, loss = 2.55480449\n",
      "Iteration 11956, loss = 2.55481059\n",
      "Iteration 11957, loss = 2.55482242\n",
      "Iteration 11958, loss = 2.55482444\n",
      "Iteration 11959, loss = 2.55471868\n",
      "Iteration 11960, loss = 2.55476873\n",
      "Iteration 11961, loss = 2.55473401\n",
      "Iteration 11962, loss = 2.55469429\n",
      "Iteration 11963, loss = 2.55466003\n",
      "Iteration 11964, loss = 2.55470307\n",
      "Iteration 11965, loss = 2.55467083\n",
      "Iteration 11966, loss = 2.55468860\n",
      "Iteration 11967, loss = 2.55469057\n",
      "Iteration 11968, loss = 2.55458221\n",
      "Iteration 11969, loss = 2.55470686\n",
      "Iteration 11970, loss = 2.55470463\n",
      "Iteration 11971, loss = 2.55471362\n",
      "Iteration 11972, loss = 2.55472660\n",
      "Iteration 11973, loss = 2.55459743\n",
      "Iteration 11974, loss = 2.55476579\n",
      "Iteration 11975, loss = 2.55462474\n",
      "Iteration 11976, loss = 2.55460850\n",
      "Iteration 11977, loss = 2.55448908\n",
      "Iteration 11978, loss = 2.55460262\n",
      "Iteration 11979, loss = 2.55445626\n",
      "Iteration 11980, loss = 2.55440952\n",
      "Iteration 11981, loss = 2.55443632\n",
      "Iteration 11982, loss = 2.55447480\n",
      "Iteration 11983, loss = 2.55447640\n",
      "Iteration 11984, loss = 2.55445053\n",
      "Iteration 11985, loss = 2.55451022\n",
      "Iteration 11986, loss = 2.55452807\n",
      "Iteration 11987, loss = 2.55434349\n",
      "Iteration 11988, loss = 2.55441470\n",
      "Iteration 11989, loss = 2.55447415\n",
      "Iteration 11990, loss = 2.55432043\n",
      "Iteration 11991, loss = 2.55439952\n",
      "Iteration 11992, loss = 2.55432935\n",
      "Iteration 11993, loss = 2.55439223\n",
      "Iteration 11994, loss = 2.55432714\n",
      "Iteration 11995, loss = 2.55423656\n",
      "Iteration 11996, loss = 2.55431728\n",
      "Iteration 11997, loss = 2.55420848\n",
      "Iteration 11998, loss = 2.55432311\n",
      "Iteration 11999, loss = 2.55415738\n",
      "Iteration 12000, loss = 2.55435883\n",
      "Iteration 12001, loss = 2.55421041\n",
      "Iteration 12002, loss = 2.55412029\n",
      "Iteration 12003, loss = 2.55416020\n",
      "Iteration 12004, loss = 2.55417544\n",
      "Iteration 12005, loss = 2.55407756\n",
      "Iteration 12006, loss = 2.55413885\n",
      "Iteration 12007, loss = 2.55411831\n",
      "Iteration 12008, loss = 2.55410880\n",
      "Iteration 12009, loss = 2.55410553\n",
      "Iteration 12010, loss = 2.55419188\n",
      "Iteration 12011, loss = 2.55402872\n",
      "Iteration 12012, loss = 2.55410987\n",
      "Iteration 12013, loss = 2.55413422\n",
      "Iteration 12014, loss = 2.55400841\n",
      "Iteration 12015, loss = 2.55406188\n",
      "Iteration 12016, loss = 2.55396051\n",
      "Iteration 12017, loss = 2.55407345\n",
      "Iteration 12018, loss = 2.55388103\n",
      "Iteration 12019, loss = 2.55385574\n",
      "Iteration 12020, loss = 2.55429824\n",
      "Iteration 12021, loss = 2.55382892\n",
      "Iteration 12022, loss = 2.55380129\n",
      "Iteration 12023, loss = 2.55393687\n",
      "Iteration 12024, loss = 2.55391337\n",
      "Iteration 12025, loss = 2.55400465\n",
      "Iteration 12026, loss = 2.55402058\n",
      "Iteration 12027, loss = 2.55388887\n",
      "Iteration 12028, loss = 2.55390761\n",
      "Iteration 12029, loss = 2.55377301\n",
      "Iteration 12030, loss = 2.55379379\n",
      "Iteration 12031, loss = 2.55371255\n",
      "Iteration 12032, loss = 2.55370267\n",
      "Iteration 12033, loss = 2.55376617\n",
      "Iteration 12034, loss = 2.55367249\n",
      "Iteration 12035, loss = 2.55368846\n",
      "Iteration 12036, loss = 2.55367975\n",
      "Iteration 12037, loss = 2.55371556\n",
      "Iteration 12038, loss = 2.55375112\n",
      "Iteration 12039, loss = 2.55368817\n",
      "Iteration 12040, loss = 2.55363890\n",
      "Iteration 12041, loss = 2.55364182\n",
      "Iteration 12042, loss = 2.55355274\n",
      "Iteration 12043, loss = 2.55370572\n",
      "Iteration 12044, loss = 2.55360963\n",
      "Iteration 12045, loss = 2.55357905\n",
      "Iteration 12046, loss = 2.55357833\n",
      "Iteration 12047, loss = 2.55353521\n",
      "Iteration 12048, loss = 2.55349687\n",
      "Iteration 12049, loss = 2.55344391\n",
      "Iteration 12050, loss = 2.55355279\n",
      "Iteration 12051, loss = 2.55348629\n",
      "Iteration 12052, loss = 2.55348158\n",
      "Iteration 12053, loss = 2.55344683\n",
      "Iteration 12054, loss = 2.55357385\n",
      "Iteration 12055, loss = 2.55341421\n",
      "Iteration 12056, loss = 2.55335726\n",
      "Iteration 12057, loss = 2.55333070\n",
      "Iteration 12058, loss = 2.55333823\n",
      "Iteration 12059, loss = 2.55338043\n",
      "Iteration 12060, loss = 2.55333049\n",
      "Iteration 12061, loss = 2.55354861\n",
      "Iteration 12062, loss = 2.55339347\n",
      "Iteration 12063, loss = 2.55331628\n",
      "Iteration 12064, loss = 2.55335062\n",
      "Iteration 12065, loss = 2.55321361\n",
      "Iteration 12066, loss = 2.55338871\n",
      "Iteration 12067, loss = 2.55317341\n",
      "Iteration 12068, loss = 2.55353010\n",
      "Iteration 12069, loss = 2.55329568\n",
      "Iteration 12070, loss = 2.55322323\n",
      "Iteration 12071, loss = 2.55321385\n",
      "Iteration 12072, loss = 2.55324142\n",
      "Iteration 12073, loss = 2.55319470\n",
      "Iteration 12074, loss = 2.55317239\n",
      "Iteration 12075, loss = 2.55315705\n",
      "Iteration 12076, loss = 2.55310905\n",
      "Iteration 12077, loss = 2.55311693\n",
      "Iteration 12078, loss = 2.55302312\n",
      "Iteration 12079, loss = 2.55308000\n",
      "Iteration 12080, loss = 2.55308505\n",
      "Iteration 12081, loss = 2.55302706\n",
      "Iteration 12082, loss = 2.55304316\n",
      "Iteration 12083, loss = 2.55303957\n",
      "Iteration 12084, loss = 2.55302188\n",
      "Iteration 12085, loss = 2.55298281\n",
      "Iteration 12086, loss = 2.55297557\n",
      "Iteration 12087, loss = 2.55295386\n",
      "Iteration 12088, loss = 2.55302492\n",
      "Iteration 12089, loss = 2.55288947\n",
      "Iteration 12090, loss = 2.55316566\n",
      "Iteration 12091, loss = 2.55295951\n",
      "Iteration 12092, loss = 2.55291244\n",
      "Iteration 12093, loss = 2.55293809\n",
      "Iteration 12094, loss = 2.55286783\n",
      "Iteration 12095, loss = 2.55285166\n",
      "Iteration 12096, loss = 2.55290133\n",
      "Iteration 12097, loss = 2.55281867\n",
      "Iteration 12098, loss = 2.55279887\n",
      "Iteration 12099, loss = 2.55283632\n",
      "Iteration 12100, loss = 2.55274679\n",
      "Iteration 12101, loss = 2.55275729\n",
      "Iteration 12102, loss = 2.55275535\n",
      "Iteration 12103, loss = 2.55264063\n",
      "Iteration 12104, loss = 2.55281789\n",
      "Iteration 12105, loss = 2.55266281\n",
      "Iteration 12106, loss = 2.55266442\n",
      "Iteration 12107, loss = 2.55277244\n",
      "Iteration 12108, loss = 2.55269646\n",
      "Iteration 12109, loss = 2.55268142\n",
      "Iteration 12110, loss = 2.55263306\n",
      "Iteration 12111, loss = 2.55269190\n",
      "Iteration 12112, loss = 2.55266648\n",
      "Iteration 12113, loss = 2.55268528\n",
      "Iteration 12114, loss = 2.55253995\n",
      "Iteration 12115, loss = 2.55270817\n",
      "Iteration 12116, loss = 2.55266972\n",
      "Iteration 12117, loss = 2.55252933\n",
      "Iteration 12118, loss = 2.55256954\n",
      "Iteration 12119, loss = 2.55250309\n",
      "Iteration 12120, loss = 2.55245512\n",
      "Iteration 12121, loss = 2.55245634\n",
      "Iteration 12122, loss = 2.55256646\n",
      "Iteration 12123, loss = 2.55246533\n",
      "Iteration 12124, loss = 2.55250216\n",
      "Iteration 12125, loss = 2.55253820\n",
      "Iteration 12126, loss = 2.55252406\n",
      "Iteration 12127, loss = 2.55237829\n",
      "Iteration 12128, loss = 2.55239819\n",
      "Iteration 12129, loss = 2.55236357\n",
      "Iteration 12130, loss = 2.55247486\n",
      "Iteration 12131, loss = 2.55240804\n",
      "Iteration 12132, loss = 2.55245085\n",
      "Iteration 12133, loss = 2.55230777\n",
      "Iteration 12134, loss = 2.55227997\n",
      "Iteration 12135, loss = 2.55233968\n",
      "Iteration 12136, loss = 2.55231181\n",
      "Iteration 12137, loss = 2.55227155\n",
      "Iteration 12138, loss = 2.55232147\n",
      "Iteration 12139, loss = 2.55228582\n",
      "Iteration 12140, loss = 2.55221059\n",
      "Iteration 12141, loss = 2.55214635\n",
      "Iteration 12142, loss = 2.55217764\n",
      "Iteration 12143, loss = 2.55221614\n",
      "Iteration 12144, loss = 2.55220537\n",
      "Iteration 12145, loss = 2.55223449\n",
      "Iteration 12146, loss = 2.55213460\n",
      "Iteration 12147, loss = 2.55204107\n",
      "Iteration 12148, loss = 2.55218070\n",
      "Iteration 12149, loss = 2.55205326\n",
      "Iteration 12150, loss = 2.55217940\n",
      "Iteration 12151, loss = 2.55220404\n",
      "Iteration 12152, loss = 2.55218802\n",
      "Iteration 12153, loss = 2.55219180\n",
      "Iteration 12154, loss = 2.55195849\n",
      "Iteration 12155, loss = 2.55204710\n",
      "Iteration 12156, loss = 2.55213287\n",
      "Iteration 12157, loss = 2.55185998\n",
      "Iteration 12158, loss = 2.55200958\n",
      "Iteration 12159, loss = 2.55197526\n",
      "Iteration 12160, loss = 2.55196350\n",
      "Iteration 12161, loss = 2.55194646\n",
      "Iteration 12162, loss = 2.55184467\n",
      "Iteration 12163, loss = 2.55198652\n",
      "Iteration 12164, loss = 2.55188457\n",
      "Iteration 12165, loss = 2.55184755\n",
      "Iteration 12166, loss = 2.55186191\n",
      "Iteration 12167, loss = 2.55178917\n",
      "Iteration 12168, loss = 2.55190707\n",
      "Iteration 12169, loss = 2.55175114\n",
      "Iteration 12170, loss = 2.55172656\n",
      "Iteration 12171, loss = 2.55172257\n",
      "Iteration 12172, loss = 2.55175127\n",
      "Iteration 12173, loss = 2.55175227\n",
      "Iteration 12174, loss = 2.55173588\n",
      "Iteration 12175, loss = 2.55174618\n",
      "Iteration 12176, loss = 2.55176949\n",
      "Iteration 12177, loss = 2.55171283\n",
      "Iteration 12178, loss = 2.55173478\n",
      "Iteration 12179, loss = 2.55176474\n",
      "Iteration 12180, loss = 2.55162150\n",
      "Iteration 12181, loss = 2.55159410\n",
      "Iteration 12182, loss = 2.55167843\n",
      "Iteration 12183, loss = 2.55159876\n",
      "Iteration 12184, loss = 2.55154964\n",
      "Iteration 12185, loss = 2.55153220\n",
      "Iteration 12186, loss = 2.55152722\n",
      "Iteration 12187, loss = 2.55146572\n",
      "Iteration 12188, loss = 2.55156176\n",
      "Iteration 12189, loss = 2.55152644\n",
      "Iteration 12190, loss = 2.55141976\n",
      "Iteration 12191, loss = 2.55148280\n",
      "Iteration 12192, loss = 2.55158457\n",
      "Iteration 12193, loss = 2.55192968\n",
      "Iteration 12194, loss = 2.55138648\n",
      "Iteration 12195, loss = 2.55173490\n",
      "Iteration 12196, loss = 2.55141586\n",
      "Iteration 12197, loss = 2.55137388\n",
      "Iteration 12198, loss = 2.55144746\n",
      "Iteration 12199, loss = 2.55131572\n",
      "Iteration 12200, loss = 2.55144017\n",
      "Iteration 12201, loss = 2.55150931\n",
      "Iteration 12202, loss = 2.55132751\n",
      "Iteration 12203, loss = 2.55147455\n",
      "Iteration 12204, loss = 2.55126680\n",
      "Iteration 12205, loss = 2.55133766\n",
      "Iteration 12206, loss = 2.55131598\n",
      "Iteration 12207, loss = 2.55122136\n",
      "Iteration 12208, loss = 2.55128217\n",
      "Iteration 12209, loss = 2.55127805\n",
      "Iteration 12210, loss = 2.55121734\n",
      "Iteration 12211, loss = 2.55122776\n",
      "Iteration 12212, loss = 2.55113643\n",
      "Iteration 12213, loss = 2.55120437\n",
      "Iteration 12214, loss = 2.55121867\n",
      "Iteration 12215, loss = 2.55115114\n",
      "Iteration 12216, loss = 2.55135459\n",
      "Iteration 12217, loss = 2.55126667\n",
      "Iteration 12218, loss = 2.55109732\n",
      "Iteration 12219, loss = 2.55114560\n",
      "Iteration 12220, loss = 2.55109640\n",
      "Iteration 12221, loss = 2.55102138\n",
      "Iteration 12222, loss = 2.55102166\n",
      "Iteration 12223, loss = 2.55099678\n",
      "Iteration 12224, loss = 2.55116235\n",
      "Iteration 12225, loss = 2.55101814\n",
      "Iteration 12226, loss = 2.55100451\n",
      "Iteration 12227, loss = 2.55106124\n",
      "Iteration 12228, loss = 2.55103877\n",
      "Iteration 12229, loss = 2.55107354\n",
      "Iteration 12230, loss = 2.55106412\n",
      "Iteration 12231, loss = 2.55093769\n",
      "Iteration 12232, loss = 2.55089313\n",
      "Iteration 12233, loss = 2.55098591\n",
      "Iteration 12234, loss = 2.55093251\n",
      "Iteration 12235, loss = 2.55095916\n",
      "Iteration 12236, loss = 2.55090890\n",
      "Iteration 12237, loss = 2.55083394\n",
      "Iteration 12238, loss = 2.55081582\n",
      "Iteration 12239, loss = 2.55076048\n",
      "Iteration 12240, loss = 2.55095409\n",
      "Iteration 12241, loss = 2.55078316\n",
      "Iteration 12242, loss = 2.55078087\n",
      "Iteration 12243, loss = 2.55080892\n",
      "Iteration 12244, loss = 2.55074493\n",
      "Iteration 12245, loss = 2.55064664\n",
      "Iteration 12246, loss = 2.55064929\n",
      "Iteration 12247, loss = 2.55067399\n",
      "Iteration 12248, loss = 2.55066675\n",
      "Iteration 12249, loss = 2.55063108\n",
      "Iteration 12250, loss = 2.55072176\n",
      "Iteration 12251, loss = 2.55072958\n",
      "Iteration 12252, loss = 2.55059551\n",
      "Iteration 12253, loss = 2.55062190\n",
      "Iteration 12254, loss = 2.55066275\n",
      "Iteration 12255, loss = 2.55050979\n",
      "Iteration 12256, loss = 2.55063636\n",
      "Iteration 12257, loss = 2.55069758\n",
      "Iteration 12258, loss = 2.55048848\n",
      "Iteration 12259, loss = 2.55055984\n",
      "Iteration 12260, loss = 2.55048535\n",
      "Iteration 12261, loss = 2.55050061\n",
      "Iteration 12262, loss = 2.55045455\n",
      "Iteration 12263, loss = 2.55041000\n",
      "Iteration 12264, loss = 2.55058063\n",
      "Iteration 12265, loss = 2.55032196\n",
      "Iteration 12266, loss = 2.55051866\n",
      "Iteration 12267, loss = 2.55040107\n",
      "Iteration 12268, loss = 2.55047266\n",
      "Iteration 12269, loss = 2.55032723\n",
      "Iteration 12270, loss = 2.55031010\n",
      "Iteration 12271, loss = 2.55040820\n",
      "Iteration 12272, loss = 2.55036794\n",
      "Iteration 12273, loss = 2.55038553\n",
      "Iteration 12274, loss = 2.55028794\n",
      "Iteration 12275, loss = 2.55027093\n",
      "Iteration 12276, loss = 2.55048275\n",
      "Iteration 12277, loss = 2.55023578\n",
      "Iteration 12278, loss = 2.55025670\n",
      "Iteration 12279, loss = 2.55025198\n",
      "Iteration 12280, loss = 2.55018703\n",
      "Iteration 12281, loss = 2.55019531\n",
      "Iteration 12282, loss = 2.55023187\n",
      "Iteration 12283, loss = 2.55016128\n",
      "Iteration 12284, loss = 2.55026114\n",
      "Iteration 12285, loss = 2.55018372\n",
      "Iteration 12286, loss = 2.55030342\n",
      "Iteration 12287, loss = 2.55015231\n",
      "Iteration 12288, loss = 2.55012005\n",
      "Iteration 12289, loss = 2.55017028\n",
      "Iteration 12290, loss = 2.55012779\n",
      "Iteration 12291, loss = 2.55009686\n",
      "Iteration 12292, loss = 2.55013270\n",
      "Iteration 12293, loss = 2.54995979\n",
      "Iteration 12294, loss = 2.55013140\n",
      "Iteration 12295, loss = 2.55007391\n",
      "Iteration 12296, loss = 2.54998964\n",
      "Iteration 12297, loss = 2.55006518\n",
      "Iteration 12298, loss = 2.55007297\n",
      "Iteration 12299, loss = 2.55001022\n",
      "Iteration 12300, loss = 2.54988369\n",
      "Iteration 12301, loss = 2.54992662\n",
      "Iteration 12302, loss = 2.54987546\n",
      "Iteration 12303, loss = 2.54986429\n",
      "Iteration 12304, loss = 2.54987066\n",
      "Iteration 12305, loss = 2.54982368\n",
      "Iteration 12306, loss = 2.54984754\n",
      "Iteration 12307, loss = 2.54991547\n",
      "Iteration 12308, loss = 2.54998763\n",
      "Iteration 12309, loss = 2.54981869\n",
      "Iteration 12310, loss = 2.54975655\n",
      "Iteration 12311, loss = 2.54992058\n",
      "Iteration 12312, loss = 2.54965620\n",
      "Iteration 12313, loss = 2.54976530\n",
      "Iteration 12314, loss = 2.54969806\n",
      "Iteration 12315, loss = 2.54992905\n",
      "Iteration 12316, loss = 2.54976513\n",
      "Iteration 12317, loss = 2.54976392\n",
      "Iteration 12318, loss = 2.54966785\n",
      "Iteration 12319, loss = 2.54974278\n",
      "Iteration 12320, loss = 2.54959313\n",
      "Iteration 12321, loss = 2.54961224\n",
      "Iteration 12322, loss = 2.54958548\n",
      "Iteration 12323, loss = 2.54961254\n",
      "Iteration 12324, loss = 2.54960253\n",
      "Iteration 12325, loss = 2.54958580\n",
      "Iteration 12326, loss = 2.54957721\n",
      "Iteration 12327, loss = 2.54953286\n",
      "Iteration 12328, loss = 2.54970787\n",
      "Iteration 12329, loss = 2.54957472\n",
      "Iteration 12330, loss = 2.54952047\n",
      "Iteration 12331, loss = 2.54949673\n",
      "Iteration 12332, loss = 2.54949004\n",
      "Iteration 12333, loss = 2.54951952\n",
      "Iteration 12334, loss = 2.54943959\n",
      "Iteration 12335, loss = 2.54954014\n",
      "Iteration 12336, loss = 2.54960029\n",
      "Iteration 12337, loss = 2.54944132\n",
      "Iteration 12338, loss = 2.54941529\n",
      "Iteration 12339, loss = 2.54959758\n",
      "Iteration 12340, loss = 2.54954756\n",
      "Iteration 12341, loss = 2.54975731\n",
      "Iteration 12342, loss = 2.54930040\n",
      "Iteration 12343, loss = 2.54929285\n",
      "Iteration 12344, loss = 2.54934764\n",
      "Iteration 12345, loss = 2.54929366\n",
      "Iteration 12346, loss = 2.54929480\n",
      "Iteration 12347, loss = 2.54929212\n",
      "Iteration 12348, loss = 2.54921299\n",
      "Iteration 12349, loss = 2.54922742\n",
      "Iteration 12350, loss = 2.54917016\n",
      "Iteration 12351, loss = 2.54917137\n",
      "Iteration 12352, loss = 2.54921686\n",
      "Iteration 12353, loss = 2.54923601\n",
      "Iteration 12354, loss = 2.54934597\n",
      "Iteration 12355, loss = 2.54915206\n",
      "Iteration 12356, loss = 2.54910468\n",
      "Iteration 12357, loss = 2.54912992\n",
      "Iteration 12358, loss = 2.54916631\n",
      "Iteration 12359, loss = 2.54903181\n",
      "Iteration 12360, loss = 2.54916407\n",
      "Iteration 12361, loss = 2.54914969\n",
      "Iteration 12362, loss = 2.54937937\n",
      "Iteration 12363, loss = 2.54903984\n",
      "Iteration 12364, loss = 2.54905017\n",
      "Iteration 12365, loss = 2.54905843\n",
      "Iteration 12366, loss = 2.54894976\n",
      "Iteration 12367, loss = 2.54897900\n",
      "Iteration 12368, loss = 2.54888643\n",
      "Iteration 12369, loss = 2.54898652\n",
      "Iteration 12370, loss = 2.54891819\n",
      "Iteration 12371, loss = 2.54908240\n",
      "Iteration 12372, loss = 2.54892445\n",
      "Iteration 12373, loss = 2.54891008\n",
      "Iteration 12374, loss = 2.54894464\n",
      "Iteration 12375, loss = 2.54888639\n",
      "Iteration 12376, loss = 2.54888082\n",
      "Iteration 12377, loss = 2.54884650\n",
      "Iteration 12378, loss = 2.54885561\n",
      "Iteration 12379, loss = 2.54878039\n",
      "Iteration 12380, loss = 2.54881811\n",
      "Iteration 12381, loss = 2.54873263\n",
      "Iteration 12382, loss = 2.54890639\n",
      "Iteration 12383, loss = 2.54873238\n",
      "Iteration 12384, loss = 2.54873995\n",
      "Iteration 12385, loss = 2.54873061\n",
      "Iteration 12386, loss = 2.54873083\n",
      "Iteration 12387, loss = 2.54866077\n",
      "Iteration 12388, loss = 2.54873066\n",
      "Iteration 12389, loss = 2.54894226\n",
      "Iteration 12390, loss = 2.54869216\n",
      "Iteration 12391, loss = 2.54868018\n",
      "Iteration 12392, loss = 2.54858269\n",
      "Iteration 12393, loss = 2.54858965\n",
      "Iteration 12394, loss = 2.54858039\n",
      "Iteration 12395, loss = 2.54857892\n",
      "Iteration 12396, loss = 2.54861250\n",
      "Iteration 12397, loss = 2.54858277\n",
      "Iteration 12398, loss = 2.54859719\n",
      "Iteration 12399, loss = 2.54852711\n",
      "Iteration 12400, loss = 2.54853653\n",
      "Iteration 12401, loss = 2.54836607\n",
      "Iteration 12402, loss = 2.54845621\n",
      "Iteration 12403, loss = 2.54860400\n",
      "Iteration 12404, loss = 2.54840781\n",
      "Iteration 12405, loss = 2.54843181\n",
      "Iteration 12406, loss = 2.54850954\n",
      "Iteration 12407, loss = 2.54843089\n",
      "Iteration 12408, loss = 2.54835141\n",
      "Iteration 12409, loss = 2.54835554\n",
      "Iteration 12410, loss = 2.54837012\n",
      "Iteration 12411, loss = 2.54831052\n",
      "Iteration 12412, loss = 2.54845337\n",
      "Iteration 12413, loss = 2.54834958\n",
      "Iteration 12414, loss = 2.54831050\n",
      "Iteration 12415, loss = 2.54825843\n",
      "Iteration 12416, loss = 2.54839285\n",
      "Iteration 12417, loss = 2.54827109\n",
      "Iteration 12418, loss = 2.54822886\n",
      "Iteration 12419, loss = 2.54834772\n",
      "Iteration 12420, loss = 2.54820232\n",
      "Iteration 12421, loss = 2.54847379\n",
      "Iteration 12422, loss = 2.54820747\n",
      "Iteration 12423, loss = 2.54824796\n",
      "Iteration 12424, loss = 2.54818788\n",
      "Iteration 12425, loss = 2.54846369\n",
      "Iteration 12426, loss = 2.54818521\n",
      "Iteration 12427, loss = 2.54809994\n",
      "Iteration 12428, loss = 2.54805564\n",
      "Iteration 12429, loss = 2.54811947\n",
      "Iteration 12430, loss = 2.54804568\n",
      "Iteration 12431, loss = 2.54809059\n",
      "Iteration 12432, loss = 2.54816568\n",
      "Iteration 12433, loss = 2.54799087\n",
      "Iteration 12434, loss = 2.54834117\n",
      "Iteration 12435, loss = 2.54817640\n",
      "Iteration 12436, loss = 2.54816862\n",
      "Iteration 12437, loss = 2.54814283\n",
      "Iteration 12438, loss = 2.54794850\n",
      "Iteration 12439, loss = 2.54803102\n",
      "Iteration 12440, loss = 2.54798992\n",
      "Iteration 12441, loss = 2.54795230\n",
      "Iteration 12442, loss = 2.54800109\n",
      "Iteration 12443, loss = 2.54826007\n",
      "Iteration 12444, loss = 2.54797830\n",
      "Iteration 12445, loss = 2.54797204\n",
      "Iteration 12446, loss = 2.54787293\n",
      "Iteration 12447, loss = 2.54777350\n",
      "Iteration 12448, loss = 2.54775441\n",
      "Iteration 12449, loss = 2.54793982\n",
      "Iteration 12450, loss = 2.54774730\n",
      "Iteration 12451, loss = 2.54776127\n",
      "Iteration 12452, loss = 2.54772239\n",
      "Iteration 12453, loss = 2.54773563\n",
      "Iteration 12454, loss = 2.54768948\n",
      "Iteration 12455, loss = 2.54776026\n",
      "Iteration 12456, loss = 2.54782294\n",
      "Iteration 12457, loss = 2.54793651\n",
      "Iteration 12458, loss = 2.54769188\n",
      "Iteration 12459, loss = 2.54768665\n",
      "Iteration 12460, loss = 2.54783485\n",
      "Iteration 12461, loss = 2.54761650\n",
      "Iteration 12462, loss = 2.54760579\n",
      "Iteration 12463, loss = 2.54755095\n",
      "Iteration 12464, loss = 2.54751883\n",
      "Iteration 12465, loss = 2.54754810\n",
      "Iteration 12466, loss = 2.54752493\n",
      "Iteration 12467, loss = 2.54770822\n",
      "Iteration 12468, loss = 2.54747629\n",
      "Iteration 12469, loss = 2.54743378\n",
      "Iteration 12470, loss = 2.54778872\n",
      "Iteration 12471, loss = 2.54759314\n",
      "Iteration 12472, loss = 2.54746847\n",
      "Iteration 12473, loss = 2.54735199\n",
      "Iteration 12474, loss = 2.54755129\n",
      "Iteration 12475, loss = 2.54742350\n",
      "Iteration 12476, loss = 2.54737112\n",
      "Iteration 12477, loss = 2.54733303\n",
      "Iteration 12478, loss = 2.54731529\n",
      "Iteration 12479, loss = 2.54733342\n",
      "Iteration 12480, loss = 2.54734209\n",
      "Iteration 12481, loss = 2.54736895\n",
      "Iteration 12482, loss = 2.54746949\n",
      "Iteration 12483, loss = 2.54730319\n",
      "Iteration 12484, loss = 2.54733098\n",
      "Iteration 12485, loss = 2.54729797\n",
      "Iteration 12486, loss = 2.54730125\n",
      "Iteration 12487, loss = 2.54718712\n",
      "Iteration 12488, loss = 2.54721229\n",
      "Iteration 12489, loss = 2.54732435\n",
      "Iteration 12490, loss = 2.54720090\n",
      "Iteration 12491, loss = 2.54717472\n",
      "Iteration 12492, loss = 2.54734836\n",
      "Iteration 12493, loss = 2.54721857\n",
      "Iteration 12494, loss = 2.54721436\n",
      "Iteration 12495, loss = 2.54710007\n",
      "Iteration 12496, loss = 2.54713305\n",
      "Iteration 12497, loss = 2.54707475\n",
      "Iteration 12498, loss = 2.54711998\n",
      "Iteration 12499, loss = 2.54722610\n",
      "Iteration 12500, loss = 2.54707726\n",
      "Iteration 12501, loss = 2.54707705\n",
      "Iteration 12502, loss = 2.54700839\n",
      "Iteration 12503, loss = 2.54696588\n",
      "Iteration 12504, loss = 2.54707542\n",
      "Iteration 12505, loss = 2.54695345\n",
      "Iteration 12506, loss = 2.54695833\n",
      "Iteration 12507, loss = 2.54696410\n",
      "Iteration 12508, loss = 2.54705937\n",
      "Iteration 12509, loss = 2.54684358\n",
      "Iteration 12510, loss = 2.54683852\n",
      "Iteration 12511, loss = 2.54691902\n",
      "Iteration 12512, loss = 2.54700896\n",
      "Iteration 12513, loss = 2.54700508\n",
      "Iteration 12514, loss = 2.54682045\n",
      "Iteration 12515, loss = 2.54687423\n",
      "Iteration 12516, loss = 2.54677734\n",
      "Iteration 12517, loss = 2.54686351\n",
      "Iteration 12518, loss = 2.54697810\n",
      "Iteration 12519, loss = 2.54680895\n",
      "Iteration 12520, loss = 2.54674978\n",
      "Iteration 12521, loss = 2.54673688\n",
      "Iteration 12522, loss = 2.54681600\n",
      "Iteration 12523, loss = 2.54674799\n",
      "Iteration 12524, loss = 2.54667029\n",
      "Iteration 12525, loss = 2.54664605\n",
      "Iteration 12526, loss = 2.54668225\n",
      "Iteration 12527, loss = 2.54668286\n",
      "Iteration 12528, loss = 2.54664030\n",
      "Iteration 12529, loss = 2.54689276\n",
      "Iteration 12530, loss = 2.54655751\n",
      "Iteration 12531, loss = 2.54682927\n",
      "Iteration 12532, loss = 2.54659315\n",
      "Iteration 12533, loss = 2.54659044\n",
      "Iteration 12534, loss = 2.54659346\n",
      "Iteration 12535, loss = 2.54649117\n",
      "Iteration 12536, loss = 2.54656409\n",
      "Iteration 12537, loss = 2.54661062\n",
      "Iteration 12538, loss = 2.54646377\n",
      "Iteration 12539, loss = 2.54657769\n",
      "Iteration 12540, loss = 2.54644737\n",
      "Iteration 12541, loss = 2.54654111\n",
      "Iteration 12542, loss = 2.54649093\n",
      "Iteration 12543, loss = 2.54678056\n",
      "Iteration 12544, loss = 2.54653887\n",
      "Iteration 12545, loss = 2.54633381\n",
      "Iteration 12546, loss = 2.54659808\n",
      "Iteration 12547, loss = 2.54641161\n",
      "Iteration 12548, loss = 2.54647711\n",
      "Iteration 12549, loss = 2.54654715\n",
      "Iteration 12550, loss = 2.54652282\n",
      "Iteration 12551, loss = 2.54623062\n",
      "Iteration 12552, loss = 2.54637387\n",
      "Iteration 12553, loss = 2.54633092\n",
      "Iteration 12554, loss = 2.54633204\n",
      "Iteration 12555, loss = 2.54624216\n",
      "Iteration 12556, loss = 2.54619653\n",
      "Iteration 12557, loss = 2.54614587\n",
      "Iteration 12558, loss = 2.54624747\n",
      "Iteration 12559, loss = 2.54632119\n",
      "Iteration 12560, loss = 2.54615424\n",
      "Iteration 12561, loss = 2.54617729\n",
      "Iteration 12562, loss = 2.54606240\n",
      "Iteration 12563, loss = 2.54615047\n",
      "Iteration 12564, loss = 2.54621177\n",
      "Iteration 12565, loss = 2.54611097\n",
      "Iteration 12566, loss = 2.54610353\n",
      "Iteration 12567, loss = 2.54601972\n",
      "Iteration 12568, loss = 2.54609918\n",
      "Iteration 12569, loss = 2.54634894\n",
      "Iteration 12570, loss = 2.54600117\n",
      "Iteration 12571, loss = 2.54599608\n",
      "Iteration 12572, loss = 2.54604709\n",
      "Iteration 12573, loss = 2.54594813\n",
      "Iteration 12574, loss = 2.54615423\n",
      "Iteration 12575, loss = 2.54595713\n",
      "Iteration 12576, loss = 2.54593688\n",
      "Iteration 12577, loss = 2.54605833\n",
      "Iteration 12578, loss = 2.54592143\n",
      "Iteration 12579, loss = 2.54588286\n",
      "Iteration 12580, loss = 2.54599301\n",
      "Iteration 12581, loss = 2.54587568\n",
      "Iteration 12582, loss = 2.54587344\n",
      "Iteration 12583, loss = 2.54582213\n",
      "Iteration 12584, loss = 2.54592158\n",
      "Iteration 12585, loss = 2.54582550\n",
      "Iteration 12586, loss = 2.54584137\n",
      "Iteration 12587, loss = 2.54575821\n",
      "Iteration 12588, loss = 2.54608273\n",
      "Iteration 12589, loss = 2.54574122\n",
      "Iteration 12590, loss = 2.54575932\n",
      "Iteration 12591, loss = 2.54568130\n",
      "Iteration 12592, loss = 2.54575864\n",
      "Iteration 12593, loss = 2.54566740\n",
      "Iteration 12594, loss = 2.54566697\n",
      "Iteration 12595, loss = 2.54568544\n",
      "Iteration 12596, loss = 2.54568386\n",
      "Iteration 12597, loss = 2.54585955\n",
      "Iteration 12598, loss = 2.54590339\n",
      "Iteration 12599, loss = 2.54566892\n",
      "Iteration 12600, loss = 2.54558722\n",
      "Iteration 12601, loss = 2.54556757\n",
      "Iteration 12602, loss = 2.54564337\n",
      "Iteration 12603, loss = 2.54562290\n",
      "Iteration 12604, loss = 2.54569527\n",
      "Iteration 12605, loss = 2.54559702\n",
      "Iteration 12606, loss = 2.54550896\n",
      "Iteration 12607, loss = 2.54576348\n",
      "Iteration 12608, loss = 2.54552720\n",
      "Iteration 12609, loss = 2.54555365\n",
      "Iteration 12610, loss = 2.54545026\n",
      "Iteration 12611, loss = 2.54569665\n",
      "Iteration 12612, loss = 2.54545781\n",
      "Iteration 12613, loss = 2.54538252\n",
      "Iteration 12614, loss = 2.54546292\n",
      "Iteration 12615, loss = 2.54541256\n",
      "Iteration 12616, loss = 2.54531781\n",
      "Iteration 12617, loss = 2.54554831\n",
      "Iteration 12618, loss = 2.54536542\n",
      "Iteration 12619, loss = 2.54541423\n",
      "Iteration 12620, loss = 2.54528709\n",
      "Iteration 12621, loss = 2.54535222\n",
      "Iteration 12622, loss = 2.54532163\n",
      "Iteration 12623, loss = 2.54536389\n",
      "Iteration 12624, loss = 2.54524762\n",
      "Iteration 12625, loss = 2.54524703\n",
      "Iteration 12626, loss = 2.54547014\n",
      "Iteration 12627, loss = 2.54527592\n",
      "Iteration 12628, loss = 2.54514034\n",
      "Iteration 12629, loss = 2.54548194\n",
      "Iteration 12630, loss = 2.54535836\n",
      "Iteration 12631, loss = 2.54513739\n",
      "Iteration 12632, loss = 2.54510657\n",
      "Iteration 12633, loss = 2.54514761\n",
      "Iteration 12634, loss = 2.54521974\n",
      "Iteration 12635, loss = 2.54503527\n",
      "Iteration 12636, loss = 2.54512567\n",
      "Iteration 12637, loss = 2.54521579\n",
      "Iteration 12638, loss = 2.54499328\n",
      "Iteration 12639, loss = 2.54507508\n",
      "Iteration 12640, loss = 2.54499270\n",
      "Iteration 12641, loss = 2.54500919\n",
      "Iteration 12642, loss = 2.54510319\n",
      "Iteration 12643, loss = 2.54506457\n",
      "Iteration 12644, loss = 2.54497644\n",
      "Iteration 12645, loss = 2.54488459\n",
      "Iteration 12646, loss = 2.54502324\n",
      "Iteration 12647, loss = 2.54504107\n",
      "Iteration 12648, loss = 2.54491074\n",
      "Iteration 12649, loss = 2.54483402\n",
      "Iteration 12650, loss = 2.54492717\n",
      "Iteration 12651, loss = 2.54482071\n",
      "Iteration 12652, loss = 2.54480664\n",
      "Iteration 12653, loss = 2.54493365\n",
      "Iteration 12654, loss = 2.54497780\n",
      "Iteration 12655, loss = 2.54478845\n",
      "Iteration 12656, loss = 2.54485326\n",
      "Iteration 12657, loss = 2.54470624\n",
      "Iteration 12658, loss = 2.54475405\n",
      "Iteration 12659, loss = 2.54467413\n",
      "Iteration 12660, loss = 2.54478654\n",
      "Iteration 12661, loss = 2.54502752\n",
      "Iteration 12662, loss = 2.54467099\n",
      "Iteration 12663, loss = 2.54473685\n",
      "Iteration 12664, loss = 2.54478418\n",
      "Iteration 12665, loss = 2.54453378\n",
      "Iteration 12666, loss = 2.54495695\n",
      "Iteration 12667, loss = 2.54500081\n",
      "Iteration 12668, loss = 2.54460259\n",
      "Iteration 12669, loss = 2.54471065\n",
      "Iteration 12670, loss = 2.54462387\n",
      "Iteration 12671, loss = 2.54455249\n",
      "Iteration 12672, loss = 2.54466005\n",
      "Iteration 12673, loss = 2.54484255\n",
      "Iteration 12674, loss = 2.54483824\n",
      "Iteration 12675, loss = 2.54457301\n",
      "Iteration 12676, loss = 2.54452197\n",
      "Iteration 12677, loss = 2.54452959\n",
      "Iteration 12678, loss = 2.54443645\n",
      "Iteration 12679, loss = 2.54447019\n",
      "Iteration 12680, loss = 2.54455523\n",
      "Iteration 12681, loss = 2.54443271\n",
      "Iteration 12682, loss = 2.54440312\n",
      "Iteration 12683, loss = 2.54445037\n",
      "Iteration 12684, loss = 2.54434463\n",
      "Iteration 12685, loss = 2.54427394\n",
      "Iteration 12686, loss = 2.54437128\n",
      "Iteration 12687, loss = 2.54431660\n",
      "Iteration 12688, loss = 2.54425716\n",
      "Iteration 12689, loss = 2.54437913\n",
      "Iteration 12690, loss = 2.54434528\n",
      "Iteration 12691, loss = 2.54434572\n",
      "Iteration 12692, loss = 2.54441802\n",
      "Iteration 12693, loss = 2.54425160\n",
      "Iteration 12694, loss = 2.54432399\n",
      "Iteration 12695, loss = 2.54421344\n",
      "Iteration 12696, loss = 2.54418397\n",
      "Iteration 12697, loss = 2.54421862\n",
      "Iteration 12698, loss = 2.54414993\n",
      "Iteration 12699, loss = 2.54423400\n",
      "Iteration 12700, loss = 2.54414575\n",
      "Iteration 12701, loss = 2.54411683\n",
      "Iteration 12702, loss = 2.54413701\n",
      "Iteration 12703, loss = 2.54416709\n",
      "Iteration 12704, loss = 2.54418759\n",
      "Iteration 12705, loss = 2.54416619\n",
      "Iteration 12706, loss = 2.54405927\n",
      "Iteration 12707, loss = 2.54403572\n",
      "Iteration 12708, loss = 2.54403093\n",
      "Iteration 12709, loss = 2.54389427\n",
      "Iteration 12710, loss = 2.54411924\n",
      "Iteration 12711, loss = 2.54404190\n",
      "Iteration 12712, loss = 2.54410968\n",
      "Iteration 12713, loss = 2.54414418\n",
      "Iteration 12714, loss = 2.54389281\n",
      "Iteration 12715, loss = 2.54395875\n",
      "Iteration 12716, loss = 2.54393340\n",
      "Iteration 12717, loss = 2.54392509\n",
      "Iteration 12718, loss = 2.54389433\n",
      "Iteration 12719, loss = 2.54386886\n",
      "Iteration 12720, loss = 2.54389938\n",
      "Iteration 12721, loss = 2.54390766\n",
      "Iteration 12722, loss = 2.54386256\n",
      "Iteration 12723, loss = 2.54385911\n",
      "Iteration 12724, loss = 2.54378089\n",
      "Iteration 12725, loss = 2.54377806\n",
      "Iteration 12726, loss = 2.54374357\n",
      "Iteration 12727, loss = 2.54368412\n",
      "Iteration 12728, loss = 2.54374579\n",
      "Iteration 12729, loss = 2.54390157\n",
      "Iteration 12730, loss = 2.54383055\n",
      "Iteration 12731, loss = 2.54368473\n",
      "Iteration 12732, loss = 2.54384308\n",
      "Iteration 12733, loss = 2.54357129\n",
      "Iteration 12734, loss = 2.54369689\n",
      "Iteration 12735, loss = 2.54360966\n",
      "Iteration 12736, loss = 2.54367823\n",
      "Iteration 12737, loss = 2.54373429\n",
      "Iteration 12738, loss = 2.54352476\n",
      "Iteration 12739, loss = 2.54351663\n",
      "Iteration 12740, loss = 2.54355751\n",
      "Iteration 12741, loss = 2.54350845\n",
      "Iteration 12742, loss = 2.54360027\n",
      "Iteration 12743, loss = 2.54354877\n",
      "Iteration 12744, loss = 2.54353135\n",
      "Iteration 12745, loss = 2.54356692\n",
      "Iteration 12746, loss = 2.54348966\n",
      "Iteration 12747, loss = 2.54343521\n",
      "Iteration 12748, loss = 2.54352022\n",
      "Iteration 12749, loss = 2.54342945\n",
      "Iteration 12750, loss = 2.54339683\n",
      "Iteration 12751, loss = 2.54341730\n",
      "Iteration 12752, loss = 2.54346207\n",
      "Iteration 12753, loss = 2.54337891\n",
      "Iteration 12754, loss = 2.54336583\n",
      "Iteration 12755, loss = 2.54332637\n",
      "Iteration 12756, loss = 2.54331345\n",
      "Iteration 12757, loss = 2.54328127\n",
      "Iteration 12758, loss = 2.54344482\n",
      "Iteration 12759, loss = 2.54342986\n",
      "Iteration 12760, loss = 2.54321412\n",
      "Iteration 12761, loss = 2.54323548\n",
      "Iteration 12762, loss = 2.54332433\n",
      "Iteration 12763, loss = 2.54324230\n",
      "Iteration 12764, loss = 2.54319016\n",
      "Iteration 12765, loss = 2.54329433\n",
      "Iteration 12766, loss = 2.54317623\n",
      "Iteration 12767, loss = 2.54331263\n",
      "Iteration 12768, loss = 2.54309263\n",
      "Iteration 12769, loss = 2.54319643\n",
      "Iteration 12770, loss = 2.54306946\n",
      "Iteration 12771, loss = 2.54308450\n",
      "Iteration 12772, loss = 2.54304665\n",
      "Iteration 12773, loss = 2.54301234\n",
      "Iteration 12774, loss = 2.54304444\n",
      "Iteration 12775, loss = 2.54320784\n",
      "Iteration 12776, loss = 2.54303698\n",
      "Iteration 12777, loss = 2.54301496\n",
      "Iteration 12778, loss = 2.54326726\n",
      "Iteration 12779, loss = 2.54299587\n",
      "Iteration 12780, loss = 2.54304684\n",
      "Iteration 12781, loss = 2.54296965\n",
      "Iteration 12782, loss = 2.54294569\n",
      "Iteration 12783, loss = 2.54292140\n",
      "Iteration 12784, loss = 2.54289191\n",
      "Iteration 12785, loss = 2.54287718\n",
      "Iteration 12786, loss = 2.54283923\n",
      "Iteration 12787, loss = 2.54285605\n",
      "Iteration 12788, loss = 2.54287646\n",
      "Iteration 12789, loss = 2.54293411\n",
      "Iteration 12790, loss = 2.54277518\n",
      "Iteration 12791, loss = 2.54290760\n",
      "Iteration 12792, loss = 2.54287445\n",
      "Iteration 12793, loss = 2.54279048\n",
      "Iteration 12794, loss = 2.54273642\n",
      "Iteration 12795, loss = 2.54271245\n",
      "Iteration 12796, loss = 2.54270363\n",
      "Iteration 12797, loss = 2.54285120\n",
      "Iteration 12798, loss = 2.54285652\n",
      "Iteration 12799, loss = 2.54272673\n",
      "Iteration 12800, loss = 2.54279106\n",
      "Iteration 12801, loss = 2.54277941\n",
      "Iteration 12802, loss = 2.54273238\n",
      "Iteration 12803, loss = 2.54274972\n",
      "Iteration 12804, loss = 2.54260593\n",
      "Iteration 12805, loss = 2.54268481\n",
      "Iteration 12806, loss = 2.54266742\n",
      "Iteration 12807, loss = 2.54276961\n",
      "Iteration 12808, loss = 2.54255122\n",
      "Iteration 12809, loss = 2.54318986\n",
      "Iteration 12810, loss = 2.54240388\n",
      "Iteration 12811, loss = 2.54270672\n",
      "Iteration 12812, loss = 2.54265897\n",
      "Iteration 12813, loss = 2.54248109\n",
      "Iteration 12814, loss = 2.54255599\n",
      "Iteration 12815, loss = 2.54250227\n",
      "Iteration 12816, loss = 2.54235228\n",
      "Iteration 12817, loss = 2.54240660\n",
      "Iteration 12818, loss = 2.54254985\n",
      "Iteration 12819, loss = 2.54238316\n",
      "Iteration 12820, loss = 2.54250141\n",
      "Iteration 12821, loss = 2.54233265\n",
      "Iteration 12822, loss = 2.54232567\n",
      "Iteration 12823, loss = 2.54244536\n",
      "Iteration 12824, loss = 2.54230662\n",
      "Iteration 12825, loss = 2.54224755\n",
      "Iteration 12826, loss = 2.54248069\n",
      "Iteration 12827, loss = 2.54236755\n",
      "Iteration 12828, loss = 2.54227337\n",
      "Iteration 12829, loss = 2.54220324\n",
      "Iteration 12830, loss = 2.54222802\n",
      "Iteration 12831, loss = 2.54229679\n",
      "Iteration 12832, loss = 2.54219396\n",
      "Iteration 12833, loss = 2.54224866\n",
      "Iteration 12834, loss = 2.54227153\n",
      "Iteration 12835, loss = 2.54208951\n",
      "Iteration 12836, loss = 2.54209139\n",
      "Iteration 12837, loss = 2.54221257\n",
      "Iteration 12838, loss = 2.54209329\n",
      "Iteration 12839, loss = 2.54209517\n",
      "Iteration 12840, loss = 2.54218606\n",
      "Iteration 12841, loss = 2.54220617\n",
      "Iteration 12842, loss = 2.54196987\n",
      "Iteration 12843, loss = 2.54217120\n",
      "Iteration 12844, loss = 2.54210167\n",
      "Iteration 12845, loss = 2.54201990\n",
      "Iteration 12846, loss = 2.54199043\n",
      "Iteration 12847, loss = 2.54195845\n",
      "Iteration 12848, loss = 2.54194850\n",
      "Iteration 12849, loss = 2.54188070\n",
      "Iteration 12850, loss = 2.54191505\n",
      "Iteration 12851, loss = 2.54185155\n",
      "Iteration 12852, loss = 2.54200503\n",
      "Iteration 12853, loss = 2.54187595\n",
      "Iteration 12854, loss = 2.54204084\n",
      "Iteration 12855, loss = 2.54190500\n",
      "Iteration 12856, loss = 2.54183655\n",
      "Iteration 12857, loss = 2.54191008\n",
      "Iteration 12858, loss = 2.54182953\n",
      "Iteration 12859, loss = 2.54190014\n",
      "Iteration 12860, loss = 2.54184961\n",
      "Iteration 12861, loss = 2.54178362\n",
      "Iteration 12862, loss = 2.54176022\n",
      "Iteration 12863, loss = 2.54175293\n",
      "Iteration 12864, loss = 2.54176137\n",
      "Iteration 12865, loss = 2.54171254\n",
      "Iteration 12866, loss = 2.54164936\n",
      "Iteration 12867, loss = 2.54170923\n",
      "Iteration 12868, loss = 2.54161562\n",
      "Iteration 12869, loss = 2.54173352\n",
      "Iteration 12870, loss = 2.54174217\n",
      "Iteration 12871, loss = 2.54164941\n",
      "Iteration 12872, loss = 2.54164800\n",
      "Iteration 12873, loss = 2.54159775\n",
      "Iteration 12874, loss = 2.54151243\n",
      "Iteration 12875, loss = 2.54152617\n",
      "Iteration 12876, loss = 2.54158349\n",
      "Iteration 12877, loss = 2.54156475\n",
      "Iteration 12878, loss = 2.54146998\n",
      "Iteration 12879, loss = 2.54152317\n",
      "Iteration 12880, loss = 2.54153478\n",
      "Iteration 12881, loss = 2.54152842\n",
      "Iteration 12882, loss = 2.54151027\n",
      "Iteration 12883, loss = 2.54138891\n",
      "Iteration 12884, loss = 2.54136080\n",
      "Iteration 12885, loss = 2.54141891\n",
      "Iteration 12886, loss = 2.54132134\n",
      "Iteration 12887, loss = 2.54130466\n",
      "Iteration 12888, loss = 2.54141635\n",
      "Iteration 12889, loss = 2.54173834\n",
      "Iteration 12890, loss = 2.54127329\n",
      "Iteration 12891, loss = 2.54161580\n",
      "Iteration 12892, loss = 2.54131800\n",
      "Iteration 12893, loss = 2.54168543\n",
      "Iteration 12894, loss = 2.54121488\n",
      "Iteration 12895, loss = 2.54138579\n",
      "Iteration 12896, loss = 2.54125786\n",
      "Iteration 12897, loss = 2.54122065\n",
      "Iteration 12898, loss = 2.54126087\n",
      "Iteration 12899, loss = 2.54117201\n",
      "Iteration 12900, loss = 2.54119770\n",
      "Iteration 12901, loss = 2.54119526\n",
      "Iteration 12902, loss = 2.54116902\n",
      "Iteration 12903, loss = 2.54118138\n",
      "Iteration 12904, loss = 2.54119897\n",
      "Iteration 12905, loss = 2.54128228\n",
      "Iteration 12906, loss = 2.54109637\n",
      "Iteration 12907, loss = 2.54109509\n",
      "Iteration 12908, loss = 2.54108902\n",
      "Iteration 12909, loss = 2.54101478\n",
      "Iteration 12910, loss = 2.54104483\n",
      "Iteration 12911, loss = 2.54096364\n",
      "Iteration 12912, loss = 2.54110242\n",
      "Iteration 12913, loss = 2.54114882\n",
      "Iteration 12914, loss = 2.54090777\n",
      "Iteration 12915, loss = 2.54106674\n",
      "Iteration 12916, loss = 2.54095930\n",
      "Iteration 12917, loss = 2.54092768\n",
      "Iteration 12918, loss = 2.54099851\n",
      "Iteration 12919, loss = 2.54094252\n",
      "Iteration 12920, loss = 2.54089666\n",
      "Iteration 12921, loss = 2.54086496\n",
      "Iteration 12922, loss = 2.54090856\n",
      "Iteration 12923, loss = 2.54098166\n",
      "Iteration 12924, loss = 2.54090906\n",
      "Iteration 12925, loss = 2.54083619\n",
      "Iteration 12926, loss = 2.54080709\n",
      "Iteration 12927, loss = 2.54083120\n",
      "Iteration 12928, loss = 2.54073221\n",
      "Iteration 12929, loss = 2.54073707\n",
      "Iteration 12930, loss = 2.54074831\n",
      "Iteration 12931, loss = 2.54083791\n",
      "Iteration 12932, loss = 2.54084053\n",
      "Iteration 12933, loss = 2.54069799\n",
      "Iteration 12934, loss = 2.54073665\n",
      "Iteration 12935, loss = 2.54079671\n",
      "Iteration 12936, loss = 2.54075308\n",
      "Iteration 12937, loss = 2.54078270\n",
      "Iteration 12938, loss = 2.54062385\n",
      "Iteration 12939, loss = 2.54076654\n",
      "Iteration 12940, loss = 2.54060612\n",
      "Iteration 12941, loss = 2.54065028\n",
      "Iteration 12942, loss = 2.54060667\n",
      "Iteration 12943, loss = 2.54055306\n",
      "Iteration 12944, loss = 2.54052383\n",
      "Iteration 12945, loss = 2.54054510\n",
      "Iteration 12946, loss = 2.54057885\n",
      "Iteration 12947, loss = 2.54073943\n",
      "Iteration 12948, loss = 2.54052354\n",
      "Iteration 12949, loss = 2.54043957\n",
      "Iteration 12950, loss = 2.54054943\n",
      "Iteration 12951, loss = 2.54047837\n",
      "Iteration 12952, loss = 2.54040385\n",
      "Iteration 12953, loss = 2.54080710\n",
      "Iteration 12954, loss = 2.54052580\n",
      "Iteration 12955, loss = 2.54039904\n",
      "Iteration 12956, loss = 2.54036018\n",
      "Iteration 12957, loss = 2.54041533\n",
      "Iteration 12958, loss = 2.54033267\n",
      "Iteration 12959, loss = 2.54031668\n",
      "Iteration 12960, loss = 2.54034458\n",
      "Iteration 12961, loss = 2.54034164\n",
      "Iteration 12962, loss = 2.54030010\n",
      "Iteration 12963, loss = 2.54033754\n",
      "Iteration 12964, loss = 2.54041391\n",
      "Iteration 12965, loss = 2.54036585\n",
      "Iteration 12966, loss = 2.54016264\n",
      "Iteration 12967, loss = 2.54019783\n",
      "Iteration 12968, loss = 2.54018471\n",
      "Iteration 12969, loss = 2.54021550\n",
      "Iteration 12970, loss = 2.54020187\n",
      "Iteration 12971, loss = 2.54018126\n",
      "Iteration 12972, loss = 2.54011636\n",
      "Iteration 12973, loss = 2.54009573\n",
      "Iteration 12974, loss = 2.54029568\n",
      "Iteration 12975, loss = 2.54028026\n",
      "Iteration 12976, loss = 2.54008135\n",
      "Iteration 12977, loss = 2.54012411\n",
      "Iteration 12978, loss = 2.54013052\n",
      "Iteration 12979, loss = 2.54003432\n",
      "Iteration 12980, loss = 2.53998233\n",
      "Iteration 12981, loss = 2.54001822\n",
      "Iteration 12982, loss = 2.54001980\n",
      "Iteration 12983, loss = 2.53986372\n",
      "Iteration 12984, loss = 2.53996665\n",
      "Iteration 12985, loss = 2.53996264\n",
      "Iteration 12986, loss = 2.53991552\n",
      "Iteration 12987, loss = 2.53988613\n",
      "Iteration 12988, loss = 2.53991153\n",
      "Iteration 12989, loss = 2.53998618\n",
      "Iteration 12990, loss = 2.53989980\n",
      "Iteration 12991, loss = 2.53979025\n",
      "Iteration 12992, loss = 2.53991748\n",
      "Iteration 12993, loss = 2.53985905\n",
      "Iteration 12994, loss = 2.53991949\n",
      "Iteration 12995, loss = 2.53979474\n",
      "Iteration 12996, loss = 2.53979057\n",
      "Iteration 12997, loss = 2.53984887\n",
      "Iteration 12998, loss = 2.53992366\n",
      "Iteration 12999, loss = 2.53996842\n",
      "Iteration 13000, loss = 2.53996495\n",
      "Iteration 13001, loss = 2.53975068\n",
      "Iteration 13002, loss = 2.53963680\n",
      "Iteration 13003, loss = 2.53974431\n",
      "Iteration 13004, loss = 2.53997210\n",
      "Iteration 13005, loss = 2.53968498\n",
      "Iteration 13006, loss = 2.53960580\n",
      "Iteration 13007, loss = 2.53963920\n",
      "Iteration 13008, loss = 2.53962193\n",
      "Iteration 13009, loss = 2.53956587\n",
      "Iteration 13010, loss = 2.53965284\n",
      "Iteration 13011, loss = 2.53951574\n",
      "Iteration 13012, loss = 2.53952068\n",
      "Iteration 13013, loss = 2.53956339\n",
      "Iteration 13014, loss = 2.53958569\n",
      "Iteration 13015, loss = 2.53952736\n",
      "Iteration 13016, loss = 2.53938545\n",
      "Iteration 13017, loss = 2.53963573\n",
      "Iteration 13018, loss = 2.53986200\n",
      "Iteration 13019, loss = 2.53939617\n",
      "Iteration 13020, loss = 2.53940044\n",
      "Iteration 13021, loss = 2.53947909\n",
      "Iteration 13022, loss = 2.53944802\n",
      "Iteration 13023, loss = 2.53950082\n",
      "Iteration 13024, loss = 2.53933408\n",
      "Iteration 13025, loss = 2.53935182\n",
      "Iteration 13026, loss = 2.53927351\n",
      "Iteration 13027, loss = 2.53926457\n",
      "Iteration 13028, loss = 2.53938046\n",
      "Iteration 13029, loss = 2.53926206\n",
      "Iteration 13030, loss = 2.53931686\n",
      "Iteration 13031, loss = 2.53919968\n",
      "Iteration 13032, loss = 2.53923428\n",
      "Iteration 13033, loss = 2.53917840\n",
      "Iteration 13034, loss = 2.53927061\n",
      "Iteration 13035, loss = 2.53938383\n",
      "Iteration 13036, loss = 2.53934387\n",
      "Iteration 13037, loss = 2.53935417\n",
      "Iteration 13038, loss = 2.53911651\n",
      "Iteration 13039, loss = 2.53908143\n",
      "Iteration 13040, loss = 2.53912610\n",
      "Iteration 13041, loss = 2.53903443\n",
      "Iteration 13042, loss = 2.53944085\n",
      "Iteration 13043, loss = 2.53902263\n",
      "Iteration 13044, loss = 2.53896958\n",
      "Iteration 13045, loss = 2.53908505\n",
      "Iteration 13046, loss = 2.53920166\n",
      "Iteration 13047, loss = 2.53907297\n",
      "Iteration 13048, loss = 2.53897225\n",
      "Iteration 13049, loss = 2.53896605\n",
      "Iteration 13050, loss = 2.53893129\n",
      "Iteration 13051, loss = 2.53897051\n",
      "Iteration 13052, loss = 2.53894886\n",
      "Iteration 13053, loss = 2.53892579\n",
      "Iteration 13054, loss = 2.53895426\n",
      "Iteration 13055, loss = 2.53888422\n",
      "Iteration 13056, loss = 2.53890029\n",
      "Iteration 13057, loss = 2.53901009\n",
      "Iteration 13058, loss = 2.53886104\n",
      "Iteration 13059, loss = 2.53889379\n",
      "Iteration 13060, loss = 2.53881695\n",
      "Iteration 13061, loss = 2.53878956\n",
      "Iteration 13062, loss = 2.53874252\n",
      "Iteration 13063, loss = 2.53868710\n",
      "Iteration 13064, loss = 2.53873192\n",
      "Iteration 13065, loss = 2.53881782\n",
      "Iteration 13066, loss = 2.53860590\n",
      "Iteration 13067, loss = 2.53884306\n",
      "Iteration 13068, loss = 2.53875148\n",
      "Iteration 13069, loss = 2.53866139\n",
      "Iteration 13070, loss = 2.53861161\n",
      "Iteration 13071, loss = 2.53862155\n",
      "Iteration 13072, loss = 2.53859147\n",
      "Iteration 13073, loss = 2.53862498\n",
      "Iteration 13074, loss = 2.53880233\n",
      "Iteration 13075, loss = 2.53867103\n",
      "Iteration 13076, loss = 2.53858061\n",
      "Iteration 13077, loss = 2.53859177\n",
      "Iteration 13078, loss = 2.53848716\n",
      "Iteration 13079, loss = 2.53854115\n",
      "Iteration 13080, loss = 2.53856555\n",
      "Iteration 13081, loss = 2.53844212\n",
      "Iteration 13082, loss = 2.53861299\n",
      "Iteration 13083, loss = 2.53850003\n",
      "Iteration 13084, loss = 2.53848029\n",
      "Iteration 13085, loss = 2.53850297\n",
      "Iteration 13086, loss = 2.53837227\n",
      "Iteration 13087, loss = 2.53848490\n",
      "Iteration 13088, loss = 2.53846437\n",
      "Iteration 13089, loss = 2.53839426\n",
      "Iteration 13090, loss = 2.53831200\n",
      "Iteration 13091, loss = 2.53831588\n",
      "Iteration 13092, loss = 2.53846638\n",
      "Iteration 13093, loss = 2.53840426\n",
      "Iteration 13094, loss = 2.53850425\n",
      "Iteration 13095, loss = 2.53834915\n",
      "Iteration 13096, loss = 2.53829951\n",
      "Iteration 13097, loss = 2.53838258\n",
      "Iteration 13098, loss = 2.53846966\n",
      "Iteration 13099, loss = 2.53830207\n",
      "Iteration 13100, loss = 2.53834115\n",
      "Iteration 13101, loss = 2.53832753\n",
      "Iteration 13102, loss = 2.53853290\n",
      "Iteration 13103, loss = 2.53830169\n",
      "Iteration 13104, loss = 2.53848212\n",
      "Iteration 13105, loss = 2.53830823\n",
      "Iteration 13106, loss = 2.53813166\n",
      "Iteration 13107, loss = 2.53814452\n",
      "Iteration 13108, loss = 2.53826454\n",
      "Iteration 13109, loss = 2.53821197\n",
      "Iteration 13110, loss = 2.53799800\n",
      "Iteration 13111, loss = 2.53800052\n",
      "Iteration 13112, loss = 2.53812015\n",
      "Iteration 13113, loss = 2.53814483\n",
      "Iteration 13114, loss = 2.53816040\n",
      "Iteration 13115, loss = 2.53790583\n",
      "Iteration 13116, loss = 2.53804174\n",
      "Iteration 13117, loss = 2.53794887\n",
      "Iteration 13118, loss = 2.53825669\n",
      "Iteration 13119, loss = 2.53816230\n",
      "Iteration 13120, loss = 2.53784508\n",
      "Iteration 13121, loss = 2.53806073\n",
      "Iteration 13122, loss = 2.53782952\n",
      "Iteration 13123, loss = 2.53795771\n",
      "Iteration 13124, loss = 2.53818207\n",
      "Iteration 13125, loss = 2.53800494\n",
      "Iteration 13126, loss = 2.53784911\n",
      "Iteration 13127, loss = 2.53789710\n",
      "Iteration 13128, loss = 2.53786285\n",
      "Iteration 13129, loss = 2.53776177\n",
      "Iteration 13130, loss = 2.53775797\n",
      "Iteration 13131, loss = 2.53782916\n",
      "Iteration 13132, loss = 2.53787819\n",
      "Iteration 13133, loss = 2.53776849\n",
      "Iteration 13134, loss = 2.53798204\n",
      "Iteration 13135, loss = 2.53775182\n",
      "Iteration 13136, loss = 2.53774366\n",
      "Iteration 13137, loss = 2.53781999\n",
      "Iteration 13138, loss = 2.53763674\n",
      "Iteration 13139, loss = 2.53760024\n",
      "Iteration 13140, loss = 2.53766563\n",
      "Iteration 13141, loss = 2.53766805\n",
      "Iteration 13142, loss = 2.53758500\n",
      "Iteration 13143, loss = 2.53760189\n",
      "Iteration 13144, loss = 2.53781581\n",
      "Iteration 13145, loss = 2.53756388\n",
      "Iteration 13146, loss = 2.53784834\n",
      "Iteration 13147, loss = 2.53771378\n",
      "Iteration 13148, loss = 2.53747133\n",
      "Iteration 13149, loss = 2.53750034\n",
      "Iteration 13150, loss = 2.53769332\n",
      "Iteration 13151, loss = 2.53767791\n",
      "Iteration 13152, loss = 2.53750327\n",
      "Iteration 13153, loss = 2.53742232\n",
      "Iteration 13154, loss = 2.53750665\n",
      "Iteration 13155, loss = 2.53734344\n",
      "Iteration 13156, loss = 2.53750886\n",
      "Iteration 13157, loss = 2.53736424\n",
      "Iteration 13158, loss = 2.53744264\n",
      "Iteration 13159, loss = 2.53735578\n",
      "Iteration 13160, loss = 2.53730140\n",
      "Iteration 13161, loss = 2.53731181\n",
      "Iteration 13162, loss = 2.53732774\n",
      "Iteration 13163, loss = 2.53742924\n",
      "Iteration 13164, loss = 2.53731083\n",
      "Iteration 13165, loss = 2.53742802\n",
      "Iteration 13166, loss = 2.53722530\n",
      "Iteration 13167, loss = 2.53708443\n",
      "Iteration 13168, loss = 2.53736307\n",
      "Iteration 13169, loss = 2.53713432\n",
      "Iteration 13170, loss = 2.53718584\n",
      "Iteration 13171, loss = 2.53718429\n",
      "Iteration 13172, loss = 2.53713915\n",
      "Iteration 13173, loss = 2.53719288\n",
      "Iteration 13174, loss = 2.53718374\n",
      "Iteration 13175, loss = 2.53744459\n",
      "Iteration 13176, loss = 2.53705201\n",
      "Iteration 13177, loss = 2.53731748\n",
      "Iteration 13178, loss = 2.53720110\n",
      "Iteration 13179, loss = 2.53702348\n",
      "Iteration 13180, loss = 2.53716402\n",
      "Iteration 13181, loss = 2.53711738\n",
      "Iteration 13182, loss = 2.53690196\n",
      "Iteration 13183, loss = 2.53697237\n",
      "Iteration 13184, loss = 2.53707725\n",
      "Iteration 13185, loss = 2.53694632\n",
      "Iteration 13186, loss = 2.53700841\n",
      "Iteration 13187, loss = 2.53699080\n",
      "Iteration 13188, loss = 2.53690898\n",
      "Iteration 13189, loss = 2.53682536\n",
      "Iteration 13190, loss = 2.53688812\n",
      "Iteration 13191, loss = 2.53688537\n",
      "Iteration 13192, loss = 2.53687412\n",
      "Iteration 13193, loss = 2.53680950\n",
      "Iteration 13194, loss = 2.53679870\n",
      "Iteration 13195, loss = 2.53677820\n",
      "Iteration 13196, loss = 2.53680814\n",
      "Iteration 13197, loss = 2.53676769\n",
      "Iteration 13198, loss = 2.53686633\n",
      "Iteration 13199, loss = 2.53684991\n",
      "Iteration 13200, loss = 2.53686363\n",
      "Iteration 13201, loss = 2.53693238\n",
      "Iteration 13202, loss = 2.53679874\n",
      "Iteration 13203, loss = 2.53668934\n",
      "Iteration 13204, loss = 2.53691044\n",
      "Iteration 13205, loss = 2.53669639\n",
      "Iteration 13206, loss = 2.53682719\n",
      "Iteration 13207, loss = 2.53670814\n",
      "Iteration 13208, loss = 2.53660172\n",
      "Iteration 13209, loss = 2.53653516\n",
      "Iteration 13210, loss = 2.53661942\n",
      "Iteration 13211, loss = 2.53656292\n",
      "Iteration 13212, loss = 2.53669068\n",
      "Iteration 13213, loss = 2.53657475\n",
      "Iteration 13214, loss = 2.53663202\n",
      "Iteration 13215, loss = 2.53645772\n",
      "Iteration 13216, loss = 2.53662443\n",
      "Iteration 13217, loss = 2.53662067\n",
      "Iteration 13218, loss = 2.53638910\n",
      "Iteration 13219, loss = 2.53659259\n",
      "Iteration 13220, loss = 2.53643641\n",
      "Iteration 13221, loss = 2.53640788\n",
      "Iteration 13222, loss = 2.53641509\n",
      "Iteration 13223, loss = 2.53656901\n",
      "Iteration 13224, loss = 2.53630396\n",
      "Iteration 13225, loss = 2.53628201\n",
      "Iteration 13226, loss = 2.53640137\n",
      "Iteration 13227, loss = 2.53635621\n",
      "Iteration 13228, loss = 2.53626068\n",
      "Iteration 13229, loss = 2.53631606\n",
      "Iteration 13230, loss = 2.53632995\n",
      "Iteration 13231, loss = 2.53620368\n",
      "Iteration 13232, loss = 2.53627857\n",
      "Iteration 13233, loss = 2.53625986\n",
      "Iteration 13234, loss = 2.53625598\n",
      "Iteration 13235, loss = 2.53632964\n",
      "Iteration 13236, loss = 2.53615825\n",
      "Iteration 13237, loss = 2.53623299\n",
      "Iteration 13238, loss = 2.53616939\n",
      "Iteration 13239, loss = 2.53619942\n",
      "Iteration 13240, loss = 2.53612698\n",
      "Iteration 13241, loss = 2.53622184\n",
      "Iteration 13242, loss = 2.53611637\n",
      "Iteration 13243, loss = 2.53606249\n",
      "Iteration 13244, loss = 2.53612597\n",
      "Iteration 13245, loss = 2.53606028\n",
      "Iteration 13246, loss = 2.53610656\n",
      "Iteration 13247, loss = 2.53608532\n",
      "Iteration 13248, loss = 2.53605602\n",
      "Iteration 13249, loss = 2.53605300\n",
      "Iteration 13250, loss = 2.53598131\n",
      "Iteration 13251, loss = 2.53599341\n",
      "Iteration 13252, loss = 2.53594336\n",
      "Iteration 13253, loss = 2.53596425\n",
      "Iteration 13254, loss = 2.53595108\n",
      "Iteration 13255, loss = 2.53598956\n",
      "Iteration 13256, loss = 2.53607983\n",
      "Iteration 13257, loss = 2.53608085\n",
      "Iteration 13258, loss = 2.53590220\n",
      "Iteration 13259, loss = 2.53588252\n",
      "Iteration 13260, loss = 2.53588384\n",
      "Iteration 13261, loss = 2.53585958\n",
      "Iteration 13262, loss = 2.53584155\n",
      "Iteration 13263, loss = 2.53604523\n",
      "Iteration 13264, loss = 2.53571910\n",
      "Iteration 13265, loss = 2.53579802\n",
      "Iteration 13266, loss = 2.53574539\n",
      "Iteration 13267, loss = 2.53576468\n",
      "Iteration 13268, loss = 2.53579438\n",
      "Iteration 13269, loss = 2.53574172\n",
      "Iteration 13270, loss = 2.53569340\n",
      "Iteration 13271, loss = 2.53570348\n",
      "Iteration 13272, loss = 2.53560583\n",
      "Iteration 13273, loss = 2.53556859\n",
      "Iteration 13274, loss = 2.53583582\n",
      "Iteration 13275, loss = 2.53565711\n",
      "Iteration 13276, loss = 2.53570313\n",
      "Iteration 13277, loss = 2.53562913\n",
      "Iteration 13278, loss = 2.53564597\n",
      "Iteration 13279, loss = 2.53557368\n",
      "Iteration 13280, loss = 2.53556622\n",
      "Iteration 13281, loss = 2.53561297\n",
      "Iteration 13282, loss = 2.53552163\n",
      "Iteration 13283, loss = 2.53571001\n",
      "Iteration 13284, loss = 2.53548381\n",
      "Iteration 13285, loss = 2.53545963\n",
      "Iteration 13286, loss = 2.53541004\n",
      "Iteration 13287, loss = 2.53538711\n",
      "Iteration 13288, loss = 2.53544764\n",
      "Iteration 13289, loss = 2.53546258\n",
      "Iteration 13290, loss = 2.53539939\n",
      "Iteration 13291, loss = 2.53551312\n",
      "Iteration 13292, loss = 2.53544388\n",
      "Iteration 13293, loss = 2.53554893\n",
      "Iteration 13294, loss = 2.53568576\n",
      "Iteration 13295, loss = 2.53539100\n",
      "Iteration 13296, loss = 2.53525591\n",
      "Iteration 13297, loss = 2.53526842\n",
      "Iteration 13298, loss = 2.53530160\n",
      "Iteration 13299, loss = 2.53546358\n",
      "Iteration 13300, loss = 2.53520659\n",
      "Iteration 13301, loss = 2.53521416\n",
      "Iteration 13302, loss = 2.53520591\n",
      "Iteration 13303, loss = 2.53525078\n",
      "Iteration 13304, loss = 2.53524016\n",
      "Iteration 13305, loss = 2.53511106\n",
      "Iteration 13306, loss = 2.53532664\n",
      "Iteration 13307, loss = 2.53507530\n",
      "Iteration 13308, loss = 2.53515350\n",
      "Iteration 13309, loss = 2.53519521\n",
      "Iteration 13310, loss = 2.53523092\n",
      "Iteration 13311, loss = 2.53514113\n",
      "Iteration 13312, loss = 2.53512869\n",
      "Iteration 13313, loss = 2.53506846\n",
      "Iteration 13314, loss = 2.53507774\n",
      "Iteration 13315, loss = 2.53500398\n",
      "Iteration 13316, loss = 2.53506672\n",
      "Iteration 13317, loss = 2.53503176\n",
      "Iteration 13318, loss = 2.53503084\n",
      "Iteration 13319, loss = 2.53486052\n",
      "Iteration 13320, loss = 2.53494908\n",
      "Iteration 13321, loss = 2.53497039\n",
      "Iteration 13322, loss = 2.53500552\n",
      "Iteration 13323, loss = 2.53502579\n",
      "Iteration 13324, loss = 2.53508573\n",
      "Iteration 13325, loss = 2.53481401\n",
      "Iteration 13326, loss = 2.53485571\n",
      "Iteration 13327, loss = 2.53489997\n",
      "Iteration 13328, loss = 2.53480944\n",
      "Iteration 13329, loss = 2.53479476\n",
      "Iteration 13330, loss = 2.53472395\n",
      "Iteration 13331, loss = 2.53474067\n",
      "Iteration 13332, loss = 2.53477133\n",
      "Iteration 13333, loss = 2.53476783\n",
      "Iteration 13334, loss = 2.53474396\n",
      "Iteration 13335, loss = 2.53487585\n",
      "Iteration 13336, loss = 2.53468863\n",
      "Iteration 13337, loss = 2.53480844\n",
      "Iteration 13338, loss = 2.53476962\n",
      "Iteration 13339, loss = 2.53478336\n",
      "Iteration 13340, loss = 2.53461073\n",
      "Iteration 13341, loss = 2.53478959\n",
      "Iteration 13342, loss = 2.53465069\n",
      "Iteration 13343, loss = 2.53455942\n",
      "Iteration 13344, loss = 2.53480616\n",
      "Iteration 13345, loss = 2.53458519\n",
      "Iteration 13346, loss = 2.53452599\n",
      "Iteration 13347, loss = 2.53453327\n",
      "Iteration 13348, loss = 2.53450561\n",
      "Iteration 13349, loss = 2.53451696\n",
      "Iteration 13350, loss = 2.53449069\n",
      "Iteration 13351, loss = 2.53449610\n",
      "Iteration 13352, loss = 2.53443895\n",
      "Iteration 13353, loss = 2.53440237\n",
      "Iteration 13354, loss = 2.53441933\n",
      "Iteration 13355, loss = 2.53432864\n",
      "Iteration 13356, loss = 2.53442237\n",
      "Iteration 13357, loss = 2.53458430\n",
      "Iteration 13358, loss = 2.53435222\n",
      "Iteration 13359, loss = 2.53440666\n",
      "Iteration 13360, loss = 2.53440494\n",
      "Iteration 13361, loss = 2.53430037\n",
      "Iteration 13362, loss = 2.53427171\n",
      "Iteration 13363, loss = 2.53432554\n",
      "Iteration 13364, loss = 2.53435899\n",
      "Iteration 13365, loss = 2.53423511\n",
      "Iteration 13366, loss = 2.53427634\n",
      "Iteration 13367, loss = 2.53418191\n",
      "Iteration 13368, loss = 2.53423886\n",
      "Iteration 13369, loss = 2.53421974\n",
      "Iteration 13370, loss = 2.53416232\n",
      "Iteration 13371, loss = 2.53420848\n",
      "Iteration 13372, loss = 2.53417718\n",
      "Iteration 13373, loss = 2.53411673\n",
      "Iteration 13374, loss = 2.53417409\n",
      "Iteration 13375, loss = 2.53406979\n",
      "Iteration 13376, loss = 2.53422977\n",
      "Iteration 13377, loss = 2.53417277\n",
      "Iteration 13378, loss = 2.53408796\n",
      "Iteration 13379, loss = 2.53407764\n",
      "Iteration 13380, loss = 2.53421234\n",
      "Iteration 13381, loss = 2.53403418\n",
      "Iteration 13382, loss = 2.53407136\n",
      "Iteration 13383, loss = 2.53399246\n",
      "Iteration 13384, loss = 2.53432201\n",
      "Iteration 13385, loss = 2.53407934\n",
      "Iteration 13386, loss = 2.53393505\n",
      "Iteration 13387, loss = 2.53397790\n",
      "Iteration 13388, loss = 2.53391320\n",
      "Iteration 13389, loss = 2.53395223\n",
      "Iteration 13390, loss = 2.53401137\n",
      "Iteration 13391, loss = 2.53389919\n",
      "Iteration 13392, loss = 2.53391504\n",
      "Iteration 13393, loss = 2.53382746\n",
      "Iteration 13394, loss = 2.53382948\n",
      "Iteration 13395, loss = 2.53372851\n",
      "Iteration 13396, loss = 2.53382255\n",
      "Iteration 13397, loss = 2.53375534\n",
      "Iteration 13398, loss = 2.53385613\n",
      "Iteration 13399, loss = 2.53383552\n",
      "Iteration 13400, loss = 2.53374589\n",
      "Iteration 13401, loss = 2.53372729\n",
      "Iteration 13402, loss = 2.53382942\n",
      "Iteration 13403, loss = 2.53385190\n",
      "Iteration 13404, loss = 2.53372007\n",
      "Iteration 13405, loss = 2.53367376\n",
      "Iteration 13406, loss = 2.53357734\n",
      "Iteration 13407, loss = 2.53367476\n",
      "Iteration 13408, loss = 2.53375702\n",
      "Iteration 13409, loss = 2.53355053\n",
      "Iteration 13410, loss = 2.53359111\n",
      "Iteration 13411, loss = 2.53354054\n",
      "Iteration 13412, loss = 2.53353109\n",
      "Iteration 13413, loss = 2.53378697\n",
      "Iteration 13414, loss = 2.53353879\n",
      "Iteration 13415, loss = 2.53347954\n",
      "Iteration 13416, loss = 2.53352848\n",
      "Iteration 13417, loss = 2.53347205\n",
      "Iteration 13418, loss = 2.53352102\n",
      "Iteration 13419, loss = 2.53345733\n",
      "Iteration 13420, loss = 2.53351311\n",
      "Iteration 13421, loss = 2.53364276\n",
      "Iteration 13422, loss = 2.53339937\n",
      "Iteration 13423, loss = 2.53340566\n",
      "Iteration 13424, loss = 2.53335050\n",
      "Iteration 13425, loss = 2.53334144\n",
      "Iteration 13426, loss = 2.53333026\n",
      "Iteration 13427, loss = 2.53329713\n",
      "Iteration 13428, loss = 2.53330957\n",
      "Iteration 13429, loss = 2.53330142\n",
      "Iteration 13430, loss = 2.53323928\n",
      "Iteration 13431, loss = 2.53325371\n",
      "Iteration 13432, loss = 2.53325436\n",
      "Iteration 13433, loss = 2.53321722\n",
      "Iteration 13434, loss = 2.53320875\n",
      "Iteration 13435, loss = 2.53313855\n",
      "Iteration 13436, loss = 2.53318737\n",
      "Iteration 13437, loss = 2.53328047\n",
      "Iteration 13438, loss = 2.53330138\n",
      "Iteration 13439, loss = 2.53340677\n",
      "Iteration 13440, loss = 2.53325369\n",
      "Iteration 13441, loss = 2.53318686\n",
      "Iteration 13442, loss = 2.53317119\n",
      "Iteration 13443, loss = 2.53319639\n",
      "Iteration 13444, loss = 2.53315478\n",
      "Iteration 13445, loss = 2.53301985\n",
      "Iteration 13446, loss = 2.53309909\n",
      "Iteration 13447, loss = 2.53299808\n",
      "Iteration 13448, loss = 2.53306700\n",
      "Iteration 13449, loss = 2.53295043\n",
      "Iteration 13450, loss = 2.53303784\n",
      "Iteration 13451, loss = 2.53296343\n",
      "Iteration 13452, loss = 2.53298811\n",
      "Iteration 13453, loss = 2.53298671\n",
      "Iteration 13454, loss = 2.53290764\n",
      "Iteration 13455, loss = 2.53302454\n",
      "Iteration 13456, loss = 2.53286704\n",
      "Iteration 13457, loss = 2.53295198\n",
      "Iteration 13458, loss = 2.53299469\n",
      "Iteration 13459, loss = 2.53281997\n",
      "Iteration 13460, loss = 2.53282901\n",
      "Iteration 13461, loss = 2.53278017\n",
      "Iteration 13462, loss = 2.53301996\n",
      "Iteration 13463, loss = 2.53273506\n",
      "Iteration 13464, loss = 2.53314767\n",
      "Iteration 13465, loss = 2.53297895\n",
      "Iteration 13466, loss = 2.53269655\n",
      "Iteration 13467, loss = 2.53271828\n",
      "Iteration 13468, loss = 2.53264343\n",
      "Iteration 13469, loss = 2.53276021\n",
      "Iteration 13470, loss = 2.53281086\n",
      "Iteration 13471, loss = 2.53265467\n",
      "Iteration 13472, loss = 2.53265777\n",
      "Iteration 13473, loss = 2.53268835\n",
      "Iteration 13474, loss = 2.53280829\n",
      "Iteration 13475, loss = 2.53264312\n",
      "Iteration 13476, loss = 2.53262156\n",
      "Iteration 13477, loss = 2.53263912\n",
      "Iteration 13478, loss = 2.53254597\n",
      "Iteration 13479, loss = 2.53256163\n",
      "Iteration 13480, loss = 2.53266272\n",
      "Iteration 13481, loss = 2.53255608\n",
      "Iteration 13482, loss = 2.53259410\n",
      "Iteration 13483, loss = 2.53249061\n",
      "Iteration 13484, loss = 2.53269973\n",
      "Iteration 13485, loss = 2.53279291\n",
      "Iteration 13486, loss = 2.53247225\n",
      "Iteration 13487, loss = 2.53238880\n",
      "Iteration 13488, loss = 2.53243217\n",
      "Iteration 13489, loss = 2.53275747\n",
      "Iteration 13490, loss = 2.53241349\n",
      "Iteration 13491, loss = 2.53259747\n",
      "Iteration 13492, loss = 2.53246833\n",
      "Iteration 13493, loss = 2.53244276\n",
      "Iteration 13494, loss = 2.53232568\n",
      "Iteration 13495, loss = 2.53227403\n",
      "Iteration 13496, loss = 2.53224756\n",
      "Iteration 13497, loss = 2.53224325\n",
      "Iteration 13498, loss = 2.53225916\n",
      "Iteration 13499, loss = 2.53241588\n",
      "Iteration 13500, loss = 2.53245325\n",
      "Iteration 13501, loss = 2.53216008\n",
      "Iteration 13502, loss = 2.53210498\n",
      "Iteration 13503, loss = 2.53215058\n",
      "Iteration 13504, loss = 2.53211359\n",
      "Iteration 13505, loss = 2.53211201\n",
      "Iteration 13506, loss = 2.53217465\n",
      "Iteration 13507, loss = 2.53210259\n",
      "Iteration 13508, loss = 2.53205312\n",
      "Iteration 13509, loss = 2.53209714\n",
      "Iteration 13510, loss = 2.53216155\n",
      "Iteration 13511, loss = 2.53205848\n",
      "Iteration 13512, loss = 2.53210673\n",
      "Iteration 13513, loss = 2.53205728\n",
      "Iteration 13514, loss = 2.53201788\n",
      "Iteration 13515, loss = 2.53212828\n",
      "Iteration 13516, loss = 2.53201857\n",
      "Iteration 13517, loss = 2.53201501\n",
      "Iteration 13518, loss = 2.53196339\n",
      "Iteration 13519, loss = 2.53195903\n",
      "Iteration 13520, loss = 2.53191029\n",
      "Iteration 13521, loss = 2.53200231\n",
      "Iteration 13522, loss = 2.53183216\n",
      "Iteration 13523, loss = 2.53193864\n",
      "Iteration 13524, loss = 2.53195155\n",
      "Iteration 13525, loss = 2.53196510\n",
      "Iteration 13526, loss = 2.53188942\n",
      "Iteration 13527, loss = 2.53184010\n",
      "Iteration 13528, loss = 2.53190439\n",
      "Iteration 13529, loss = 2.53192059\n",
      "Iteration 13530, loss = 2.53175199\n",
      "Iteration 13531, loss = 2.53182064\n",
      "Iteration 13532, loss = 2.53170349\n",
      "Iteration 13533, loss = 2.53169145\n",
      "Iteration 13534, loss = 2.53167576\n",
      "Iteration 13535, loss = 2.53174185\n",
      "Iteration 13536, loss = 2.53159498\n",
      "Iteration 13537, loss = 2.53159047\n",
      "Iteration 13538, loss = 2.53163980\n",
      "Iteration 13539, loss = 2.53168098\n",
      "Iteration 13540, loss = 2.53155038\n",
      "Iteration 13541, loss = 2.53163302\n",
      "Iteration 13542, loss = 2.53164126\n",
      "Iteration 13543, loss = 2.53150209\n",
      "Iteration 13544, loss = 2.53152898\n",
      "Iteration 13545, loss = 2.53161235\n",
      "Iteration 13546, loss = 2.53151781\n",
      "Iteration 13547, loss = 2.53164381\n",
      "Iteration 13548, loss = 2.53149274\n",
      "Iteration 13549, loss = 2.53145274\n",
      "Iteration 13550, loss = 2.53139781\n",
      "Iteration 13551, loss = 2.53152430\n",
      "Iteration 13552, loss = 2.53138549\n",
      "Iteration 13553, loss = 2.53143325\n",
      "Iteration 13554, loss = 2.53142728\n",
      "Iteration 13555, loss = 2.53137654\n",
      "Iteration 13556, loss = 2.53138774\n",
      "Iteration 13557, loss = 2.53144289\n",
      "Iteration 13558, loss = 2.53139283\n",
      "Iteration 13559, loss = 2.53138843\n",
      "Iteration 13560, loss = 2.53144271\n",
      "Iteration 13561, loss = 2.53137785\n",
      "Iteration 13562, loss = 2.53126167\n",
      "Iteration 13563, loss = 2.53135942\n",
      "Iteration 13564, loss = 2.53137131\n",
      "Iteration 13565, loss = 2.53118397\n",
      "Iteration 13566, loss = 2.53126253\n",
      "Iteration 13567, loss = 2.53124502\n",
      "Iteration 13568, loss = 2.53115890\n",
      "Iteration 13569, loss = 2.53121472\n",
      "Iteration 13570, loss = 2.53118878\n",
      "Iteration 13571, loss = 2.53113051\n",
      "Iteration 13572, loss = 2.53131659\n",
      "Iteration 13573, loss = 2.53108565\n",
      "Iteration 13574, loss = 2.53109332\n",
      "Iteration 13575, loss = 2.53128257\n",
      "Iteration 13576, loss = 2.53106269\n",
      "Iteration 13577, loss = 2.53106141\n",
      "Iteration 13578, loss = 2.53105113\n",
      "Iteration 13579, loss = 2.53111818\n",
      "Iteration 13580, loss = 2.53099516\n",
      "Iteration 13581, loss = 2.53108549\n",
      "Iteration 13582, loss = 2.53092090\n",
      "Iteration 13583, loss = 2.53103378\n",
      "Iteration 13584, loss = 2.53095056\n",
      "Iteration 13585, loss = 2.53087110\n",
      "Iteration 13586, loss = 2.53086829\n",
      "Iteration 13587, loss = 2.53087930\n",
      "Iteration 13588, loss = 2.53088112\n",
      "Iteration 13589, loss = 2.53108706\n",
      "Iteration 13590, loss = 2.53084174\n",
      "Iteration 13591, loss = 2.53082637\n",
      "Iteration 13592, loss = 2.53095107\n",
      "Iteration 13593, loss = 2.53119388\n",
      "Iteration 13594, loss = 2.53085884\n",
      "Iteration 13595, loss = 2.53083379\n",
      "Iteration 13596, loss = 2.53081730\n",
      "Iteration 13597, loss = 2.53082415\n",
      "Iteration 13598, loss = 2.53097656\n",
      "Iteration 13599, loss = 2.53073575\n",
      "Iteration 13600, loss = 2.53071294\n",
      "Iteration 13601, loss = 2.53090548\n",
      "Iteration 13602, loss = 2.53071263\n",
      "Iteration 13603, loss = 2.53089294\n",
      "Iteration 13604, loss = 2.53067845\n",
      "Iteration 13605, loss = 2.53053335\n",
      "Iteration 13606, loss = 2.53069279\n",
      "Iteration 13607, loss = 2.53060634\n",
      "Iteration 13608, loss = 2.53065573\n",
      "Iteration 13609, loss = 2.53060618\n",
      "Iteration 13610, loss = 2.53068329\n",
      "Iteration 13611, loss = 2.53050036\n",
      "Iteration 13612, loss = 2.53052948\n",
      "Iteration 13613, loss = 2.53046699\n",
      "Iteration 13614, loss = 2.53060134\n",
      "Iteration 13615, loss = 2.53059411\n",
      "Iteration 13616, loss = 2.53072774\n",
      "Iteration 13617, loss = 2.53059397\n",
      "Iteration 13618, loss = 2.53058557\n",
      "Iteration 13619, loss = 2.53045176\n",
      "Iteration 13620, loss = 2.53045487\n",
      "Iteration 13621, loss = 2.53039640\n",
      "Iteration 13622, loss = 2.53079140\n",
      "Iteration 13623, loss = 2.53025452\n",
      "Iteration 13624, loss = 2.53037651\n",
      "Iteration 13625, loss = 2.53038789\n",
      "Iteration 13626, loss = 2.53060039\n",
      "Iteration 13627, loss = 2.53059166\n",
      "Iteration 13628, loss = 2.53025165\n",
      "Iteration 13629, loss = 2.53037502\n",
      "Iteration 13630, loss = 2.53025768\n",
      "Iteration 13631, loss = 2.53020386\n",
      "Iteration 13632, loss = 2.53025024\n",
      "Iteration 13633, loss = 2.53018721\n",
      "Iteration 13634, loss = 2.53013146\n",
      "Iteration 13635, loss = 2.53022588\n",
      "Iteration 13636, loss = 2.53028427\n",
      "Iteration 13637, loss = 2.53029332\n",
      "Iteration 13638, loss = 2.53015151\n",
      "Iteration 13639, loss = 2.53006236\n",
      "Iteration 13640, loss = 2.53015039\n",
      "Iteration 13641, loss = 2.53010852\n",
      "Iteration 13642, loss = 2.53005417\n",
      "Iteration 13643, loss = 2.53015835\n",
      "Iteration 13644, loss = 2.53023279\n",
      "Iteration 13645, loss = 2.53000102\n",
      "Iteration 13646, loss = 2.53002398\n",
      "Iteration 13647, loss = 2.53012986\n",
      "Iteration 13648, loss = 2.53034160\n",
      "Iteration 13649, loss = 2.52990526\n",
      "Iteration 13650, loss = 2.53005498\n",
      "Iteration 13651, loss = 2.52996011\n",
      "Iteration 13652, loss = 2.52988735\n",
      "Iteration 13653, loss = 2.53010550\n",
      "Iteration 13654, loss = 2.52987888\n",
      "Iteration 13655, loss = 2.52991705\n",
      "Iteration 13656, loss = 2.52993418\n",
      "Iteration 13657, loss = 2.52997497\n",
      "Iteration 13658, loss = 2.52997874\n",
      "Iteration 13659, loss = 2.52981657\n",
      "Iteration 13660, loss = 2.52981342\n",
      "Iteration 13661, loss = 2.52979098\n",
      "Iteration 13662, loss = 2.52996607\n",
      "Iteration 13663, loss = 2.52978444\n",
      "Iteration 13664, loss = 2.52974495\n",
      "Iteration 13665, loss = 2.52959659\n",
      "Iteration 13666, loss = 2.52982156\n",
      "Iteration 13667, loss = 2.52966565\n",
      "Iteration 13668, loss = 2.52974256\n",
      "Iteration 13669, loss = 2.52958695\n",
      "Iteration 13670, loss = 2.52967900\n",
      "Iteration 13671, loss = 2.52961788\n",
      "Iteration 13672, loss = 2.52958655\n",
      "Iteration 13673, loss = 2.52961497\n",
      "Iteration 13674, loss = 2.52957926\n",
      "Iteration 13675, loss = 2.52976123\n",
      "Iteration 13676, loss = 2.52948681\n",
      "Iteration 13677, loss = 2.52952111\n",
      "Iteration 13678, loss = 2.52966131\n",
      "Iteration 13679, loss = 2.52962533\n",
      "Iteration 13680, loss = 2.52943694\n",
      "Iteration 13681, loss = 2.52947085\n",
      "Iteration 13682, loss = 2.52968714\n",
      "Iteration 13683, loss = 2.52953227\n",
      "Iteration 13684, loss = 2.52947907\n",
      "Iteration 13685, loss = 2.52949634\n",
      "Iteration 13686, loss = 2.52943977\n",
      "Iteration 13687, loss = 2.52931659\n",
      "Iteration 13688, loss = 2.52946633\n",
      "Iteration 13689, loss = 2.52941280\n",
      "Iteration 13690, loss = 2.52950624\n",
      "Iteration 13691, loss = 2.52947300\n",
      "Iteration 13692, loss = 2.52931864\n",
      "Iteration 13693, loss = 2.52923844\n",
      "Iteration 13694, loss = 2.52943728\n",
      "Iteration 13695, loss = 2.52923220\n",
      "Iteration 13696, loss = 2.52924943\n",
      "Iteration 13697, loss = 2.52919038\n",
      "Iteration 13698, loss = 2.52918399\n",
      "Iteration 13699, loss = 2.52908810\n",
      "Iteration 13700, loss = 2.52926023\n",
      "Iteration 13701, loss = 2.52923535\n",
      "Iteration 13702, loss = 2.52922816\n",
      "Iteration 13703, loss = 2.52908788\n",
      "Iteration 13704, loss = 2.52911530\n",
      "Iteration 13705, loss = 2.52911849\n",
      "Iteration 13706, loss = 2.52908628\n",
      "Iteration 13707, loss = 2.52915967\n",
      "Iteration 13708, loss = 2.52900754\n",
      "Iteration 13709, loss = 2.52905932\n",
      "Iteration 13710, loss = 2.52901245\n",
      "Iteration 13711, loss = 2.52897176\n",
      "Iteration 13712, loss = 2.52906014\n",
      "Iteration 13713, loss = 2.52904550\n",
      "Iteration 13714, loss = 2.52891681\n",
      "Iteration 13715, loss = 2.52891195\n",
      "Iteration 13716, loss = 2.52889798\n",
      "Iteration 13717, loss = 2.52889851\n",
      "Iteration 13718, loss = 2.52882522\n",
      "Iteration 13719, loss = 2.52896426\n",
      "Iteration 13720, loss = 2.52883446\n",
      "Iteration 13721, loss = 2.52896173\n",
      "Iteration 13722, loss = 2.52895100\n",
      "Iteration 13723, loss = 2.52891376\n",
      "Iteration 13724, loss = 2.52886122\n",
      "Iteration 13725, loss = 2.52878419\n",
      "Iteration 13726, loss = 2.52880406\n",
      "Iteration 13727, loss = 2.52881631\n",
      "Iteration 13728, loss = 2.52885277\n",
      "Iteration 13729, loss = 2.52877174\n",
      "Iteration 13730, loss = 2.52875904\n",
      "Iteration 13731, loss = 2.52872914\n",
      "Iteration 13732, loss = 2.52864497\n",
      "Iteration 13733, loss = 2.52874554\n",
      "Iteration 13734, loss = 2.52865121\n",
      "Iteration 13735, loss = 2.52872896\n",
      "Iteration 13736, loss = 2.52866718\n",
      "Iteration 13737, loss = 2.52865918\n",
      "Iteration 13738, loss = 2.52873652\n",
      "Iteration 13739, loss = 2.52857055\n",
      "Iteration 13740, loss = 2.52858033\n",
      "Iteration 13741, loss = 2.52864309\n",
      "Iteration 13742, loss = 2.52851249\n",
      "Iteration 13743, loss = 2.52865672\n",
      "Iteration 13744, loss = 2.52870970\n",
      "Iteration 13745, loss = 2.52852692\n",
      "Iteration 13746, loss = 2.52885502\n",
      "Iteration 13747, loss = 2.52839116\n",
      "Iteration 13748, loss = 2.52847947\n",
      "Iteration 13749, loss = 2.52852975\n",
      "Iteration 13750, loss = 2.52835950\n",
      "Iteration 13751, loss = 2.52845265\n",
      "Iteration 13752, loss = 2.52839993\n",
      "Iteration 13753, loss = 2.52843645\n",
      "Iteration 13754, loss = 2.52844880\n",
      "Iteration 13755, loss = 2.52841638\n",
      "Iteration 13756, loss = 2.52836332\n",
      "Iteration 13757, loss = 2.52887250\n",
      "Iteration 13758, loss = 2.52831713\n",
      "Iteration 13759, loss = 2.52837732\n",
      "Iteration 13760, loss = 2.52827152\n",
      "Iteration 13761, loss = 2.52826528\n",
      "Iteration 13762, loss = 2.52839635\n",
      "Iteration 13763, loss = 2.52821698\n",
      "Iteration 13764, loss = 2.52826044\n",
      "Iteration 13765, loss = 2.52824760\n",
      "Iteration 13766, loss = 2.52821167\n",
      "Iteration 13767, loss = 2.52822531\n",
      "Iteration 13768, loss = 2.52821581\n",
      "Iteration 13769, loss = 2.52821294\n",
      "Iteration 13770, loss = 2.52807544\n",
      "Iteration 13771, loss = 2.52815655\n",
      "Iteration 13772, loss = 2.52817290\n",
      "Iteration 13773, loss = 2.52805113\n",
      "Iteration 13774, loss = 2.52804634\n",
      "Iteration 13775, loss = 2.52813044\n",
      "Iteration 13776, loss = 2.52803729\n",
      "Iteration 13777, loss = 2.52806622\n",
      "Iteration 13778, loss = 2.52799806\n",
      "Iteration 13779, loss = 2.52798078\n",
      "Iteration 13780, loss = 2.52799064\n",
      "Iteration 13781, loss = 2.52809806\n",
      "Iteration 13782, loss = 2.52792604\n",
      "Iteration 13783, loss = 2.52819979\n",
      "Iteration 13784, loss = 2.52799221\n",
      "Iteration 13785, loss = 2.52791634\n",
      "Iteration 13786, loss = 2.52793916\n",
      "Iteration 13787, loss = 2.52783553\n",
      "Iteration 13788, loss = 2.52781386\n",
      "Iteration 13789, loss = 2.52788267\n",
      "Iteration 13790, loss = 2.52799402\n",
      "Iteration 13791, loss = 2.52780110\n",
      "Iteration 13792, loss = 2.52776475\n",
      "Iteration 13793, loss = 2.52776126\n",
      "Iteration 13794, loss = 2.52780706\n",
      "Iteration 13795, loss = 2.52776875\n",
      "Iteration 13796, loss = 2.52779372\n",
      "Iteration 13797, loss = 2.52783116\n",
      "Iteration 13798, loss = 2.52767765\n",
      "Iteration 13799, loss = 2.52770707\n",
      "Iteration 13800, loss = 2.52765824\n",
      "Iteration 13801, loss = 2.52772319\n",
      "Iteration 13802, loss = 2.52760879\n",
      "Iteration 13803, loss = 2.52766023\n",
      "Iteration 13804, loss = 2.52756143\n",
      "Iteration 13805, loss = 2.52760532\n",
      "Iteration 13806, loss = 2.52761037\n",
      "Iteration 13807, loss = 2.52758091\n",
      "Iteration 13808, loss = 2.52767559\n",
      "Iteration 13809, loss = 2.52749785\n",
      "Iteration 13810, loss = 2.52749862\n",
      "Iteration 13811, loss = 2.52753550\n",
      "Iteration 13812, loss = 2.52752952\n",
      "Iteration 13813, loss = 2.52739497\n",
      "Iteration 13814, loss = 2.52754640\n",
      "Iteration 13815, loss = 2.52749300\n",
      "Iteration 13816, loss = 2.52752862\n",
      "Iteration 13817, loss = 2.52734156\n",
      "Iteration 13818, loss = 2.52740431\n",
      "Iteration 13819, loss = 2.52732906\n",
      "Iteration 13820, loss = 2.52733085\n",
      "Iteration 13821, loss = 2.52739761\n",
      "Iteration 13822, loss = 2.52757522\n",
      "Iteration 13823, loss = 2.52735670\n",
      "Iteration 13824, loss = 2.52734259\n",
      "Iteration 13825, loss = 2.52728238\n",
      "Iteration 13826, loss = 2.52726615\n",
      "Iteration 13827, loss = 2.52731701\n",
      "Iteration 13828, loss = 2.52728676\n",
      "Iteration 13829, loss = 2.52708157\n",
      "Iteration 13830, loss = 2.52735541\n",
      "Iteration 13831, loss = 2.52719979\n",
      "Iteration 13832, loss = 2.52717609\n",
      "Iteration 13833, loss = 2.52727028\n",
      "Iteration 13834, loss = 2.52717050\n",
      "Iteration 13835, loss = 2.52721031\n",
      "Iteration 13836, loss = 2.52729719\n",
      "Iteration 13837, loss = 2.52716536\n",
      "Iteration 13838, loss = 2.52709227\n",
      "Iteration 13839, loss = 2.52696534\n",
      "Iteration 13840, loss = 2.52704727\n",
      "Iteration 13841, loss = 2.52703039\n",
      "Iteration 13842, loss = 2.52703229\n",
      "Iteration 13843, loss = 2.52694021\n",
      "Iteration 13844, loss = 2.52694881\n",
      "Iteration 13845, loss = 2.52706155\n",
      "Iteration 13846, loss = 2.52698381\n",
      "Iteration 13847, loss = 2.52689851\n",
      "Iteration 13848, loss = 2.52693531\n",
      "Iteration 13849, loss = 2.52689205\n",
      "Iteration 13850, loss = 2.52693046\n",
      "Iteration 13851, loss = 2.52687684\n",
      "Iteration 13852, loss = 2.52694706\n",
      "Iteration 13853, loss = 2.52689248\n",
      "Iteration 13854, loss = 2.52681493\n",
      "Iteration 13855, loss = 2.52699272\n",
      "Iteration 13856, loss = 2.52683187\n",
      "Iteration 13857, loss = 2.52683719\n",
      "Iteration 13858, loss = 2.52671775\n",
      "Iteration 13859, loss = 2.52679966\n",
      "Iteration 13860, loss = 2.52675177\n",
      "Iteration 13861, loss = 2.52682396\n",
      "Iteration 13862, loss = 2.52670733\n",
      "Iteration 13863, loss = 2.52686597\n",
      "Iteration 13864, loss = 2.52669447\n",
      "Iteration 13865, loss = 2.52672131\n",
      "Iteration 13866, loss = 2.52680512\n",
      "Iteration 13867, loss = 2.52661114\n",
      "Iteration 13868, loss = 2.52679477\n",
      "Iteration 13869, loss = 2.52670532\n",
      "Iteration 13870, loss = 2.52658042\n",
      "Iteration 13871, loss = 2.52656745\n",
      "Iteration 13872, loss = 2.52651034\n",
      "Iteration 13873, loss = 2.52678779\n",
      "Iteration 13874, loss = 2.52658806\n",
      "Iteration 13875, loss = 2.52657149\n",
      "Iteration 13876, loss = 2.52673693\n",
      "Iteration 13877, loss = 2.52643364\n",
      "Iteration 13878, loss = 2.52649673\n",
      "Iteration 13879, loss = 2.52653147\n",
      "Iteration 13880, loss = 2.52638866\n",
      "Iteration 13881, loss = 2.52633511\n",
      "Iteration 13882, loss = 2.52643858\n",
      "Iteration 13883, loss = 2.52661712\n",
      "Iteration 13884, loss = 2.52627787\n",
      "Iteration 13885, loss = 2.52658778\n",
      "Iteration 13886, loss = 2.52635751\n",
      "Iteration 13887, loss = 2.52634304\n",
      "Iteration 13888, loss = 2.52637905\n",
      "Iteration 13889, loss = 2.52627951\n",
      "Iteration 13890, loss = 2.52637232\n",
      "Iteration 13891, loss = 2.52624558\n",
      "Iteration 13892, loss = 2.52626173\n",
      "Iteration 13893, loss = 2.52633527\n",
      "Iteration 13894, loss = 2.52623441\n",
      "Iteration 13895, loss = 2.52624628\n",
      "Iteration 13896, loss = 2.52614694\n",
      "Iteration 13897, loss = 2.52618460\n",
      "Iteration 13898, loss = 2.52626394\n",
      "Iteration 13899, loss = 2.52617961\n",
      "Iteration 13900, loss = 2.52613906\n",
      "Iteration 13901, loss = 2.52633973\n",
      "Iteration 13902, loss = 2.52619710\n",
      "Iteration 13903, loss = 2.52610123\n",
      "Iteration 13904, loss = 2.52614051\n",
      "Iteration 13905, loss = 2.52609904\n",
      "Iteration 13906, loss = 2.52600850\n",
      "Iteration 13907, loss = 2.52613761\n",
      "Iteration 13908, loss = 2.52607820\n",
      "Iteration 13909, loss = 2.52622510\n",
      "Iteration 13910, loss = 2.52601500\n",
      "Iteration 13911, loss = 2.52636302\n",
      "Iteration 13912, loss = 2.52585882\n",
      "Iteration 13913, loss = 2.52594897\n",
      "Iteration 13914, loss = 2.52593786\n",
      "Iteration 13915, loss = 2.52598918\n",
      "Iteration 13916, loss = 2.52589502\n",
      "Iteration 13917, loss = 2.52592616\n",
      "Iteration 13918, loss = 2.52585355\n",
      "Iteration 13919, loss = 2.52581132\n",
      "Iteration 13920, loss = 2.52584330\n",
      "Iteration 13921, loss = 2.52589422\n",
      "Iteration 13922, loss = 2.52586241\n",
      "Iteration 13923, loss = 2.52583482\n",
      "Iteration 13924, loss = 2.52577538\n",
      "Iteration 13925, loss = 2.52586131\n",
      "Iteration 13926, loss = 2.52584053\n",
      "Iteration 13927, loss = 2.52568631\n",
      "Iteration 13928, loss = 2.52573912\n",
      "Iteration 13929, loss = 2.52572945\n",
      "Iteration 13930, loss = 2.52568514\n",
      "Iteration 13931, loss = 2.52562845\n",
      "Iteration 13932, loss = 2.52571277\n",
      "Iteration 13933, loss = 2.52581890\n",
      "Iteration 13934, loss = 2.52574015\n",
      "Iteration 13935, loss = 2.52567347\n",
      "Iteration 13936, loss = 2.52565071\n",
      "Iteration 13937, loss = 2.52563972\n",
      "Iteration 13938, loss = 2.52564513\n",
      "Iteration 13939, loss = 2.52561745\n",
      "Iteration 13940, loss = 2.52554616\n",
      "Iteration 13941, loss = 2.52556447\n",
      "Iteration 13942, loss = 2.52555134\n",
      "Iteration 13943, loss = 2.52540992\n",
      "Iteration 13944, loss = 2.52546707\n",
      "Iteration 13945, loss = 2.52566330\n",
      "Iteration 13946, loss = 2.52551838\n",
      "Iteration 13947, loss = 2.52553574\n",
      "Iteration 13948, loss = 2.52539577\n",
      "Iteration 13949, loss = 2.52539186\n",
      "Iteration 13950, loss = 2.52531671\n",
      "Iteration 13951, loss = 2.52539322\n",
      "Iteration 13952, loss = 2.52534020\n",
      "Iteration 13953, loss = 2.52531379\n",
      "Iteration 13954, loss = 2.52533192\n",
      "Iteration 13955, loss = 2.52540891\n",
      "Iteration 13956, loss = 2.52534425\n",
      "Iteration 13957, loss = 2.52528534\n",
      "Iteration 13958, loss = 2.52532956\n",
      "Iteration 13959, loss = 2.52530344\n",
      "Iteration 13960, loss = 2.52518531\n",
      "Iteration 13961, loss = 2.52523433\n",
      "Iteration 13962, loss = 2.52520855\n",
      "Iteration 13963, loss = 2.52508824\n",
      "Iteration 13964, loss = 2.52542204\n",
      "Iteration 13965, loss = 2.52517697\n",
      "Iteration 13966, loss = 2.52524192\n",
      "Iteration 13967, loss = 2.52522863\n",
      "Iteration 13968, loss = 2.52526578\n",
      "Iteration 13969, loss = 2.52507831\n",
      "Iteration 13970, loss = 2.52512836\n",
      "Iteration 13971, loss = 2.52515077\n",
      "Iteration 13972, loss = 2.52500520\n",
      "Iteration 13973, loss = 2.52511686\n",
      "Iteration 13974, loss = 2.52506275\n",
      "Iteration 13975, loss = 2.52489378\n",
      "Iteration 13976, loss = 2.52513321\n",
      "Iteration 13977, loss = 2.52488676\n",
      "Iteration 13978, loss = 2.52500703\n",
      "Iteration 13979, loss = 2.52485873\n",
      "Iteration 13980, loss = 2.52509654\n",
      "Iteration 13981, loss = 2.52500024\n",
      "Iteration 13982, loss = 2.52487628\n",
      "Iteration 13983, loss = 2.52487088\n",
      "Iteration 13984, loss = 2.52489652\n",
      "Iteration 13985, loss = 2.52483238\n",
      "Iteration 13986, loss = 2.52488221\n",
      "Iteration 13987, loss = 2.52480305\n",
      "Iteration 13988, loss = 2.52483428\n",
      "Iteration 13989, loss = 2.52476463\n",
      "Iteration 13990, loss = 2.52480382\n",
      "Iteration 13991, loss = 2.52484976\n",
      "Iteration 13992, loss = 2.52464455\n",
      "Iteration 13993, loss = 2.52473543\n",
      "Iteration 13994, loss = 2.52475947\n",
      "Iteration 13995, loss = 2.52463986\n",
      "Iteration 13996, loss = 2.52477588\n",
      "Iteration 13997, loss = 2.52472077\n",
      "Iteration 13998, loss = 2.52462049\n",
      "Iteration 13999, loss = 2.52463643\n",
      "Iteration 14000, loss = 2.52491754\n",
      "Iteration 14001, loss = 2.52464109\n",
      "Iteration 14002, loss = 2.52464706\n",
      "Iteration 14003, loss = 2.52448329\n",
      "Iteration 14004, loss = 2.52446317\n",
      "Iteration 14005, loss = 2.52459465\n",
      "Iteration 14006, loss = 2.52453757\n",
      "Iteration 14007, loss = 2.52461826\n",
      "Iteration 14008, loss = 2.52455495\n",
      "Iteration 14009, loss = 2.52473505\n",
      "Iteration 14010, loss = 2.52444849\n",
      "Iteration 14011, loss = 2.52441793\n",
      "Iteration 14012, loss = 2.52441090\n",
      "Iteration 14013, loss = 2.52447236\n",
      "Iteration 14014, loss = 2.52442023\n",
      "Iteration 14015, loss = 2.52445085\n",
      "Iteration 14016, loss = 2.52447746\n",
      "Iteration 14017, loss = 2.52443493\n",
      "Iteration 14018, loss = 2.52453099\n",
      "Iteration 14019, loss = 2.52430794\n",
      "Iteration 14020, loss = 2.52444363\n",
      "Iteration 14021, loss = 2.52431323\n",
      "Iteration 14022, loss = 2.52431691\n",
      "Iteration 14023, loss = 2.52435136\n",
      "Iteration 14024, loss = 2.52427653\n",
      "Iteration 14025, loss = 2.52418902\n",
      "Iteration 14026, loss = 2.52424755\n",
      "Iteration 14027, loss = 2.52411573\n",
      "Iteration 14028, loss = 2.52424635\n",
      "Iteration 14029, loss = 2.52411023\n",
      "Iteration 14030, loss = 2.52421760\n",
      "Iteration 14031, loss = 2.52410072\n",
      "Iteration 14032, loss = 2.52407124\n",
      "Iteration 14033, loss = 2.52417976\n",
      "Iteration 14034, loss = 2.52406895\n",
      "Iteration 14035, loss = 2.52425227\n",
      "Iteration 14036, loss = 2.52406114\n",
      "Iteration 14037, loss = 2.52411394\n",
      "Iteration 14038, loss = 2.52403724\n",
      "Iteration 14039, loss = 2.52399784\n",
      "Iteration 14040, loss = 2.52411815\n",
      "Iteration 14041, loss = 2.52450924\n",
      "Iteration 14042, loss = 2.52412556\n",
      "Iteration 14043, loss = 2.52392974\n",
      "Iteration 14044, loss = 2.52398528\n",
      "Iteration 14045, loss = 2.52391096\n",
      "Iteration 14046, loss = 2.52404596\n",
      "Iteration 14047, loss = 2.52388573\n",
      "Iteration 14048, loss = 2.52383578\n",
      "Iteration 14049, loss = 2.52398632\n",
      "Iteration 14050, loss = 2.52387388\n",
      "Iteration 14051, loss = 2.52384812\n",
      "Iteration 14052, loss = 2.52385604\n",
      "Iteration 14053, loss = 2.52388969\n",
      "Iteration 14054, loss = 2.52381813\n",
      "Iteration 14055, loss = 2.52380306\n",
      "Iteration 14056, loss = 2.52373914\n",
      "Iteration 14057, loss = 2.52371353\n",
      "Iteration 14058, loss = 2.52369239\n",
      "Iteration 14059, loss = 2.52377821\n",
      "Iteration 14060, loss = 2.52373354\n",
      "Iteration 14061, loss = 2.52366283\n",
      "Iteration 14062, loss = 2.52356445\n",
      "Iteration 14063, loss = 2.52376190\n",
      "Iteration 14064, loss = 2.52364566\n",
      "Iteration 14065, loss = 2.52373108\n",
      "Iteration 14066, loss = 2.52365643\n",
      "Iteration 14067, loss = 2.52358920\n",
      "Iteration 14068, loss = 2.52363447\n",
      "Iteration 14069, loss = 2.52355892\n",
      "Iteration 14070, loss = 2.52352227\n",
      "Iteration 14071, loss = 2.52365663\n",
      "Iteration 14072, loss = 2.52359250\n",
      "Iteration 14073, loss = 2.52356439\n",
      "Iteration 14074, loss = 2.52350941\n",
      "Iteration 14075, loss = 2.52348463\n",
      "Iteration 14076, loss = 2.52338519\n",
      "Iteration 14077, loss = 2.52340100\n",
      "Iteration 14078, loss = 2.52342954\n",
      "Iteration 14079, loss = 2.52366956\n",
      "Iteration 14080, loss = 2.52351361\n",
      "Iteration 14081, loss = 2.52333187\n",
      "Iteration 14082, loss = 2.52332468\n",
      "Iteration 14083, loss = 2.52324213\n",
      "Iteration 14084, loss = 2.52335310\n",
      "Iteration 14085, loss = 2.52333624\n",
      "Iteration 14086, loss = 2.52327087\n",
      "Iteration 14087, loss = 2.52329392\n",
      "Iteration 14088, loss = 2.52319559\n",
      "Iteration 14089, loss = 2.52327357\n",
      "Iteration 14090, loss = 2.52343772\n",
      "Iteration 14091, loss = 2.52317530\n",
      "Iteration 14092, loss = 2.52351283\n",
      "Iteration 14093, loss = 2.52320965\n",
      "Iteration 14094, loss = 2.52323674\n",
      "Iteration 14095, loss = 2.52315266\n",
      "Iteration 14096, loss = 2.52337422\n",
      "Iteration 14097, loss = 2.52303159\n",
      "Iteration 14098, loss = 2.52313045\n",
      "Iteration 14099, loss = 2.52308764\n",
      "Iteration 14100, loss = 2.52337203\n",
      "Iteration 14101, loss = 2.52316773\n",
      "Iteration 14102, loss = 2.52313654\n",
      "Iteration 14103, loss = 2.52310051\n",
      "Iteration 14104, loss = 2.52298887\n",
      "Iteration 14105, loss = 2.52300645\n",
      "Iteration 14106, loss = 2.52294949\n",
      "Iteration 14107, loss = 2.52301090\n",
      "Iteration 14108, loss = 2.52300914\n",
      "Iteration 14109, loss = 2.52305302\n",
      "Iteration 14110, loss = 2.52303432\n",
      "Iteration 14111, loss = 2.52287216\n",
      "Iteration 14112, loss = 2.52290573\n",
      "Iteration 14113, loss = 2.52297006\n",
      "Iteration 14114, loss = 2.52288098\n",
      "Iteration 14115, loss = 2.52307699\n",
      "Iteration 14116, loss = 2.52300242\n",
      "Iteration 14117, loss = 2.52285162\n",
      "Iteration 14118, loss = 2.52288225\n",
      "Iteration 14119, loss = 2.52296359\n",
      "Iteration 14120, loss = 2.52279380\n",
      "Iteration 14121, loss = 2.52272171\n",
      "Iteration 14122, loss = 2.52268384\n",
      "Iteration 14123, loss = 2.52280890\n",
      "Iteration 14124, loss = 2.52274467\n",
      "Iteration 14125, loss = 2.52269088\n",
      "Iteration 14126, loss = 2.52270673\n",
      "Iteration 14127, loss = 2.52284191\n",
      "Iteration 14128, loss = 2.52258156\n",
      "Iteration 14129, loss = 2.52281289\n",
      "Iteration 14130, loss = 2.52256703\n",
      "Iteration 14131, loss = 2.52266664\n",
      "Iteration 14132, loss = 2.52263230\n",
      "Iteration 14133, loss = 2.52258850\n",
      "Iteration 14134, loss = 2.52259290\n",
      "Iteration 14135, loss = 2.52254986\n",
      "Iteration 14136, loss = 2.52262810\n",
      "Iteration 14137, loss = 2.52255701\n",
      "Iteration 14138, loss = 2.52251561\n",
      "Iteration 14139, loss = 2.52252391\n",
      "Iteration 14140, loss = 2.52248863\n",
      "Iteration 14141, loss = 2.52249876\n",
      "Iteration 14142, loss = 2.52249035\n",
      "Iteration 14143, loss = 2.52248982\n",
      "Iteration 14144, loss = 2.52246043\n",
      "Iteration 14145, loss = 2.52238684\n",
      "Iteration 14146, loss = 2.52232051\n",
      "Iteration 14147, loss = 2.52251466\n",
      "Iteration 14148, loss = 2.52297049\n",
      "Iteration 14149, loss = 2.52238002\n",
      "Iteration 14150, loss = 2.52252179\n",
      "Iteration 14151, loss = 2.52226198\n",
      "Iteration 14152, loss = 2.52231086\n",
      "Iteration 14153, loss = 2.52222992\n",
      "Iteration 14154, loss = 2.52233776\n",
      "Iteration 14155, loss = 2.52222041\n",
      "Iteration 14156, loss = 2.52230488\n",
      "Iteration 14157, loss = 2.52240733\n",
      "Iteration 14158, loss = 2.52212830\n",
      "Iteration 14159, loss = 2.52228632\n",
      "Iteration 14160, loss = 2.52217143\n",
      "Iteration 14161, loss = 2.52212201\n",
      "Iteration 14162, loss = 2.52221524\n",
      "Iteration 14163, loss = 2.52213857\n",
      "Iteration 14164, loss = 2.52219528\n",
      "Iteration 14165, loss = 2.52221561\n",
      "Iteration 14166, loss = 2.52242395\n",
      "Iteration 14167, loss = 2.52201450\n",
      "Iteration 14168, loss = 2.52198761\n",
      "Iteration 14169, loss = 2.52209328\n",
      "Iteration 14170, loss = 2.52197456\n",
      "Iteration 14171, loss = 2.52213310\n",
      "Iteration 14172, loss = 2.52181495\n",
      "Iteration 14173, loss = 2.52209940\n",
      "Iteration 14174, loss = 2.52192126\n",
      "Iteration 14175, loss = 2.52192517\n",
      "Iteration 14176, loss = 2.52190941\n",
      "Iteration 14177, loss = 2.52192342\n",
      "Iteration 14178, loss = 2.52191119\n",
      "Iteration 14179, loss = 2.52190173\n",
      "Iteration 14180, loss = 2.52187763\n",
      "Iteration 14181, loss = 2.52201383\n",
      "Iteration 14182, loss = 2.52175677\n",
      "Iteration 14183, loss = 2.52177874\n",
      "Iteration 14184, loss = 2.52186164\n",
      "Iteration 14185, loss = 2.52173520\n",
      "Iteration 14186, loss = 2.52182098\n",
      "Iteration 14187, loss = 2.52171769\n",
      "Iteration 14188, loss = 2.52179399\n",
      "Iteration 14189, loss = 2.52198469\n",
      "Iteration 14190, loss = 2.52162933\n",
      "Iteration 14191, loss = 2.52213298\n",
      "Iteration 14192, loss = 2.52165568\n",
      "Iteration 14193, loss = 2.52200600\n",
      "Iteration 14194, loss = 2.52160079\n",
      "Iteration 14195, loss = 2.52174178\n",
      "Iteration 14196, loss = 2.52153836\n",
      "Iteration 14197, loss = 2.52177756\n",
      "Iteration 14198, loss = 2.52157214\n",
      "Iteration 14199, loss = 2.52153291\n",
      "Iteration 14200, loss = 2.52154315\n",
      "Iteration 14201, loss = 2.52151675\n",
      "Iteration 14202, loss = 2.52159995\n",
      "Iteration 14203, loss = 2.52141385\n",
      "Iteration 14204, loss = 2.52160203\n",
      "Iteration 14205, loss = 2.52145701\n",
      "Iteration 14206, loss = 2.52151114\n",
      "Iteration 14207, loss = 2.52149336\n",
      "Iteration 14208, loss = 2.52156505\n",
      "Iteration 14209, loss = 2.52146336\n",
      "Iteration 14210, loss = 2.52139801\n",
      "Iteration 14211, loss = 2.52136506\n",
      "Iteration 14212, loss = 2.52130387\n",
      "Iteration 14213, loss = 2.52131748\n",
      "Iteration 14214, loss = 2.52135336\n",
      "Iteration 14215, loss = 2.52128644\n",
      "Iteration 14216, loss = 2.52137218\n",
      "Iteration 14217, loss = 2.52136647\n",
      "Iteration 14218, loss = 2.52127812\n",
      "Iteration 14219, loss = 2.52131967\n",
      "Iteration 14220, loss = 2.52121340\n",
      "Iteration 14221, loss = 2.52122530\n",
      "Iteration 14222, loss = 2.52125167\n",
      "Iteration 14223, loss = 2.52121117\n",
      "Iteration 14224, loss = 2.52115741\n",
      "Iteration 14225, loss = 2.52127453\n",
      "Iteration 14226, loss = 2.52112245\n",
      "Iteration 14227, loss = 2.52106002\n",
      "Iteration 14228, loss = 2.52110655\n",
      "Iteration 14229, loss = 2.52105019\n",
      "Iteration 14230, loss = 2.52110361\n",
      "Iteration 14231, loss = 2.52100654\n",
      "Iteration 14232, loss = 2.52115683\n",
      "Iteration 14233, loss = 2.52103626\n",
      "Iteration 14234, loss = 2.52127708\n",
      "Iteration 14235, loss = 2.52117175\n",
      "Iteration 14236, loss = 2.52093979\n",
      "Iteration 14237, loss = 2.52096637\n",
      "Iteration 14238, loss = 2.52091734\n",
      "Iteration 14239, loss = 2.52123474\n",
      "Iteration 14240, loss = 2.52093887\n",
      "Iteration 14241, loss = 2.52106444\n",
      "Iteration 14242, loss = 2.52102679\n",
      "Iteration 14243, loss = 2.52084544\n",
      "Iteration 14244, loss = 2.52091075\n",
      "Iteration 14245, loss = 2.52085919\n",
      "Iteration 14246, loss = 2.52087340\n",
      "Iteration 14247, loss = 2.52078254\n",
      "Iteration 14248, loss = 2.52084131\n",
      "Iteration 14249, loss = 2.52076543\n",
      "Iteration 14250, loss = 2.52074757\n",
      "Iteration 14251, loss = 2.52084566\n",
      "Iteration 14252, loss = 2.52064154\n",
      "Iteration 14253, loss = 2.52080233\n",
      "Iteration 14254, loss = 2.52082122\n",
      "Iteration 14255, loss = 2.52068601\n",
      "Iteration 14256, loss = 2.52073097\n",
      "Iteration 14257, loss = 2.52090834\n",
      "Iteration 14258, loss = 2.52074592\n",
      "Iteration 14259, loss = 2.52087812\n",
      "Iteration 14260, loss = 2.52062405\n",
      "Iteration 14261, loss = 2.52056845\n",
      "Iteration 14262, loss = 2.52062216\n",
      "Iteration 14263, loss = 2.52058632\n",
      "Iteration 14264, loss = 2.52063209\n",
      "Iteration 14265, loss = 2.52060514\n",
      "Iteration 14266, loss = 2.52053139\n",
      "Iteration 14267, loss = 2.52053152\n",
      "Iteration 14268, loss = 2.52064946\n",
      "Iteration 14269, loss = 2.52057761\n",
      "Iteration 14270, loss = 2.52051441\n",
      "Iteration 14271, loss = 2.52046198\n",
      "Iteration 14272, loss = 2.52048842\n",
      "Iteration 14273, loss = 2.52034290\n",
      "Iteration 14274, loss = 2.52041877\n",
      "Iteration 14275, loss = 2.52038983\n",
      "Iteration 14276, loss = 2.52040688\n",
      "Iteration 14277, loss = 2.52030840\n",
      "Iteration 14278, loss = 2.52055176\n",
      "Iteration 14279, loss = 2.52054740\n",
      "Iteration 14280, loss = 2.52035468\n",
      "Iteration 14281, loss = 2.52018487\n",
      "Iteration 14282, loss = 2.52028599\n",
      "Iteration 14283, loss = 2.52020622\n",
      "Iteration 14284, loss = 2.52021222\n",
      "Iteration 14285, loss = 2.52035529\n",
      "Iteration 14286, loss = 2.52019883\n",
      "Iteration 14287, loss = 2.52031984\n",
      "Iteration 14288, loss = 2.52018776\n",
      "Iteration 14289, loss = 2.52018890\n",
      "Iteration 14290, loss = 2.52029669\n",
      "Iteration 14291, loss = 2.52019627\n",
      "Iteration 14292, loss = 2.52015910\n",
      "Iteration 14293, loss = 2.52031350\n",
      "Iteration 14294, loss = 2.52010516\n",
      "Iteration 14295, loss = 2.52009212\n",
      "Iteration 14296, loss = 2.51999787\n",
      "Iteration 14297, loss = 2.52000496\n",
      "Iteration 14298, loss = 2.51997118\n",
      "Iteration 14299, loss = 2.52005840\n",
      "Iteration 14300, loss = 2.51992040\n",
      "Iteration 14301, loss = 2.51994028\n",
      "Iteration 14302, loss = 2.51996344\n",
      "Iteration 14303, loss = 2.51997984\n",
      "Iteration 14304, loss = 2.51992194\n",
      "Iteration 14305, loss = 2.51998619\n",
      "Iteration 14306, loss = 2.51997421\n",
      "Iteration 14307, loss = 2.52003848\n",
      "Iteration 14308, loss = 2.51992325\n",
      "Iteration 14309, loss = 2.51986501\n",
      "Iteration 14310, loss = 2.51994349\n",
      "Iteration 14311, loss = 2.51978093\n",
      "Iteration 14312, loss = 2.52001510\n",
      "Iteration 14313, loss = 2.52003222\n",
      "Iteration 14314, loss = 2.51993971\n",
      "Iteration 14315, loss = 2.51984982\n",
      "Iteration 14316, loss = 2.52011501\n",
      "Iteration 14317, loss = 2.51977137\n",
      "Iteration 14318, loss = 2.51974104\n",
      "Iteration 14319, loss = 2.51980018\n",
      "Iteration 14320, loss = 2.51963605\n",
      "Iteration 14321, loss = 2.51974801\n",
      "Iteration 14322, loss = 2.51970289\n",
      "Iteration 14323, loss = 2.51974730\n",
      "Iteration 14324, loss = 2.51992587\n",
      "Iteration 14325, loss = 2.51968280\n",
      "Iteration 14326, loss = 2.51957926\n",
      "Iteration 14327, loss = 2.51956429\n",
      "Iteration 14328, loss = 2.51955596\n",
      "Iteration 14329, loss = 2.51951230\n",
      "Iteration 14330, loss = 2.51947851\n",
      "Iteration 14331, loss = 2.51966701\n",
      "Iteration 14332, loss = 2.51962756\n",
      "Iteration 14333, loss = 2.51968938\n",
      "Iteration 14334, loss = 2.51949666\n",
      "Iteration 14335, loss = 2.51942206\n",
      "Iteration 14336, loss = 2.51953338\n",
      "Iteration 14337, loss = 2.51949509\n",
      "Iteration 14338, loss = 2.51939254\n",
      "Iteration 14339, loss = 2.51936981\n",
      "Iteration 14340, loss = 2.51939209\n",
      "Iteration 14341, loss = 2.51937957\n",
      "Iteration 14342, loss = 2.51935507\n",
      "Iteration 14343, loss = 2.51927474\n",
      "Iteration 14344, loss = 2.51946122\n",
      "Iteration 14345, loss = 2.51946720\n",
      "Iteration 14346, loss = 2.51932561\n",
      "Iteration 14347, loss = 2.51959082\n",
      "Iteration 14348, loss = 2.51920760\n",
      "Iteration 14349, loss = 2.51933704\n",
      "Iteration 14350, loss = 2.51934228\n",
      "Iteration 14351, loss = 2.51920100\n",
      "Iteration 14352, loss = 2.51941123\n",
      "Iteration 14353, loss = 2.51910543\n",
      "Iteration 14354, loss = 2.51920711\n",
      "Iteration 14355, loss = 2.51905505\n",
      "Iteration 14356, loss = 2.51921776\n",
      "Iteration 14357, loss = 2.51917209\n",
      "Iteration 14358, loss = 2.51931866\n",
      "Iteration 14359, loss = 2.51908381\n",
      "Iteration 14360, loss = 2.51920013\n",
      "Iteration 14361, loss = 2.51904691\n",
      "Iteration 14362, loss = 2.51934991\n",
      "Iteration 14363, loss = 2.51929864\n",
      "Iteration 14364, loss = 2.51921105\n",
      "Iteration 14365, loss = 2.51910355\n",
      "Iteration 14366, loss = 2.51898012\n",
      "Iteration 14367, loss = 2.51891691\n",
      "Iteration 14368, loss = 2.51913722\n",
      "Iteration 14369, loss = 2.51888232\n",
      "Iteration 14370, loss = 2.51900551\n",
      "Iteration 14371, loss = 2.51891377\n",
      "Iteration 14372, loss = 2.51898180\n",
      "Iteration 14373, loss = 2.51886714\n",
      "Iteration 14374, loss = 2.51889096\n",
      "Iteration 14375, loss = 2.51886934\n",
      "Iteration 14376, loss = 2.51881792\n",
      "Iteration 14377, loss = 2.51872240\n",
      "Iteration 14378, loss = 2.51904907\n",
      "Iteration 14379, loss = 2.51882014\n",
      "Iteration 14380, loss = 2.51881357\n",
      "Iteration 14381, loss = 2.51876692\n",
      "Iteration 14382, loss = 2.51872579\n",
      "Iteration 14383, loss = 2.51872963\n",
      "Iteration 14384, loss = 2.51878639\n",
      "Iteration 14385, loss = 2.51876929\n",
      "Iteration 14386, loss = 2.51862614\n",
      "Iteration 14387, loss = 2.51866818\n",
      "Iteration 14388, loss = 2.51865479\n",
      "Iteration 14389, loss = 2.51860779\n",
      "Iteration 14390, loss = 2.51864195\n",
      "Iteration 14391, loss = 2.51850813\n",
      "Iteration 14392, loss = 2.51866328\n",
      "Iteration 14393, loss = 2.51856859\n",
      "Iteration 14394, loss = 2.51860736\n",
      "Iteration 14395, loss = 2.51859058\n",
      "Iteration 14396, loss = 2.51863771\n",
      "Iteration 14397, loss = 2.51845332\n",
      "Iteration 14398, loss = 2.51869929\n",
      "Iteration 14399, loss = 2.51846897\n",
      "Iteration 14400, loss = 2.51841622\n",
      "Iteration 14401, loss = 2.51848217\n",
      "Iteration 14402, loss = 2.51848897\n",
      "Iteration 14403, loss = 2.51846385\n",
      "Iteration 14404, loss = 2.51851042\n",
      "Iteration 14405, loss = 2.51840507\n",
      "Iteration 14406, loss = 2.51857103\n",
      "Iteration 14407, loss = 2.51843752\n",
      "Iteration 14408, loss = 2.51843770\n",
      "Iteration 14409, loss = 2.51839338\n",
      "Iteration 14410, loss = 2.51843983\n",
      "Iteration 14411, loss = 2.51831211\n",
      "Iteration 14412, loss = 2.51825010\n",
      "Iteration 14413, loss = 2.51842253\n",
      "Iteration 14414, loss = 2.51832312\n",
      "Iteration 14415, loss = 2.51831966\n",
      "Iteration 14416, loss = 2.51821135\n",
      "Iteration 14417, loss = 2.51823597\n",
      "Iteration 14418, loss = 2.51816923\n",
      "Iteration 14419, loss = 2.51816778\n",
      "Iteration 14420, loss = 2.51827162\n",
      "Iteration 14421, loss = 2.51808686\n",
      "Iteration 14422, loss = 2.51814900\n",
      "Iteration 14423, loss = 2.51809856\n",
      "Iteration 14424, loss = 2.51829388\n",
      "Iteration 14425, loss = 2.51805643\n",
      "Iteration 14426, loss = 2.51813290\n",
      "Iteration 14427, loss = 2.51800507\n",
      "Iteration 14428, loss = 2.51800804\n",
      "Iteration 14429, loss = 2.51803061\n",
      "Iteration 14430, loss = 2.51816913\n",
      "Iteration 14431, loss = 2.51815630\n",
      "Iteration 14432, loss = 2.51805074\n",
      "Iteration 14433, loss = 2.51796314\n",
      "Iteration 14434, loss = 2.51804035\n",
      "Iteration 14435, loss = 2.51793866\n",
      "Iteration 14436, loss = 2.51784347\n",
      "Iteration 14437, loss = 2.51791683\n",
      "Iteration 14438, loss = 2.51789730\n",
      "Iteration 14439, loss = 2.51788795\n",
      "Iteration 14440, loss = 2.51780325\n",
      "Iteration 14441, loss = 2.51772611\n",
      "Iteration 14442, loss = 2.51786274\n",
      "Iteration 14443, loss = 2.51807481\n",
      "Iteration 14444, loss = 2.51774747\n",
      "Iteration 14445, loss = 2.51789175\n",
      "Iteration 14446, loss = 2.51772500\n",
      "Iteration 14447, loss = 2.51773839\n",
      "Iteration 14448, loss = 2.51770843\n",
      "Iteration 14449, loss = 2.51773883\n",
      "Iteration 14450, loss = 2.51775997\n",
      "Iteration 14451, loss = 2.51774638\n",
      "Iteration 14452, loss = 2.51763376\n",
      "Iteration 14453, loss = 2.51764969\n",
      "Iteration 14454, loss = 2.51761456\n",
      "Iteration 14455, loss = 2.51772777\n",
      "Iteration 14456, loss = 2.51756704\n",
      "Iteration 14457, loss = 2.51757448\n",
      "Iteration 14458, loss = 2.51774965\n",
      "Iteration 14459, loss = 2.51747760\n",
      "Iteration 14460, loss = 2.51750945\n",
      "Iteration 14461, loss = 2.51763762\n",
      "Iteration 14462, loss = 2.51748718\n",
      "Iteration 14463, loss = 2.51749326\n",
      "Iteration 14464, loss = 2.51768791\n",
      "Iteration 14465, loss = 2.51742407\n",
      "Iteration 14466, loss = 2.51751006\n",
      "Iteration 14467, loss = 2.51734776\n",
      "Iteration 14468, loss = 2.51739547\n",
      "Iteration 14469, loss = 2.51737013\n",
      "Iteration 14470, loss = 2.51744710\n",
      "Iteration 14471, loss = 2.51752343\n",
      "Iteration 14472, loss = 2.51733550\n",
      "Iteration 14473, loss = 2.51736133\n",
      "Iteration 14474, loss = 2.51732411\n",
      "Iteration 14475, loss = 2.51731479\n",
      "Iteration 14476, loss = 2.51728205\n",
      "Iteration 14477, loss = 2.51723628\n",
      "Iteration 14478, loss = 2.51723688\n",
      "Iteration 14479, loss = 2.51722536\n",
      "Iteration 14480, loss = 2.51722003\n",
      "Iteration 14481, loss = 2.51715276\n",
      "Iteration 14482, loss = 2.51715492\n",
      "Iteration 14483, loss = 2.51712844\n",
      "Iteration 14484, loss = 2.51724662\n",
      "Iteration 14485, loss = 2.51717527\n",
      "Iteration 14486, loss = 2.51716455\n",
      "Iteration 14487, loss = 2.51705843\n",
      "Iteration 14488, loss = 2.51708096\n",
      "Iteration 14489, loss = 2.51709282\n",
      "Iteration 14490, loss = 2.51714449\n",
      "Iteration 14491, loss = 2.51728299\n",
      "Iteration 14492, loss = 2.51721895\n",
      "Iteration 14493, loss = 2.51708215\n",
      "Iteration 14494, loss = 2.51717837\n",
      "Iteration 14495, loss = 2.51699693\n",
      "Iteration 14496, loss = 2.51704464\n",
      "Iteration 14497, loss = 2.51722749\n",
      "Iteration 14498, loss = 2.51707249\n",
      "Iteration 14499, loss = 2.51713191\n",
      "Iteration 14500, loss = 2.51700798\n",
      "Iteration 14501, loss = 2.51705268\n",
      "Iteration 14502, loss = 2.51696245\n",
      "Iteration 14503, loss = 2.51683616\n",
      "Iteration 14504, loss = 2.51685045\n",
      "Iteration 14505, loss = 2.51683115\n",
      "Iteration 14506, loss = 2.51703365\n",
      "Iteration 14507, loss = 2.51696271\n",
      "Iteration 14508, loss = 2.51691035\n",
      "Iteration 14509, loss = 2.51709287\n",
      "Iteration 14510, loss = 2.51680472\n",
      "Iteration 14511, loss = 2.51678688\n",
      "Iteration 14512, loss = 2.51670165\n",
      "Iteration 14513, loss = 2.51668358\n",
      "Iteration 14514, loss = 2.51671499\n",
      "Iteration 14515, loss = 2.51677050\n",
      "Iteration 14516, loss = 2.51665635\n",
      "Iteration 14517, loss = 2.51668664\n",
      "Iteration 14518, loss = 2.51663915\n",
      "Iteration 14519, loss = 2.51664920\n",
      "Iteration 14520, loss = 2.51670327\n",
      "Iteration 14521, loss = 2.51679108\n",
      "Iteration 14522, loss = 2.51654947\n",
      "Iteration 14523, loss = 2.51668521\n",
      "Iteration 14524, loss = 2.51652193\n",
      "Iteration 14525, loss = 2.51666064\n",
      "Iteration 14526, loss = 2.51649340\n",
      "Iteration 14527, loss = 2.51652059\n",
      "Iteration 14528, loss = 2.51656085\n",
      "Iteration 14529, loss = 2.51656607\n",
      "Iteration 14530, loss = 2.51644559\n",
      "Iteration 14531, loss = 2.51642590\n",
      "Iteration 14532, loss = 2.51649318\n",
      "Iteration 14533, loss = 2.51640624\n",
      "Iteration 14534, loss = 2.51650844\n",
      "Iteration 14535, loss = 2.51636461\n",
      "Iteration 14536, loss = 2.51642611\n",
      "Iteration 14537, loss = 2.51629731\n",
      "Iteration 14538, loss = 2.51639239\n",
      "Iteration 14539, loss = 2.51628187\n",
      "Iteration 14540, loss = 2.51663745\n",
      "Iteration 14541, loss = 2.51636773\n",
      "Iteration 14542, loss = 2.51627956\n",
      "Iteration 14543, loss = 2.51640447\n",
      "Iteration 14544, loss = 2.51624847\n",
      "Iteration 14545, loss = 2.51620590\n",
      "Iteration 14546, loss = 2.51617182\n",
      "Iteration 14547, loss = 2.51612673\n",
      "Iteration 14548, loss = 2.51613962\n",
      "Iteration 14549, loss = 2.51621427\n",
      "Iteration 14550, loss = 2.51611883\n",
      "Iteration 14551, loss = 2.51630679\n",
      "Iteration 14552, loss = 2.51601584\n",
      "Iteration 14553, loss = 2.51627741\n",
      "Iteration 14554, loss = 2.51609470\n",
      "Iteration 14555, loss = 2.51621001\n",
      "Iteration 14556, loss = 2.51605755\n",
      "Iteration 14557, loss = 2.51603576\n",
      "Iteration 14558, loss = 2.51631591\n",
      "Iteration 14559, loss = 2.51592334\n",
      "Iteration 14560, loss = 2.51599442\n",
      "Iteration 14561, loss = 2.51596448\n",
      "Iteration 14562, loss = 2.51603651\n",
      "Iteration 14563, loss = 2.51591175\n",
      "Iteration 14564, loss = 2.51604226\n",
      "Iteration 14565, loss = 2.51598412\n",
      "Iteration 14566, loss = 2.51610137\n",
      "Iteration 14567, loss = 2.51616943\n",
      "Iteration 14568, loss = 2.51579905\n",
      "Iteration 14569, loss = 2.51596376\n",
      "Iteration 14570, loss = 2.51610201\n",
      "Iteration 14571, loss = 2.51581883\n",
      "Iteration 14572, loss = 2.51590459\n",
      "Iteration 14573, loss = 2.51581333\n",
      "Iteration 14574, loss = 2.51608561\n",
      "Iteration 14575, loss = 2.51580149\n",
      "Iteration 14576, loss = 2.51594363\n",
      "Iteration 14577, loss = 2.51568791\n",
      "Iteration 14578, loss = 2.51591773\n",
      "Iteration 14579, loss = 2.51570015\n",
      "Iteration 14580, loss = 2.51565549\n",
      "Iteration 14581, loss = 2.51573204\n",
      "Iteration 14582, loss = 2.51565538\n",
      "Iteration 14583, loss = 2.51567229\n",
      "Iteration 14584, loss = 2.51576362\n",
      "Iteration 14585, loss = 2.51576133\n",
      "Iteration 14586, loss = 2.51565256\n",
      "Iteration 14587, loss = 2.51563554\n",
      "Iteration 14588, loss = 2.51560467\n",
      "Iteration 14589, loss = 2.51549863\n",
      "Iteration 14590, loss = 2.51550300\n",
      "Iteration 14591, loss = 2.51543060\n",
      "Iteration 14592, loss = 2.51548300\n",
      "Iteration 14593, loss = 2.51548216\n",
      "Iteration 14594, loss = 2.51551563\n",
      "Iteration 14595, loss = 2.51545132\n",
      "Iteration 14596, loss = 2.51536560\n",
      "Iteration 14597, loss = 2.51548978\n",
      "Iteration 14598, loss = 2.51544062\n",
      "Iteration 14599, loss = 2.51537826\n",
      "Iteration 14600, loss = 2.51534456\n",
      "Iteration 14601, loss = 2.51536313\n",
      "Iteration 14602, loss = 2.51550044\n",
      "Iteration 14603, loss = 2.51551973\n",
      "Iteration 14604, loss = 2.51529998\n",
      "Iteration 14605, loss = 2.51551357\n",
      "Iteration 14606, loss = 2.51524979\n",
      "Iteration 14607, loss = 2.51540312\n",
      "Iteration 14608, loss = 2.51535587\n",
      "Iteration 14609, loss = 2.51519623\n",
      "Iteration 14610, loss = 2.51527077\n",
      "Iteration 14611, loss = 2.51515933\n",
      "Iteration 14612, loss = 2.51519806\n",
      "Iteration 14613, loss = 2.51509406\n",
      "Iteration 14614, loss = 2.51525355\n",
      "Iteration 14615, loss = 2.51513508\n",
      "Iteration 14616, loss = 2.51514460\n",
      "Iteration 14617, loss = 2.51512996\n",
      "Iteration 14618, loss = 2.51543207\n",
      "Iteration 14619, loss = 2.51517553\n",
      "Iteration 14620, loss = 2.51523075\n",
      "Iteration 14621, loss = 2.51518617\n",
      "Iteration 14622, loss = 2.51516586\n",
      "Iteration 14623, loss = 2.51497775\n",
      "Iteration 14624, loss = 2.51531249\n",
      "Iteration 14625, loss = 2.51501283\n",
      "Iteration 14626, loss = 2.51505336\n",
      "Iteration 14627, loss = 2.51512250\n",
      "Iteration 14628, loss = 2.51493976\n",
      "Iteration 14629, loss = 2.51484106\n",
      "Iteration 14630, loss = 2.51491498\n",
      "Iteration 14631, loss = 2.51493903\n",
      "Iteration 14632, loss = 2.51495039\n",
      "Iteration 14633, loss = 2.51485706\n",
      "Iteration 14634, loss = 2.51502445\n",
      "Iteration 14635, loss = 2.51479061\n",
      "Iteration 14636, loss = 2.51479208\n",
      "Iteration 14637, loss = 2.51478657\n",
      "Iteration 14638, loss = 2.51483049\n",
      "Iteration 14639, loss = 2.51477213\n",
      "Iteration 14640, loss = 2.51478499\n",
      "Iteration 14641, loss = 2.51486829\n",
      "Iteration 14642, loss = 2.51468958\n",
      "Iteration 14643, loss = 2.51483838\n",
      "Iteration 14644, loss = 2.51463338\n",
      "Iteration 14645, loss = 2.51456746\n",
      "Iteration 14646, loss = 2.51465783\n",
      "Iteration 14647, loss = 2.51482054\n",
      "Iteration 14648, loss = 2.51471506\n",
      "Iteration 14649, loss = 2.51461112\n",
      "Iteration 14650, loss = 2.51470627\n",
      "Iteration 14651, loss = 2.51462316\n",
      "Iteration 14652, loss = 2.51471629\n",
      "Iteration 14653, loss = 2.51449198\n",
      "Iteration 14654, loss = 2.51448752\n",
      "Iteration 14655, loss = 2.51455294\n",
      "Iteration 14656, loss = 2.51459604\n",
      "Iteration 14657, loss = 2.51464561\n",
      "Iteration 14658, loss = 2.51468892\n",
      "Iteration 14659, loss = 2.51472626\n",
      "Iteration 14660, loss = 2.51458737\n",
      "Iteration 14661, loss = 2.51436459\n",
      "Iteration 14662, loss = 2.51442754\n",
      "Iteration 14663, loss = 2.51437749\n",
      "Iteration 14664, loss = 2.51446487\n",
      "Iteration 14665, loss = 2.51425693\n",
      "Iteration 14666, loss = 2.51447503\n",
      "Iteration 14667, loss = 2.51443421\n",
      "Iteration 14668, loss = 2.51439782\n",
      "Iteration 14669, loss = 2.51430749\n",
      "Iteration 14670, loss = 2.51444970\n",
      "Iteration 14671, loss = 2.51464599\n",
      "Iteration 14672, loss = 2.51416951\n",
      "Iteration 14673, loss = 2.51427855\n",
      "Iteration 14674, loss = 2.51443408\n",
      "Iteration 14675, loss = 2.51420389\n",
      "Iteration 14676, loss = 2.51432718\n",
      "Iteration 14677, loss = 2.51414763\n",
      "Iteration 14678, loss = 2.51419499\n",
      "Iteration 14679, loss = 2.51413536\n",
      "Iteration 14680, loss = 2.51411551\n",
      "Iteration 14681, loss = 2.51425614\n",
      "Iteration 14682, loss = 2.51406697\n",
      "Iteration 14683, loss = 2.51411185\n",
      "Iteration 14684, loss = 2.51397161\n",
      "Iteration 14685, loss = 2.51404312\n",
      "Iteration 14686, loss = 2.51430400\n",
      "Iteration 14687, loss = 2.51410452\n",
      "Iteration 14688, loss = 2.51402857\n",
      "Iteration 14689, loss = 2.51394014\n",
      "Iteration 14690, loss = 2.51391318\n",
      "Iteration 14691, loss = 2.51394416\n",
      "Iteration 14692, loss = 2.51399997\n",
      "Iteration 14693, loss = 2.51397536\n",
      "Iteration 14694, loss = 2.51394005\n",
      "Iteration 14695, loss = 2.51387265\n",
      "Iteration 14696, loss = 2.51382878\n",
      "Iteration 14697, loss = 2.51398484\n",
      "Iteration 14698, loss = 2.51403523\n",
      "Iteration 14699, loss = 2.51378137\n",
      "Iteration 14700, loss = 2.51382308\n",
      "Iteration 14701, loss = 2.51390218\n",
      "Iteration 14702, loss = 2.51387294\n",
      "Iteration 14703, loss = 2.51376675\n",
      "Iteration 14704, loss = 2.51376856\n",
      "Iteration 14705, loss = 2.51380929\n",
      "Iteration 14706, loss = 2.51383054\n",
      "Iteration 14707, loss = 2.51364367\n",
      "Iteration 14708, loss = 2.51369296\n",
      "Iteration 14709, loss = 2.51375255\n",
      "Iteration 14710, loss = 2.51364083\n",
      "Iteration 14711, loss = 2.51360844\n",
      "Iteration 14712, loss = 2.51362346\n",
      "Iteration 14713, loss = 2.51364697\n",
      "Iteration 14714, loss = 2.51371932\n",
      "Iteration 14715, loss = 2.51361723\n",
      "Iteration 14716, loss = 2.51374522\n",
      "Iteration 14717, loss = 2.51362669\n",
      "Iteration 14718, loss = 2.51356950\n",
      "Iteration 14719, loss = 2.51356305\n",
      "Iteration 14720, loss = 2.51363738\n",
      "Iteration 14721, loss = 2.51345745\n",
      "Iteration 14722, loss = 2.51341637\n",
      "Iteration 14723, loss = 2.51339835\n",
      "Iteration 14724, loss = 2.51353807\n",
      "Iteration 14725, loss = 2.51346373\n",
      "Iteration 14726, loss = 2.51364610\n",
      "Iteration 14727, loss = 2.51343685\n",
      "Iteration 14728, loss = 2.51346742\n",
      "Iteration 14729, loss = 2.51327788\n",
      "Iteration 14730, loss = 2.51341336\n",
      "Iteration 14731, loss = 2.51343803\n",
      "Iteration 14732, loss = 2.51327451\n",
      "Iteration 14733, loss = 2.51328363\n",
      "Iteration 14734, loss = 2.51340377\n",
      "Iteration 14735, loss = 2.51334765\n",
      "Iteration 14736, loss = 2.51334136\n",
      "Iteration 14737, loss = 2.51320330\n",
      "Iteration 14738, loss = 2.51332957\n",
      "Iteration 14739, loss = 2.51348135\n",
      "Iteration 14740, loss = 2.51319158\n",
      "Iteration 14741, loss = 2.51323440\n",
      "Iteration 14742, loss = 2.51323129\n",
      "Iteration 14743, loss = 2.51330260\n",
      "Iteration 14744, loss = 2.51322994\n",
      "Iteration 14745, loss = 2.51302672\n",
      "Iteration 14746, loss = 2.51304619\n",
      "Iteration 14747, loss = 2.51311190\n",
      "Iteration 14748, loss = 2.51312398\n",
      "Iteration 14749, loss = 2.51306816\n",
      "Iteration 14750, loss = 2.51303861\n",
      "Iteration 14751, loss = 2.51296365\n",
      "Iteration 14752, loss = 2.51314896\n",
      "Iteration 14753, loss = 2.51320451\n",
      "Iteration 14754, loss = 2.51309696\n",
      "Iteration 14755, loss = 2.51300592\n",
      "Iteration 14756, loss = 2.51286150\n",
      "Iteration 14757, loss = 2.51290093\n",
      "Iteration 14758, loss = 2.51302008\n",
      "Iteration 14759, loss = 2.51292829\n",
      "Iteration 14760, loss = 2.51281577\n",
      "Iteration 14761, loss = 2.51288280\n",
      "Iteration 14762, loss = 2.51285658\n",
      "Iteration 14763, loss = 2.51292808\n",
      "Iteration 14764, loss = 2.51283751\n",
      "Iteration 14765, loss = 2.51275340\n",
      "Iteration 14766, loss = 2.51284099\n",
      "Iteration 14767, loss = 2.51289972\n",
      "Iteration 14768, loss = 2.51274287\n",
      "Iteration 14769, loss = 2.51277421\n",
      "Iteration 14770, loss = 2.51274163\n",
      "Iteration 14771, loss = 2.51274113\n",
      "Iteration 14772, loss = 2.51265234\n",
      "Iteration 14773, loss = 2.51270798\n",
      "Iteration 14774, loss = 2.51265726\n",
      "Iteration 14775, loss = 2.51272789\n",
      "Iteration 14776, loss = 2.51286617\n",
      "Iteration 14777, loss = 2.51294009\n",
      "Iteration 14778, loss = 2.51278397\n",
      "Iteration 14779, loss = 2.51263428\n",
      "Iteration 14780, loss = 2.51257740\n",
      "Iteration 14781, loss = 2.51251457\n",
      "Iteration 14782, loss = 2.51253419\n",
      "Iteration 14783, loss = 2.51249767\n",
      "Iteration 14784, loss = 2.51248880\n",
      "Iteration 14785, loss = 2.51254791\n",
      "Iteration 14786, loss = 2.51261013\n",
      "Iteration 14787, loss = 2.51266186\n",
      "Iteration 14788, loss = 2.51244894\n",
      "Iteration 14789, loss = 2.51241274\n",
      "Iteration 14790, loss = 2.51239176\n",
      "Iteration 14791, loss = 2.51251469\n",
      "Iteration 14792, loss = 2.51236682\n",
      "Iteration 14793, loss = 2.51241849\n",
      "Iteration 14794, loss = 2.51246333\n",
      "Iteration 14795, loss = 2.51231910\n",
      "Iteration 14796, loss = 2.51232236\n",
      "Iteration 14797, loss = 2.51233320\n",
      "Iteration 14798, loss = 2.51234926\n",
      "Iteration 14799, loss = 2.51250180\n",
      "Iteration 14800, loss = 2.51219383\n",
      "Iteration 14801, loss = 2.51225094\n",
      "Iteration 14802, loss = 2.51218240\n",
      "Iteration 14803, loss = 2.51225362\n",
      "Iteration 14804, loss = 2.51230418\n",
      "Iteration 14805, loss = 2.51227633\n",
      "Iteration 14806, loss = 2.51232808\n",
      "Iteration 14807, loss = 2.51214983\n",
      "Iteration 14808, loss = 2.51216029\n",
      "Iteration 14809, loss = 2.51217284\n",
      "Iteration 14810, loss = 2.51214105\n",
      "Iteration 14811, loss = 2.51229926\n",
      "Iteration 14812, loss = 2.51201982\n",
      "Iteration 14813, loss = 2.51226234\n",
      "Iteration 14814, loss = 2.51223795\n",
      "Iteration 14815, loss = 2.51200653\n",
      "Iteration 14816, loss = 2.51206952\n",
      "Iteration 14817, loss = 2.51203972\n",
      "Iteration 14818, loss = 2.51195713\n",
      "Iteration 14819, loss = 2.51199130\n",
      "Iteration 14820, loss = 2.51206200\n",
      "Iteration 14821, loss = 2.51187980\n",
      "Iteration 14822, loss = 2.51205807\n",
      "Iteration 14823, loss = 2.51184941\n",
      "Iteration 14824, loss = 2.51184944\n",
      "Iteration 14825, loss = 2.51196910\n",
      "Iteration 14826, loss = 2.51191883\n",
      "Iteration 14827, loss = 2.51191839\n",
      "Iteration 14828, loss = 2.51213235\n",
      "Iteration 14829, loss = 2.51172975\n",
      "Iteration 14830, loss = 2.51237650\n",
      "Iteration 14831, loss = 2.51194990\n",
      "Iteration 14832, loss = 2.51203019\n",
      "Iteration 14833, loss = 2.51170793\n",
      "Iteration 14834, loss = 2.51187468\n",
      "Iteration 14835, loss = 2.51174427\n",
      "Iteration 14836, loss = 2.51176918\n",
      "Iteration 14837, loss = 2.51159558\n",
      "Iteration 14838, loss = 2.51169453\n",
      "Iteration 14839, loss = 2.51169557\n",
      "Iteration 14840, loss = 2.51170194\n",
      "Iteration 14841, loss = 2.51164243\n",
      "Iteration 14842, loss = 2.51183768\n",
      "Iteration 14843, loss = 2.51161716\n",
      "Iteration 14844, loss = 2.51159622\n",
      "Iteration 14845, loss = 2.51156309\n",
      "Iteration 14846, loss = 2.51150244\n",
      "Iteration 14847, loss = 2.51160961\n",
      "Iteration 14848, loss = 2.51165807\n",
      "Iteration 14849, loss = 2.51155745\n",
      "Iteration 14850, loss = 2.51147477\n",
      "Iteration 14851, loss = 2.51145179\n",
      "Iteration 14852, loss = 2.51139881\n",
      "Iteration 14853, loss = 2.51140517\n",
      "Iteration 14854, loss = 2.51144907\n",
      "Iteration 14855, loss = 2.51137687\n",
      "Iteration 14856, loss = 2.51149481\n",
      "Iteration 14857, loss = 2.51143087\n",
      "Iteration 14858, loss = 2.51131258\n",
      "Iteration 14859, loss = 2.51139474\n",
      "Iteration 14860, loss = 2.51138057\n",
      "Iteration 14861, loss = 2.51132690\n",
      "Iteration 14862, loss = 2.51125944\n",
      "Iteration 14863, loss = 2.51133510\n",
      "Iteration 14864, loss = 2.51124141\n",
      "Iteration 14865, loss = 2.51123200\n",
      "Iteration 14866, loss = 2.51119086\n",
      "Iteration 14867, loss = 2.51131992\n",
      "Iteration 14868, loss = 2.51136143\n",
      "Iteration 14869, loss = 2.51111134\n",
      "Iteration 14870, loss = 2.51114219\n",
      "Iteration 14871, loss = 2.51120643\n",
      "Iteration 14872, loss = 2.51109895\n",
      "Iteration 14873, loss = 2.51110785\n",
      "Iteration 14874, loss = 2.51121813\n",
      "Iteration 14875, loss = 2.51113373\n",
      "Iteration 14876, loss = 2.51116029\n",
      "Iteration 14877, loss = 2.51106865\n",
      "Iteration 14878, loss = 2.51114593\n",
      "Iteration 14879, loss = 2.51111391\n",
      "Iteration 14880, loss = 2.51099005\n",
      "Iteration 14881, loss = 2.51117547\n",
      "Iteration 14882, loss = 2.51103977\n",
      "Iteration 14883, loss = 2.51106926\n",
      "Iteration 14884, loss = 2.51116941\n",
      "Iteration 14885, loss = 2.51119782\n",
      "Iteration 14886, loss = 2.51095393\n",
      "Iteration 14887, loss = 2.51091637\n",
      "Iteration 14888, loss = 2.51091273\n",
      "Iteration 14889, loss = 2.51087874\n",
      "Iteration 14890, loss = 2.51094516\n",
      "Iteration 14891, loss = 2.51081674\n",
      "Iteration 14892, loss = 2.51085053\n",
      "Iteration 14893, loss = 2.51093273\n",
      "Iteration 14894, loss = 2.51079203\n",
      "Iteration 14895, loss = 2.51081720\n",
      "Iteration 14896, loss = 2.51089009\n",
      "Iteration 14897, loss = 2.51083902\n",
      "Iteration 14898, loss = 2.51077128\n",
      "Iteration 14899, loss = 2.51073109\n",
      "Iteration 14900, loss = 2.51082142\n",
      "Iteration 14901, loss = 2.51066403\n",
      "Iteration 14902, loss = 2.51071895\n",
      "Iteration 14903, loss = 2.51068085\n",
      "Iteration 14904, loss = 2.51057491\n",
      "Iteration 14905, loss = 2.51068547\n",
      "Iteration 14906, loss = 2.51075782\n",
      "Iteration 14907, loss = 2.51056958\n",
      "Iteration 14908, loss = 2.51054755\n",
      "Iteration 14909, loss = 2.51074678\n",
      "Iteration 14910, loss = 2.51067212\n",
      "Iteration 14911, loss = 2.51065458\n",
      "Iteration 14912, loss = 2.51057202\n",
      "Iteration 14913, loss = 2.51052757\n",
      "Iteration 14914, loss = 2.51054265\n",
      "Iteration 14915, loss = 2.51054868\n",
      "Iteration 14916, loss = 2.51053131\n",
      "Iteration 14917, loss = 2.51045502\n",
      "Iteration 14918, loss = 2.51043794\n",
      "Iteration 14919, loss = 2.51078987\n",
      "Iteration 14920, loss = 2.51045610\n",
      "Iteration 14921, loss = 2.51050396\n",
      "Iteration 14922, loss = 2.51033084\n",
      "Iteration 14923, loss = 2.51021487\n",
      "Iteration 14924, loss = 2.51036163\n",
      "Iteration 14925, loss = 2.51038546\n",
      "Iteration 14926, loss = 2.51038928\n",
      "Iteration 14927, loss = 2.51022320\n",
      "Iteration 14928, loss = 2.51027490\n",
      "Iteration 14929, loss = 2.51035499\n",
      "Iteration 14930, loss = 2.51044922\n",
      "Iteration 14931, loss = 2.51031830\n",
      "Iteration 14932, loss = 2.51036811\n",
      "Iteration 14933, loss = 2.51018269\n",
      "Iteration 14934, loss = 2.51019746\n",
      "Iteration 14935, loss = 2.51022676\n",
      "Iteration 14936, loss = 2.51014967\n",
      "Iteration 14937, loss = 2.51009566\n",
      "Iteration 14938, loss = 2.51017030\n",
      "Iteration 14939, loss = 2.51009167\n",
      "Iteration 14940, loss = 2.51006259\n",
      "Iteration 14941, loss = 2.50998877\n",
      "Iteration 14942, loss = 2.51018943\n",
      "Iteration 14943, loss = 2.51001893\n",
      "Iteration 14944, loss = 2.51046428\n",
      "Iteration 14945, loss = 2.51011646\n",
      "Iteration 14946, loss = 2.51009215\n",
      "Iteration 14947, loss = 2.50996316\n",
      "Iteration 14948, loss = 2.50993239\n",
      "Iteration 14949, loss = 2.51004086\n",
      "Iteration 14950, loss = 2.50994458\n",
      "Iteration 14951, loss = 2.50995261\n",
      "Iteration 14952, loss = 2.51002457\n",
      "Iteration 14953, loss = 2.50983368\n",
      "Iteration 14954, loss = 2.51002228\n",
      "Iteration 14955, loss = 2.51004538\n",
      "Iteration 14956, loss = 2.50977277\n",
      "Iteration 14957, loss = 2.50988064\n",
      "Iteration 14958, loss = 2.50987659\n",
      "Iteration 14959, loss = 2.50995096\n",
      "Iteration 14960, loss = 2.50974833\n",
      "Iteration 14961, loss = 2.50981042\n",
      "Iteration 14962, loss = 2.50974760\n",
      "Iteration 14963, loss = 2.50981408\n",
      "Iteration 14964, loss = 2.50974059\n",
      "Iteration 14965, loss = 2.50968925\n",
      "Iteration 14966, loss = 2.50981112\n",
      "Iteration 14967, loss = 2.50980592\n",
      "Iteration 14968, loss = 2.50962668\n",
      "Iteration 14969, loss = 2.50975104\n",
      "Iteration 14970, loss = 2.50960830\n",
      "Iteration 14971, loss = 2.50962509\n",
      "Iteration 14972, loss = 2.50950699\n",
      "Iteration 14973, loss = 2.50981052\n",
      "Iteration 14974, loss = 2.50946363\n",
      "Iteration 14975, loss = 2.50952127\n",
      "Iteration 14976, loss = 2.50958396\n",
      "Iteration 14977, loss = 2.50954015\n",
      "Iteration 14978, loss = 2.50953391\n",
      "Iteration 14979, loss = 2.50952464\n",
      "Iteration 14980, loss = 2.50939373\n",
      "Iteration 14981, loss = 2.50949898\n",
      "Iteration 14982, loss = 2.50950940\n",
      "Iteration 14983, loss = 2.50963810\n",
      "Iteration 14984, loss = 2.50934943\n",
      "Iteration 14985, loss = 2.50938702\n",
      "Iteration 14986, loss = 2.50938526\n",
      "Iteration 14987, loss = 2.50933491\n",
      "Iteration 14988, loss = 2.50924154\n",
      "Iteration 14989, loss = 2.50940660\n",
      "Iteration 14990, loss = 2.50942930\n",
      "Iteration 14991, loss = 2.50929055\n",
      "Iteration 14992, loss = 2.50936884\n",
      "Iteration 14993, loss = 2.50927757\n",
      "Iteration 14994, loss = 2.50940464\n",
      "Iteration 14995, loss = 2.50924498\n",
      "Iteration 14996, loss = 2.50928441\n",
      "Iteration 14997, loss = 2.50923764\n",
      "Iteration 14998, loss = 2.50916947\n",
      "Iteration 14999, loss = 2.50918248\n",
      "Iteration 15000, loss = 2.50927258\n",
      "Iteration 15001, loss = 2.50912962\n",
      "Iteration 15002, loss = 2.50911811\n",
      "Iteration 15003, loss = 2.50913513\n",
      "Iteration 15004, loss = 2.50914815\n",
      "Iteration 15005, loss = 2.50909998\n",
      "Iteration 15006, loss = 2.50903822\n",
      "Iteration 15007, loss = 2.50909137\n",
      "Iteration 15008, loss = 2.50904601\n",
      "Iteration 15009, loss = 2.50901806\n",
      "Iteration 15010, loss = 2.50901960\n",
      "Iteration 15011, loss = 2.50916379\n",
      "Iteration 15012, loss = 2.50912585\n",
      "Iteration 15013, loss = 2.50908613\n",
      "Iteration 15014, loss = 2.50900667\n",
      "Iteration 15015, loss = 2.50889073\n",
      "Iteration 15016, loss = 2.50896479\n",
      "Iteration 15017, loss = 2.50887439\n",
      "Iteration 15018, loss = 2.50899514\n",
      "Iteration 15019, loss = 2.50882141\n",
      "Iteration 15020, loss = 2.50885086\n",
      "Iteration 15021, loss = 2.50886473\n",
      "Iteration 15022, loss = 2.50879560\n",
      "Iteration 15023, loss = 2.50882621\n",
      "Iteration 15024, loss = 2.50875351\n",
      "Iteration 15025, loss = 2.50878118\n",
      "Iteration 15026, loss = 2.50876382\n",
      "Iteration 15027, loss = 2.50878019\n",
      "Iteration 15028, loss = 2.50889753\n",
      "Iteration 15029, loss = 2.50881367\n",
      "Iteration 15030, loss = 2.50876547\n",
      "Iteration 15031, loss = 2.50876653\n",
      "Iteration 15032, loss = 2.50884835\n",
      "Iteration 15033, loss = 2.50876813\n",
      "Iteration 15034, loss = 2.50868596\n",
      "Iteration 15035, loss = 2.50878127\n",
      "Iteration 15036, loss = 2.50859343\n",
      "Iteration 15037, loss = 2.50858011\n",
      "Iteration 15038, loss = 2.50858352\n",
      "Iteration 15039, loss = 2.50874651\n",
      "Iteration 15040, loss = 2.50870139\n",
      "Iteration 15041, loss = 2.50857632\n",
      "Iteration 15042, loss = 2.50861184\n",
      "Iteration 15043, loss = 2.50855597\n",
      "Iteration 15044, loss = 2.50856593\n",
      "Iteration 15045, loss = 2.50857784\n",
      "Iteration 15046, loss = 2.50845601\n",
      "Iteration 15047, loss = 2.50839707\n",
      "Iteration 15048, loss = 2.50841964\n",
      "Iteration 15049, loss = 2.50839243\n",
      "Iteration 15050, loss = 2.50846557\n",
      "Iteration 15051, loss = 2.50839397\n",
      "Iteration 15052, loss = 2.50847604\n",
      "Iteration 15053, loss = 2.50860110\n",
      "Iteration 15054, loss = 2.50831577\n",
      "Iteration 15055, loss = 2.50830524\n",
      "Iteration 15056, loss = 2.50824625\n",
      "Iteration 15057, loss = 2.50829202\n",
      "Iteration 15058, loss = 2.50824875\n",
      "Iteration 15059, loss = 2.50833387\n",
      "Iteration 15060, loss = 2.50858255\n",
      "Iteration 15061, loss = 2.50842596\n",
      "Iteration 15062, loss = 2.50831239\n",
      "Iteration 15063, loss = 2.50826918\n",
      "Iteration 15064, loss = 2.50837765\n",
      "Iteration 15065, loss = 2.50818580\n",
      "Iteration 15066, loss = 2.50823602\n",
      "Iteration 15067, loss = 2.50819055\n",
      "Iteration 15068, loss = 2.50818646\n",
      "Iteration 15069, loss = 2.50821538\n",
      "Iteration 15070, loss = 2.50813005\n",
      "Iteration 15071, loss = 2.50804107\n",
      "Iteration 15072, loss = 2.50800821\n",
      "Iteration 15073, loss = 2.50811117\n",
      "Iteration 15074, loss = 2.50809800\n",
      "Iteration 15075, loss = 2.50808640\n",
      "Iteration 15076, loss = 2.50795233\n",
      "Iteration 15077, loss = 2.50795689\n",
      "Iteration 15078, loss = 2.50784679\n",
      "Iteration 15079, loss = 2.50791874\n",
      "Iteration 15080, loss = 2.50789459\n",
      "Iteration 15081, loss = 2.50807573\n",
      "Iteration 15082, loss = 2.50807603\n",
      "Iteration 15083, loss = 2.50798674\n",
      "Iteration 15084, loss = 2.50795890\n",
      "Iteration 15085, loss = 2.50789765\n",
      "Iteration 15086, loss = 2.50784537\n",
      "Iteration 15087, loss = 2.50779035\n",
      "Iteration 15088, loss = 2.50791051\n",
      "Iteration 15089, loss = 2.50777145\n",
      "Iteration 15090, loss = 2.50785681\n",
      "Iteration 15091, loss = 2.50775468\n",
      "Iteration 15092, loss = 2.50779583\n",
      "Iteration 15093, loss = 2.50769406\n",
      "Iteration 15094, loss = 2.50763909\n",
      "Iteration 15095, loss = 2.50772781\n",
      "Iteration 15096, loss = 2.50775730\n",
      "Iteration 15097, loss = 2.50769400\n",
      "Iteration 15098, loss = 2.50770422\n",
      "Iteration 15099, loss = 2.50778934\n",
      "Iteration 15100, loss = 2.50753720\n",
      "Iteration 15101, loss = 2.50756159\n",
      "Iteration 15102, loss = 2.50764439\n",
      "Iteration 15103, loss = 2.50768866\n",
      "Iteration 15104, loss = 2.50753112\n",
      "Iteration 15105, loss = 2.50792471\n",
      "Iteration 15106, loss = 2.50761984\n",
      "Iteration 15107, loss = 2.50750578\n",
      "Iteration 15108, loss = 2.50750788\n",
      "Iteration 15109, loss = 2.50740933\n",
      "Iteration 15110, loss = 2.50752473\n",
      "Iteration 15111, loss = 2.50739634\n",
      "Iteration 15112, loss = 2.50743447\n",
      "Iteration 15113, loss = 2.50735693\n",
      "Iteration 15114, loss = 2.50758156\n",
      "Iteration 15115, loss = 2.50731552\n",
      "Iteration 15116, loss = 2.50734423\n",
      "Iteration 15117, loss = 2.50753388\n",
      "Iteration 15118, loss = 2.50754579\n",
      "Iteration 15119, loss = 2.50742857\n",
      "Iteration 15120, loss = 2.50732148\n",
      "Iteration 15121, loss = 2.50726788\n",
      "Iteration 15122, loss = 2.50739977\n",
      "Iteration 15123, loss = 2.50740232\n",
      "Iteration 15124, loss = 2.50744424\n",
      "Iteration 15125, loss = 2.50730595\n",
      "Iteration 15126, loss = 2.50736682\n",
      "Iteration 15127, loss = 2.50723457\n",
      "Iteration 15128, loss = 2.50722019\n",
      "Iteration 15129, loss = 2.50712814\n",
      "Iteration 15130, loss = 2.50749601\n",
      "Iteration 15131, loss = 2.50721286\n",
      "Iteration 15132, loss = 2.50719148\n",
      "Iteration 15133, loss = 2.50723848\n",
      "Iteration 15134, loss = 2.50699782\n",
      "Iteration 15135, loss = 2.50703747\n",
      "Iteration 15136, loss = 2.50705954\n",
      "Iteration 15137, loss = 2.50721910\n",
      "Iteration 15138, loss = 2.50713992\n",
      "Iteration 15139, loss = 2.50708445\n",
      "Iteration 15140, loss = 2.50706028\n",
      "Iteration 15141, loss = 2.50700619\n",
      "Iteration 15142, loss = 2.50700928\n",
      "Iteration 15143, loss = 2.50694218\n",
      "Iteration 15144, loss = 2.50752931\n",
      "Iteration 15145, loss = 2.50688956\n",
      "Iteration 15146, loss = 2.50705107\n",
      "Iteration 15147, loss = 2.50695204\n",
      "Iteration 15148, loss = 2.50680710\n",
      "Iteration 15149, loss = 2.50687965\n",
      "Iteration 15150, loss = 2.50682661\n",
      "Iteration 15151, loss = 2.50684734\n",
      "Iteration 15152, loss = 2.50678953\n",
      "Iteration 15153, loss = 2.50680967\n",
      "Iteration 15154, loss = 2.50672026\n",
      "Iteration 15155, loss = 2.50672471\n",
      "Iteration 15156, loss = 2.50675369\n",
      "Iteration 15157, loss = 2.50676763\n",
      "Iteration 15158, loss = 2.50680010\n",
      "Iteration 15159, loss = 2.50666198\n",
      "Iteration 15160, loss = 2.50681025\n",
      "Iteration 15161, loss = 2.50667286\n",
      "Iteration 15162, loss = 2.50658515\n",
      "Iteration 15163, loss = 2.50664040\n",
      "Iteration 15164, loss = 2.50664187\n",
      "Iteration 15165, loss = 2.50668387\n",
      "Iteration 15166, loss = 2.50657540\n",
      "Iteration 15167, loss = 2.50659478\n",
      "Iteration 15168, loss = 2.50652445\n",
      "Iteration 15169, loss = 2.50673685\n",
      "Iteration 15170, loss = 2.50657964\n",
      "Iteration 15171, loss = 2.50652661\n",
      "Iteration 15172, loss = 2.50652848\n",
      "Iteration 15173, loss = 2.50650428\n",
      "Iteration 15174, loss = 2.50643901\n",
      "Iteration 15175, loss = 2.50656880\n",
      "Iteration 15176, loss = 2.50636112\n",
      "Iteration 15177, loss = 2.50645485\n",
      "Iteration 15178, loss = 2.50648496\n",
      "Iteration 15179, loss = 2.50638619\n",
      "Iteration 15180, loss = 2.50654181\n",
      "Iteration 15181, loss = 2.50657737\n",
      "Iteration 15182, loss = 2.50635311\n",
      "Iteration 15183, loss = 2.50638770\n",
      "Iteration 15184, loss = 2.50631887\n",
      "Iteration 15185, loss = 2.50637620\n",
      "Iteration 15186, loss = 2.50636118\n",
      "Iteration 15187, loss = 2.50633841\n",
      "Iteration 15188, loss = 2.50610920\n",
      "Iteration 15189, loss = 2.50645594\n",
      "Iteration 15190, loss = 2.50611199\n",
      "Iteration 15191, loss = 2.50641204\n",
      "Iteration 15192, loss = 2.50631966\n",
      "Iteration 15193, loss = 2.50644045\n",
      "Iteration 15194, loss = 2.50615048\n",
      "Iteration 15195, loss = 2.50610282\n",
      "Iteration 15196, loss = 2.50632259\n",
      "Iteration 15197, loss = 2.50601466\n",
      "Iteration 15198, loss = 2.50603110\n",
      "Iteration 15199, loss = 2.50622984\n",
      "Iteration 15200, loss = 2.50612018\n",
      "Iteration 15201, loss = 2.50603983\n",
      "Iteration 15202, loss = 2.50616769\n",
      "Iteration 15203, loss = 2.50600469\n",
      "Iteration 15204, loss = 2.50598963\n",
      "Iteration 15205, loss = 2.50597530\n",
      "Iteration 15206, loss = 2.50597388\n",
      "Iteration 15207, loss = 2.50598513\n",
      "Iteration 15208, loss = 2.50621074\n",
      "Iteration 15209, loss = 2.50587302\n",
      "Iteration 15210, loss = 2.50584288\n",
      "Iteration 15211, loss = 2.50590084\n",
      "Iteration 15212, loss = 2.50583458\n",
      "Iteration 15213, loss = 2.50593045\n",
      "Iteration 15214, loss = 2.50598230\n",
      "Iteration 15215, loss = 2.50599277\n",
      "Iteration 15216, loss = 2.50579497\n",
      "Iteration 15217, loss = 2.50581196\n",
      "Iteration 15218, loss = 2.50571073\n",
      "Iteration 15219, loss = 2.50584014\n",
      "Iteration 15220, loss = 2.50570602\n",
      "Iteration 15221, loss = 2.50584371\n",
      "Iteration 15222, loss = 2.50578608\n",
      "Iteration 15223, loss = 2.50568265\n",
      "Iteration 15224, loss = 2.50571592\n",
      "Iteration 15225, loss = 2.50588096\n",
      "Iteration 15226, loss = 2.50569952\n",
      "Iteration 15227, loss = 2.50567095\n",
      "Iteration 15228, loss = 2.50567520\n",
      "Iteration 15229, loss = 2.50567493\n",
      "Iteration 15230, loss = 2.50557426\n",
      "Iteration 15231, loss = 2.50553142\n",
      "Iteration 15232, loss = 2.50558984\n",
      "Iteration 15233, loss = 2.50566769\n",
      "Iteration 15234, loss = 2.50565900\n",
      "Iteration 15235, loss = 2.50553442\n",
      "Iteration 15236, loss = 2.50543397\n",
      "Iteration 15237, loss = 2.50552021\n",
      "Iteration 15238, loss = 2.50559255\n",
      "Iteration 15239, loss = 2.50539566\n",
      "Iteration 15240, loss = 2.50560012\n",
      "Iteration 15241, loss = 2.50549875\n",
      "Iteration 15242, loss = 2.50549566\n",
      "Iteration 15243, loss = 2.50545517\n",
      "Iteration 15244, loss = 2.50548020\n",
      "Iteration 15245, loss = 2.50537837\n",
      "Iteration 15246, loss = 2.50551460\n",
      "Iteration 15247, loss = 2.50543908\n",
      "Iteration 15248, loss = 2.50537982\n",
      "Iteration 15249, loss = 2.50540495\n",
      "Iteration 15250, loss = 2.50528476\n",
      "Iteration 15251, loss = 2.50538177\n",
      "Iteration 15252, loss = 2.50527107\n",
      "Iteration 15253, loss = 2.50537544\n",
      "Iteration 15254, loss = 2.50523782\n",
      "Iteration 15255, loss = 2.50514968\n",
      "Iteration 15256, loss = 2.50515058\n",
      "Iteration 15257, loss = 2.50533693\n",
      "Iteration 15258, loss = 2.50518440\n",
      "Iteration 15259, loss = 2.50514717\n",
      "Iteration 15260, loss = 2.50529652\n",
      "Iteration 15261, loss = 2.50508441\n",
      "Iteration 15262, loss = 2.50514716\n",
      "Iteration 15263, loss = 2.50509231\n",
      "Iteration 15264, loss = 2.50513342\n",
      "Iteration 15265, loss = 2.50499300\n",
      "Iteration 15266, loss = 2.50518225\n",
      "Iteration 15267, loss = 2.50507606\n",
      "Iteration 15268, loss = 2.50494522\n",
      "Iteration 15269, loss = 2.50497981\n",
      "Iteration 15270, loss = 2.50519173\n",
      "Iteration 15271, loss = 2.50502189\n",
      "Iteration 15272, loss = 2.50503526\n",
      "Iteration 15273, loss = 2.50487945\n",
      "Iteration 15274, loss = 2.50494065\n",
      "Iteration 15275, loss = 2.50495137\n",
      "Iteration 15276, loss = 2.50487683\n",
      "Iteration 15277, loss = 2.50489459\n",
      "Iteration 15278, loss = 2.50485575\n",
      "Iteration 15279, loss = 2.50488807\n",
      "Iteration 15280, loss = 2.50491690\n",
      "Iteration 15281, loss = 2.50513004\n",
      "Iteration 15282, loss = 2.50480676\n",
      "Iteration 15283, loss = 2.50470661\n",
      "Iteration 15284, loss = 2.50472493\n",
      "Iteration 15285, loss = 2.50473026\n",
      "Iteration 15286, loss = 2.50465753\n",
      "Iteration 15287, loss = 2.50470326\n",
      "Iteration 15288, loss = 2.50495155\n",
      "Iteration 15289, loss = 2.50466816\n",
      "Iteration 15290, loss = 2.50473196\n",
      "Iteration 15291, loss = 2.50490040\n",
      "Iteration 15292, loss = 2.50466789\n",
      "Iteration 15293, loss = 2.50489567\n",
      "Iteration 15294, loss = 2.50480446\n",
      "Iteration 15295, loss = 2.50466068\n",
      "Iteration 15296, loss = 2.50504248\n",
      "Iteration 15297, loss = 2.50444096\n",
      "Iteration 15298, loss = 2.50452816\n",
      "Iteration 15299, loss = 2.50467467\n",
      "Iteration 15300, loss = 2.50441512\n",
      "Iteration 15301, loss = 2.50477381\n",
      "Iteration 15302, loss = 2.50482009\n",
      "Iteration 15303, loss = 2.50441286\n",
      "Iteration 15304, loss = 2.50467750\n",
      "Iteration 15305, loss = 2.50465462\n",
      "Iteration 15306, loss = 2.50444719\n",
      "Iteration 15307, loss = 2.50446490\n",
      "Iteration 15308, loss = 2.50482599\n",
      "Iteration 15309, loss = 2.50436580\n",
      "Iteration 15310, loss = 2.50454742\n",
      "Iteration 15311, loss = 2.50424668\n",
      "Iteration 15312, loss = 2.50438762\n",
      "Iteration 15313, loss = 2.50443845\n",
      "Iteration 15314, loss = 2.50425093\n",
      "Iteration 15315, loss = 2.50446946\n",
      "Iteration 15316, loss = 2.50432589\n",
      "Iteration 15317, loss = 2.50430298\n",
      "Iteration 15318, loss = 2.50420219\n",
      "Iteration 15319, loss = 2.50425258\n",
      "Iteration 15320, loss = 2.50418664\n",
      "Iteration 15321, loss = 2.50422510\n",
      "Iteration 15322, loss = 2.50414216\n",
      "Iteration 15323, loss = 2.50417305\n",
      "Iteration 15324, loss = 2.50413364\n",
      "Iteration 15325, loss = 2.50403606\n",
      "Iteration 15326, loss = 2.50413141\n",
      "Iteration 15327, loss = 2.50407956\n",
      "Iteration 15328, loss = 2.50411322\n",
      "Iteration 15329, loss = 2.50422017\n",
      "Iteration 15330, loss = 2.50397480\n",
      "Iteration 15331, loss = 2.50411477\n",
      "Iteration 15332, loss = 2.50420366\n",
      "Iteration 15333, loss = 2.50387931\n",
      "Iteration 15334, loss = 2.50417568\n",
      "Iteration 15335, loss = 2.50396521\n",
      "Iteration 15336, loss = 2.50411380\n",
      "Iteration 15337, loss = 2.50398228\n",
      "Iteration 15338, loss = 2.50401098\n",
      "Iteration 15339, loss = 2.50390751\n",
      "Iteration 15340, loss = 2.50382364\n",
      "Iteration 15341, loss = 2.50390954\n",
      "Iteration 15342, loss = 2.50380988\n",
      "Iteration 15343, loss = 2.50404348\n",
      "Iteration 15344, loss = 2.50397393\n",
      "Iteration 15345, loss = 2.50396570\n",
      "Iteration 15346, loss = 2.50380802\n",
      "Iteration 15347, loss = 2.50372532\n",
      "Iteration 15348, loss = 2.50385507\n",
      "Iteration 15349, loss = 2.50389428\n",
      "Iteration 15350, loss = 2.50379214\n",
      "Iteration 15351, loss = 2.50385497\n",
      "Iteration 15352, loss = 2.50375700\n",
      "Iteration 15353, loss = 2.50396178\n",
      "Iteration 15354, loss = 2.50364264\n",
      "Iteration 15355, loss = 2.50365877\n",
      "Iteration 15356, loss = 2.50365240\n",
      "Iteration 15357, loss = 2.50386257\n",
      "Iteration 15358, loss = 2.50359051\n",
      "Iteration 15359, loss = 2.50361930\n",
      "Iteration 15360, loss = 2.50360788\n",
      "Iteration 15361, loss = 2.50355160\n",
      "Iteration 15362, loss = 2.50371093\n",
      "Iteration 15363, loss = 2.50346926\n",
      "Iteration 15364, loss = 2.50357079\n",
      "Iteration 15365, loss = 2.50354806\n",
      "Iteration 15366, loss = 2.50346027\n",
      "Iteration 15367, loss = 2.50352883\n",
      "Iteration 15368, loss = 2.50345421\n",
      "Iteration 15369, loss = 2.50343241\n",
      "Iteration 15370, loss = 2.50340553\n",
      "Iteration 15371, loss = 2.50339378\n",
      "Iteration 15372, loss = 2.50364127\n",
      "Iteration 15373, loss = 2.50343396\n",
      "Iteration 15374, loss = 2.50339314\n",
      "Iteration 15375, loss = 2.50339791\n",
      "Iteration 15376, loss = 2.50389391\n",
      "Iteration 15377, loss = 2.50340330\n",
      "Iteration 15378, loss = 2.50332502\n",
      "Iteration 15379, loss = 2.50330747\n",
      "Iteration 15380, loss = 2.50326356\n",
      "Iteration 15381, loss = 2.50325827\n",
      "Iteration 15382, loss = 2.50324188\n",
      "Iteration 15383, loss = 2.50318213\n",
      "Iteration 15384, loss = 2.50324793\n",
      "Iteration 15385, loss = 2.50319955\n",
      "Iteration 15386, loss = 2.50326459\n",
      "Iteration 15387, loss = 2.50312879\n",
      "Iteration 15388, loss = 2.50305487\n",
      "Iteration 15389, loss = 2.50315734\n",
      "Iteration 15390, loss = 2.50321310\n",
      "Iteration 15391, loss = 2.50305495\n",
      "Iteration 15392, loss = 2.50304163\n",
      "Iteration 15393, loss = 2.50311703\n",
      "Iteration 15394, loss = 2.50315231\n",
      "Iteration 15395, loss = 2.50303108\n",
      "Iteration 15396, loss = 2.50305239\n",
      "Iteration 15397, loss = 2.50310989\n",
      "Iteration 15398, loss = 2.50304684\n",
      "Iteration 15399, loss = 2.50308551\n",
      "Iteration 15400, loss = 2.50300588\n",
      "Iteration 15401, loss = 2.50301505\n",
      "Iteration 15402, loss = 2.50290335\n",
      "Iteration 15403, loss = 2.50305161\n",
      "Iteration 15404, loss = 2.50300103\n",
      "Iteration 15405, loss = 2.50293189\n",
      "Iteration 15406, loss = 2.50301692\n",
      "Iteration 15407, loss = 2.50278795\n",
      "Iteration 15408, loss = 2.50303201\n",
      "Iteration 15409, loss = 2.50277279\n",
      "Iteration 15410, loss = 2.50302148\n",
      "Iteration 15411, loss = 2.50285187\n",
      "Iteration 15412, loss = 2.50277375\n",
      "Iteration 15413, loss = 2.50274515\n",
      "Iteration 15414, loss = 2.50274776\n",
      "Iteration 15415, loss = 2.50270330\n",
      "Iteration 15416, loss = 2.50268123\n",
      "Iteration 15417, loss = 2.50265317\n",
      "Iteration 15418, loss = 2.50275855\n",
      "Iteration 15419, loss = 2.50269112\n",
      "Iteration 15420, loss = 2.50275053\n",
      "Iteration 15421, loss = 2.50259579\n",
      "Iteration 15422, loss = 2.50268782\n",
      "Iteration 15423, loss = 2.50259413\n",
      "Iteration 15424, loss = 2.50283900\n",
      "Iteration 15425, loss = 2.50259272\n",
      "Iteration 15426, loss = 2.50253938\n",
      "Iteration 15427, loss = 2.50254373\n",
      "Iteration 15428, loss = 2.50259742\n",
      "Iteration 15429, loss = 2.50269101\n",
      "Iteration 15430, loss = 2.50252666\n",
      "Iteration 15431, loss = 2.50251746\n",
      "Iteration 15432, loss = 2.50255889\n",
      "Iteration 15433, loss = 2.50233191\n",
      "Iteration 15434, loss = 2.50261204\n",
      "Iteration 15435, loss = 2.50247860\n",
      "Iteration 15436, loss = 2.50245215\n",
      "Iteration 15437, loss = 2.50233950\n",
      "Iteration 15438, loss = 2.50233842\n",
      "Iteration 15439, loss = 2.50254093\n",
      "Iteration 15440, loss = 2.50238997\n",
      "Iteration 15441, loss = 2.50233244\n",
      "Iteration 15442, loss = 2.50235825\n",
      "Iteration 15443, loss = 2.50232846\n",
      "Iteration 15444, loss = 2.50228859\n",
      "Iteration 15445, loss = 2.50245177\n",
      "Iteration 15446, loss = 2.50226366\n",
      "Iteration 15447, loss = 2.50222262\n",
      "Iteration 15448, loss = 2.50233276\n",
      "Iteration 15449, loss = 2.50232653\n",
      "Iteration 15450, loss = 2.50218981\n",
      "Iteration 15451, loss = 2.50220011\n",
      "Iteration 15452, loss = 2.50213131\n",
      "Iteration 15453, loss = 2.50212071\n",
      "Iteration 15454, loss = 2.50242611\n",
      "Iteration 15455, loss = 2.50205997\n",
      "Iteration 15456, loss = 2.50213794\n",
      "Iteration 15457, loss = 2.50208039\n",
      "Iteration 15458, loss = 2.50211109\n",
      "Iteration 15459, loss = 2.50199917\n",
      "Iteration 15460, loss = 2.50204523\n",
      "Iteration 15461, loss = 2.50203252\n",
      "Iteration 15462, loss = 2.50197346\n",
      "Iteration 15463, loss = 2.50213463\n",
      "Iteration 15464, loss = 2.50190982\n",
      "Iteration 15465, loss = 2.50198764\n",
      "Iteration 15466, loss = 2.50220977\n",
      "Iteration 15467, loss = 2.50185655\n",
      "Iteration 15468, loss = 2.50196681\n",
      "Iteration 15469, loss = 2.50186398\n",
      "Iteration 15470, loss = 2.50200654\n",
      "Iteration 15471, loss = 2.50190638\n",
      "Iteration 15472, loss = 2.50188776\n",
      "Iteration 15473, loss = 2.50175059\n",
      "Iteration 15474, loss = 2.50194366\n",
      "Iteration 15475, loss = 2.50173048\n",
      "Iteration 15476, loss = 2.50174674\n",
      "Iteration 15477, loss = 2.50208777\n",
      "Iteration 15478, loss = 2.50170804\n",
      "Iteration 15479, loss = 2.50183292\n",
      "Iteration 15480, loss = 2.50170614\n",
      "Iteration 15481, loss = 2.50176482\n",
      "Iteration 15482, loss = 2.50172906\n",
      "Iteration 15483, loss = 2.50169620\n",
      "Iteration 15484, loss = 2.50158280\n",
      "Iteration 15485, loss = 2.50163032\n",
      "Iteration 15486, loss = 2.50182246\n",
      "Iteration 15487, loss = 2.50164144\n",
      "Iteration 15488, loss = 2.50152146\n",
      "Iteration 15489, loss = 2.50154889\n",
      "Iteration 15490, loss = 2.50156084\n",
      "Iteration 15491, loss = 2.50167639\n",
      "Iteration 15492, loss = 2.50187603\n",
      "Iteration 15493, loss = 2.50167124\n",
      "Iteration 15494, loss = 2.50151695\n",
      "Iteration 15495, loss = 2.50166628\n",
      "Iteration 15496, loss = 2.50171682\n",
      "Iteration 15497, loss = 2.50149255\n",
      "Iteration 15498, loss = 2.50143055\n",
      "Iteration 15499, loss = 2.50148052\n",
      "Iteration 15500, loss = 2.50156254\n",
      "Iteration 15501, loss = 2.50152268\n",
      "Iteration 15502, loss = 2.50153835\n",
      "Iteration 15503, loss = 2.50134029\n",
      "Iteration 15504, loss = 2.50132546\n",
      "Iteration 15505, loss = 2.50132402\n",
      "Iteration 15506, loss = 2.50133283\n",
      "Iteration 15507, loss = 2.50141545\n",
      "Iteration 15508, loss = 2.50136686\n",
      "Iteration 15509, loss = 2.50126242\n",
      "Iteration 15510, loss = 2.50150455\n",
      "Iteration 15511, loss = 2.50120821\n",
      "Iteration 15512, loss = 2.50128521\n",
      "Iteration 15513, loss = 2.50131241\n",
      "Iteration 15514, loss = 2.50115444\n",
      "Iteration 15515, loss = 2.50123705\n",
      "Iteration 15516, loss = 2.50116234\n",
      "Iteration 15517, loss = 2.50114488\n",
      "Iteration 15518, loss = 2.50116999\n",
      "Iteration 15519, loss = 2.50110514\n",
      "Iteration 15520, loss = 2.50109580\n",
      "Iteration 15521, loss = 2.50102332\n",
      "Iteration 15522, loss = 2.50104869\n",
      "Iteration 15523, loss = 2.50110151\n",
      "Iteration 15524, loss = 2.50105038\n",
      "Iteration 15525, loss = 2.50112418\n",
      "Iteration 15526, loss = 2.50097193\n",
      "Iteration 15527, loss = 2.50099221\n",
      "Iteration 15528, loss = 2.50093922\n",
      "Iteration 15529, loss = 2.50087509\n",
      "Iteration 15530, loss = 2.50102848\n",
      "Iteration 15531, loss = 2.50092360\n",
      "Iteration 15532, loss = 2.50106672\n",
      "Iteration 15533, loss = 2.50083481\n",
      "Iteration 15534, loss = 2.50087569\n",
      "Iteration 15535, loss = 2.50090132\n",
      "Iteration 15536, loss = 2.50094831\n",
      "Iteration 15537, loss = 2.50095292\n",
      "Iteration 15538, loss = 2.50087232\n",
      "Iteration 15539, loss = 2.50118256\n",
      "Iteration 15540, loss = 2.50083091\n",
      "Iteration 15541, loss = 2.50087973\n",
      "Iteration 15542, loss = 2.50065424\n",
      "Iteration 15543, loss = 2.50081664\n",
      "Iteration 15544, loss = 2.50076308\n",
      "Iteration 15545, loss = 2.50064688\n",
      "Iteration 15546, loss = 2.50084714\n",
      "Iteration 15547, loss = 2.50072075\n",
      "Iteration 15548, loss = 2.50056730\n",
      "Iteration 15549, loss = 2.50084341\n",
      "Iteration 15550, loss = 2.50068145\n",
      "Iteration 15551, loss = 2.50062118\n",
      "Iteration 15552, loss = 2.50078676\n",
      "Iteration 15553, loss = 2.50086750\n",
      "Iteration 15554, loss = 2.50063179\n",
      "Iteration 15555, loss = 2.50058863\n",
      "Iteration 15556, loss = 2.50061935\n",
      "Iteration 15557, loss = 2.50056483\n",
      "Iteration 15558, loss = 2.50076980\n",
      "Iteration 15559, loss = 2.50066068\n",
      "Iteration 15560, loss = 2.50048426\n",
      "Iteration 15561, loss = 2.50041361\n",
      "Iteration 15562, loss = 2.50048646\n",
      "Iteration 15563, loss = 2.50048574\n",
      "Iteration 15564, loss = 2.50044702\n",
      "Iteration 15565, loss = 2.50046935\n",
      "Iteration 15566, loss = 2.50051641\n",
      "Iteration 15567, loss = 2.50043962\n",
      "Iteration 15568, loss = 2.50047134\n",
      "Iteration 15569, loss = 2.50041259\n",
      "Iteration 15570, loss = 2.50038496\n",
      "Iteration 15571, loss = 2.50028374\n",
      "Iteration 15572, loss = 2.50038483\n",
      "Iteration 15573, loss = 2.50027058\n",
      "Iteration 15574, loss = 2.50030841\n",
      "Iteration 15575, loss = 2.50015489\n",
      "Iteration 15576, loss = 2.50024297\n",
      "Iteration 15577, loss = 2.50020279\n",
      "Iteration 15578, loss = 2.50029887\n",
      "Iteration 15579, loss = 2.50008488\n",
      "Iteration 15580, loss = 2.50015011\n",
      "Iteration 15581, loss = 2.50017344\n",
      "Iteration 15582, loss = 2.50013890\n",
      "Iteration 15583, loss = 2.50007779\n",
      "Iteration 15584, loss = 2.50014487\n",
      "Iteration 15585, loss = 2.50030050\n",
      "Iteration 15586, loss = 2.50001425\n",
      "Iteration 15587, loss = 2.50014249\n",
      "Iteration 15588, loss = 2.50021572\n",
      "Iteration 15589, loss = 2.50002477\n",
      "Iteration 15590, loss = 2.50007154\n",
      "Iteration 15591, loss = 2.50002474\n",
      "Iteration 15592, loss = 2.50009904\n",
      "Iteration 15593, loss = 2.49989719\n",
      "Iteration 15594, loss = 2.49992847\n",
      "Iteration 15595, loss = 2.49990767\n",
      "Iteration 15596, loss = 2.49987435\n",
      "Iteration 15597, loss = 2.49989451\n",
      "Iteration 15598, loss = 2.50000569\n",
      "Iteration 15599, loss = 2.49987421\n",
      "Iteration 15600, loss = 2.49978596\n",
      "Iteration 15601, loss = 2.49994372\n",
      "Iteration 15602, loss = 2.49980645\n",
      "Iteration 15603, loss = 2.49986883\n",
      "Iteration 15604, loss = 2.49999098\n",
      "Iteration 15605, loss = 2.49984227\n",
      "Iteration 15606, loss = 2.49978631\n",
      "Iteration 15607, loss = 2.49969077\n",
      "Iteration 15608, loss = 2.49978850\n",
      "Iteration 15609, loss = 2.49961656\n",
      "Iteration 15610, loss = 2.49976979\n",
      "Iteration 15611, loss = 2.49962939\n",
      "Iteration 15612, loss = 2.49974881\n",
      "Iteration 15613, loss = 2.49963096\n",
      "Iteration 15614, loss = 2.49983028\n",
      "Iteration 15615, loss = 2.50005440\n",
      "Iteration 15616, loss = 2.49970388\n",
      "Iteration 15617, loss = 2.49955203\n",
      "Iteration 15618, loss = 2.49971062\n",
      "Iteration 15619, loss = 2.49964337\n",
      "Iteration 15620, loss = 2.49953845\n",
      "Iteration 15621, loss = 2.49953527\n",
      "Iteration 15622, loss = 2.49950466\n",
      "Iteration 15623, loss = 2.49950482\n",
      "Iteration 15624, loss = 2.49943282\n",
      "Iteration 15625, loss = 2.49944798\n",
      "Iteration 15626, loss = 2.49939304\n",
      "Iteration 15627, loss = 2.49949271\n",
      "Iteration 15628, loss = 2.49933431\n",
      "Iteration 15629, loss = 2.49936092\n",
      "Iteration 15630, loss = 2.49948140\n",
      "Iteration 15631, loss = 2.49943084\n",
      "Iteration 15632, loss = 2.49924593\n",
      "Iteration 15633, loss = 2.49933636\n",
      "Iteration 15634, loss = 2.49924431\n",
      "Iteration 15635, loss = 2.49931792\n",
      "Iteration 15636, loss = 2.49923557\n",
      "Iteration 15637, loss = 2.49941971\n",
      "Iteration 15638, loss = 2.49920486\n",
      "Iteration 15639, loss = 2.49962003\n",
      "Iteration 15640, loss = 2.49934326\n",
      "Iteration 15641, loss = 2.49932074\n",
      "Iteration 15642, loss = 2.49935901\n",
      "Iteration 15643, loss = 2.49920793\n",
      "Iteration 15644, loss = 2.49927761\n",
      "Iteration 15645, loss = 2.49925876\n",
      "Iteration 15646, loss = 2.49929508\n",
      "Iteration 15647, loss = 2.49918021\n",
      "Iteration 15648, loss = 2.49902101\n",
      "Iteration 15649, loss = 2.49914271\n",
      "Iteration 15650, loss = 2.49907289\n",
      "Iteration 15651, loss = 2.49923309\n",
      "Iteration 15652, loss = 2.49902529\n",
      "Iteration 15653, loss = 2.49945176\n",
      "Iteration 15654, loss = 2.49911302\n",
      "Iteration 15655, loss = 2.49909603\n",
      "Iteration 15656, loss = 2.49922216\n",
      "Iteration 15657, loss = 2.49897769\n",
      "Iteration 15658, loss = 2.49888650\n",
      "Iteration 15659, loss = 2.49899650\n",
      "Iteration 15660, loss = 2.49899167\n",
      "Iteration 15661, loss = 2.49889064\n",
      "Iteration 15662, loss = 2.49886533\n",
      "Iteration 15663, loss = 2.49903215\n",
      "Iteration 15664, loss = 2.49884980\n",
      "Iteration 15665, loss = 2.49887776\n",
      "Iteration 15666, loss = 2.49889860\n",
      "Iteration 15667, loss = 2.49881800\n",
      "Iteration 15668, loss = 2.49907496\n",
      "Iteration 15669, loss = 2.49880606\n",
      "Iteration 15670, loss = 2.49890027\n",
      "Iteration 15671, loss = 2.49901941\n",
      "Iteration 15672, loss = 2.49869297\n",
      "Iteration 15673, loss = 2.49888198\n",
      "Iteration 15674, loss = 2.49891272\n",
      "Iteration 15675, loss = 2.49884120\n",
      "Iteration 15676, loss = 2.49882998\n",
      "Iteration 15677, loss = 2.49861122\n",
      "Iteration 15678, loss = 2.49881978\n",
      "Iteration 15679, loss = 2.49854017\n",
      "Iteration 15680, loss = 2.49876337\n",
      "Iteration 15681, loss = 2.49859811\n",
      "Iteration 15682, loss = 2.49855391\n",
      "Iteration 15683, loss = 2.49853708\n",
      "Iteration 15684, loss = 2.49854178\n",
      "Iteration 15685, loss = 2.49851577\n",
      "Iteration 15686, loss = 2.49868572\n",
      "Iteration 15687, loss = 2.49842454\n",
      "Iteration 15688, loss = 2.49857103\n",
      "Iteration 15689, loss = 2.49872071\n",
      "Iteration 15690, loss = 2.49857272\n",
      "Iteration 15691, loss = 2.49848754\n",
      "Iteration 15692, loss = 2.49841964\n",
      "Iteration 15693, loss = 2.49843433\n",
      "Iteration 15694, loss = 2.49834245\n",
      "Iteration 15695, loss = 2.49836313\n",
      "Iteration 15696, loss = 2.49834847\n",
      "Iteration 15697, loss = 2.49829972\n",
      "Iteration 15698, loss = 2.49825750\n",
      "Iteration 15699, loss = 2.49835786\n",
      "Iteration 15700, loss = 2.49833813\n",
      "Iteration 15701, loss = 2.49850703\n",
      "Iteration 15702, loss = 2.49822631\n",
      "Iteration 15703, loss = 2.49824828\n",
      "Iteration 15704, loss = 2.49864193\n",
      "Iteration 15705, loss = 2.49823239\n",
      "Iteration 15706, loss = 2.49812544\n",
      "Iteration 15707, loss = 2.49824413\n",
      "Iteration 15708, loss = 2.49835244\n",
      "Iteration 15709, loss = 2.49816582\n",
      "Iteration 15710, loss = 2.49815422\n",
      "Iteration 15711, loss = 2.49806425\n",
      "Iteration 15712, loss = 2.49815976\n",
      "Iteration 15713, loss = 2.49807524\n",
      "Iteration 15714, loss = 2.49814939\n",
      "Iteration 15715, loss = 2.49819792\n",
      "Iteration 15716, loss = 2.49799086\n",
      "Iteration 15717, loss = 2.49814458\n",
      "Iteration 15718, loss = 2.49800446\n",
      "Iteration 15719, loss = 2.49814737\n",
      "Iteration 15720, loss = 2.49793445\n",
      "Iteration 15721, loss = 2.49796791\n",
      "Iteration 15722, loss = 2.49801574\n",
      "Iteration 15723, loss = 2.49787279\n",
      "Iteration 15724, loss = 2.49796808\n",
      "Iteration 15725, loss = 2.49795816\n",
      "Iteration 15726, loss = 2.49791027\n",
      "Iteration 15727, loss = 2.49781193\n",
      "Iteration 15728, loss = 2.49778168\n",
      "Iteration 15729, loss = 2.49782464\n",
      "Iteration 15730, loss = 2.49792515\n",
      "Iteration 15731, loss = 2.49785860\n",
      "Iteration 15732, loss = 2.49782933\n",
      "Iteration 15733, loss = 2.49785905\n",
      "Iteration 15734, loss = 2.49780649\n",
      "Iteration 15735, loss = 2.49764292\n",
      "Iteration 15736, loss = 2.49776192\n",
      "Iteration 15737, loss = 2.49776174\n",
      "Iteration 15738, loss = 2.49774485\n",
      "Iteration 15739, loss = 2.49774118\n",
      "Iteration 15740, loss = 2.49775693\n",
      "Iteration 15741, loss = 2.49765413\n",
      "Iteration 15742, loss = 2.49770078\n",
      "Iteration 15743, loss = 2.49774446\n",
      "Iteration 15744, loss = 2.49757215\n",
      "Iteration 15745, loss = 2.49761267\n",
      "Iteration 15746, loss = 2.49753526\n",
      "Iteration 15747, loss = 2.49767480\n",
      "Iteration 15748, loss = 2.49762660\n",
      "Iteration 15749, loss = 2.49769367\n",
      "Iteration 15750, loss = 2.49745584\n",
      "Iteration 15751, loss = 2.49774148\n",
      "Iteration 15752, loss = 2.49748725\n",
      "Iteration 15753, loss = 2.49751219\n",
      "Iteration 15754, loss = 2.49750229\n",
      "Iteration 15755, loss = 2.49737254\n",
      "Iteration 15756, loss = 2.49745743\n",
      "Iteration 15757, loss = 2.49735356\n",
      "Iteration 15758, loss = 2.49741502\n",
      "Iteration 15759, loss = 2.49747168\n",
      "Iteration 15760, loss = 2.49760328\n",
      "Iteration 15761, loss = 2.49739434\n",
      "Iteration 15762, loss = 2.49734175\n",
      "Iteration 15763, loss = 2.49729872\n",
      "Iteration 15764, loss = 2.49723395\n",
      "Iteration 15765, loss = 2.49737143\n",
      "Iteration 15766, loss = 2.49735563\n",
      "Iteration 15767, loss = 2.49719960\n",
      "Iteration 15768, loss = 2.49732903\n",
      "Iteration 15769, loss = 2.49742035\n",
      "Iteration 15770, loss = 2.49725918\n",
      "Iteration 15771, loss = 2.49725378\n",
      "Iteration 15772, loss = 2.49723074\n",
      "Iteration 15773, loss = 2.49717945\n",
      "Iteration 15774, loss = 2.49719331\n",
      "Iteration 15775, loss = 2.49718954\n",
      "Iteration 15776, loss = 2.49710752\n",
      "Iteration 15777, loss = 2.49708137\n",
      "Iteration 15778, loss = 2.49711212\n",
      "Iteration 15779, loss = 2.49697501\n",
      "Iteration 15780, loss = 2.49705565\n",
      "Iteration 15781, loss = 2.49714794\n",
      "Iteration 15782, loss = 2.49703037\n",
      "Iteration 15783, loss = 2.49712382\n",
      "Iteration 15784, loss = 2.49727400\n",
      "Iteration 15785, loss = 2.49697603\n",
      "Iteration 15786, loss = 2.49698034\n",
      "Iteration 15787, loss = 2.49699654\n",
      "Iteration 15788, loss = 2.49706970\n",
      "Iteration 15789, loss = 2.49682089\n",
      "Iteration 15790, loss = 2.49702153\n",
      "Iteration 15791, loss = 2.49708370\n",
      "Iteration 15792, loss = 2.49695360\n",
      "Iteration 15793, loss = 2.49683910\n",
      "Iteration 15794, loss = 2.49686456\n",
      "Iteration 15795, loss = 2.49678114\n",
      "Iteration 15796, loss = 2.49679611\n",
      "Iteration 15797, loss = 2.49677561\n",
      "Iteration 15798, loss = 2.49695888\n",
      "Iteration 15799, loss = 2.49675556\n",
      "Iteration 15800, loss = 2.49670960\n",
      "Iteration 15801, loss = 2.49664714\n",
      "Iteration 15802, loss = 2.49675611\n",
      "Iteration 15803, loss = 2.49663345\n",
      "Iteration 15804, loss = 2.49677458\n",
      "Iteration 15805, loss = 2.49670483\n",
      "Iteration 15806, loss = 2.49678144\n",
      "Iteration 15807, loss = 2.49660583\n",
      "Iteration 15808, loss = 2.49672044\n",
      "Iteration 15809, loss = 2.49690545\n",
      "Iteration 15810, loss = 2.49653421\n",
      "Iteration 15811, loss = 2.49658446\n",
      "Iteration 15812, loss = 2.49668750\n",
      "Iteration 15813, loss = 2.49649133\n",
      "Iteration 15814, loss = 2.49647191\n",
      "Iteration 15815, loss = 2.49654220\n",
      "Iteration 15816, loss = 2.49675109\n",
      "Iteration 15817, loss = 2.49656372\n",
      "Iteration 15818, loss = 2.49664895\n",
      "Iteration 15819, loss = 2.49641844\n",
      "Iteration 15820, loss = 2.49640564\n",
      "Iteration 15821, loss = 2.49663552\n",
      "Iteration 15822, loss = 2.49652017\n",
      "Iteration 15823, loss = 2.49642164\n",
      "Iteration 15824, loss = 2.49649416\n",
      "Iteration 15825, loss = 2.49661983\n",
      "Iteration 15826, loss = 2.49679719\n",
      "Iteration 15827, loss = 2.49632074\n",
      "Iteration 15828, loss = 2.49629009\n",
      "Iteration 15829, loss = 2.49640472\n",
      "Iteration 15830, loss = 2.49644025\n",
      "Iteration 15831, loss = 2.49635469\n",
      "Iteration 15832, loss = 2.49622087\n",
      "Iteration 15833, loss = 2.49626168\n",
      "Iteration 15834, loss = 2.49624602\n",
      "Iteration 15835, loss = 2.49623412\n",
      "Iteration 15836, loss = 2.49618321\n",
      "Iteration 15837, loss = 2.49617977\n",
      "Iteration 15838, loss = 2.49613337\n",
      "Iteration 15839, loss = 2.49604843\n",
      "Iteration 15840, loss = 2.49641113\n",
      "Iteration 15841, loss = 2.49612108\n",
      "Iteration 15842, loss = 2.49619706\n",
      "Iteration 15843, loss = 2.49619409\n",
      "Iteration 15844, loss = 2.49606559\n",
      "Iteration 15845, loss = 2.49622200\n",
      "Iteration 15846, loss = 2.49601748\n",
      "Iteration 15847, loss = 2.49605250\n",
      "Iteration 15848, loss = 2.49606332\n",
      "Iteration 15849, loss = 2.49611373\n",
      "Iteration 15850, loss = 2.49601805\n",
      "Iteration 15851, loss = 2.49596667\n",
      "Iteration 15852, loss = 2.49619013\n",
      "Iteration 15853, loss = 2.49624370\n",
      "Iteration 15854, loss = 2.49591481\n",
      "Iteration 15855, loss = 2.49586586\n",
      "Iteration 15856, loss = 2.49599547\n",
      "Iteration 15857, loss = 2.49591000\n",
      "Iteration 15858, loss = 2.49598918\n",
      "Iteration 15859, loss = 2.49576464\n",
      "Iteration 15860, loss = 2.49583737\n",
      "Iteration 15861, loss = 2.49588267\n",
      "Iteration 15862, loss = 2.49572479\n",
      "Iteration 15863, loss = 2.49578832\n",
      "Iteration 15864, loss = 2.49583359\n",
      "Iteration 15865, loss = 2.49588846\n",
      "Iteration 15866, loss = 2.49571454\n",
      "Iteration 15867, loss = 2.49572675\n",
      "Iteration 15868, loss = 2.49570007\n",
      "Iteration 15869, loss = 2.49575406\n",
      "Iteration 15870, loss = 2.49568516\n",
      "Iteration 15871, loss = 2.49565633\n",
      "Iteration 15872, loss = 2.49558708\n",
      "Iteration 15873, loss = 2.49565713\n",
      "Iteration 15874, loss = 2.49565285\n",
      "Iteration 15875, loss = 2.49564737\n",
      "Iteration 15876, loss = 2.49554674\n",
      "Iteration 15877, loss = 2.49565538\n",
      "Iteration 15878, loss = 2.49557366\n",
      "Iteration 15879, loss = 2.49563941\n",
      "Iteration 15880, loss = 2.49562382\n",
      "Iteration 15881, loss = 2.49562565\n",
      "Iteration 15882, loss = 2.49545925\n",
      "Iteration 15883, loss = 2.49558933\n",
      "Iteration 15884, loss = 2.49562447\n",
      "Iteration 15885, loss = 2.49541391\n",
      "Iteration 15886, loss = 2.49543724\n",
      "Iteration 15887, loss = 2.49542227\n",
      "Iteration 15888, loss = 2.49587289\n",
      "Iteration 15889, loss = 2.49531030\n",
      "Iteration 15890, loss = 2.49531325\n",
      "Iteration 15891, loss = 2.49527377\n",
      "Iteration 15892, loss = 2.49531573\n",
      "Iteration 15893, loss = 2.49522144\n",
      "Iteration 15894, loss = 2.49537839\n",
      "Iteration 15895, loss = 2.49541921\n",
      "Iteration 15896, loss = 2.49535144\n",
      "Iteration 15897, loss = 2.49531559\n",
      "Iteration 15898, loss = 2.49537683\n",
      "Iteration 15899, loss = 2.49522472\n",
      "Iteration 15900, loss = 2.49526322\n",
      "Iteration 15901, loss = 2.49513731\n",
      "Iteration 15902, loss = 2.49519335\n",
      "Iteration 15903, loss = 2.49523603\n",
      "Iteration 15904, loss = 2.49522575\n",
      "Iteration 15905, loss = 2.49519746\n",
      "Iteration 15906, loss = 2.49503365\n",
      "Iteration 15907, loss = 2.49511885\n",
      "Iteration 15908, loss = 2.49510394\n",
      "Iteration 15909, loss = 2.49503346\n",
      "Iteration 15910, loss = 2.49502822\n",
      "Iteration 15911, loss = 2.49509573\n",
      "Iteration 15912, loss = 2.49506486\n",
      "Iteration 15913, loss = 2.49516961\n",
      "Iteration 15914, loss = 2.49552727\n",
      "Iteration 15915, loss = 2.49492480\n",
      "Iteration 15916, loss = 2.49493118\n",
      "Iteration 15917, loss = 2.49490793\n",
      "Iteration 15918, loss = 2.49502292\n",
      "Iteration 15919, loss = 2.49492971\n",
      "Iteration 15920, loss = 2.49498829\n",
      "Iteration 15921, loss = 2.49481417\n",
      "Iteration 15922, loss = 2.49490066\n",
      "Iteration 15923, loss = 2.49494013\n",
      "Iteration 15924, loss = 2.49477146\n",
      "Iteration 15925, loss = 2.49475330\n",
      "Iteration 15926, loss = 2.49485894\n",
      "Iteration 15927, loss = 2.49487501\n",
      "Iteration 15928, loss = 2.49518151\n",
      "Iteration 15929, loss = 2.49470497\n",
      "Iteration 15930, loss = 2.49495295\n",
      "Iteration 15931, loss = 2.49476182\n",
      "Iteration 15932, loss = 2.49471428\n",
      "Iteration 15933, loss = 2.49465766\n",
      "Iteration 15934, loss = 2.49474776\n",
      "Iteration 15935, loss = 2.49465715\n",
      "Iteration 15936, loss = 2.49466482\n",
      "Iteration 15937, loss = 2.49491614\n",
      "Iteration 15938, loss = 2.49463382\n",
      "Iteration 15939, loss = 2.49451531\n",
      "Iteration 15940, loss = 2.49458292\n",
      "Iteration 15941, loss = 2.49467609\n",
      "Iteration 15942, loss = 2.49450320\n",
      "Iteration 15943, loss = 2.49474466\n",
      "Iteration 15944, loss = 2.49469751\n",
      "Iteration 15945, loss = 2.49461817\n",
      "Iteration 15946, loss = 2.49483148\n",
      "Iteration 15947, loss = 2.49440802\n",
      "Iteration 15948, loss = 2.49451971\n",
      "Iteration 15949, loss = 2.49456073\n",
      "Iteration 15950, loss = 2.49441305\n",
      "Iteration 15951, loss = 2.49435521\n",
      "Iteration 15952, loss = 2.49450041\n",
      "Iteration 15953, loss = 2.49456140\n",
      "Iteration 15954, loss = 2.49442360\n",
      "Iteration 15955, loss = 2.49433204\n",
      "Iteration 15956, loss = 2.49437710\n",
      "Iteration 15957, loss = 2.49438186\n",
      "Iteration 15958, loss = 2.49432634\n",
      "Iteration 15959, loss = 2.49425630\n",
      "Iteration 15960, loss = 2.49428548\n",
      "Iteration 15961, loss = 2.49427167\n",
      "Iteration 15962, loss = 2.49410232\n",
      "Iteration 15963, loss = 2.49429413\n",
      "Iteration 15964, loss = 2.49432915\n",
      "Iteration 15965, loss = 2.49412000\n",
      "Iteration 15966, loss = 2.49420928\n",
      "Iteration 15967, loss = 2.49419546\n",
      "Iteration 15968, loss = 2.49423656\n",
      "Iteration 15969, loss = 2.49441951\n",
      "Iteration 15970, loss = 2.49432047\n",
      "Iteration 15971, loss = 2.49406410\n",
      "Iteration 15972, loss = 2.49413587\n",
      "Iteration 15973, loss = 2.49410147\n",
      "Iteration 15974, loss = 2.49394622\n",
      "Iteration 15975, loss = 2.49410843\n",
      "Iteration 15976, loss = 2.49399714\n",
      "Iteration 15977, loss = 2.49418097\n",
      "Iteration 15978, loss = 2.49400955\n",
      "Iteration 15979, loss = 2.49394742\n",
      "Iteration 15980, loss = 2.49402512\n",
      "Iteration 15981, loss = 2.49409786\n",
      "Iteration 15982, loss = 2.49384435\n",
      "Iteration 15983, loss = 2.49385162\n",
      "Iteration 15984, loss = 2.49384598\n",
      "Iteration 15985, loss = 2.49408900\n",
      "Iteration 15986, loss = 2.49414971\n",
      "Iteration 15987, loss = 2.49380527\n",
      "Iteration 15988, loss = 2.49392457\n",
      "Iteration 15989, loss = 2.49388032\n",
      "Iteration 15990, loss = 2.49376556\n",
      "Iteration 15991, loss = 2.49402340\n",
      "Iteration 15992, loss = 2.49382051\n",
      "Iteration 15993, loss = 2.49393245\n",
      "Iteration 15994, loss = 2.49395607\n",
      "Iteration 15995, loss = 2.49372578\n",
      "Iteration 15996, loss = 2.49370003\n",
      "Iteration 15997, loss = 2.49364663\n",
      "Iteration 15998, loss = 2.49368481\n",
      "Iteration 15999, loss = 2.49366313\n",
      "Iteration 16000, loss = 2.49365079\n",
      "Iteration 16001, loss = 2.49370992\n",
      "Iteration 16002, loss = 2.49374733\n",
      "Iteration 16003, loss = 2.49365028\n",
      "Iteration 16004, loss = 2.49369368\n",
      "Iteration 16005, loss = 2.49352200\n",
      "Iteration 16006, loss = 2.49370917\n",
      "Iteration 16007, loss = 2.49371703\n",
      "Iteration 16008, loss = 2.49357488\n",
      "Iteration 16009, loss = 2.49346148\n",
      "Iteration 16010, loss = 2.49353229\n",
      "Iteration 16011, loss = 2.49338572\n",
      "Iteration 16012, loss = 2.49350979\n",
      "Iteration 16013, loss = 2.49348385\n",
      "Iteration 16014, loss = 2.49339478\n",
      "Iteration 16015, loss = 2.49350744\n",
      "Iteration 16016, loss = 2.49345913\n",
      "Iteration 16017, loss = 2.49335448\n",
      "Iteration 16018, loss = 2.49337567\n",
      "Iteration 16019, loss = 2.49339010\n",
      "Iteration 16020, loss = 2.49330269\n",
      "Iteration 16021, loss = 2.49336550\n",
      "Iteration 16022, loss = 2.49339070\n",
      "Iteration 16023, loss = 2.49337981\n",
      "Iteration 16024, loss = 2.49322768\n",
      "Iteration 16025, loss = 2.49322674\n",
      "Iteration 16026, loss = 2.49342821\n",
      "Iteration 16027, loss = 2.49335496\n",
      "Iteration 16028, loss = 2.49334720\n",
      "Iteration 16029, loss = 2.49323414\n",
      "Iteration 16030, loss = 2.49317851\n",
      "Iteration 16031, loss = 2.49308519\n",
      "Iteration 16032, loss = 2.49312980\n",
      "Iteration 16033, loss = 2.49309848\n",
      "Iteration 16034, loss = 2.49321548\n",
      "Iteration 16035, loss = 2.49309830\n",
      "Iteration 16036, loss = 2.49304061\n",
      "Iteration 16037, loss = 2.49338280\n",
      "Iteration 16038, loss = 2.49318096\n",
      "Iteration 16039, loss = 2.49316499\n",
      "Iteration 16040, loss = 2.49299650\n",
      "Iteration 16041, loss = 2.49308724\n",
      "Iteration 16042, loss = 2.49318944\n",
      "Iteration 16043, loss = 2.49297156\n",
      "Iteration 16044, loss = 2.49287899\n",
      "Iteration 16045, loss = 2.49301066\n",
      "Iteration 16046, loss = 2.49287152\n",
      "Iteration 16047, loss = 2.49300333\n",
      "Iteration 16048, loss = 2.49291987\n",
      "Iteration 16049, loss = 2.49286835\n",
      "Iteration 16050, loss = 2.49288977\n",
      "Iteration 16051, loss = 2.49291987\n",
      "Iteration 16052, loss = 2.49277060\n",
      "Iteration 16053, loss = 2.49283407\n",
      "Iteration 16054, loss = 2.49284339\n",
      "Iteration 16055, loss = 2.49297451\n",
      "Iteration 16056, loss = 2.49283376\n",
      "Iteration 16057, loss = 2.49272510\n",
      "Iteration 16058, loss = 2.49272002\n",
      "Iteration 16059, loss = 2.49284172\n",
      "Iteration 16060, loss = 2.49268449\n",
      "Iteration 16061, loss = 2.49271469\n",
      "Iteration 16062, loss = 2.49270058\n",
      "Iteration 16063, loss = 2.49269427\n",
      "Iteration 16064, loss = 2.49262189\n",
      "Iteration 16065, loss = 2.49260412\n",
      "Iteration 16066, loss = 2.49261738\n",
      "Iteration 16067, loss = 2.49273611\n",
      "Iteration 16068, loss = 2.49272606\n",
      "Iteration 16069, loss = 2.49273126\n",
      "Iteration 16070, loss = 2.49260366\n",
      "Iteration 16071, loss = 2.49259557\n",
      "Iteration 16072, loss = 2.49261630\n",
      "Iteration 16073, loss = 2.49257379\n",
      "Iteration 16074, loss = 2.49260714\n",
      "Iteration 16075, loss = 2.49261119\n",
      "Iteration 16076, loss = 2.49241308\n",
      "Iteration 16077, loss = 2.49262898\n",
      "Iteration 16078, loss = 2.49256868\n",
      "Iteration 16079, loss = 2.49234907\n",
      "Iteration 16080, loss = 2.49235296\n",
      "Iteration 16081, loss = 2.49229172\n",
      "Iteration 16082, loss = 2.49246421\n",
      "Iteration 16083, loss = 2.49249608\n",
      "Iteration 16084, loss = 2.49244840\n",
      "Iteration 16085, loss = 2.49247268\n",
      "Iteration 16086, loss = 2.49239451\n",
      "Iteration 16087, loss = 2.49228159\n",
      "Iteration 16088, loss = 2.49226561\n",
      "Iteration 16089, loss = 2.49229075\n",
      "Iteration 16090, loss = 2.49233729\n",
      "Iteration 16091, loss = 2.49238568\n",
      "Iteration 16092, loss = 2.49230963\n",
      "Iteration 16093, loss = 2.49237823\n",
      "Iteration 16094, loss = 2.49222598\n",
      "Iteration 16095, loss = 2.49214881\n",
      "Iteration 16096, loss = 2.49215478\n",
      "Iteration 16097, loss = 2.49212628\n",
      "Iteration 16098, loss = 2.49214417\n",
      "Iteration 16099, loss = 2.49217223\n",
      "Iteration 16100, loss = 2.49221791\n",
      "Iteration 16101, loss = 2.49222589\n",
      "Iteration 16102, loss = 2.49209041\n",
      "Iteration 16103, loss = 2.49260050\n",
      "Iteration 16104, loss = 2.49212415\n",
      "Iteration 16105, loss = 2.49207777\n",
      "Iteration 16106, loss = 2.49228200\n",
      "Iteration 16107, loss = 2.49216474\n",
      "Iteration 16108, loss = 2.49225416\n",
      "Iteration 16109, loss = 2.49192505\n",
      "Iteration 16110, loss = 2.49197677\n",
      "Iteration 16111, loss = 2.49194277\n",
      "Iteration 16112, loss = 2.49228461\n",
      "Iteration 16113, loss = 2.49214541\n",
      "Iteration 16114, loss = 2.49192552\n",
      "Iteration 16115, loss = 2.49186421\n",
      "Iteration 16116, loss = 2.49179374\n",
      "Iteration 16117, loss = 2.49184734\n",
      "Iteration 16118, loss = 2.49183701\n",
      "Iteration 16119, loss = 2.49188338\n",
      "Iteration 16120, loss = 2.49178634\n",
      "Iteration 16121, loss = 2.49185570\n",
      "Iteration 16122, loss = 2.49194046\n",
      "Iteration 16123, loss = 2.49175107\n",
      "Iteration 16124, loss = 2.49191704\n",
      "Iteration 16125, loss = 2.49183283\n",
      "Iteration 16126, loss = 2.49189407\n",
      "Iteration 16127, loss = 2.49166616\n",
      "Iteration 16128, loss = 2.49175820\n",
      "Iteration 16129, loss = 2.49183644\n",
      "Iteration 16130, loss = 2.49174812\n",
      "Iteration 16131, loss = 2.49179879\n",
      "Iteration 16132, loss = 2.49180818\n",
      "Iteration 16133, loss = 2.49187201\n",
      "Iteration 16134, loss = 2.49162860\n",
      "Iteration 16135, loss = 2.49166209\n",
      "Iteration 16136, loss = 2.49155461\n",
      "Iteration 16137, loss = 2.49151996\n",
      "Iteration 16138, loss = 2.49160507\n",
      "Iteration 16139, loss = 2.49155877\n",
      "Iteration 16140, loss = 2.49154483\n",
      "Iteration 16141, loss = 2.49156159\n",
      "Iteration 16142, loss = 2.49159470\n",
      "Iteration 16143, loss = 2.49145466\n",
      "Iteration 16144, loss = 2.49144338\n",
      "Iteration 16145, loss = 2.49153540\n",
      "Iteration 16146, loss = 2.49144061\n",
      "Iteration 16147, loss = 2.49148054\n",
      "Iteration 16148, loss = 2.49131518\n",
      "Iteration 16149, loss = 2.49141860\n",
      "Iteration 16150, loss = 2.49142685\n",
      "Iteration 16151, loss = 2.49155999\n",
      "Iteration 16152, loss = 2.49143120\n",
      "Iteration 16153, loss = 2.49132484\n",
      "Iteration 16154, loss = 2.49137387\n",
      "Iteration 16155, loss = 2.49147941\n",
      "Iteration 16156, loss = 2.49128526\n",
      "Iteration 16157, loss = 2.49122862\n",
      "Iteration 16158, loss = 2.49118982\n",
      "Iteration 16159, loss = 2.49118274\n",
      "Iteration 16160, loss = 2.49128728\n",
      "Iteration 16161, loss = 2.49128249\n",
      "Iteration 16162, loss = 2.49154655\n",
      "Iteration 16163, loss = 2.49105253\n",
      "Iteration 16164, loss = 2.49116398\n",
      "Iteration 16165, loss = 2.49112919\n",
      "Iteration 16166, loss = 2.49143276\n",
      "Iteration 16167, loss = 2.49121863\n",
      "Iteration 16168, loss = 2.49104487\n",
      "Iteration 16169, loss = 2.49104658\n",
      "Iteration 16170, loss = 2.49115997\n",
      "Iteration 16171, loss = 2.49110940\n",
      "Iteration 16172, loss = 2.49092200\n",
      "Iteration 16173, loss = 2.49099382\n",
      "Iteration 16174, loss = 2.49101774\n",
      "Iteration 16175, loss = 2.49094775\n",
      "Iteration 16176, loss = 2.49087950\n",
      "Iteration 16177, loss = 2.49091284\n",
      "Iteration 16178, loss = 2.49096564\n",
      "Iteration 16179, loss = 2.49101853\n",
      "Iteration 16180, loss = 2.49089030\n",
      "Iteration 16181, loss = 2.49098639\n",
      "Iteration 16182, loss = 2.49086279\n",
      "Iteration 16183, loss = 2.49091008\n",
      "Iteration 16184, loss = 2.49075885\n",
      "Iteration 16185, loss = 2.49089167\n",
      "Iteration 16186, loss = 2.49073162\n",
      "Iteration 16187, loss = 2.49079378\n",
      "Iteration 16188, loss = 2.49078865\n",
      "Iteration 16189, loss = 2.49076045\n",
      "Iteration 16190, loss = 2.49062955\n",
      "Iteration 16191, loss = 2.49072346\n",
      "Iteration 16192, loss = 2.49068033\n",
      "Iteration 16193, loss = 2.49069097\n",
      "Iteration 16194, loss = 2.49055592\n",
      "Iteration 16195, loss = 2.49072067\n",
      "Iteration 16196, loss = 2.49061401\n",
      "Iteration 16197, loss = 2.49063705\n",
      "Iteration 16198, loss = 2.49058386\n",
      "Iteration 16199, loss = 2.49062366\n",
      "Iteration 16200, loss = 2.49062549\n",
      "Iteration 16201, loss = 2.49055239\n",
      "Iteration 16202, loss = 2.49051615\n",
      "Iteration 16203, loss = 2.49055616\n",
      "Iteration 16204, loss = 2.49101557\n",
      "Iteration 16205, loss = 2.49041657\n",
      "Iteration 16206, loss = 2.49048111\n",
      "Iteration 16207, loss = 2.49060115\n",
      "Iteration 16208, loss = 2.49045146\n",
      "Iteration 16209, loss = 2.49056426\n",
      "Iteration 16210, loss = 2.49039941\n",
      "Iteration 16211, loss = 2.49041022\n",
      "Iteration 16212, loss = 2.49034522\n",
      "Iteration 16213, loss = 2.49032148\n",
      "Iteration 16214, loss = 2.49048988\n",
      "Iteration 16215, loss = 2.49045438\n",
      "Iteration 16216, loss = 2.49028699\n",
      "Iteration 16217, loss = 2.49028319\n",
      "Iteration 16218, loss = 2.49030887\n",
      "Iteration 16219, loss = 2.49028579\n",
      "Iteration 16220, loss = 2.49022073\n",
      "Iteration 16221, loss = 2.49024749\n",
      "Iteration 16222, loss = 2.49015072\n",
      "Iteration 16223, loss = 2.49041465\n",
      "Iteration 16224, loss = 2.49021789\n",
      "Iteration 16225, loss = 2.49031724\n",
      "Iteration 16226, loss = 2.49059142\n",
      "Iteration 16227, loss = 2.49012887\n",
      "Iteration 16228, loss = 2.49011111\n",
      "Iteration 16229, loss = 2.49007316\n",
      "Iteration 16230, loss = 2.49014621\n",
      "Iteration 16231, loss = 2.49009157\n",
      "Iteration 16232, loss = 2.49015812\n",
      "Iteration 16233, loss = 2.49003658\n",
      "Iteration 16234, loss = 2.49015485\n",
      "Iteration 16235, loss = 2.49013530\n",
      "Iteration 16236, loss = 2.48997717\n",
      "Iteration 16237, loss = 2.48999304\n",
      "Iteration 16238, loss = 2.48997585\n",
      "Iteration 16239, loss = 2.49029343\n",
      "Iteration 16240, loss = 2.48989430\n",
      "Iteration 16241, loss = 2.48996628\n",
      "Iteration 16242, loss = 2.48998604\n",
      "Iteration 16243, loss = 2.48998326\n",
      "Iteration 16244, loss = 2.48978513\n",
      "Iteration 16245, loss = 2.49006304\n",
      "Iteration 16246, loss = 2.49000776\n",
      "Iteration 16247, loss = 2.48983984\n",
      "Iteration 16248, loss = 2.48983178\n",
      "Iteration 16249, loss = 2.48975488\n",
      "Iteration 16250, loss = 2.48990373\n",
      "Iteration 16251, loss = 2.48983379\n",
      "Iteration 16252, loss = 2.48977499\n",
      "Iteration 16253, loss = 2.48984147\n",
      "Iteration 16254, loss = 2.48975992\n",
      "Iteration 16255, loss = 2.48976618\n",
      "Iteration 16256, loss = 2.48962470\n",
      "Iteration 16257, loss = 2.48967711\n",
      "Iteration 16258, loss = 2.48963692\n",
      "Iteration 16259, loss = 2.48977716\n",
      "Iteration 16260, loss = 2.48964490\n",
      "Iteration 16261, loss = 2.48965485\n",
      "Iteration 16262, loss = 2.48966538\n",
      "Iteration 16263, loss = 2.48958084\n",
      "Iteration 16264, loss = 2.48963196\n",
      "Iteration 16265, loss = 2.48954019\n",
      "Iteration 16266, loss = 2.48959829\n",
      "Iteration 16267, loss = 2.48958731\n",
      "Iteration 16268, loss = 2.48946491\n",
      "Iteration 16269, loss = 2.48947499\n",
      "Iteration 16270, loss = 2.48951422\n",
      "Iteration 16271, loss = 2.48939485\n",
      "Iteration 16272, loss = 2.48965949\n",
      "Iteration 16273, loss = 2.48964733\n",
      "Iteration 16274, loss = 2.48934931\n",
      "Iteration 16275, loss = 2.48968810\n",
      "Iteration 16276, loss = 2.48942964\n",
      "Iteration 16277, loss = 2.48948561\n",
      "Iteration 16278, loss = 2.48936354\n",
      "Iteration 16279, loss = 2.48957626\n",
      "Iteration 16280, loss = 2.48948262\n",
      "Iteration 16281, loss = 2.48928239\n",
      "Iteration 16282, loss = 2.48922241\n",
      "Iteration 16283, loss = 2.48922224\n",
      "Iteration 16284, loss = 2.48937565\n",
      "Iteration 16285, loss = 2.48933908\n",
      "Iteration 16286, loss = 2.48948975\n",
      "Iteration 16287, loss = 2.48909866\n",
      "Iteration 16288, loss = 2.48924229\n",
      "Iteration 16289, loss = 2.48913691\n",
      "Iteration 16290, loss = 2.48918959\n",
      "Iteration 16291, loss = 2.48921683\n",
      "Iteration 16292, loss = 2.48907821\n",
      "Iteration 16293, loss = 2.48921030\n",
      "Iteration 16294, loss = 2.48924525\n",
      "Iteration 16295, loss = 2.48909862\n",
      "Iteration 16296, loss = 2.48905935\n",
      "Iteration 16297, loss = 2.48897914\n",
      "Iteration 16298, loss = 2.48906805\n",
      "Iteration 16299, loss = 2.48905775\n",
      "Iteration 16300, loss = 2.48935682\n",
      "Iteration 16301, loss = 2.48907600\n",
      "Iteration 16302, loss = 2.48914858\n",
      "Iteration 16303, loss = 2.48891153\n",
      "Iteration 16304, loss = 2.48914103\n",
      "Iteration 16305, loss = 2.48890107\n",
      "Iteration 16306, loss = 2.48903367\n",
      "Iteration 16307, loss = 2.48898263\n",
      "Iteration 16308, loss = 2.48888317\n",
      "Iteration 16309, loss = 2.48903742\n",
      "Iteration 16310, loss = 2.48883873\n",
      "Iteration 16311, loss = 2.48876134\n",
      "Iteration 16312, loss = 2.48882021\n",
      "Iteration 16313, loss = 2.48892396\n",
      "Iteration 16314, loss = 2.48879967\n",
      "Iteration 16315, loss = 2.48871638\n",
      "Iteration 16316, loss = 2.48874410\n",
      "Iteration 16317, loss = 2.48871184\n",
      "Iteration 16318, loss = 2.48867850\n",
      "Iteration 16319, loss = 2.48873873\n",
      "Iteration 16320, loss = 2.48868427\n",
      "Iteration 16321, loss = 2.48878303\n",
      "Iteration 16322, loss = 2.48895157\n",
      "Iteration 16323, loss = 2.48874429\n",
      "Iteration 16324, loss = 2.48854324\n",
      "Iteration 16325, loss = 2.48860369\n",
      "Iteration 16326, loss = 2.48876450\n",
      "Iteration 16327, loss = 2.48867759\n",
      "Iteration 16328, loss = 2.48860207\n",
      "Iteration 16329, loss = 2.48869819\n",
      "Iteration 16330, loss = 2.48866767\n",
      "Iteration 16331, loss = 2.48857783\n",
      "Iteration 16332, loss = 2.48855927\n",
      "Iteration 16333, loss = 2.48856281\n",
      "Iteration 16334, loss = 2.48858903\n",
      "Iteration 16335, loss = 2.48856894\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 28.68697098\n",
      "Iteration 2, loss = 3.65104800\n",
      "Iteration 3, loss = 3.65146101\n",
      "Iteration 4, loss = 3.65117247\n",
      "Iteration 5, loss = 3.65082931\n",
      "Iteration 6, loss = 3.65046641\n",
      "Iteration 7, loss = 3.65011989\n",
      "Iteration 8, loss = 3.64974066\n",
      "Iteration 9, loss = 3.64937484\n",
      "Iteration 10, loss = 3.64897046\n",
      "Iteration 11, loss = 3.64863779\n",
      "Iteration 12, loss = 3.64827436\n",
      "Iteration 13, loss = 3.64791622\n",
      "Iteration 14, loss = 3.64757982\n",
      "Iteration 15, loss = 3.64727573\n",
      "Iteration 16, loss = 3.64695908\n",
      "Iteration 17, loss = 3.64665582\n",
      "Iteration 18, loss = 3.64629710\n",
      "Iteration 19, loss = 3.64598412\n",
      "Iteration 20, loss = 3.64568903\n",
      "Iteration 21, loss = 3.64542048\n",
      "Iteration 22, loss = 3.64509207\n",
      "Iteration 23, loss = 3.64478725\n",
      "Iteration 24, loss = 3.64455412\n",
      "Iteration 25, loss = 3.64426542\n",
      "Iteration 26, loss = 3.64399398\n",
      "Iteration 27, loss = 3.64376192\n",
      "Iteration 28, loss = 3.64349657\n",
      "Iteration 29, loss = 3.64327144\n",
      "Iteration 30, loss = 3.64304328\n",
      "Iteration 31, loss = 3.64279363\n",
      "Iteration 32, loss = 3.64256903\n",
      "Iteration 33, loss = 3.64237431\n",
      "Iteration 34, loss = 3.64219832\n",
      "Iteration 35, loss = 3.64198695\n",
      "Iteration 36, loss = 3.64177919\n",
      "Iteration 37, loss = 3.64158141\n",
      "Iteration 38, loss = 3.64142929\n",
      "Iteration 39, loss = 3.64124767\n",
      "Iteration 40, loss = 3.64106556\n",
      "Iteration 41, loss = 3.64088423\n",
      "Iteration 42, loss = 3.64071705\n",
      "Iteration 43, loss = 3.64054985\n",
      "Iteration 44, loss = 3.64040463\n",
      "Iteration 45, loss = 3.64023363\n",
      "Iteration 46, loss = 3.64009610\n",
      "Iteration 47, loss = 3.63993789\n",
      "Iteration 48, loss = 3.63980080\n",
      "Iteration 49, loss = 3.63965282\n",
      "Iteration 50, loss = 3.63950761\n",
      "Iteration 51, loss = 3.63940709\n",
      "Iteration 52, loss = 3.63926064\n",
      "Iteration 53, loss = 3.63917189\n",
      "Iteration 54, loss = 3.63902365\n",
      "Iteration 55, loss = 3.63892173\n",
      "Iteration 56, loss = 3.63881848\n",
      "Iteration 57, loss = 3.63869244\n",
      "Iteration 58, loss = 3.63858129\n",
      "Iteration 59, loss = 3.63847176\n",
      "Iteration 60, loss = 3.63835050\n",
      "Iteration 61, loss = 3.63825157\n",
      "Iteration 62, loss = 3.63817263\n",
      "Iteration 63, loss = 3.63804323\n",
      "Iteration 64, loss = 3.63796381\n",
      "Iteration 65, loss = 3.63787138\n",
      "Iteration 66, loss = 3.63777976\n",
      "Iteration 67, loss = 3.63771505\n",
      "Iteration 68, loss = 3.63762550\n",
      "Iteration 69, loss = 3.63756165\n",
      "Iteration 70, loss = 3.63749017\n",
      "Iteration 71, loss = 3.63741123\n",
      "Iteration 72, loss = 3.63733395\n",
      "Iteration 73, loss = 3.63726831\n",
      "Iteration 74, loss = 3.63718723\n",
      "Iteration 75, loss = 3.63713907\n",
      "Iteration 76, loss = 3.63706681\n",
      "Iteration 77, loss = 3.63698587\n",
      "Iteration 78, loss = 3.63694109\n",
      "Iteration 79, loss = 3.63690928\n",
      "Iteration 80, loss = 3.63683125\n",
      "Iteration 81, loss = 3.63678973\n",
      "Iteration 82, loss = 3.63671195\n",
      "Iteration 83, loss = 3.63668036\n",
      "Iteration 84, loss = 3.63661851\n",
      "Iteration 85, loss = 3.63657852\n",
      "Iteration 86, loss = 3.63653457\n",
      "Iteration 87, loss = 3.63648248\n",
      "Iteration 88, loss = 3.63644625\n",
      "Iteration 89, loss = 3.63639559\n",
      "Iteration 90, loss = 3.63634487\n",
      "Iteration 91, loss = 3.63629678\n",
      "Iteration 92, loss = 3.63625277\n",
      "Iteration 93, loss = 3.63621595\n",
      "Iteration 94, loss = 3.63618279\n",
      "Iteration 95, loss = 3.63613446\n",
      "Iteration 96, loss = 3.63611049\n",
      "Iteration 97, loss = 3.63607395\n",
      "Iteration 98, loss = 3.63604255\n",
      "Iteration 99, loss = 3.63600892\n",
      "Iteration 100, loss = 3.63596705\n",
      "Iteration 101, loss = 3.63593606\n",
      "Iteration 102, loss = 3.63591096\n",
      "Iteration 103, loss = 3.63588778\n",
      "Iteration 104, loss = 3.63583246\n",
      "Iteration 105, loss = 3.63580307\n",
      "Iteration 106, loss = 3.63577725\n",
      "Iteration 107, loss = 3.63575883\n",
      "Iteration 108, loss = 3.63574924\n",
      "Iteration 109, loss = 3.63568995\n",
      "Iteration 110, loss = 3.63567467\n",
      "Iteration 111, loss = 3.63563723\n",
      "Iteration 112, loss = 3.63560389\n",
      "Iteration 113, loss = 3.63558571\n",
      "Iteration 114, loss = 3.63556401\n",
      "Iteration 115, loss = 3.63552126\n",
      "Iteration 116, loss = 3.63552636\n",
      "Iteration 117, loss = 3.63549599\n",
      "Iteration 118, loss = 3.63546842\n",
      "Iteration 119, loss = 3.63544578\n",
      "Iteration 120, loss = 3.63543606\n",
      "Iteration 121, loss = 3.63539758\n",
      "Iteration 122, loss = 3.63539597\n",
      "Iteration 123, loss = 3.63537128\n",
      "Iteration 124, loss = 3.63535694\n",
      "Iteration 125, loss = 3.63531905\n",
      "Iteration 126, loss = 3.63532372\n",
      "Iteration 127, loss = 3.63529546\n",
      "Iteration 128, loss = 3.63527468\n",
      "Iteration 129, loss = 3.63525949\n",
      "Iteration 130, loss = 3.63524152\n",
      "Iteration 131, loss = 3.63523816\n",
      "Iteration 132, loss = 3.63519362\n",
      "Iteration 133, loss = 3.63519198\n",
      "Iteration 134, loss = 3.63517299\n",
      "Iteration 135, loss = 3.63517443\n",
      "Iteration 136, loss = 3.63515458\n",
      "Iteration 137, loss = 3.63511992\n",
      "Iteration 138, loss = 3.63513340\n",
      "Iteration 139, loss = 3.63510448\n",
      "Iteration 140, loss = 3.63508589\n",
      "Iteration 141, loss = 3.63508370\n",
      "Iteration 142, loss = 3.63507303\n",
      "Iteration 143, loss = 3.63506117\n",
      "Iteration 144, loss = 3.63504762\n",
      "Iteration 145, loss = 3.63504211\n",
      "Iteration 146, loss = 3.63504700\n",
      "Iteration 147, loss = 3.63504908\n",
      "Iteration 148, loss = 3.63501317\n",
      "Iteration 149, loss = 3.63500541\n",
      "Iteration 150, loss = 3.63500354\n",
      "Iteration 151, loss = 3.63499344\n",
      "Iteration 152, loss = 3.63498818\n",
      "Iteration 153, loss = 3.63497733\n",
      "Iteration 154, loss = 3.63497501\n",
      "Iteration 155, loss = 3.63496163\n",
      "Iteration 156, loss = 3.63495444\n",
      "Iteration 157, loss = 3.63495208\n",
      "Iteration 158, loss = 3.63494447\n",
      "Iteration 159, loss = 3.63493974\n",
      "Iteration 160, loss = 3.63490752\n",
      "Iteration 161, loss = 3.63491296\n",
      "Iteration 162, loss = 3.63490372\n",
      "Iteration 163, loss = 3.63490596\n",
      "Iteration 164, loss = 3.63489904\n",
      "Iteration 165, loss = 3.63488237\n",
      "Iteration 166, loss = 3.63489142\n",
      "Iteration 167, loss = 3.63488044\n",
      "Iteration 168, loss = 3.63488576\n",
      "Iteration 169, loss = 3.63487587\n",
      "Iteration 170, loss = 3.63486984\n",
      "Iteration 171, loss = 3.63486758\n",
      "Iteration 172, loss = 3.63486206\n",
      "Iteration 173, loss = 3.63485904\n",
      "Iteration 174, loss = 3.63483849\n",
      "Iteration 175, loss = 3.63483911\n",
      "Iteration 176, loss = 3.63484256\n",
      "Iteration 177, loss = 3.63483395\n",
      "Iteration 178, loss = 3.63483297\n",
      "Iteration 179, loss = 3.63481579\n",
      "Iteration 180, loss = 3.63483458\n",
      "Iteration 181, loss = 3.63479871\n",
      "Iteration 182, loss = 3.63481195\n",
      "Iteration 183, loss = 3.63481675\n",
      "Iteration 184, loss = 3.63478871\n",
      "Iteration 185, loss = 3.63479165\n",
      "Iteration 186, loss = 3.63478086\n",
      "Iteration 187, loss = 3.63478035\n",
      "Iteration 188, loss = 3.63479311\n",
      "Iteration 189, loss = 3.63477082\n",
      "Iteration 190, loss = 3.63476667\n",
      "Iteration 191, loss = 3.63475872\n",
      "Iteration 192, loss = 3.63478153\n",
      "Iteration 193, loss = 3.63475135\n",
      "Iteration 194, loss = 3.63476288\n",
      "Iteration 195, loss = 3.63474529\n",
      "Iteration 196, loss = 3.63474296\n",
      "Iteration 197, loss = 3.63475811\n",
      "Iteration 198, loss = 3.63473239\n",
      "Iteration 199, loss = 3.63472345\n",
      "Iteration 200, loss = 3.63472801\n",
      "Iteration 201, loss = 3.63472146\n",
      "Iteration 202, loss = 3.63472685\n",
      "Iteration 203, loss = 3.63471058\n",
      "Iteration 204, loss = 3.63471589\n",
      "Iteration 205, loss = 3.63470559\n",
      "Iteration 206, loss = 3.63470218\n",
      "Iteration 207, loss = 3.63470317\n",
      "Iteration 208, loss = 3.63471469\n",
      "Iteration 209, loss = 3.63470662\n",
      "Iteration 210, loss = 3.63469704\n",
      "Iteration 211, loss = 3.63472098\n",
      "Iteration 212, loss = 3.63472271\n",
      "Iteration 213, loss = 3.63470034\n",
      "Iteration 214, loss = 3.63467919\n",
      "Iteration 215, loss = 3.63468533\n",
      "Iteration 216, loss = 3.63469877\n",
      "Iteration 217, loss = 3.63470126\n",
      "Iteration 218, loss = 3.63470014\n",
      "Iteration 219, loss = 3.63471159\n",
      "Iteration 220, loss = 3.63468347\n",
      "Iteration 221, loss = 3.63468564\n",
      "Iteration 222, loss = 3.63468781\n",
      "Iteration 223, loss = 3.63469421\n",
      "Iteration 224, loss = 3.63469171\n",
      "Iteration 225, loss = 3.63467608\n",
      "Iteration 226, loss = 3.63467865\n",
      "Iteration 227, loss = 3.63468322\n",
      "Iteration 228, loss = 3.63469242\n",
      "Iteration 229, loss = 3.63467292\n",
      "Iteration 230, loss = 3.63467845\n",
      "Iteration 231, loss = 3.63467240\n",
      "Iteration 232, loss = 3.63467620\n",
      "Iteration 233, loss = 3.63469693\n",
      "Iteration 234, loss = 3.63466817\n",
      "Iteration 235, loss = 3.63467141\n",
      "Iteration 236, loss = 3.63465920\n",
      "Iteration 237, loss = 3.63467121\n",
      "Iteration 238, loss = 3.63468678\n",
      "Iteration 239, loss = 3.63466499\n",
      "Iteration 240, loss = 3.63467113\n",
      "Iteration 241, loss = 3.63466484\n",
      "Iteration 242, loss = 3.63468384\n",
      "Iteration 243, loss = 3.63466901\n",
      "Iteration 244, loss = 3.63467161\n",
      "Iteration 245, loss = 3.63466108\n",
      "Iteration 246, loss = 3.63466786\n",
      "Iteration 247, loss = 3.63466340\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.30213838\n",
      "Iteration 2, loss = 8.65840505\n",
      "Iteration 3, loss = 7.27211305\n",
      "Iteration 4, loss = 6.19432501\n",
      "Iteration 5, loss = 5.46126870\n",
      "Iteration 6, loss = 4.93977469\n",
      "Iteration 7, loss = 4.51673669\n",
      "Iteration 8, loss = 4.16774139\n",
      "Iteration 9, loss = 3.90167894\n",
      "Iteration 10, loss = 3.68973235\n",
      "Iteration 11, loss = 3.52827169\n",
      "Iteration 12, loss = 3.39318827\n",
      "Iteration 13, loss = 3.28657490\n",
      "Iteration 14, loss = 3.20052980\n",
      "Iteration 15, loss = 3.12991408\n",
      "Iteration 16, loss = 3.07389000\n",
      "Iteration 17, loss = 3.02650779\n",
      "Iteration 18, loss = 2.98329977\n",
      "Iteration 19, loss = 2.95108194\n",
      "Iteration 20, loss = 2.92082829\n",
      "Iteration 21, loss = 2.89458077\n",
      "Iteration 22, loss = 2.87438669\n",
      "Iteration 23, loss = 2.85442872\n",
      "Iteration 24, loss = 2.83747728\n",
      "Iteration 25, loss = 2.82539982\n",
      "Iteration 26, loss = 2.81489052\n",
      "Iteration 27, loss = 2.80316806\n",
      "Iteration 28, loss = 2.79607381\n",
      "Iteration 29, loss = 2.78828719\n",
      "Iteration 30, loss = 2.78125421\n",
      "Iteration 31, loss = 2.77343311\n",
      "Iteration 32, loss = 2.76964678\n",
      "Iteration 33, loss = 2.76334575\n",
      "Iteration 34, loss = 2.75889484\n",
      "Iteration 35, loss = 2.75385355\n",
      "Iteration 36, loss = 2.75105113\n",
      "Iteration 37, loss = 2.74645610\n",
      "Iteration 38, loss = 2.74408126\n",
      "Iteration 39, loss = 2.74020254\n",
      "Iteration 40, loss = 2.73450196\n",
      "Iteration 41, loss = 2.73332649\n",
      "Iteration 42, loss = 2.73070493\n",
      "Iteration 43, loss = 2.72498098\n",
      "Iteration 44, loss = 2.72278566\n",
      "Iteration 45, loss = 2.72101392\n",
      "Iteration 46, loss = 2.71546239\n",
      "Iteration 47, loss = 2.71335477\n",
      "Iteration 48, loss = 2.71194510\n",
      "Iteration 49, loss = 2.70873403\n",
      "Iteration 50, loss = 2.70801019\n",
      "Iteration 51, loss = 2.70802280\n",
      "Iteration 52, loss = 2.70255778\n",
      "Iteration 53, loss = 2.70180097\n",
      "Iteration 54, loss = 2.69916389\n",
      "Iteration 55, loss = 2.69641099\n",
      "Iteration 56, loss = 2.69425869\n",
      "Iteration 57, loss = 2.69376455\n",
      "Iteration 58, loss = 2.69038098\n",
      "Iteration 59, loss = 2.69062977\n",
      "Iteration 60, loss = 2.68779650\n",
      "Iteration 61, loss = 2.68504940\n",
      "Iteration 62, loss = 2.68361452\n",
      "Iteration 63, loss = 2.68242117\n",
      "Iteration 64, loss = 2.68250763\n",
      "Iteration 65, loss = 2.67988236\n",
      "Iteration 66, loss = 2.67807572\n",
      "Iteration 67, loss = 2.67698785\n",
      "Iteration 68, loss = 2.67490016\n",
      "Iteration 69, loss = 2.67122831\n",
      "Iteration 70, loss = 2.67246705\n",
      "Iteration 71, loss = 2.67054869\n",
      "Iteration 72, loss = 2.66916895\n",
      "Iteration 73, loss = 2.66994270\n",
      "Iteration 74, loss = 2.66672907\n",
      "Iteration 75, loss = 2.66547413\n",
      "Iteration 76, loss = 2.66402103\n",
      "Iteration 77, loss = 2.66389873\n",
      "Iteration 78, loss = 2.66162852\n",
      "Iteration 79, loss = 2.65938137\n",
      "Iteration 80, loss = 2.65958337\n",
      "Iteration 81, loss = 2.65766601\n",
      "Iteration 82, loss = 2.65597621\n",
      "Iteration 83, loss = 2.65507697\n",
      "Iteration 84, loss = 2.65300450\n",
      "Iteration 85, loss = 2.65207204\n",
      "Iteration 86, loss = 2.65193677\n",
      "Iteration 87, loss = 2.64741170\n",
      "Iteration 88, loss = 2.65080024\n",
      "Iteration 89, loss = 2.64705236\n",
      "Iteration 90, loss = 2.64728371\n",
      "Iteration 91, loss = 2.64762548\n",
      "Iteration 92, loss = 2.64548782\n",
      "Iteration 93, loss = 2.64240496\n",
      "Iteration 94, loss = 2.64176875\n",
      "Iteration 95, loss = 2.64150849\n",
      "Iteration 96, loss = 2.63997416\n",
      "Iteration 97, loss = 2.63823130\n",
      "Iteration 98, loss = 2.63878674\n",
      "Iteration 99, loss = 2.63744675\n",
      "Iteration 100, loss = 2.63605498\n",
      "Iteration 101, loss = 2.63401340\n",
      "Iteration 102, loss = 2.63504092\n",
      "Iteration 103, loss = 2.63157768\n",
      "Iteration 104, loss = 2.63319654\n",
      "Iteration 105, loss = 2.63015518\n",
      "Iteration 106, loss = 2.63008858\n",
      "Iteration 107, loss = 2.62880296\n",
      "Iteration 108, loss = 2.63099159\n",
      "Iteration 109, loss = 2.62625222\n",
      "Iteration 110, loss = 2.62632714\n",
      "Iteration 111, loss = 2.62703646\n",
      "Iteration 112, loss = 2.62758599\n",
      "Iteration 113, loss = 2.62400179\n",
      "Iteration 114, loss = 2.62390610\n",
      "Iteration 115, loss = 2.62149849\n",
      "Iteration 116, loss = 2.62276413\n",
      "Iteration 117, loss = 2.62090551\n",
      "Iteration 118, loss = 2.61873217\n",
      "Iteration 119, loss = 2.61963658\n",
      "Iteration 120, loss = 2.61758210\n",
      "Iteration 121, loss = 2.61479757\n",
      "Iteration 122, loss = 2.61930807\n",
      "Iteration 123, loss = 2.61548975\n",
      "Iteration 124, loss = 2.61296278\n",
      "Iteration 125, loss = 2.61109606\n",
      "Iteration 126, loss = 2.61179824\n",
      "Iteration 127, loss = 2.60953535\n",
      "Iteration 128, loss = 2.60896636\n",
      "Iteration 129, loss = 2.61004640\n",
      "Iteration 130, loss = 2.60883133\n",
      "Iteration 131, loss = 2.60832551\n",
      "Iteration 132, loss = 2.60869653\n",
      "Iteration 133, loss = 2.60688785\n",
      "Iteration 134, loss = 2.60740047\n",
      "Iteration 135, loss = 2.60521814\n",
      "Iteration 136, loss = 2.60532919\n",
      "Iteration 137, loss = 2.60350573\n",
      "Iteration 138, loss = 2.60211330\n",
      "Iteration 139, loss = 2.60242967\n",
      "Iteration 140, loss = 2.60240818\n",
      "Iteration 141, loss = 2.59870416\n",
      "Iteration 142, loss = 2.59951440\n",
      "Iteration 143, loss = 2.59710405\n",
      "Iteration 144, loss = 2.59742472\n",
      "Iteration 145, loss = 2.59611578\n",
      "Iteration 146, loss = 2.59711949\n",
      "Iteration 147, loss = 2.59685284\n",
      "Iteration 148, loss = 2.59347066\n",
      "Iteration 149, loss = 2.59257302\n",
      "Iteration 150, loss = 2.59272683\n",
      "Iteration 151, loss = 2.59228695\n",
      "Iteration 152, loss = 2.59374655\n",
      "Iteration 153, loss = 2.58819526\n",
      "Iteration 154, loss = 2.59271609\n",
      "Iteration 155, loss = 2.59324865\n",
      "Iteration 156, loss = 2.59086188\n",
      "Iteration 157, loss = 2.58740519\n",
      "Iteration 158, loss = 2.58735789\n",
      "Iteration 159, loss = 2.58739770\n",
      "Iteration 160, loss = 2.59028274\n",
      "Iteration 161, loss = 2.58551460\n",
      "Iteration 162, loss = 2.58789335\n",
      "Iteration 163, loss = 2.58387110\n",
      "Iteration 164, loss = 2.58593864\n",
      "Iteration 165, loss = 2.58340005\n",
      "Iteration 166, loss = 2.58242671\n",
      "Iteration 167, loss = 2.58381652\n",
      "Iteration 168, loss = 2.58433692\n",
      "Iteration 169, loss = 2.58178991\n",
      "Iteration 170, loss = 2.58141633\n",
      "Iteration 171, loss = 2.57977526\n",
      "Iteration 172, loss = 2.57913107\n",
      "Iteration 173, loss = 2.58065669\n",
      "Iteration 174, loss = 2.57761744\n",
      "Iteration 175, loss = 2.58210956\n",
      "Iteration 176, loss = 2.57737061\n",
      "Iteration 177, loss = 2.57579043\n",
      "Iteration 178, loss = 2.57341693\n",
      "Iteration 179, loss = 2.57417804\n",
      "Iteration 180, loss = 2.57319773\n",
      "Iteration 181, loss = 2.57213906\n",
      "Iteration 182, loss = 2.57133238\n",
      "Iteration 183, loss = 2.57159838\n",
      "Iteration 184, loss = 2.57027564\n",
      "Iteration 185, loss = 2.57525277\n",
      "Iteration 186, loss = 2.57043786\n",
      "Iteration 187, loss = 2.56959630\n",
      "Iteration 188, loss = 2.56854379\n",
      "Iteration 189, loss = 2.56939935\n",
      "Iteration 190, loss = 2.57196213\n",
      "Iteration 191, loss = 2.56728826\n",
      "Iteration 192, loss = 2.56611710\n",
      "Iteration 193, loss = 2.56667538\n",
      "Iteration 194, loss = 2.56382004\n",
      "Iteration 195, loss = 2.56595590\n",
      "Iteration 196, loss = 2.56460825\n",
      "Iteration 197, loss = 2.56370503\n",
      "Iteration 198, loss = 2.56331006\n",
      "Iteration 199, loss = 2.56300019\n",
      "Iteration 200, loss = 2.56215220\n",
      "Iteration 201, loss = 2.56055956\n",
      "Iteration 202, loss = 2.56106172\n",
      "Iteration 203, loss = 2.55782134\n",
      "Iteration 204, loss = 2.55926605\n",
      "Iteration 205, loss = 2.55897856\n",
      "Iteration 206, loss = 2.55875256\n",
      "Iteration 207, loss = 2.55870495\n",
      "Iteration 208, loss = 2.55371057\n",
      "Iteration 209, loss = 2.55491828\n",
      "Iteration 210, loss = 2.55444524\n",
      "Iteration 211, loss = 2.55384742\n",
      "Iteration 212, loss = 2.55683115\n",
      "Iteration 213, loss = 2.55473037\n",
      "Iteration 214, loss = 2.55455556\n",
      "Iteration 215, loss = 2.54958862\n",
      "Iteration 216, loss = 2.55336936\n",
      "Iteration 217, loss = 2.54947247\n",
      "Iteration 218, loss = 2.55038735\n",
      "Iteration 219, loss = 2.55052685\n",
      "Iteration 220, loss = 2.55089142\n",
      "Iteration 221, loss = 2.54911690\n",
      "Iteration 222, loss = 2.55106336\n",
      "Iteration 223, loss = 2.54742226\n",
      "Iteration 224, loss = 2.54749139\n",
      "Iteration 225, loss = 2.54585967\n",
      "Iteration 226, loss = 2.54577231\n",
      "Iteration 227, loss = 2.54389243\n",
      "Iteration 228, loss = 2.54293510\n",
      "Iteration 229, loss = 2.54351363\n",
      "Iteration 230, loss = 2.54086010\n",
      "Iteration 231, loss = 2.54319340\n",
      "Iteration 232, loss = 2.54395376\n",
      "Iteration 233, loss = 2.54072444\n",
      "Iteration 234, loss = 2.54342906\n",
      "Iteration 235, loss = 2.54285581\n",
      "Iteration 236, loss = 2.54148809\n",
      "Iteration 237, loss = 2.54093368\n",
      "Iteration 238, loss = 2.53922657\n",
      "Iteration 239, loss = 2.53808653\n",
      "Iteration 240, loss = 2.53835968\n",
      "Iteration 241, loss = 2.53679376\n",
      "Iteration 242, loss = 2.53791375\n",
      "Iteration 243, loss = 2.53567107\n",
      "Iteration 244, loss = 2.53505390\n",
      "Iteration 245, loss = 2.53759694\n",
      "Iteration 246, loss = 2.53344725\n",
      "Iteration 247, loss = 2.53352514\n",
      "Iteration 248, loss = 2.53625036\n",
      "Iteration 249, loss = 2.53276359\n",
      "Iteration 250, loss = 2.53336242\n",
      "Iteration 251, loss = 2.53337180\n",
      "Iteration 252, loss = 2.53405956\n",
      "Iteration 253, loss = 2.53084663\n",
      "Iteration 254, loss = 2.53012286\n",
      "Iteration 255, loss = 2.53015151\n",
      "Iteration 256, loss = 2.52850622\n",
      "Iteration 257, loss = 2.52915395\n",
      "Iteration 258, loss = 2.52981449\n",
      "Iteration 259, loss = 2.52701793\n",
      "Iteration 260, loss = 2.52856039\n",
      "Iteration 261, loss = 2.52682354\n",
      "Iteration 262, loss = 2.52629540\n",
      "Iteration 263, loss = 2.52682335\n",
      "Iteration 264, loss = 2.52366760\n",
      "Iteration 265, loss = 2.52672857\n",
      "Iteration 266, loss = 2.52685388\n",
      "Iteration 267, loss = 2.52501386\n",
      "Iteration 268, loss = 2.52391751\n",
      "Iteration 269, loss = 2.52426671\n",
      "Iteration 270, loss = 2.52517826\n",
      "Iteration 271, loss = 2.52312901\n",
      "Iteration 272, loss = 2.51940535\n",
      "Iteration 273, loss = 2.51802804\n",
      "Iteration 274, loss = 2.52172093\n",
      "Iteration 275, loss = 2.51854992\n",
      "Iteration 276, loss = 2.51852836\n",
      "Iteration 277, loss = 2.51801631\n",
      "Iteration 278, loss = 2.51566279\n",
      "Iteration 279, loss = 2.51778198\n",
      "Iteration 280, loss = 2.52078752\n",
      "Iteration 281, loss = 2.51764127\n",
      "Iteration 282, loss = 2.51703270\n",
      "Iteration 283, loss = 2.51367163\n",
      "Iteration 284, loss = 2.51516048\n",
      "Iteration 285, loss = 2.51666117\n",
      "Iteration 286, loss = 2.51349024\n",
      "Iteration 287, loss = 2.51623930\n",
      "Iteration 288, loss = 2.51290823\n",
      "Iteration 289, loss = 2.51443901\n",
      "Iteration 290, loss = 2.50954313\n",
      "Iteration 291, loss = 2.51327614\n",
      "Iteration 292, loss = 2.51349393\n",
      "Iteration 293, loss = 2.51370768\n",
      "Iteration 294, loss = 2.51317077\n",
      "Iteration 295, loss = 2.51006655\n",
      "Iteration 296, loss = 2.50847184\n",
      "Iteration 297, loss = 2.50879445\n",
      "Iteration 298, loss = 2.50711624\n",
      "Iteration 299, loss = 2.50729814\n",
      "Iteration 300, loss = 2.50852216\n",
      "Iteration 301, loss = 2.50786059\n",
      "Iteration 302, loss = 2.50602864\n",
      "Iteration 303, loss = 2.50767563\n",
      "Iteration 304, loss = 2.50850815\n",
      "Iteration 305, loss = 2.50475893\n",
      "Iteration 306, loss = 2.50530123\n",
      "Iteration 307, loss = 2.50362228\n",
      "Iteration 308, loss = 2.50484978\n",
      "Iteration 309, loss = 2.50051943\n",
      "Iteration 310, loss = 2.50359674\n",
      "Iteration 311, loss = 2.50153575\n",
      "Iteration 312, loss = 2.50344807\n",
      "Iteration 313, loss = 2.50132195\n",
      "Iteration 314, loss = 2.49888301\n",
      "Iteration 315, loss = 2.49870666\n",
      "Iteration 316, loss = 2.49753666\n",
      "Iteration 317, loss = 2.49779577\n",
      "Iteration 318, loss = 2.50007693\n",
      "Iteration 319, loss = 2.49571721\n",
      "Iteration 320, loss = 2.49962644\n",
      "Iteration 321, loss = 2.49730695\n",
      "Iteration 322, loss = 2.49554345\n",
      "Iteration 323, loss = 2.49768797\n",
      "Iteration 324, loss = 2.49391260\n",
      "Iteration 325, loss = 2.49334926\n",
      "Iteration 326, loss = 2.49520085\n",
      "Iteration 327, loss = 2.49443802\n",
      "Iteration 328, loss = 2.49458104\n",
      "Iteration 329, loss = 2.49188021\n",
      "Iteration 330, loss = 2.49166229\n",
      "Iteration 331, loss = 2.49475256\n",
      "Iteration 332, loss = 2.49390353\n",
      "Iteration 333, loss = 2.49023518\n",
      "Iteration 334, loss = 2.49196553\n",
      "Iteration 335, loss = 2.49105112\n",
      "Iteration 336, loss = 2.48648051\n",
      "Iteration 337, loss = 2.49029813\n",
      "Iteration 338, loss = 2.48579500\n",
      "Iteration 339, loss = 2.48697374\n",
      "Iteration 340, loss = 2.48824875\n",
      "Iteration 341, loss = 2.48599218\n",
      "Iteration 342, loss = 2.48648952\n",
      "Iteration 343, loss = 2.48528215\n",
      "Iteration 344, loss = 2.48510025\n",
      "Iteration 345, loss = 2.48281556\n",
      "Iteration 346, loss = 2.48582896\n",
      "Iteration 347, loss = 2.48572819\n",
      "Iteration 348, loss = 2.48256026\n",
      "Iteration 349, loss = 2.48343340\n",
      "Iteration 350, loss = 2.48262435\n",
      "Iteration 351, loss = 2.48106709\n",
      "Iteration 352, loss = 2.48172310\n",
      "Iteration 353, loss = 2.47968925\n",
      "Iteration 354, loss = 2.47991502\n",
      "Iteration 355, loss = 2.47916404\n",
      "Iteration 356, loss = 2.48151244\n",
      "Iteration 357, loss = 2.47726215\n",
      "Iteration 358, loss = 2.48086541\n",
      "Iteration 359, loss = 2.47774068\n",
      "Iteration 360, loss = 2.47793075\n",
      "Iteration 361, loss = 2.47432655\n",
      "Iteration 362, loss = 2.47403282\n",
      "Iteration 363, loss = 2.47662237\n",
      "Iteration 364, loss = 2.47518332\n",
      "Iteration 365, loss = 2.47567379\n",
      "Iteration 366, loss = 2.47357683\n",
      "Iteration 367, loss = 2.47502186\n",
      "Iteration 368, loss = 2.47441830\n",
      "Iteration 369, loss = 2.47354580\n",
      "Iteration 370, loss = 2.46966337\n",
      "Iteration 371, loss = 2.47127872\n",
      "Iteration 372, loss = 2.47005622\n",
      "Iteration 373, loss = 2.46908420\n",
      "Iteration 374, loss = 2.46992264\n",
      "Iteration 375, loss = 2.46932447\n",
      "Iteration 376, loss = 2.46954975\n",
      "Iteration 377, loss = 2.46620100\n",
      "Iteration 378, loss = 2.47132650\n",
      "Iteration 379, loss = 2.46897546\n",
      "Iteration 380, loss = 2.46792201\n",
      "Iteration 381, loss = 2.46892424\n",
      "Iteration 382, loss = 2.46701704\n",
      "Iteration 383, loss = 2.47011497\n",
      "Iteration 384, loss = 2.46436385\n",
      "Iteration 385, loss = 2.46492014\n",
      "Iteration 386, loss = 2.47030093\n",
      "Iteration 387, loss = 2.46470435\n",
      "Iteration 388, loss = 2.46345227\n",
      "Iteration 389, loss = 2.46295537\n",
      "Iteration 390, loss = 2.46339960\n",
      "Iteration 391, loss = 2.46540530\n",
      "Iteration 392, loss = 2.46572479\n",
      "Iteration 393, loss = 2.46218240\n",
      "Iteration 394, loss = 2.46028606\n",
      "Iteration 395, loss = 2.46041445\n",
      "Iteration 396, loss = 2.45926640\n",
      "Iteration 397, loss = 2.46135490\n",
      "Iteration 398, loss = 2.45960584\n",
      "Iteration 399, loss = 2.45855124\n",
      "Iteration 400, loss = 2.45889207\n",
      "Iteration 401, loss = 2.45976194\n",
      "Iteration 402, loss = 2.45540732\n",
      "Iteration 403, loss = 2.45752579\n",
      "Iteration 404, loss = 2.45805134\n",
      "Iteration 405, loss = 2.45515489\n",
      "Iteration 406, loss = 2.45679329\n",
      "Iteration 407, loss = 2.45799344\n",
      "Iteration 408, loss = 2.45605859\n",
      "Iteration 409, loss = 2.45738243\n",
      "Iteration 410, loss = 2.45322181\n",
      "Iteration 411, loss = 2.45588072\n",
      "Iteration 412, loss = 2.45104781\n",
      "Iteration 413, loss = 2.45622149\n",
      "Iteration 414, loss = 2.45095153\n",
      "Iteration 415, loss = 2.45254968\n",
      "Iteration 416, loss = 2.45002119\n",
      "Iteration 417, loss = 2.45366010\n",
      "Iteration 418, loss = 2.45069075\n",
      "Iteration 419, loss = 2.44940138\n",
      "Iteration 420, loss = 2.44956806\n",
      "Iteration 421, loss = 2.45113478\n",
      "Iteration 422, loss = 2.44813133\n",
      "Iteration 423, loss = 2.44863043\n",
      "Iteration 424, loss = 2.44975532\n",
      "Iteration 425, loss = 2.45082569\n",
      "Iteration 426, loss = 2.44697879\n",
      "Iteration 427, loss = 2.44900518\n",
      "Iteration 428, loss = 2.44757200\n",
      "Iteration 429, loss = 2.44374547\n",
      "Iteration 430, loss = 2.44419236\n",
      "Iteration 431, loss = 2.44623346\n",
      "Iteration 432, loss = 2.44408215\n",
      "Iteration 433, loss = 2.44432492\n",
      "Iteration 434, loss = 2.44723440\n",
      "Iteration 435, loss = 2.44517502\n",
      "Iteration 436, loss = 2.44190152\n",
      "Iteration 437, loss = 2.44351078\n",
      "Iteration 438, loss = 2.44211420\n",
      "Iteration 439, loss = 2.44022760\n",
      "Iteration 440, loss = 2.44060695\n",
      "Iteration 441, loss = 2.43965796\n",
      "Iteration 442, loss = 2.43844509\n",
      "Iteration 443, loss = 2.44262362\n",
      "Iteration 444, loss = 2.43907936\n",
      "Iteration 445, loss = 2.44467427\n",
      "Iteration 446, loss = 2.43865253\n",
      "Iteration 447, loss = 2.43925332\n",
      "Iteration 448, loss = 2.43953674\n",
      "Iteration 449, loss = 2.43664115\n",
      "Iteration 450, loss = 2.43959140\n",
      "Iteration 451, loss = 2.43865275\n",
      "Iteration 452, loss = 2.43687834\n",
      "Iteration 453, loss = 2.43447922\n",
      "Iteration 454, loss = 2.43804104\n",
      "Iteration 455, loss = 2.43460571\n",
      "Iteration 456, loss = 2.43470744\n",
      "Iteration 457, loss = 2.43313337\n",
      "Iteration 458, loss = 2.43270086\n",
      "Iteration 459, loss = 2.43004073\n",
      "Iteration 460, loss = 2.43080485\n",
      "Iteration 461, loss = 2.43215650\n",
      "Iteration 462, loss = 2.43312758\n",
      "Iteration 463, loss = 2.42973256\n",
      "Iteration 464, loss = 2.43197010\n",
      "Iteration 465, loss = 2.43375711\n",
      "Iteration 466, loss = 2.43026876\n",
      "Iteration 467, loss = 2.42872910\n",
      "Iteration 468, loss = 2.42770050\n",
      "Iteration 469, loss = 2.42960404\n",
      "Iteration 470, loss = 2.42783695\n",
      "Iteration 471, loss = 2.42810773\n",
      "Iteration 472, loss = 2.42696886\n",
      "Iteration 473, loss = 2.42653545\n",
      "Iteration 474, loss = 2.42375008\n",
      "Iteration 475, loss = 2.42615913\n",
      "Iteration 476, loss = 2.42330157\n",
      "Iteration 477, loss = 2.42676234\n",
      "Iteration 478, loss = 2.42591011\n",
      "Iteration 479, loss = 2.42573161\n",
      "Iteration 480, loss = 2.42130513\n",
      "Iteration 481, loss = 2.42430492\n",
      "Iteration 482, loss = 2.42930935\n",
      "Iteration 483, loss = 2.42414323\n",
      "Iteration 484, loss = 2.42284698\n",
      "Iteration 485, loss = 2.42299866\n",
      "Iteration 486, loss = 2.42047937\n",
      "Iteration 487, loss = 2.41819873\n",
      "Iteration 488, loss = 2.42060057\n",
      "Iteration 489, loss = 2.41880687\n",
      "Iteration 490, loss = 2.41767271\n",
      "Iteration 491, loss = 2.41763238\n",
      "Iteration 492, loss = 2.41854352\n",
      "Iteration 493, loss = 2.41750762\n",
      "Iteration 494, loss = 2.41825272\n",
      "Iteration 495, loss = 2.41501431\n",
      "Iteration 496, loss = 2.41709783\n",
      "Iteration 497, loss = 2.41555428\n",
      "Iteration 498, loss = 2.41624843\n",
      "Iteration 499, loss = 2.41805890\n",
      "Iteration 500, loss = 2.41429019\n",
      "Iteration 501, loss = 2.41618442\n",
      "Iteration 502, loss = 2.41348343\n",
      "Iteration 503, loss = 2.41320698\n",
      "Iteration 504, loss = 2.41565539\n",
      "Iteration 505, loss = 2.41404367\n",
      "Iteration 506, loss = 2.41107028\n",
      "Iteration 507, loss = 2.41492264\n",
      "Iteration 508, loss = 2.41436677\n",
      "Iteration 509, loss = 2.41036438\n",
      "Iteration 510, loss = 2.41114532\n",
      "Iteration 511, loss = 2.41123132\n",
      "Iteration 512, loss = 2.40854925\n",
      "Iteration 513, loss = 2.41210897\n",
      "Iteration 514, loss = 2.41201487\n",
      "Iteration 515, loss = 2.40799570\n",
      "Iteration 516, loss = 2.40831188\n",
      "Iteration 517, loss = 2.40733165\n",
      "Iteration 518, loss = 2.40708184\n",
      "Iteration 519, loss = 2.40573239\n",
      "Iteration 520, loss = 2.40817401\n",
      "Iteration 521, loss = 2.40687931\n",
      "Iteration 522, loss = 2.40541587\n",
      "Iteration 523, loss = 2.40519720\n",
      "Iteration 524, loss = 2.40498676\n",
      "Iteration 525, loss = 2.40379302\n",
      "Iteration 526, loss = 2.40178531\n",
      "Iteration 527, loss = 2.40243573\n",
      "Iteration 528, loss = 2.40192732\n",
      "Iteration 529, loss = 2.40160849\n",
      "Iteration 530, loss = 2.40108558\n",
      "Iteration 531, loss = 2.40206981\n",
      "Iteration 532, loss = 2.40611133\n",
      "Iteration 533, loss = 2.40050575\n",
      "Iteration 534, loss = 2.40217559\n",
      "Iteration 535, loss = 2.40271929\n",
      "Iteration 536, loss = 2.39797975\n",
      "Iteration 537, loss = 2.39740361\n",
      "Iteration 538, loss = 2.39977851\n",
      "Iteration 539, loss = 2.40079987\n",
      "Iteration 540, loss = 2.39840361\n",
      "Iteration 541, loss = 2.39845126\n",
      "Iteration 542, loss = 2.39907599\n",
      "Iteration 543, loss = 2.39571175\n",
      "Iteration 544, loss = 2.39653710\n",
      "Iteration 545, loss = 2.39435336\n",
      "Iteration 546, loss = 2.39625574\n",
      "Iteration 547, loss = 2.39727837\n",
      "Iteration 548, loss = 2.39596265\n",
      "Iteration 549, loss = 2.39745289\n",
      "Iteration 550, loss = 2.39270380\n",
      "Iteration 551, loss = 2.39540269\n",
      "Iteration 552, loss = 2.39003490\n",
      "Iteration 553, loss = 2.39123921\n",
      "Iteration 554, loss = 2.38963935\n",
      "Iteration 555, loss = 2.39115161\n",
      "Iteration 556, loss = 2.39041509\n",
      "Iteration 557, loss = 2.39866135\n",
      "Iteration 558, loss = 2.39114379\n",
      "Iteration 559, loss = 2.38957336\n",
      "Iteration 560, loss = 2.38969466\n",
      "Iteration 561, loss = 2.38658076\n",
      "Iteration 562, loss = 2.38875255\n",
      "Iteration 563, loss = 2.39073660\n",
      "Iteration 564, loss = 2.38821364\n",
      "Iteration 565, loss = 2.38909733\n",
      "Iteration 566, loss = 2.38892186\n",
      "Iteration 567, loss = 2.38747238\n",
      "Iteration 568, loss = 2.38997423\n",
      "Iteration 569, loss = 2.38377010\n",
      "Iteration 570, loss = 2.38509092\n",
      "Iteration 571, loss = 2.38543792\n",
      "Iteration 572, loss = 2.38432947\n",
      "Iteration 573, loss = 2.38405043\n",
      "Iteration 574, loss = 2.38474050\n",
      "Iteration 575, loss = 2.38191610\n",
      "Iteration 576, loss = 2.38404453\n",
      "Iteration 577, loss = 2.38403529\n",
      "Iteration 578, loss = 2.38119991\n",
      "Iteration 579, loss = 2.38320873\n",
      "Iteration 580, loss = 2.38052780\n",
      "Iteration 581, loss = 2.38314939\n",
      "Iteration 582, loss = 2.37864317\n",
      "Iteration 583, loss = 2.37812280\n",
      "Iteration 584, loss = 2.38146713\n",
      "Iteration 585, loss = 2.37731299\n",
      "Iteration 586, loss = 2.37954585\n",
      "Iteration 587, loss = 2.37687780\n",
      "Iteration 588, loss = 2.37965980\n",
      "Iteration 589, loss = 2.37857927\n",
      "Iteration 590, loss = 2.37681840\n",
      "Iteration 591, loss = 2.37699178\n",
      "Iteration 592, loss = 2.37901382\n",
      "Iteration 593, loss = 2.37568040\n",
      "Iteration 594, loss = 2.37927404\n",
      "Iteration 595, loss = 2.37434172\n",
      "Iteration 596, loss = 2.37770765\n",
      "Iteration 597, loss = 2.37363203\n",
      "Iteration 598, loss = 2.37405616\n",
      "Iteration 599, loss = 2.37400825\n",
      "Iteration 600, loss = 2.37427425\n",
      "Iteration 601, loss = 2.37062503\n",
      "Iteration 602, loss = 2.37256609\n",
      "Iteration 603, loss = 2.37346607\n",
      "Iteration 604, loss = 2.37105671\n",
      "Iteration 605, loss = 2.37297737\n",
      "Iteration 606, loss = 2.37149762\n",
      "Iteration 607, loss = 2.36862246\n",
      "Iteration 608, loss = 2.37063522\n",
      "Iteration 609, loss = 2.36897762\n",
      "Iteration 610, loss = 2.36761103\n",
      "Iteration 611, loss = 2.36833470\n",
      "Iteration 612, loss = 2.36727031\n",
      "Iteration 613, loss = 2.36564065\n",
      "Iteration 614, loss = 2.36648229\n",
      "Iteration 615, loss = 2.36740006\n",
      "Iteration 616, loss = 2.36874124\n",
      "Iteration 617, loss = 2.36783365\n",
      "Iteration 618, loss = 2.36756028\n",
      "Iteration 619, loss = 2.36904091\n",
      "Iteration 620, loss = 2.36486881\n",
      "Iteration 621, loss = 2.36569820\n",
      "Iteration 622, loss = 2.36551238\n",
      "Iteration 623, loss = 2.36451188\n",
      "Iteration 624, loss = 2.36291827\n",
      "Iteration 625, loss = 2.36225664\n",
      "Iteration 626, loss = 2.36110360\n",
      "Iteration 627, loss = 2.35961136\n",
      "Iteration 628, loss = 2.36086784\n",
      "Iteration 629, loss = 2.36224050\n",
      "Iteration 630, loss = 2.35945672\n",
      "Iteration 631, loss = 2.35992089\n",
      "Iteration 632, loss = 2.36038445\n",
      "Iteration 633, loss = 2.36041378\n",
      "Iteration 634, loss = 2.35854181\n",
      "Iteration 635, loss = 2.35858595\n",
      "Iteration 636, loss = 2.35855541\n",
      "Iteration 637, loss = 2.36042662\n",
      "Iteration 638, loss = 2.35864197\n",
      "Iteration 639, loss = 2.35795952\n",
      "Iteration 640, loss = 2.35963245\n",
      "Iteration 641, loss = 2.35729196\n",
      "Iteration 642, loss = 2.35741251\n",
      "Iteration 643, loss = 2.35410291\n",
      "Iteration 644, loss = 2.35683553\n",
      "Iteration 645, loss = 2.35397755\n",
      "Iteration 646, loss = 2.35631211\n",
      "Iteration 647, loss = 2.35568319\n",
      "Iteration 648, loss = 2.35530878\n",
      "Iteration 649, loss = 2.35254852\n",
      "Iteration 650, loss = 2.35110815\n",
      "Iteration 651, loss = 2.34980450\n",
      "Iteration 652, loss = 2.35154895\n",
      "Iteration 653, loss = 2.34933312\n",
      "Iteration 654, loss = 2.34965411\n",
      "Iteration 655, loss = 2.35168245\n",
      "Iteration 656, loss = 2.34965712\n",
      "Iteration 657, loss = 2.34971522\n",
      "Iteration 658, loss = 2.34957191\n",
      "Iteration 659, loss = 2.34980296\n",
      "Iteration 660, loss = 2.34922782\n",
      "Iteration 661, loss = 2.34901608\n",
      "Iteration 662, loss = 2.34869846\n",
      "Iteration 663, loss = 2.34772684\n",
      "Iteration 664, loss = 2.34840482\n",
      "Iteration 665, loss = 2.34624914\n",
      "Iteration 666, loss = 2.34809617\n",
      "Iteration 667, loss = 2.34395596\n",
      "Iteration 668, loss = 2.34563205\n",
      "Iteration 669, loss = 2.34745995\n",
      "Iteration 670, loss = 2.34171258\n",
      "Iteration 671, loss = 2.34373756\n",
      "Iteration 672, loss = 2.34315075\n",
      "Iteration 673, loss = 2.34204032\n",
      "Iteration 674, loss = 2.34485432\n",
      "Iteration 675, loss = 2.34071809\n",
      "Iteration 676, loss = 2.33959466\n",
      "Iteration 677, loss = 2.34129171\n",
      "Iteration 678, loss = 2.33869975\n",
      "Iteration 679, loss = 2.33857893\n",
      "Iteration 680, loss = 2.34031633\n",
      "Iteration 681, loss = 2.33908366\n",
      "Iteration 682, loss = 2.33558234\n",
      "Iteration 683, loss = 2.33757132\n",
      "Iteration 684, loss = 2.34037472\n",
      "Iteration 685, loss = 2.33999240\n",
      "Iteration 686, loss = 2.34035884\n",
      "Iteration 687, loss = 2.33778891\n",
      "Iteration 688, loss = 2.33514796\n",
      "Iteration 689, loss = 2.33795214\n",
      "Iteration 690, loss = 2.33203894\n",
      "Iteration 691, loss = 2.33407429\n",
      "Iteration 692, loss = 2.33377559\n",
      "Iteration 693, loss = 2.33546211\n",
      "Iteration 694, loss = 2.33489621\n",
      "Iteration 695, loss = 2.33352814\n",
      "Iteration 696, loss = 2.33081885\n",
      "Iteration 697, loss = 2.33224919\n",
      "Iteration 698, loss = 2.33219331\n",
      "Iteration 699, loss = 2.32842931\n",
      "Iteration 700, loss = 2.33385067\n",
      "Iteration 701, loss = 2.33263312\n",
      "Iteration 702, loss = 2.33229150\n",
      "Iteration 703, loss = 2.32825001\n",
      "Iteration 704, loss = 2.33082325\n",
      "Iteration 705, loss = 2.32919004\n",
      "Iteration 706, loss = 2.32622270\n",
      "Iteration 707, loss = 2.32647789\n",
      "Iteration 708, loss = 2.32947510\n",
      "Iteration 709, loss = 2.32510589\n",
      "Iteration 710, loss = 2.32556953\n",
      "Iteration 711, loss = 2.32669274\n",
      "Iteration 712, loss = 2.32670132\n",
      "Iteration 713, loss = 2.32303935\n",
      "Iteration 714, loss = 2.32337029\n",
      "Iteration 715, loss = 2.32349946\n",
      "Iteration 716, loss = 2.32377075\n",
      "Iteration 717, loss = 2.32248726\n",
      "Iteration 718, loss = 2.32526365\n",
      "Iteration 719, loss = 2.32092324\n",
      "Iteration 720, loss = 2.32146980\n",
      "Iteration 721, loss = 2.32175009\n",
      "Iteration 722, loss = 2.31956123\n",
      "Iteration 723, loss = 2.31843958\n",
      "Iteration 724, loss = 2.32083309\n",
      "Iteration 725, loss = 2.31851714\n",
      "Iteration 726, loss = 2.31893344\n",
      "Iteration 727, loss = 2.31927697\n",
      "Iteration 728, loss = 2.31849893\n",
      "Iteration 729, loss = 2.31493844\n",
      "Iteration 730, loss = 2.31827849\n",
      "Iteration 731, loss = 2.32112139\n",
      "Iteration 732, loss = 2.31773598\n",
      "Iteration 733, loss = 2.31692268\n",
      "Iteration 734, loss = 2.31572355\n",
      "Iteration 735, loss = 2.31664989\n",
      "Iteration 736, loss = 2.31270081\n",
      "Iteration 737, loss = 2.31559379\n",
      "Iteration 738, loss = 2.31249916\n",
      "Iteration 739, loss = 2.31415590\n",
      "Iteration 740, loss = 2.31176318\n",
      "Iteration 741, loss = 2.31201200\n",
      "Iteration 742, loss = 2.31146538\n",
      "Iteration 743, loss = 2.31398295\n",
      "Iteration 744, loss = 2.31327143\n",
      "Iteration 745, loss = 2.31190295\n",
      "Iteration 746, loss = 2.31288675\n",
      "Iteration 747, loss = 2.31077159\n",
      "Iteration 748, loss = 2.31185286\n",
      "Iteration 749, loss = 2.30757045\n",
      "Iteration 750, loss = 2.31067361\n",
      "Iteration 751, loss = 2.31008947\n",
      "Iteration 752, loss = 2.31041956\n",
      "Iteration 753, loss = 2.30573275\n",
      "Iteration 754, loss = 2.30761186\n",
      "Iteration 755, loss = 2.30871829\n",
      "Iteration 756, loss = 2.30480674\n",
      "Iteration 757, loss = 2.30908463\n",
      "Iteration 758, loss = 2.30952795\n",
      "Iteration 759, loss = 2.30898871\n",
      "Iteration 760, loss = 2.30699441\n",
      "Iteration 761, loss = 2.30551081\n",
      "Iteration 762, loss = 2.30226983\n",
      "Iteration 763, loss = 2.30629171\n",
      "Iteration 764, loss = 2.30312408\n",
      "Iteration 765, loss = 2.30305944\n",
      "Iteration 766, loss = 2.30249922\n",
      "Iteration 767, loss = 2.30402205\n",
      "Iteration 768, loss = 2.30117684\n",
      "Iteration 769, loss = 2.30215691\n",
      "Iteration 770, loss = 2.30328635\n",
      "Iteration 771, loss = 2.30416588\n",
      "Iteration 772, loss = 2.30220557\n",
      "Iteration 773, loss = 2.30555000\n",
      "Iteration 774, loss = 2.29902446\n",
      "Iteration 775, loss = 2.30142920\n",
      "Iteration 776, loss = 2.30072403\n",
      "Iteration 777, loss = 2.30181832\n",
      "Iteration 778, loss = 2.30070763\n",
      "Iteration 779, loss = 2.29599145\n",
      "Iteration 780, loss = 2.30143142\n",
      "Iteration 781, loss = 2.29848288\n",
      "Iteration 782, loss = 2.29864586\n",
      "Iteration 783, loss = 2.30160684\n",
      "Iteration 784, loss = 2.29647703\n",
      "Iteration 785, loss = 2.29846696\n",
      "Iteration 786, loss = 2.29528491\n",
      "Iteration 787, loss = 2.29467436\n",
      "Iteration 788, loss = 2.29354983\n",
      "Iteration 789, loss = 2.29479206\n",
      "Iteration 790, loss = 2.29423100\n",
      "Iteration 791, loss = 2.29198192\n",
      "Iteration 792, loss = 2.29119449\n",
      "Iteration 793, loss = 2.29376395\n",
      "Iteration 794, loss = 2.29562565\n",
      "Iteration 795, loss = 2.29225537\n",
      "Iteration 796, loss = 2.29230379\n",
      "Iteration 797, loss = 2.29319882\n",
      "Iteration 798, loss = 2.28801109\n",
      "Iteration 799, loss = 2.29064067\n",
      "Iteration 800, loss = 2.28719218\n",
      "Iteration 801, loss = 2.29131040\n",
      "Iteration 802, loss = 2.28707597\n",
      "Iteration 803, loss = 2.28995715\n",
      "Iteration 804, loss = 2.28827484\n",
      "Iteration 805, loss = 2.28776742\n",
      "Iteration 806, loss = 2.28624408\n",
      "Iteration 807, loss = 2.28578258\n",
      "Iteration 808, loss = 2.28685843\n",
      "Iteration 809, loss = 2.29022298\n",
      "Iteration 810, loss = 2.28481542\n",
      "Iteration 811, loss = 2.28616439\n",
      "Iteration 812, loss = 2.28522473\n",
      "Iteration 813, loss = 2.28301349\n",
      "Iteration 814, loss = 2.28505531\n",
      "Iteration 815, loss = 2.28300233\n",
      "Iteration 816, loss = 2.28419553\n",
      "Iteration 817, loss = 2.28710543\n",
      "Iteration 818, loss = 2.28303080\n",
      "Iteration 819, loss = 2.28317435\n",
      "Iteration 820, loss = 2.28160929\n",
      "Iteration 821, loss = 2.28432966\n",
      "Iteration 822, loss = 2.28405320\n",
      "Iteration 823, loss = 2.28191105\n",
      "Iteration 824, loss = 2.28416266\n",
      "Iteration 825, loss = 2.28179241\n",
      "Iteration 826, loss = 2.27923716\n",
      "Iteration 827, loss = 2.27865047\n",
      "Iteration 828, loss = 2.28122623\n",
      "Iteration 829, loss = 2.27854328\n",
      "Iteration 830, loss = 2.27780243\n",
      "Iteration 831, loss = 2.27753953\n",
      "Iteration 832, loss = 2.27960282\n",
      "Iteration 833, loss = 2.27835939\n",
      "Iteration 834, loss = 2.27591915\n",
      "Iteration 835, loss = 2.27566824\n",
      "Iteration 836, loss = 2.27538860\n",
      "Iteration 837, loss = 2.27641111\n",
      "Iteration 838, loss = 2.27615737\n",
      "Iteration 839, loss = 2.27444161\n",
      "Iteration 840, loss = 2.27359759\n",
      "Iteration 841, loss = 2.27186467\n",
      "Iteration 842, loss = 2.27738945\n",
      "Iteration 843, loss = 2.27406979\n",
      "Iteration 844, loss = 2.27082731\n",
      "Iteration 845, loss = 2.27365849\n",
      "Iteration 846, loss = 2.27159484\n",
      "Iteration 847, loss = 2.27187843\n",
      "Iteration 848, loss = 2.27058587\n",
      "Iteration 849, loss = 2.26684518\n",
      "Iteration 850, loss = 2.26678450\n",
      "Iteration 851, loss = 2.26869362\n",
      "Iteration 852, loss = 2.26696698\n",
      "Iteration 853, loss = 2.26772334\n",
      "Iteration 854, loss = 2.26419565\n",
      "Iteration 855, loss = 2.26909204\n",
      "Iteration 856, loss = 2.26744573\n",
      "Iteration 857, loss = 2.26892158\n",
      "Iteration 858, loss = 2.26712001\n",
      "Iteration 859, loss = 2.26898766\n",
      "Iteration 860, loss = 2.26604457\n",
      "Iteration 861, loss = 2.26405101\n",
      "Iteration 862, loss = 2.26904195\n",
      "Iteration 863, loss = 2.26750717\n",
      "Iteration 864, loss = 2.26282781\n",
      "Iteration 865, loss = 2.26754761\n",
      "Iteration 866, loss = 2.26531005\n",
      "Iteration 867, loss = 2.26132558\n",
      "Iteration 868, loss = 2.26437183\n",
      "Iteration 869, loss = 2.26100619\n",
      "Iteration 870, loss = 2.26245122\n",
      "Iteration 871, loss = 2.26169267\n",
      "Iteration 872, loss = 2.25893778\n",
      "Iteration 873, loss = 2.26395185\n",
      "Iteration 874, loss = 2.26057463\n",
      "Iteration 875, loss = 2.25923249\n",
      "Iteration 876, loss = 2.25914177\n",
      "Iteration 877, loss = 2.25854130\n",
      "Iteration 878, loss = 2.25655704\n",
      "Iteration 879, loss = 2.25933359\n",
      "Iteration 880, loss = 2.25632893\n",
      "Iteration 881, loss = 2.25562722\n",
      "Iteration 882, loss = 2.25400505\n",
      "Iteration 883, loss = 2.25897553\n",
      "Iteration 884, loss = 2.25541053\n",
      "Iteration 885, loss = 2.25410849\n",
      "Iteration 886, loss = 2.25784436\n",
      "Iteration 887, loss = 2.25343062\n",
      "Iteration 888, loss = 2.25231066\n",
      "Iteration 889, loss = 2.25638443\n",
      "Iteration 890, loss = 2.25328874\n",
      "Iteration 891, loss = 2.25198268\n",
      "Iteration 892, loss = 2.24974275\n",
      "Iteration 893, loss = 2.25136740\n",
      "Iteration 894, loss = 2.25308139\n",
      "Iteration 895, loss = 2.24836384\n",
      "Iteration 896, loss = 2.25081551\n",
      "Iteration 897, loss = 2.25023289\n",
      "Iteration 898, loss = 2.24629601\n",
      "Iteration 899, loss = 2.25041127\n",
      "Iteration 900, loss = 2.24494579\n",
      "Iteration 901, loss = 2.24479825\n",
      "Iteration 902, loss = 2.24612483\n",
      "Iteration 903, loss = 2.24309364\n",
      "Iteration 904, loss = 2.24535655\n",
      "Iteration 905, loss = 2.24318565\n",
      "Iteration 906, loss = 2.24355482\n",
      "Iteration 907, loss = 2.24296420\n",
      "Iteration 908, loss = 2.24332345\n",
      "Iteration 909, loss = 2.24378745\n",
      "Iteration 910, loss = 2.24211721\n",
      "Iteration 911, loss = 2.24316369\n",
      "Iteration 912, loss = 2.24917826\n",
      "Iteration 913, loss = 2.23983416\n",
      "Iteration 914, loss = 2.24380682\n",
      "Iteration 915, loss = 2.23805750\n",
      "Iteration 916, loss = 2.24137925\n",
      "Iteration 917, loss = 2.24108241\n",
      "Iteration 918, loss = 2.23778392\n",
      "Iteration 919, loss = 2.23644019\n",
      "Iteration 920, loss = 2.23741528\n",
      "Iteration 921, loss = 2.24095165\n",
      "Iteration 922, loss = 2.24064735\n",
      "Iteration 923, loss = 2.24064169\n",
      "Iteration 924, loss = 2.23589912\n",
      "Iteration 925, loss = 2.23924589\n",
      "Iteration 926, loss = 2.23347831\n",
      "Iteration 927, loss = 2.23789042\n",
      "Iteration 928, loss = 2.23817325\n",
      "Iteration 929, loss = 2.23624552\n",
      "Iteration 930, loss = 2.23513012\n",
      "Iteration 931, loss = 2.23447841\n",
      "Iteration 932, loss = 2.23223974\n",
      "Iteration 933, loss = 2.23303033\n",
      "Iteration 934, loss = 2.23132632\n",
      "Iteration 935, loss = 2.22994179\n",
      "Iteration 936, loss = 2.23118666\n",
      "Iteration 937, loss = 2.23210665\n",
      "Iteration 938, loss = 2.23098676\n",
      "Iteration 939, loss = 2.23040194\n",
      "Iteration 940, loss = 2.22859066\n",
      "Iteration 941, loss = 2.23107488\n",
      "Iteration 942, loss = 2.22891801\n",
      "Iteration 943, loss = 2.22847244\n",
      "Iteration 944, loss = 2.22742037\n",
      "Iteration 945, loss = 2.23121266\n",
      "Iteration 946, loss = 2.22732134\n",
      "Iteration 947, loss = 2.22542417\n",
      "Iteration 948, loss = 2.22643649\n",
      "Iteration 949, loss = 2.22465810\n",
      "Iteration 950, loss = 2.22538815\n",
      "Iteration 951, loss = 2.22457438\n",
      "Iteration 952, loss = 2.22938590\n",
      "Iteration 953, loss = 2.22660684\n",
      "Iteration 954, loss = 2.22466779\n",
      "Iteration 955, loss = 2.22383197\n",
      "Iteration 956, loss = 2.22175060\n",
      "Iteration 957, loss = 2.21980939\n",
      "Iteration 958, loss = 2.22481631\n",
      "Iteration 959, loss = 2.22501977\n",
      "Iteration 960, loss = 2.22516640\n",
      "Iteration 961, loss = 2.21924014\n",
      "Iteration 962, loss = 2.22328851\n",
      "Iteration 963, loss = 2.21818121\n",
      "Iteration 964, loss = 2.22212965\n",
      "Iteration 965, loss = 2.21986635\n",
      "Iteration 966, loss = 2.22024374\n",
      "Iteration 967, loss = 2.21856628\n",
      "Iteration 968, loss = 2.21874027\n",
      "Iteration 969, loss = 2.22038764\n",
      "Iteration 970, loss = 2.21660694\n",
      "Iteration 971, loss = 2.21706618\n",
      "Iteration 972, loss = 2.21803712\n",
      "Iteration 973, loss = 2.21954850\n",
      "Iteration 974, loss = 2.21490474\n",
      "Iteration 975, loss = 2.21875794\n",
      "Iteration 976, loss = 2.21646119\n",
      "Iteration 977, loss = 2.21778241\n",
      "Iteration 978, loss = 2.21473522\n",
      "Iteration 979, loss = 2.21315231\n",
      "Iteration 980, loss = 2.21717399\n",
      "Iteration 981, loss = 2.21329486\n",
      "Iteration 982, loss = 2.21470005\n",
      "Iteration 983, loss = 2.20994570\n",
      "Iteration 984, loss = 2.21259689\n",
      "Iteration 985, loss = 2.21386647\n",
      "Iteration 986, loss = 2.21687206\n",
      "Iteration 987, loss = 2.21160124\n",
      "Iteration 988, loss = 2.21226750\n",
      "Iteration 989, loss = 2.21267152\n",
      "Iteration 990, loss = 2.21206535\n",
      "Iteration 991, loss = 2.21106606\n",
      "Iteration 992, loss = 2.21114430\n",
      "Iteration 993, loss = 2.20990194\n",
      "Iteration 994, loss = 2.21133530\n",
      "Iteration 995, loss = 2.20869585\n",
      "Iteration 996, loss = 2.20675429\n",
      "Iteration 997, loss = 2.20778781\n",
      "Iteration 998, loss = 2.20675858\n",
      "Iteration 999, loss = 2.20683924\n",
      "Iteration 1000, loss = 2.20900847\n",
      "Iteration 1001, loss = 2.20674256\n",
      "Iteration 1002, loss = 2.20875246\n",
      "Iteration 1003, loss = 2.20567626\n",
      "Iteration 1004, loss = 2.20554454\n",
      "Iteration 1005, loss = 2.20542516\n",
      "Iteration 1006, loss = 2.20411481\n",
      "Iteration 1007, loss = 2.20579140\n",
      "Iteration 1008, loss = 2.20285327\n",
      "Iteration 1009, loss = 2.20240196\n",
      "Iteration 1010, loss = 2.20325307\n",
      "Iteration 1011, loss = 2.20355277\n",
      "Iteration 1012, loss = 2.20179197\n",
      "Iteration 1013, loss = 2.20330278\n",
      "Iteration 1014, loss = 2.19805326\n",
      "Iteration 1015, loss = 2.19905667\n",
      "Iteration 1016, loss = 2.19999216\n",
      "Iteration 1017, loss = 2.19893616\n",
      "Iteration 1018, loss = 2.19717198\n",
      "Iteration 1019, loss = 2.19871717\n",
      "Iteration 1020, loss = 2.19789002\n",
      "Iteration 1021, loss = 2.19852289\n",
      "Iteration 1022, loss = 2.19713757\n",
      "Iteration 1023, loss = 2.19678530\n",
      "Iteration 1024, loss = 2.20043778\n",
      "Iteration 1025, loss = 2.19833151\n",
      "Iteration 1026, loss = 2.19583995\n",
      "Iteration 1027, loss = 2.19392791\n",
      "Iteration 1028, loss = 2.19243221\n",
      "Iteration 1029, loss = 2.19274033\n",
      "Iteration 1030, loss = 2.19564546\n",
      "Iteration 1031, loss = 2.19266458\n",
      "Iteration 1032, loss = 2.19300592\n",
      "Iteration 1033, loss = 2.19506071\n",
      "Iteration 1034, loss = 2.19454797\n",
      "Iteration 1035, loss = 2.19034102\n",
      "Iteration 1036, loss = 2.19222365\n",
      "Iteration 1037, loss = 2.18810471\n",
      "Iteration 1038, loss = 2.18866779\n",
      "Iteration 1039, loss = 2.18944470\n",
      "Iteration 1040, loss = 2.18925538\n",
      "Iteration 1041, loss = 2.18942684\n",
      "Iteration 1042, loss = 2.18942199\n",
      "Iteration 1043, loss = 2.19038764\n",
      "Iteration 1044, loss = 2.18771546\n",
      "Iteration 1045, loss = 2.18768011\n",
      "Iteration 1046, loss = 2.18662386\n",
      "Iteration 1047, loss = 2.18831200\n",
      "Iteration 1048, loss = 2.18560295\n",
      "Iteration 1049, loss = 2.18815758\n",
      "Iteration 1050, loss = 2.19069533\n",
      "Iteration 1051, loss = 2.19172174\n",
      "Iteration 1052, loss = 2.18745296\n",
      "Iteration 1053, loss = 2.19044497\n",
      "Iteration 1054, loss = 2.18945656\n",
      "Iteration 1055, loss = 2.18448626\n",
      "Iteration 1056, loss = 2.18244059\n",
      "Iteration 1057, loss = 2.18282741\n",
      "Iteration 1058, loss = 2.18739667\n",
      "Iteration 1059, loss = 2.18734840\n",
      "Iteration 1060, loss = 2.18256620\n",
      "Iteration 1061, loss = 2.18334479\n",
      "Iteration 1062, loss = 2.18336108\n",
      "Iteration 1063, loss = 2.18737515\n",
      "Iteration 1064, loss = 2.18126566\n",
      "Iteration 1065, loss = 2.18383979\n",
      "Iteration 1066, loss = 2.17916311\n",
      "Iteration 1067, loss = 2.18341664\n",
      "Iteration 1068, loss = 2.18484397\n",
      "Iteration 1069, loss = 2.17814474\n",
      "Iteration 1070, loss = 2.17942498\n",
      "Iteration 1071, loss = 2.18189696\n",
      "Iteration 1072, loss = 2.18032847\n",
      "Iteration 1073, loss = 2.17983585\n",
      "Iteration 1074, loss = 2.18123578\n",
      "Iteration 1075, loss = 2.18183397\n",
      "Iteration 1076, loss = 2.17950256\n",
      "Iteration 1077, loss = 2.17754964\n",
      "Iteration 1078, loss = 2.18335205\n",
      "Iteration 1079, loss = 2.17654016\n",
      "Iteration 1080, loss = 2.17778505\n",
      "Iteration 1081, loss = 2.17520177\n",
      "Iteration 1082, loss = 2.17332163\n",
      "Iteration 1083, loss = 2.17259674\n",
      "Iteration 1084, loss = 2.17238833\n",
      "Iteration 1085, loss = 2.17320067\n",
      "Iteration 1086, loss = 2.17303622\n",
      "Iteration 1087, loss = 2.17396149\n",
      "Iteration 1088, loss = 2.17152748\n",
      "Iteration 1089, loss = 2.17676177\n",
      "Iteration 1090, loss = 2.17402907\n",
      "Iteration 1091, loss = 2.17317595\n",
      "Iteration 1092, loss = 2.17373367\n",
      "Iteration 1093, loss = 2.17405432\n",
      "Iteration 1094, loss = 2.17035707\n",
      "Iteration 1095, loss = 2.17189328\n",
      "Iteration 1096, loss = 2.16908505\n",
      "Iteration 1097, loss = 2.17148281\n",
      "Iteration 1098, loss = 2.16549714\n",
      "Iteration 1099, loss = 2.17146036\n",
      "Iteration 1100, loss = 2.16660327\n",
      "Iteration 1101, loss = 2.16613177\n",
      "Iteration 1102, loss = 2.16754344\n",
      "Iteration 1103, loss = 2.16707384\n",
      "Iteration 1104, loss = 2.16584857\n",
      "Iteration 1105, loss = 2.16707447\n",
      "Iteration 1106, loss = 2.16547684\n",
      "Iteration 1107, loss = 2.17092624\n",
      "Iteration 1108, loss = 2.16736548\n",
      "Iteration 1109, loss = 2.16458753\n",
      "Iteration 1110, loss = 2.16642009\n",
      "Iteration 1111, loss = 2.16170193\n",
      "Iteration 1112, loss = 2.17044893\n",
      "Iteration 1113, loss = 2.16859256\n",
      "Iteration 1114, loss = 2.16485286\n",
      "Iteration 1115, loss = 2.16425227\n",
      "Iteration 1116, loss = 2.16384628\n",
      "Iteration 1117, loss = 2.16111656\n",
      "Iteration 1118, loss = 2.16316399\n",
      "Iteration 1119, loss = 2.16627766\n",
      "Iteration 1120, loss = 2.16209490\n",
      "Iteration 1121, loss = 2.16362255\n",
      "Iteration 1122, loss = 2.16203905\n",
      "Iteration 1123, loss = 2.16228597\n",
      "Iteration 1124, loss = 2.16204038\n",
      "Iteration 1125, loss = 2.16143090\n",
      "Iteration 1126, loss = 2.15829707\n",
      "Iteration 1127, loss = 2.16014576\n",
      "Iteration 1128, loss = 2.15950327\n",
      "Iteration 1129, loss = 2.15991193\n",
      "Iteration 1130, loss = 2.15857730\n",
      "Iteration 1131, loss = 2.16037301\n",
      "Iteration 1132, loss = 2.15445943\n",
      "Iteration 1133, loss = 2.15870904\n",
      "Iteration 1134, loss = 2.15535396\n",
      "Iteration 1135, loss = 2.15558261\n",
      "Iteration 1136, loss = 2.15839566\n",
      "Iteration 1137, loss = 2.15669457\n",
      "Iteration 1138, loss = 2.15760827\n",
      "Iteration 1139, loss = 2.15417335\n",
      "Iteration 1140, loss = 2.15586447\n",
      "Iteration 1141, loss = 2.15662406\n",
      "Iteration 1142, loss = 2.15752611\n",
      "Iteration 1143, loss = 2.15409889\n",
      "Iteration 1144, loss = 2.15555397\n",
      "Iteration 1145, loss = 2.15580017\n",
      "Iteration 1146, loss = 2.15391012\n",
      "Iteration 1147, loss = 2.15327907\n",
      "Iteration 1148, loss = 2.15541360\n",
      "Iteration 1149, loss = 2.15305660\n",
      "Iteration 1150, loss = 2.15457846\n",
      "Iteration 1151, loss = 2.15358596\n",
      "Iteration 1152, loss = 2.15156521\n",
      "Iteration 1153, loss = 2.15343621\n",
      "Iteration 1154, loss = 2.15264992\n",
      "Iteration 1155, loss = 2.15328674\n",
      "Iteration 1156, loss = 2.15501677\n",
      "Iteration 1157, loss = 2.15279086\n",
      "Iteration 1158, loss = 2.14947412\n",
      "Iteration 1159, loss = 2.15494015\n",
      "Iteration 1160, loss = 2.14920314\n",
      "Iteration 1161, loss = 2.14655656\n",
      "Iteration 1162, loss = 2.14938240\n",
      "Iteration 1163, loss = 2.14659057\n",
      "Iteration 1164, loss = 2.14939704\n",
      "Iteration 1165, loss = 2.14622738\n",
      "Iteration 1166, loss = 2.14740782\n",
      "Iteration 1167, loss = 2.14722512\n",
      "Iteration 1168, loss = 2.14620382\n",
      "Iteration 1169, loss = 2.14669574\n",
      "Iteration 1170, loss = 2.15069855\n",
      "Iteration 1171, loss = 2.14195185\n",
      "Iteration 1172, loss = 2.14510671\n",
      "Iteration 1173, loss = 2.14527141\n",
      "Iteration 1174, loss = 2.14712889\n",
      "Iteration 1175, loss = 2.14928828\n",
      "Iteration 1176, loss = 2.14607467\n",
      "Iteration 1177, loss = 2.14306910\n",
      "Iteration 1178, loss = 2.14842293\n",
      "Iteration 1179, loss = 2.14266240\n",
      "Iteration 1180, loss = 2.14259109\n",
      "Iteration 1181, loss = 2.14277870\n",
      "Iteration 1182, loss = 2.14049298\n",
      "Iteration 1183, loss = 2.14473342\n",
      "Iteration 1184, loss = 2.14559370\n",
      "Iteration 1185, loss = 2.14126778\n",
      "Iteration 1186, loss = 2.14450025\n",
      "Iteration 1187, loss = 2.14075181\n",
      "Iteration 1188, loss = 2.14275838\n",
      "Iteration 1189, loss = 2.14109785\n",
      "Iteration 1190, loss = 2.14008864\n",
      "Iteration 1191, loss = 2.13915208\n",
      "Iteration 1192, loss = 2.14332389\n",
      "Iteration 1193, loss = 2.14306034\n",
      "Iteration 1194, loss = 2.13855010\n",
      "Iteration 1195, loss = 2.13793078\n",
      "Iteration 1196, loss = 2.14123767\n",
      "Iteration 1197, loss = 2.13989949\n",
      "Iteration 1198, loss = 2.13982764\n",
      "Iteration 1199, loss = 2.13636899\n",
      "Iteration 1200, loss = 2.13556373\n",
      "Iteration 1201, loss = 2.14063745\n",
      "Iteration 1202, loss = 2.13863637\n",
      "Iteration 1203, loss = 2.13820887\n",
      "Iteration 1204, loss = 2.13544112\n",
      "Iteration 1205, loss = 2.13740175\n",
      "Iteration 1206, loss = 2.13272065\n",
      "Iteration 1207, loss = 2.13492352\n",
      "Iteration 1208, loss = 2.13354736\n",
      "Iteration 1209, loss = 2.13240100\n",
      "Iteration 1210, loss = 2.13339106\n",
      "Iteration 1211, loss = 2.13352698\n",
      "Iteration 1212, loss = 2.13474899\n",
      "Iteration 1213, loss = 2.13639865\n",
      "Iteration 1214, loss = 2.13505255\n",
      "Iteration 1215, loss = 2.13064640\n",
      "Iteration 1216, loss = 2.13170045\n",
      "Iteration 1217, loss = 2.13270235\n",
      "Iteration 1218, loss = 2.13455959\n",
      "Iteration 1219, loss = 2.13202998\n",
      "Iteration 1220, loss = 2.13632416\n",
      "Iteration 1221, loss = 2.13415377\n",
      "Iteration 1222, loss = 2.13145296\n",
      "Iteration 1223, loss = 2.13381069\n",
      "Iteration 1224, loss = 2.13265041\n",
      "Iteration 1225, loss = 2.12987994\n",
      "Iteration 1226, loss = 2.13109964\n",
      "Iteration 1227, loss = 2.13004014\n",
      "Iteration 1228, loss = 2.13193473\n",
      "Iteration 1229, loss = 2.12744521\n",
      "Iteration 1230, loss = 2.12887920\n",
      "Iteration 1231, loss = 2.12849027\n",
      "Iteration 1232, loss = 2.12649860\n",
      "Iteration 1233, loss = 2.12482553\n",
      "Iteration 1234, loss = 2.12812058\n",
      "Iteration 1235, loss = 2.12555985\n",
      "Iteration 1236, loss = 2.12814907\n",
      "Iteration 1237, loss = 2.12621587\n",
      "Iteration 1238, loss = 2.12410870\n",
      "Iteration 1239, loss = 2.12681803\n",
      "Iteration 1240, loss = 2.12607168\n",
      "Iteration 1241, loss = 2.12600957\n",
      "Iteration 1242, loss = 2.12649538\n",
      "Iteration 1243, loss = 2.12690313\n",
      "Iteration 1244, loss = 2.12348485\n",
      "Iteration 1245, loss = 2.12927959\n",
      "Iteration 1246, loss = 2.12520825\n",
      "Iteration 1247, loss = 2.12238733\n",
      "Iteration 1248, loss = 2.12372415\n",
      "Iteration 1249, loss = 2.12298746\n",
      "Iteration 1250, loss = 2.12217656\n",
      "Iteration 1251, loss = 2.12281513\n",
      "Iteration 1252, loss = 2.12406887\n",
      "Iteration 1253, loss = 2.12312698\n",
      "Iteration 1254, loss = 2.11892741\n",
      "Iteration 1255, loss = 2.12323704\n",
      "Iteration 1256, loss = 2.12210481\n",
      "Iteration 1257, loss = 2.12009556\n",
      "Iteration 1258, loss = 2.12090883\n",
      "Iteration 1259, loss = 2.11925448\n",
      "Iteration 1260, loss = 2.12230837\n",
      "Iteration 1261, loss = 2.11897347\n",
      "Iteration 1262, loss = 2.12142301\n",
      "Iteration 1263, loss = 2.12082425\n",
      "Iteration 1264, loss = 2.11721428\n",
      "Iteration 1265, loss = 2.11614292\n",
      "Iteration 1266, loss = 2.11942013\n",
      "Iteration 1267, loss = 2.11446785\n",
      "Iteration 1268, loss = 2.11962958\n",
      "Iteration 1269, loss = 2.11417576\n",
      "Iteration 1270, loss = 2.11805247\n",
      "Iteration 1271, loss = 2.11773802\n",
      "Iteration 1272, loss = 2.11506515\n",
      "Iteration 1273, loss = 2.11853447\n",
      "Iteration 1274, loss = 2.11442702\n",
      "Iteration 1275, loss = 2.11891959\n",
      "Iteration 1276, loss = 2.12335307\n",
      "Iteration 1277, loss = 2.12011985\n",
      "Iteration 1278, loss = 2.11930751\n",
      "Iteration 1279, loss = 2.11304519\n",
      "Iteration 1280, loss = 2.11776983\n",
      "Iteration 1281, loss = 2.11508655\n",
      "Iteration 1282, loss = 2.11461585\n",
      "Iteration 1283, loss = 2.11704095\n",
      "Iteration 1284, loss = 2.11520608\n",
      "Iteration 1285, loss = 2.11500899\n",
      "Iteration 1286, loss = 2.11152897\n",
      "Iteration 1287, loss = 2.11231983\n",
      "Iteration 1288, loss = 2.10914115\n",
      "Iteration 1289, loss = 2.11079754\n",
      "Iteration 1290, loss = 2.11238775\n",
      "Iteration 1291, loss = 2.11377255\n",
      "Iteration 1292, loss = 2.11502316\n",
      "Iteration 1293, loss = 2.11046723\n",
      "Iteration 1294, loss = 2.10954427\n",
      "Iteration 1295, loss = 2.10660294\n",
      "Iteration 1296, loss = 2.11452653\n",
      "Iteration 1297, loss = 2.10812803\n",
      "Iteration 1298, loss = 2.10958059\n",
      "Iteration 1299, loss = 2.10902426\n",
      "Iteration 1300, loss = 2.10812562\n",
      "Iteration 1301, loss = 2.10893797\n",
      "Iteration 1302, loss = 2.10881785\n",
      "Iteration 1303, loss = 2.10919599\n",
      "Iteration 1304, loss = 2.10585186\n",
      "Iteration 1305, loss = 2.10520309\n",
      "Iteration 1306, loss = 2.10827132\n",
      "Iteration 1307, loss = 2.10821103\n",
      "Iteration 1308, loss = 2.10751970\n",
      "Iteration 1309, loss = 2.11053400\n",
      "Iteration 1310, loss = 2.10697263\n",
      "Iteration 1311, loss = 2.10844217\n",
      "Iteration 1312, loss = 2.10534585\n",
      "Iteration 1313, loss = 2.10617795\n",
      "Iteration 1314, loss = 2.10358370\n",
      "Iteration 1315, loss = 2.10502391\n",
      "Iteration 1316, loss = 2.10348013\n",
      "Iteration 1317, loss = 2.10814313\n",
      "Iteration 1318, loss = 2.11100042\n",
      "Iteration 1319, loss = 2.10339375\n",
      "Iteration 1320, loss = 2.10252591\n",
      "Iteration 1321, loss = 2.10937105\n",
      "Iteration 1322, loss = 2.10509370\n",
      "Iteration 1323, loss = 2.10843017\n",
      "Iteration 1324, loss = 2.10222265\n",
      "Iteration 1325, loss = 2.10117638\n",
      "Iteration 1326, loss = 2.10307875\n",
      "Iteration 1327, loss = 2.10366889\n",
      "Iteration 1328, loss = 2.10706506\n",
      "Iteration 1329, loss = 2.10282933\n",
      "Iteration 1330, loss = 2.10441798\n",
      "Iteration 1331, loss = 2.10451730\n",
      "Iteration 1332, loss = 2.10079863\n",
      "Iteration 1333, loss = 2.10259022\n",
      "Iteration 1334, loss = 2.10507721\n",
      "Iteration 1335, loss = 2.10142567\n",
      "Iteration 1336, loss = 2.09790861\n",
      "Iteration 1337, loss = 2.09790643\n",
      "Iteration 1338, loss = 2.10054009\n",
      "Iteration 1339, loss = 2.09810786\n",
      "Iteration 1340, loss = 2.09748886\n",
      "Iteration 1341, loss = 2.10094129\n",
      "Iteration 1342, loss = 2.09692932\n",
      "Iteration 1343, loss = 2.09635797\n",
      "Iteration 1344, loss = 2.09861924\n",
      "Iteration 1345, loss = 2.09566333\n",
      "Iteration 1346, loss = 2.09652419\n",
      "Iteration 1347, loss = 2.09679036\n",
      "Iteration 1348, loss = 2.09639652\n",
      "Iteration 1349, loss = 2.09966299\n",
      "Iteration 1350, loss = 2.09661792\n",
      "Iteration 1351, loss = 2.09655227\n",
      "Iteration 1352, loss = 2.10023815\n",
      "Iteration 1353, loss = 2.09463032\n",
      "Iteration 1354, loss = 2.09965214\n",
      "Iteration 1355, loss = 2.09631878\n",
      "Iteration 1356, loss = 2.09474496\n",
      "Iteration 1357, loss = 2.09392279\n",
      "Iteration 1358, loss = 2.09601331\n",
      "Iteration 1359, loss = 2.09211142\n",
      "Iteration 1360, loss = 2.09512712\n",
      "Iteration 1361, loss = 2.09455043\n",
      "Iteration 1362, loss = 2.09696545\n",
      "Iteration 1363, loss = 2.09044572\n",
      "Iteration 1364, loss = 2.09245852\n",
      "Iteration 1365, loss = 2.09751991\n",
      "Iteration 1366, loss = 2.09312849\n",
      "Iteration 1367, loss = 2.09305373\n",
      "Iteration 1368, loss = 2.09301345\n",
      "Iteration 1369, loss = 2.08933865\n",
      "Iteration 1370, loss = 2.09327597\n",
      "Iteration 1371, loss = 2.09199005\n",
      "Iteration 1372, loss = 2.09204890\n",
      "Iteration 1373, loss = 2.08962490\n",
      "Iteration 1374, loss = 2.08905102\n",
      "Iteration 1375, loss = 2.08735567\n",
      "Iteration 1376, loss = 2.09106277\n",
      "Iteration 1377, loss = 2.09066655\n",
      "Iteration 1378, loss = 2.09161310\n",
      "Iteration 1379, loss = 2.08961772\n",
      "Iteration 1380, loss = 2.09064787\n",
      "Iteration 1381, loss = 2.08751477\n",
      "Iteration 1382, loss = 2.08866379\n",
      "Iteration 1383, loss = 2.08697024\n",
      "Iteration 1384, loss = 2.08595598\n",
      "Iteration 1385, loss = 2.08899302\n",
      "Iteration 1386, loss = 2.08722399\n",
      "Iteration 1387, loss = 2.08568739\n",
      "Iteration 1388, loss = 2.08673826\n",
      "Iteration 1389, loss = 2.08534296\n",
      "Iteration 1390, loss = 2.08843156\n",
      "Iteration 1391, loss = 2.08931046\n",
      "Iteration 1392, loss = 2.08855465\n",
      "Iteration 1393, loss = 2.08589278\n",
      "Iteration 1394, loss = 2.08528334\n",
      "Iteration 1395, loss = 2.08918479\n",
      "Iteration 1396, loss = 2.08125432\n",
      "Iteration 1397, loss = 2.08641402\n",
      "Iteration 1398, loss = 2.08907093\n",
      "Iteration 1399, loss = 2.08939842\n",
      "Iteration 1400, loss = 2.08297261\n",
      "Iteration 1401, loss = 2.08295486\n",
      "Iteration 1402, loss = 2.08400958\n",
      "Iteration 1403, loss = 2.08013219\n",
      "Iteration 1404, loss = 2.08145775\n",
      "Iteration 1405, loss = 2.08904186\n",
      "Iteration 1406, loss = 2.08470652\n",
      "Iteration 1407, loss = 2.08269223\n",
      "Iteration 1408, loss = 2.08012086\n",
      "Iteration 1409, loss = 2.08335815\n",
      "Iteration 1410, loss = 2.08230892\n",
      "Iteration 1411, loss = 2.08110608\n",
      "Iteration 1412, loss = 2.08358250\n",
      "Iteration 1413, loss = 2.08500204\n",
      "Iteration 1414, loss = 2.08173138\n",
      "Iteration 1415, loss = 2.08039730\n",
      "Iteration 1416, loss = 2.07946088\n",
      "Iteration 1417, loss = 2.08457419\n",
      "Iteration 1418, loss = 2.07841546\n",
      "Iteration 1419, loss = 2.08005678\n",
      "Iteration 1420, loss = 2.08395363\n",
      "Iteration 1421, loss = 2.07852794\n",
      "Iteration 1422, loss = 2.07947197\n",
      "Iteration 1423, loss = 2.07970290\n",
      "Iteration 1424, loss = 2.07676196\n",
      "Iteration 1425, loss = 2.07870152\n",
      "Iteration 1426, loss = 2.07718000\n",
      "Iteration 1427, loss = 2.08048897\n",
      "Iteration 1428, loss = 2.07482562\n",
      "Iteration 1429, loss = 2.07817020\n",
      "Iteration 1430, loss = 2.07595662\n",
      "Iteration 1431, loss = 2.08121010\n",
      "Iteration 1432, loss = 2.07644225\n",
      "Iteration 1433, loss = 2.07967609\n",
      "Iteration 1434, loss = 2.07616262\n",
      "Iteration 1435, loss = 2.07720046\n",
      "Iteration 1436, loss = 2.07520364\n",
      "Iteration 1437, loss = 2.07560849\n",
      "Iteration 1438, loss = 2.07669037\n",
      "Iteration 1439, loss = 2.08016970\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.57148925\n",
      "Iteration 2, loss = 3.65238007\n",
      "Iteration 3, loss = 3.65238530\n",
      "Iteration 4, loss = 3.65233840\n",
      "Iteration 5, loss = 3.65227633\n",
      "Iteration 6, loss = 3.65219957\n",
      "Iteration 7, loss = 3.65212975\n",
      "Iteration 8, loss = 3.65205057\n",
      "Iteration 9, loss = 3.65197423\n",
      "Iteration 10, loss = 3.65189281\n",
      "Iteration 11, loss = 3.65181861\n",
      "Iteration 12, loss = 3.65174358\n",
      "Iteration 13, loss = 3.65167001\n",
      "Iteration 14, loss = 3.65159552\n",
      "Iteration 15, loss = 3.65152088\n",
      "Iteration 16, loss = 3.65144298\n",
      "Iteration 17, loss = 3.65136948\n",
      "Iteration 18, loss = 3.65129356\n",
      "Iteration 19, loss = 3.65121521\n",
      "Iteration 20, loss = 3.65114015\n",
      "Iteration 21, loss = 3.65106405\n",
      "Iteration 22, loss = 3.65098991\n",
      "Iteration 23, loss = 3.65091865\n",
      "Iteration 24, loss = 3.65084966\n",
      "Iteration 25, loss = 3.65077404\n",
      "Iteration 26, loss = 3.65070260\n",
      "Iteration 27, loss = 3.65062971\n",
      "Iteration 28, loss = 3.65055851\n",
      "Iteration 29, loss = 3.65048645\n",
      "Iteration 30, loss = 3.65041554\n",
      "Iteration 31, loss = 3.65034448\n",
      "Iteration 32, loss = 3.65027102\n",
      "Iteration 33, loss = 3.65020255\n",
      "Iteration 34, loss = 3.65013310\n",
      "Iteration 35, loss = 3.65006230\n",
      "Iteration 36, loss = 3.64999401\n",
      "Iteration 37, loss = 3.64992236\n",
      "Iteration 38, loss = 3.64985147\n",
      "Iteration 39, loss = 3.64978787\n",
      "Iteration 40, loss = 3.64971825\n",
      "Iteration 41, loss = 3.64965695\n",
      "Iteration 42, loss = 3.64958949\n",
      "Iteration 43, loss = 3.64952384\n",
      "Iteration 44, loss = 3.64945621\n",
      "Iteration 45, loss = 3.64939275\n",
      "Iteration 46, loss = 3.64932865\n",
      "Iteration 47, loss = 3.64926807\n",
      "Iteration 48, loss = 3.64920323\n",
      "Iteration 49, loss = 3.64913331\n",
      "Iteration 50, loss = 3.64907216\n",
      "Iteration 51, loss = 3.64900653\n",
      "Iteration 52, loss = 3.64894725\n",
      "Iteration 53, loss = 3.64888769\n",
      "Iteration 54, loss = 3.64881838\n",
      "Iteration 55, loss = 3.64876104\n",
      "Iteration 56, loss = 3.64869345\n",
      "Iteration 57, loss = 3.64862802\n",
      "Iteration 58, loss = 3.64856620\n",
      "Iteration 59, loss = 3.64850420\n",
      "Iteration 60, loss = 3.64844119\n",
      "Iteration 61, loss = 3.64838195\n",
      "Iteration 62, loss = 3.64831830\n",
      "Iteration 63, loss = 3.64825651\n",
      "Iteration 64, loss = 3.64819417\n",
      "Iteration 65, loss = 3.64813315\n",
      "Iteration 66, loss = 3.64807035\n",
      "Iteration 67, loss = 3.64800903\n",
      "Iteration 68, loss = 3.64795015\n",
      "Iteration 69, loss = 3.64789605\n",
      "Iteration 70, loss = 3.64783681\n",
      "Iteration 71, loss = 3.64777788\n",
      "Iteration 72, loss = 3.64771474\n",
      "Iteration 73, loss = 3.64765336\n",
      "Iteration 74, loss = 3.64759568\n",
      "Iteration 75, loss = 3.64753588\n",
      "Iteration 76, loss = 3.64748150\n",
      "Iteration 77, loss = 3.64742097\n",
      "Iteration 78, loss = 3.64736755\n",
      "Iteration 79, loss = 3.64731303\n",
      "Iteration 80, loss = 3.64725536\n",
      "Iteration 81, loss = 3.64719970\n",
      "Iteration 82, loss = 3.64714113\n",
      "Iteration 83, loss = 3.64708575\n",
      "Iteration 84, loss = 3.64702712\n",
      "Iteration 85, loss = 3.64697314\n",
      "Iteration 86, loss = 3.64691938\n",
      "Iteration 87, loss = 3.64686214\n",
      "Iteration 88, loss = 3.64680857\n",
      "Iteration 89, loss = 3.64675167\n",
      "Iteration 90, loss = 3.64669399\n",
      "Iteration 91, loss = 3.64664217\n",
      "Iteration 92, loss = 3.64658792\n",
      "Iteration 93, loss = 3.64653207\n",
      "Iteration 94, loss = 3.64647965\n",
      "Iteration 95, loss = 3.64642838\n",
      "Iteration 96, loss = 3.64637807\n",
      "Iteration 97, loss = 3.64632431\n",
      "Iteration 98, loss = 3.64627597\n",
      "Iteration 99, loss = 3.64622379\n",
      "Iteration 100, loss = 3.64617865\n",
      "Iteration 101, loss = 3.64612297\n",
      "Iteration 102, loss = 3.64607945\n",
      "Iteration 103, loss = 3.64601918\n",
      "Iteration 104, loss = 3.64597371\n",
      "Iteration 105, loss = 3.64592415\n",
      "Iteration 106, loss = 3.64587248\n",
      "Iteration 107, loss = 3.64582138\n",
      "Iteration 108, loss = 3.64577079\n",
      "Iteration 109, loss = 3.64572082\n",
      "Iteration 110, loss = 3.64567341\n",
      "Iteration 111, loss = 3.64562543\n",
      "Iteration 112, loss = 3.64557216\n",
      "Iteration 113, loss = 3.64552641\n",
      "Iteration 114, loss = 3.64547639\n",
      "Iteration 115, loss = 3.64542700\n",
      "Iteration 116, loss = 3.64537796\n",
      "Iteration 117, loss = 3.64533050\n",
      "Iteration 118, loss = 3.64528355\n",
      "Iteration 119, loss = 3.64523687\n",
      "Iteration 120, loss = 3.64518755\n",
      "Iteration 121, loss = 3.64514208\n",
      "Iteration 122, loss = 3.64509173\n",
      "Iteration 123, loss = 3.64504456\n",
      "Iteration 124, loss = 3.64499819\n",
      "Iteration 125, loss = 3.64495388\n",
      "Iteration 126, loss = 3.64490659\n",
      "Iteration 127, loss = 3.64485909\n",
      "Iteration 128, loss = 3.64481510\n",
      "Iteration 129, loss = 3.64476977\n",
      "Iteration 130, loss = 3.64472109\n",
      "Iteration 131, loss = 3.64467953\n",
      "Iteration 132, loss = 3.64463113\n",
      "Iteration 133, loss = 3.64458904\n",
      "Iteration 134, loss = 3.64454729\n",
      "Iteration 135, loss = 3.64450179\n",
      "Iteration 136, loss = 3.64445853\n",
      "Iteration 137, loss = 3.64441336\n",
      "Iteration 138, loss = 3.64436978\n",
      "Iteration 139, loss = 3.64432724\n",
      "Iteration 140, loss = 3.64428216\n",
      "Iteration 141, loss = 3.64424166\n",
      "Iteration 142, loss = 3.64419767\n",
      "Iteration 143, loss = 3.64415758\n",
      "Iteration 144, loss = 3.64411294\n",
      "Iteration 145, loss = 3.64407216\n",
      "Iteration 146, loss = 3.64403204\n",
      "Iteration 147, loss = 3.64398745\n",
      "Iteration 148, loss = 3.64394627\n",
      "Iteration 149, loss = 3.64390797\n",
      "Iteration 150, loss = 3.64386550\n",
      "Iteration 151, loss = 3.64382746\n",
      "Iteration 152, loss = 3.64379123\n",
      "Iteration 153, loss = 3.64374996\n",
      "Iteration 154, loss = 3.64371341\n",
      "Iteration 155, loss = 3.64367112\n",
      "Iteration 156, loss = 3.64363435\n",
      "Iteration 157, loss = 3.64359534\n",
      "Iteration 158, loss = 3.64355496\n",
      "Iteration 159, loss = 3.64351725\n",
      "Iteration 160, loss = 3.64347260\n",
      "Iteration 161, loss = 3.64343921\n",
      "Iteration 162, loss = 3.64339682\n",
      "Iteration 163, loss = 3.64335572\n",
      "Iteration 164, loss = 3.64332000\n",
      "Iteration 165, loss = 3.64327915\n",
      "Iteration 166, loss = 3.64324137\n",
      "Iteration 167, loss = 3.64320045\n",
      "Iteration 168, loss = 3.64316410\n",
      "Iteration 169, loss = 3.64312315\n",
      "Iteration 170, loss = 3.64308728\n",
      "Iteration 171, loss = 3.64305310\n",
      "Iteration 172, loss = 3.64301221\n",
      "Iteration 173, loss = 3.64297539\n",
      "Iteration 174, loss = 3.64293660\n",
      "Iteration 175, loss = 3.64290026\n",
      "Iteration 176, loss = 3.64286469\n",
      "Iteration 177, loss = 3.64282865\n",
      "Iteration 178, loss = 3.64279218\n",
      "Iteration 179, loss = 3.64275819\n",
      "Iteration 180, loss = 3.64272137\n",
      "Iteration 181, loss = 3.64268626\n",
      "Iteration 182, loss = 3.64265100\n",
      "Iteration 183, loss = 3.64262040\n",
      "Iteration 184, loss = 3.64258435\n",
      "Iteration 185, loss = 3.64255147\n",
      "Iteration 186, loss = 3.64251594\n",
      "Iteration 187, loss = 3.64248065\n",
      "Iteration 188, loss = 3.64244902\n",
      "Iteration 189, loss = 3.64241150\n",
      "Iteration 190, loss = 3.64237513\n",
      "Iteration 191, loss = 3.64234014\n",
      "Iteration 192, loss = 3.64230357\n",
      "Iteration 193, loss = 3.64226973\n",
      "Iteration 194, loss = 3.64223824\n",
      "Iteration 195, loss = 3.64220186\n",
      "Iteration 196, loss = 3.64216594\n",
      "Iteration 197, loss = 3.64213611\n",
      "Iteration 198, loss = 3.64210148\n",
      "Iteration 199, loss = 3.64207141\n",
      "Iteration 200, loss = 3.64203762\n",
      "Iteration 201, loss = 3.64200492\n",
      "Iteration 202, loss = 3.64197653\n",
      "Iteration 203, loss = 3.64194223\n",
      "Iteration 204, loss = 3.64190908\n",
      "Iteration 205, loss = 3.64188151\n",
      "Iteration 206, loss = 3.64184740\n",
      "Iteration 207, loss = 3.64181474\n",
      "Iteration 208, loss = 3.64178248\n",
      "Iteration 209, loss = 3.64175442\n",
      "Iteration 210, loss = 3.64172035\n",
      "Iteration 211, loss = 3.64168748\n",
      "Iteration 212, loss = 3.64165795\n",
      "Iteration 213, loss = 3.64162292\n",
      "Iteration 214, loss = 3.64159349\n",
      "Iteration 215, loss = 3.64156138\n",
      "Iteration 216, loss = 3.64153183\n",
      "Iteration 217, loss = 3.64149691\n",
      "Iteration 218, loss = 3.64146510\n",
      "Iteration 219, loss = 3.64143208\n",
      "Iteration 220, loss = 3.64140111\n",
      "Iteration 221, loss = 3.64137203\n",
      "Iteration 222, loss = 3.64134211\n",
      "Iteration 223, loss = 3.64131221\n",
      "Iteration 224, loss = 3.64128373\n",
      "Iteration 225, loss = 3.64125263\n",
      "Iteration 226, loss = 3.64122378\n",
      "Iteration 227, loss = 3.64119210\n",
      "Iteration 228, loss = 3.64116076\n",
      "Iteration 229, loss = 3.64113003\n",
      "Iteration 230, loss = 3.64110537\n",
      "Iteration 231, loss = 3.64107368\n",
      "Iteration 232, loss = 3.64104795\n",
      "Iteration 233, loss = 3.64101827\n",
      "Iteration 234, loss = 3.64098947\n",
      "Iteration 235, loss = 3.64096296\n",
      "Iteration 236, loss = 3.64093544\n",
      "Iteration 237, loss = 3.64090927\n",
      "Iteration 238, loss = 3.64088001\n",
      "Iteration 239, loss = 3.64085300\n",
      "Iteration 240, loss = 3.64082485\n",
      "Iteration 241, loss = 3.64079675\n",
      "Iteration 242, loss = 3.64077094\n",
      "Iteration 243, loss = 3.64074342\n",
      "Iteration 244, loss = 3.64071373\n",
      "Iteration 245, loss = 3.64069009\n",
      "Iteration 246, loss = 3.64066754\n",
      "Iteration 247, loss = 3.64063576\n",
      "Iteration 248, loss = 3.64060900\n",
      "Iteration 249, loss = 3.64058552\n",
      "Iteration 250, loss = 3.64055872\n",
      "Iteration 251, loss = 3.64053156\n",
      "Iteration 252, loss = 3.64050570\n",
      "Iteration 253, loss = 3.64048173\n",
      "Iteration 254, loss = 3.64045403\n",
      "Iteration 255, loss = 3.64042866\n",
      "Iteration 256, loss = 3.64040295\n",
      "Iteration 257, loss = 3.64037988\n",
      "Iteration 258, loss = 3.64035118\n",
      "Iteration 259, loss = 3.64032816\n",
      "Iteration 260, loss = 3.64030356\n",
      "Iteration 261, loss = 3.64027906\n",
      "Iteration 262, loss = 3.64025505\n",
      "Iteration 263, loss = 3.64023084\n",
      "Iteration 264, loss = 3.64020762\n",
      "Iteration 265, loss = 3.64017781\n",
      "Iteration 266, loss = 3.64015513\n",
      "Iteration 267, loss = 3.64013262\n",
      "Iteration 268, loss = 3.64010449\n",
      "Iteration 269, loss = 3.64008280\n",
      "Iteration 270, loss = 3.64005605\n",
      "Iteration 271, loss = 3.64003166\n",
      "Iteration 272, loss = 3.64001114\n",
      "Iteration 273, loss = 3.63998716\n",
      "Iteration 274, loss = 3.63996488\n",
      "Iteration 275, loss = 3.63993975\n",
      "Iteration 276, loss = 3.63991966\n",
      "Iteration 277, loss = 3.63989422\n",
      "Iteration 278, loss = 3.63987041\n",
      "Iteration 279, loss = 3.63984543\n",
      "Iteration 280, loss = 3.63982044\n",
      "Iteration 281, loss = 3.63979795\n",
      "Iteration 282, loss = 3.63977527\n",
      "Iteration 283, loss = 3.63975455\n",
      "Iteration 284, loss = 3.63973409\n",
      "Iteration 285, loss = 3.63971237\n",
      "Iteration 286, loss = 3.63968844\n",
      "Iteration 287, loss = 3.63966674\n",
      "Iteration 288, loss = 3.63964564\n",
      "Iteration 289, loss = 3.63962407\n",
      "Iteration 290, loss = 3.63959760\n",
      "Iteration 291, loss = 3.63958044\n",
      "Iteration 292, loss = 3.63955776\n",
      "Iteration 293, loss = 3.63953495\n",
      "Iteration 294, loss = 3.63951556\n",
      "Iteration 295, loss = 3.63949537\n",
      "Iteration 296, loss = 3.63947160\n",
      "Iteration 297, loss = 3.63944744\n",
      "Iteration 298, loss = 3.63942354\n",
      "Iteration 299, loss = 3.63940618\n",
      "Iteration 300, loss = 3.63938225\n",
      "Iteration 301, loss = 3.63936333\n",
      "Iteration 302, loss = 3.63934161\n",
      "Iteration 303, loss = 3.63932654\n",
      "Iteration 304, loss = 3.63930119\n",
      "Iteration 305, loss = 3.63928373\n",
      "Iteration 306, loss = 3.63925895\n",
      "Iteration 307, loss = 3.63923817\n",
      "Iteration 308, loss = 3.63922101\n",
      "Iteration 309, loss = 3.63920033\n",
      "Iteration 310, loss = 3.63917998\n",
      "Iteration 311, loss = 3.63916141\n",
      "Iteration 312, loss = 3.63914170\n",
      "Iteration 313, loss = 3.63912336\n",
      "Iteration 314, loss = 3.63910248\n",
      "Iteration 315, loss = 3.63908272\n",
      "Iteration 316, loss = 3.63906708\n",
      "Iteration 317, loss = 3.63904481\n",
      "Iteration 318, loss = 3.63902578\n",
      "Iteration 319, loss = 3.63900805\n",
      "Iteration 320, loss = 3.63898971\n",
      "Iteration 321, loss = 3.63897337\n",
      "Iteration 322, loss = 3.63895601\n",
      "Iteration 323, loss = 3.63893374\n",
      "Iteration 324, loss = 3.63891451\n",
      "Iteration 325, loss = 3.63889914\n",
      "Iteration 326, loss = 3.63888064\n",
      "Iteration 327, loss = 3.63886221\n",
      "Iteration 328, loss = 3.63884314\n",
      "Iteration 329, loss = 3.63882735\n",
      "Iteration 330, loss = 3.63880730\n",
      "Iteration 331, loss = 3.63878748\n",
      "Iteration 332, loss = 3.63877236\n",
      "Iteration 333, loss = 3.63875502\n",
      "Iteration 334, loss = 3.63873511\n",
      "Iteration 335, loss = 3.63871903\n",
      "Iteration 336, loss = 3.63870324\n",
      "Iteration 337, loss = 3.63868608\n",
      "Iteration 338, loss = 3.63866518\n",
      "Iteration 339, loss = 3.63864929\n",
      "Iteration 340, loss = 3.63862917\n",
      "Iteration 341, loss = 3.63861386\n",
      "Iteration 342, loss = 3.63859621\n",
      "Iteration 343, loss = 3.63857690\n",
      "Iteration 344, loss = 3.63856195\n",
      "Iteration 345, loss = 3.63854315\n",
      "Iteration 346, loss = 3.63852676\n",
      "Iteration 347, loss = 3.63851219\n",
      "Iteration 348, loss = 3.63849170\n",
      "Iteration 349, loss = 3.63847644\n",
      "Iteration 350, loss = 3.63845784\n",
      "Iteration 351, loss = 3.63844405\n",
      "Iteration 352, loss = 3.63842688\n",
      "Iteration 353, loss = 3.63841259\n",
      "Iteration 354, loss = 3.63839688\n",
      "Iteration 355, loss = 3.63838142\n",
      "Iteration 356, loss = 3.63836259\n",
      "Iteration 357, loss = 3.63834854\n",
      "Iteration 358, loss = 3.63832995\n",
      "Iteration 359, loss = 3.63831286\n",
      "Iteration 360, loss = 3.63829755\n",
      "Iteration 361, loss = 3.63828154\n",
      "Iteration 362, loss = 3.63826802\n",
      "Iteration 363, loss = 3.63825111\n",
      "Iteration 364, loss = 3.63823608\n",
      "Iteration 365, loss = 3.63822269\n",
      "Iteration 366, loss = 3.63820597\n",
      "Iteration 367, loss = 3.63819237\n",
      "Iteration 368, loss = 3.63817679\n",
      "Iteration 369, loss = 3.63815973\n",
      "Iteration 370, loss = 3.63814701\n",
      "Iteration 371, loss = 3.63813123\n",
      "Iteration 372, loss = 3.63811803\n",
      "Iteration 373, loss = 3.63810101\n",
      "Iteration 374, loss = 3.63808732\n",
      "Iteration 375, loss = 3.63807258\n",
      "Iteration 376, loss = 3.63805758\n",
      "Iteration 377, loss = 3.63804277\n",
      "Iteration 378, loss = 3.63802704\n",
      "Iteration 379, loss = 3.63801532\n",
      "Iteration 380, loss = 3.63800013\n",
      "Iteration 381, loss = 3.63798601\n",
      "Iteration 382, loss = 3.63797199\n",
      "Iteration 383, loss = 3.63795884\n",
      "Iteration 384, loss = 3.63794256\n",
      "Iteration 385, loss = 3.63792990\n",
      "Iteration 386, loss = 3.63791550\n",
      "Iteration 387, loss = 3.63790124\n",
      "Iteration 388, loss = 3.63788873\n",
      "Iteration 389, loss = 3.63787534\n",
      "Iteration 390, loss = 3.63785940\n",
      "Iteration 391, loss = 3.63784666\n",
      "Iteration 392, loss = 3.63783538\n",
      "Iteration 393, loss = 3.63782099\n",
      "Iteration 394, loss = 3.63780809\n",
      "Iteration 395, loss = 3.63779489\n",
      "Iteration 396, loss = 3.63778357\n",
      "Iteration 397, loss = 3.63777358\n",
      "Iteration 398, loss = 3.63775591\n",
      "Iteration 399, loss = 3.63774468\n",
      "Iteration 400, loss = 3.63773068\n",
      "Iteration 401, loss = 3.63771639\n",
      "Iteration 402, loss = 3.63770195\n",
      "Iteration 403, loss = 3.63768923\n",
      "Iteration 404, loss = 3.63767622\n",
      "Iteration 405, loss = 3.63766165\n",
      "Iteration 406, loss = 3.63765216\n",
      "Iteration 407, loss = 3.63764006\n",
      "Iteration 408, loss = 3.63762813\n",
      "Iteration 409, loss = 3.63761956\n",
      "Iteration 410, loss = 3.63760393\n",
      "Iteration 411, loss = 3.63759388\n",
      "Iteration 412, loss = 3.63758143\n",
      "Iteration 413, loss = 3.63756678\n",
      "Iteration 414, loss = 3.63755544\n",
      "Iteration 415, loss = 3.63754308\n",
      "Iteration 416, loss = 3.63753099\n",
      "Iteration 417, loss = 3.63751855\n",
      "Iteration 418, loss = 3.63750629\n",
      "Iteration 419, loss = 3.63749430\n",
      "Iteration 420, loss = 3.63748257\n",
      "Iteration 421, loss = 3.63747091\n",
      "Iteration 422, loss = 3.63745931\n",
      "Iteration 423, loss = 3.63744700\n",
      "Iteration 424, loss = 3.63743640\n",
      "Iteration 425, loss = 3.63742570\n",
      "Iteration 426, loss = 3.63741292\n",
      "Iteration 427, loss = 3.63740018\n",
      "Iteration 428, loss = 3.63738891\n",
      "Iteration 429, loss = 3.63737904\n",
      "Iteration 430, loss = 3.63736585\n",
      "Iteration 431, loss = 3.63735663\n",
      "Iteration 432, loss = 3.63734225\n",
      "Iteration 433, loss = 3.63733048\n",
      "Iteration 434, loss = 3.63732098\n",
      "Iteration 435, loss = 3.63730893\n",
      "Iteration 436, loss = 3.63729579\n",
      "Iteration 437, loss = 3.63728756\n",
      "Iteration 438, loss = 3.63727325\n",
      "Iteration 439, loss = 3.63726155\n",
      "Iteration 440, loss = 3.63725226\n",
      "Iteration 441, loss = 3.63724015\n",
      "Iteration 442, loss = 3.63722813\n",
      "Iteration 443, loss = 3.63721674\n",
      "Iteration 444, loss = 3.63720959\n",
      "Iteration 445, loss = 3.63719758\n",
      "Iteration 446, loss = 3.63718631\n",
      "Iteration 447, loss = 3.63717730\n",
      "Iteration 448, loss = 3.63716731\n",
      "Iteration 449, loss = 3.63715635\n",
      "Iteration 450, loss = 3.63714600\n",
      "Iteration 451, loss = 3.63713759\n",
      "Iteration 452, loss = 3.63712258\n",
      "Iteration 453, loss = 3.63711591\n",
      "Iteration 454, loss = 3.63710303\n",
      "Iteration 455, loss = 3.63709367\n",
      "Iteration 456, loss = 3.63708451\n",
      "Iteration 457, loss = 3.63707725\n",
      "Iteration 458, loss = 3.63706277\n",
      "Iteration 459, loss = 3.63705620\n",
      "Iteration 460, loss = 3.63704788\n",
      "Iteration 461, loss = 3.63703982\n",
      "Iteration 462, loss = 3.63702869\n",
      "Iteration 463, loss = 3.63701964\n",
      "Iteration 464, loss = 3.63701216\n",
      "Iteration 465, loss = 3.63700264\n",
      "Iteration 466, loss = 3.63699285\n",
      "Iteration 467, loss = 3.63698328\n",
      "Iteration 468, loss = 3.63697556\n",
      "Iteration 469, loss = 3.63696441\n",
      "Iteration 470, loss = 3.63695634\n",
      "Iteration 471, loss = 3.63694621\n",
      "Iteration 472, loss = 3.63693832\n",
      "Iteration 473, loss = 3.63693046\n",
      "Iteration 474, loss = 3.63691895\n",
      "Iteration 475, loss = 3.63690842\n",
      "Iteration 476, loss = 3.63689840\n",
      "Iteration 477, loss = 3.63689141\n",
      "Iteration 478, loss = 3.63688036\n",
      "Iteration 479, loss = 3.63687102\n",
      "Iteration 480, loss = 3.63686176\n",
      "Iteration 481, loss = 3.63685105\n",
      "Iteration 482, loss = 3.63684515\n",
      "Iteration 483, loss = 3.63683461\n",
      "Iteration 484, loss = 3.63682838\n",
      "Iteration 485, loss = 3.63681952\n",
      "Iteration 486, loss = 3.63680933\n",
      "Iteration 487, loss = 3.63680190\n",
      "Iteration 488, loss = 3.63679424\n",
      "Iteration 489, loss = 3.63678721\n",
      "Iteration 490, loss = 3.63677685\n",
      "Iteration 491, loss = 3.63676722\n",
      "Iteration 492, loss = 3.63675862\n",
      "Iteration 493, loss = 3.63675414\n",
      "Iteration 494, loss = 3.63674071\n",
      "Iteration 495, loss = 3.63673363\n",
      "Iteration 496, loss = 3.63672511\n",
      "Iteration 497, loss = 3.63671783\n",
      "Iteration 498, loss = 3.63670748\n",
      "Iteration 499, loss = 3.63669994\n",
      "Iteration 500, loss = 3.63669023\n",
      "Iteration 501, loss = 3.63668327\n",
      "Iteration 502, loss = 3.63667488\n",
      "Iteration 503, loss = 3.63666607\n",
      "Iteration 504, loss = 3.63665717\n",
      "Iteration 505, loss = 3.63665049\n",
      "Iteration 506, loss = 3.63664229\n",
      "Iteration 507, loss = 3.63663396\n",
      "Iteration 508, loss = 3.63662657\n",
      "Iteration 509, loss = 3.63662178\n",
      "Iteration 510, loss = 3.63661386\n",
      "Iteration 511, loss = 3.63660558\n",
      "Iteration 512, loss = 3.63659979\n",
      "Iteration 513, loss = 3.63659099\n",
      "Iteration 514, loss = 3.63658319\n",
      "Iteration 515, loss = 3.63657485\n",
      "Iteration 516, loss = 3.63656746\n",
      "Iteration 517, loss = 3.63655768\n",
      "Iteration 518, loss = 3.63654911\n",
      "Iteration 519, loss = 3.63654204\n",
      "Iteration 520, loss = 3.63653780\n",
      "Iteration 521, loss = 3.63652977\n",
      "Iteration 522, loss = 3.63652090\n",
      "Iteration 523, loss = 3.63651680\n",
      "Iteration 524, loss = 3.63650973\n",
      "Iteration 525, loss = 3.63650265\n",
      "Iteration 526, loss = 3.63649557\n",
      "Iteration 527, loss = 3.63648795\n",
      "Iteration 528, loss = 3.63647901\n",
      "Iteration 529, loss = 3.63647255\n",
      "Iteration 530, loss = 3.63646432\n",
      "Iteration 531, loss = 3.63645568\n",
      "Iteration 532, loss = 3.63645152\n",
      "Iteration 533, loss = 3.63644206\n",
      "Iteration 534, loss = 3.63643675\n",
      "Iteration 535, loss = 3.63642869\n",
      "Iteration 536, loss = 3.63642119\n",
      "Iteration 537, loss = 3.63641650\n",
      "Iteration 538, loss = 3.63640951\n",
      "Iteration 539, loss = 3.63640160\n",
      "Iteration 540, loss = 3.63639504\n",
      "Iteration 541, loss = 3.63638679\n",
      "Iteration 542, loss = 3.63638014\n",
      "Iteration 543, loss = 3.63637241\n",
      "Iteration 544, loss = 3.63636573\n",
      "Iteration 545, loss = 3.63635952\n",
      "Iteration 546, loss = 3.63635141\n",
      "Iteration 547, loss = 3.63634474\n",
      "Iteration 548, loss = 3.63633868\n",
      "Iteration 549, loss = 3.63633409\n",
      "Iteration 550, loss = 3.63632810\n",
      "Iteration 551, loss = 3.63631870\n",
      "Iteration 552, loss = 3.63631401\n",
      "Iteration 553, loss = 3.63630895\n",
      "Iteration 554, loss = 3.63630175\n",
      "Iteration 555, loss = 3.63629457\n",
      "Iteration 556, loss = 3.63628766\n",
      "Iteration 557, loss = 3.63628352\n",
      "Iteration 558, loss = 3.63627813\n",
      "Iteration 559, loss = 3.63627462\n",
      "Iteration 560, loss = 3.63626702\n",
      "Iteration 561, loss = 3.63626078\n",
      "Iteration 562, loss = 3.63625526\n",
      "Iteration 563, loss = 3.63624741\n",
      "Iteration 564, loss = 3.63624099\n",
      "Iteration 565, loss = 3.63623477\n",
      "Iteration 566, loss = 3.63622773\n",
      "Iteration 567, loss = 3.63621975\n",
      "Iteration 568, loss = 3.63621426\n",
      "Iteration 569, loss = 3.63620620\n",
      "Iteration 570, loss = 3.63619912\n",
      "Iteration 571, loss = 3.63619492\n",
      "Iteration 572, loss = 3.63618888\n",
      "Iteration 573, loss = 3.63618207\n",
      "Iteration 574, loss = 3.63617726\n",
      "Iteration 575, loss = 3.63617023\n",
      "Iteration 576, loss = 3.63616493\n",
      "Iteration 577, loss = 3.63615958\n",
      "Iteration 578, loss = 3.63615336\n",
      "Iteration 579, loss = 3.63614643\n",
      "Iteration 580, loss = 3.63614120\n",
      "Iteration 581, loss = 3.63613881\n",
      "Iteration 582, loss = 3.63613055\n",
      "Iteration 583, loss = 3.63612377\n",
      "Iteration 584, loss = 3.63612014\n",
      "Iteration 585, loss = 3.63611318\n",
      "Iteration 586, loss = 3.63610808\n",
      "Iteration 587, loss = 3.63610274\n",
      "Iteration 588, loss = 3.63609759\n",
      "Iteration 589, loss = 3.63609026\n",
      "Iteration 590, loss = 3.63608529\n",
      "Iteration 591, loss = 3.63608076\n",
      "Iteration 592, loss = 3.63607310\n",
      "Iteration 593, loss = 3.63606932\n",
      "Iteration 594, loss = 3.63606214\n",
      "Iteration 595, loss = 3.63606042\n",
      "Iteration 596, loss = 3.63605519\n",
      "Iteration 597, loss = 3.63604801\n",
      "Iteration 598, loss = 3.63604337\n",
      "Iteration 599, loss = 3.63603751\n",
      "Iteration 600, loss = 3.63603240\n",
      "Iteration 601, loss = 3.63602603\n",
      "Iteration 602, loss = 3.63601967\n",
      "Iteration 603, loss = 3.63601622\n",
      "Iteration 604, loss = 3.63600719\n",
      "Iteration 605, loss = 3.63600407\n",
      "Iteration 606, loss = 3.63599955\n",
      "Iteration 607, loss = 3.63599298\n",
      "Iteration 608, loss = 3.63598760\n",
      "Iteration 609, loss = 3.63598217\n",
      "Iteration 610, loss = 3.63597711\n",
      "Iteration 611, loss = 3.63597258\n",
      "Iteration 612, loss = 3.63597065\n",
      "Iteration 613, loss = 3.63596550\n",
      "Iteration 614, loss = 3.63595793\n",
      "Iteration 615, loss = 3.63595169\n",
      "Iteration 616, loss = 3.63594983\n",
      "Iteration 617, loss = 3.63594106\n",
      "Iteration 618, loss = 3.63593897\n",
      "Iteration 619, loss = 3.63593216\n",
      "Iteration 620, loss = 3.63592757\n",
      "Iteration 621, loss = 3.63592055\n",
      "Iteration 622, loss = 3.63591653\n",
      "Iteration 623, loss = 3.63591037\n",
      "Iteration 624, loss = 3.63590924\n",
      "Iteration 625, loss = 3.63590473\n",
      "Iteration 626, loss = 3.63589971\n",
      "Iteration 627, loss = 3.63589526\n",
      "Iteration 628, loss = 3.63588944\n",
      "Iteration 629, loss = 3.63588447\n",
      "Iteration 630, loss = 3.63588176\n",
      "Iteration 631, loss = 3.63587613\n",
      "Iteration 632, loss = 3.63587458\n",
      "Iteration 633, loss = 3.63586794\n",
      "Iteration 634, loss = 3.63586124\n",
      "Iteration 635, loss = 3.63585850\n",
      "Iteration 636, loss = 3.63585105\n",
      "Iteration 637, loss = 3.63585011\n",
      "Iteration 638, loss = 3.63584457\n",
      "Iteration 639, loss = 3.63584055\n",
      "Iteration 640, loss = 3.63583504\n",
      "Iteration 641, loss = 3.63583116\n",
      "Iteration 642, loss = 3.63582710\n",
      "Iteration 643, loss = 3.63582461\n",
      "Iteration 644, loss = 3.63581732\n",
      "Iteration 645, loss = 3.63581035\n",
      "Iteration 646, loss = 3.63580964\n",
      "Iteration 647, loss = 3.63580455\n",
      "Iteration 648, loss = 3.63580110\n",
      "Iteration 649, loss = 3.63579763\n",
      "Iteration 650, loss = 3.63579314\n",
      "Iteration 651, loss = 3.63579029\n",
      "Iteration 652, loss = 3.63578839\n",
      "Iteration 653, loss = 3.63578234\n",
      "Iteration 654, loss = 3.63577840\n",
      "Iteration 655, loss = 3.63577450\n",
      "Iteration 656, loss = 3.63577063\n",
      "Iteration 657, loss = 3.63576659\n",
      "Iteration 658, loss = 3.63576072\n",
      "Iteration 659, loss = 3.63575828\n",
      "Iteration 660, loss = 3.63575378\n",
      "Iteration 661, loss = 3.63575317\n",
      "Iteration 662, loss = 3.63574583\n",
      "Iteration 663, loss = 3.63574390\n",
      "Iteration 664, loss = 3.63574108\n",
      "Iteration 665, loss = 3.63573561\n",
      "Iteration 666, loss = 3.63573260\n",
      "Iteration 667, loss = 3.63572884\n",
      "Iteration 668, loss = 3.63572329\n",
      "Iteration 669, loss = 3.63572035\n",
      "Iteration 670, loss = 3.63571482\n",
      "Iteration 671, loss = 3.63571025\n",
      "Iteration 672, loss = 3.63570850\n",
      "Iteration 673, loss = 3.63570494\n",
      "Iteration 674, loss = 3.63570031\n",
      "Iteration 675, loss = 3.63569611\n",
      "Iteration 676, loss = 3.63569171\n",
      "Iteration 677, loss = 3.63568878\n",
      "Iteration 678, loss = 3.63568638\n",
      "Iteration 679, loss = 3.63568336\n",
      "Iteration 680, loss = 3.63567977\n",
      "Iteration 681, loss = 3.63567576\n",
      "Iteration 682, loss = 3.63567098\n",
      "Iteration 683, loss = 3.63566773\n",
      "Iteration 684, loss = 3.63566374\n",
      "Iteration 685, loss = 3.63566041\n",
      "Iteration 686, loss = 3.63565708\n",
      "Iteration 687, loss = 3.63565385\n",
      "Iteration 688, loss = 3.63565102\n",
      "Iteration 689, loss = 3.63564703\n",
      "Iteration 690, loss = 3.63564659\n",
      "Iteration 691, loss = 3.63563983\n",
      "Iteration 692, loss = 3.63563862\n",
      "Iteration 693, loss = 3.63563470\n",
      "Iteration 694, loss = 3.63563363\n",
      "Iteration 695, loss = 3.63562795\n",
      "Iteration 696, loss = 3.63562462\n",
      "Iteration 697, loss = 3.63562104\n",
      "Iteration 698, loss = 3.63561877\n",
      "Iteration 699, loss = 3.63561509\n",
      "Iteration 700, loss = 3.63561358\n",
      "Iteration 701, loss = 3.63560803\n",
      "Iteration 702, loss = 3.63560693\n",
      "Iteration 703, loss = 3.63559906\n",
      "Iteration 704, loss = 3.63559680\n",
      "Iteration 705, loss = 3.63559574\n",
      "Iteration 706, loss = 3.63559021\n",
      "Iteration 707, loss = 3.63558612\n",
      "Iteration 708, loss = 3.63558510\n",
      "Iteration 709, loss = 3.63558048\n",
      "Iteration 710, loss = 3.63557853\n",
      "Iteration 711, loss = 3.63557356\n",
      "Iteration 712, loss = 3.63557024\n",
      "Iteration 713, loss = 3.63556930\n",
      "Iteration 714, loss = 3.63556306\n",
      "Iteration 715, loss = 3.63556047\n",
      "Iteration 716, loss = 3.63555724\n",
      "Iteration 717, loss = 3.63555392\n",
      "Iteration 718, loss = 3.63555094\n",
      "Iteration 719, loss = 3.63554925\n",
      "Iteration 720, loss = 3.63554466\n",
      "Iteration 721, loss = 3.63554449\n",
      "Iteration 722, loss = 3.63554092\n",
      "Iteration 723, loss = 3.63553843\n",
      "Iteration 724, loss = 3.63553494\n",
      "Iteration 725, loss = 3.63552981\n",
      "Iteration 726, loss = 3.63552671\n",
      "Iteration 727, loss = 3.63552302\n",
      "Iteration 728, loss = 3.63552184\n",
      "Iteration 729, loss = 3.63551795\n",
      "Iteration 730, loss = 3.63551598\n",
      "Iteration 731, loss = 3.63551213\n",
      "Iteration 732, loss = 3.63551171\n",
      "Iteration 733, loss = 3.63550749\n",
      "Iteration 734, loss = 3.63550293\n",
      "Iteration 735, loss = 3.63550075\n",
      "Iteration 736, loss = 3.63549929\n",
      "Iteration 737, loss = 3.63549516\n",
      "Iteration 738, loss = 3.63549485\n",
      "Iteration 739, loss = 3.63549001\n",
      "Iteration 740, loss = 3.63548688\n",
      "Iteration 741, loss = 3.63548585\n",
      "Iteration 742, loss = 3.63548454\n",
      "Iteration 743, loss = 3.63548284\n",
      "Iteration 744, loss = 3.63547912\n",
      "Iteration 745, loss = 3.63547633\n",
      "Iteration 746, loss = 3.63547565\n",
      "Iteration 747, loss = 3.63547276\n",
      "Iteration 748, loss = 3.63547009\n",
      "Iteration 749, loss = 3.63546682\n",
      "Iteration 750, loss = 3.63546363\n",
      "Iteration 751, loss = 3.63545991\n",
      "Iteration 752, loss = 3.63545988\n",
      "Iteration 753, loss = 3.63545612\n",
      "Iteration 754, loss = 3.63545117\n",
      "Iteration 755, loss = 3.63544997\n",
      "Iteration 756, loss = 3.63544937\n",
      "Iteration 757, loss = 3.63544420\n",
      "Iteration 758, loss = 3.63544251\n",
      "Iteration 759, loss = 3.63543997\n",
      "Iteration 760, loss = 3.63543863\n",
      "Iteration 761, loss = 3.63543481\n",
      "Iteration 762, loss = 3.63543404\n",
      "Iteration 763, loss = 3.63543009\n",
      "Iteration 764, loss = 3.63542860\n",
      "Iteration 765, loss = 3.63542324\n",
      "Iteration 766, loss = 3.63542406\n",
      "Iteration 767, loss = 3.63542253\n",
      "Iteration 768, loss = 3.63542131\n",
      "Iteration 769, loss = 3.63541815\n",
      "Iteration 770, loss = 3.63541517\n",
      "Iteration 771, loss = 3.63541483\n",
      "Iteration 772, loss = 3.63541369\n",
      "Iteration 773, loss = 3.63540854\n",
      "Iteration 774, loss = 3.63540789\n",
      "Iteration 775, loss = 3.63540790\n",
      "Iteration 776, loss = 3.63540243\n",
      "Iteration 777, loss = 3.63540109\n",
      "Iteration 778, loss = 3.63540037\n",
      "Iteration 779, loss = 3.63539436\n",
      "Iteration 780, loss = 3.63539405\n",
      "Iteration 781, loss = 3.63539297\n",
      "Iteration 782, loss = 3.63539067\n",
      "Iteration 783, loss = 3.63538864\n",
      "Iteration 784, loss = 3.63538615\n",
      "Iteration 785, loss = 3.63538488\n",
      "Iteration 786, loss = 3.63538211\n",
      "Iteration 787, loss = 3.63537855\n",
      "Iteration 788, loss = 3.63537853\n",
      "Iteration 789, loss = 3.63537542\n",
      "Iteration 790, loss = 3.63537376\n",
      "Iteration 791, loss = 3.63537244\n",
      "Iteration 792, loss = 3.63536849\n",
      "Iteration 793, loss = 3.63536871\n",
      "Iteration 794, loss = 3.63536504\n",
      "Iteration 795, loss = 3.63536410\n",
      "Iteration 796, loss = 3.63536118\n",
      "Iteration 797, loss = 3.63535840\n",
      "Iteration 798, loss = 3.63535695\n",
      "Iteration 799, loss = 3.63535626\n",
      "Iteration 800, loss = 3.63535457\n",
      "Iteration 801, loss = 3.63535093\n",
      "Iteration 802, loss = 3.63534920\n",
      "Iteration 803, loss = 3.63534587\n",
      "Iteration 804, loss = 3.63534382\n",
      "Iteration 805, loss = 3.63534149\n",
      "Iteration 806, loss = 3.63534120\n",
      "Iteration 807, loss = 3.63533847\n",
      "Iteration 808, loss = 3.63533608\n",
      "Iteration 809, loss = 3.63533189\n",
      "Iteration 810, loss = 3.63533149\n",
      "Iteration 811, loss = 3.63532844\n",
      "Iteration 812, loss = 3.63532721\n",
      "Iteration 813, loss = 3.63532510\n",
      "Iteration 814, loss = 3.63532377\n",
      "Iteration 815, loss = 3.63532032\n",
      "Iteration 816, loss = 3.63532000\n",
      "Iteration 817, loss = 3.63531770\n",
      "Iteration 818, loss = 3.63531364\n",
      "Iteration 819, loss = 3.63531450\n",
      "Iteration 820, loss = 3.63531106\n",
      "Iteration 821, loss = 3.63530979\n",
      "Iteration 822, loss = 3.63530656\n",
      "Iteration 823, loss = 3.63530610\n",
      "Iteration 824, loss = 3.63530307\n",
      "Iteration 825, loss = 3.63530152\n",
      "Iteration 826, loss = 3.63529897\n",
      "Iteration 827, loss = 3.63529879\n",
      "Iteration 828, loss = 3.63529606\n",
      "Iteration 829, loss = 3.63529372\n",
      "Iteration 830, loss = 3.63529024\n",
      "Iteration 831, loss = 3.63528823\n",
      "Iteration 832, loss = 3.63528594\n",
      "Iteration 833, loss = 3.63528502\n",
      "Iteration 834, loss = 3.63528185\n",
      "Iteration 835, loss = 3.63528169\n",
      "Iteration 836, loss = 3.63527907\n",
      "Iteration 837, loss = 3.63527885\n",
      "Iteration 838, loss = 3.63527678\n",
      "Iteration 839, loss = 3.63527373\n",
      "Iteration 840, loss = 3.63527247\n",
      "Iteration 841, loss = 3.63527090\n",
      "Iteration 842, loss = 3.63526772\n",
      "Iteration 843, loss = 3.63526823\n",
      "Iteration 844, loss = 3.63526528\n",
      "Iteration 845, loss = 3.63526562\n",
      "Iteration 846, loss = 3.63526296\n",
      "Iteration 847, loss = 3.63526184\n",
      "Iteration 848, loss = 3.63525951\n",
      "Iteration 849, loss = 3.63525757\n",
      "Iteration 850, loss = 3.63525750\n",
      "Iteration 851, loss = 3.63525621\n",
      "Iteration 852, loss = 3.63525266\n",
      "Iteration 853, loss = 3.63525252\n",
      "Iteration 854, loss = 3.63525042\n",
      "Iteration 855, loss = 3.63524916\n",
      "Iteration 856, loss = 3.63524814\n",
      "Iteration 857, loss = 3.63524626\n",
      "Iteration 858, loss = 3.63524460\n",
      "Iteration 859, loss = 3.63524287\n",
      "Iteration 860, loss = 3.63524164\n",
      "Iteration 861, loss = 3.63524021\n",
      "Iteration 862, loss = 3.63524013\n",
      "Iteration 863, loss = 3.63523982\n",
      "Iteration 864, loss = 3.63523714\n",
      "Iteration 865, loss = 3.63523527\n",
      "Iteration 866, loss = 3.63523397\n",
      "Iteration 867, loss = 3.63523246\n",
      "Iteration 868, loss = 3.63523129\n",
      "Iteration 869, loss = 3.63523054\n",
      "Iteration 870, loss = 3.63522883\n",
      "Iteration 871, loss = 3.63522704\n",
      "Iteration 872, loss = 3.63522599\n",
      "Iteration 873, loss = 3.63522328\n",
      "Iteration 874, loss = 3.63522278\n",
      "Iteration 875, loss = 3.63522233\n",
      "Iteration 876, loss = 3.63521969\n",
      "Iteration 877, loss = 3.63522067\n",
      "Iteration 878, loss = 3.63521661\n",
      "Iteration 879, loss = 3.63521734\n",
      "Iteration 880, loss = 3.63521531\n",
      "Iteration 881, loss = 3.63521357\n",
      "Iteration 882, loss = 3.63520934\n",
      "Iteration 883, loss = 3.63521172\n",
      "Iteration 884, loss = 3.63520948\n",
      "Iteration 885, loss = 3.63520794\n",
      "Iteration 886, loss = 3.63520651\n",
      "Iteration 887, loss = 3.63520539\n",
      "Iteration 888, loss = 3.63520297\n",
      "Iteration 889, loss = 3.63520213\n",
      "Iteration 890, loss = 3.63520219\n",
      "Iteration 891, loss = 3.63520124\n",
      "Iteration 892, loss = 3.63520158\n",
      "Iteration 893, loss = 3.63519822\n",
      "Iteration 894, loss = 3.63519685\n",
      "Iteration 895, loss = 3.63519814\n",
      "Iteration 896, loss = 3.63519685\n",
      "Iteration 897, loss = 3.63519497\n",
      "Iteration 898, loss = 3.63519479\n",
      "Iteration 899, loss = 3.63519099\n",
      "Iteration 900, loss = 3.63519114\n",
      "Iteration 901, loss = 3.63519188\n",
      "Iteration 902, loss = 3.63518949\n",
      "Iteration 903, loss = 3.63518574\n",
      "Iteration 904, loss = 3.63518580\n",
      "Iteration 905, loss = 3.63518362\n",
      "Iteration 906, loss = 3.63518510\n",
      "Iteration 907, loss = 3.63518236\n",
      "Iteration 908, loss = 3.63517993\n",
      "Iteration 909, loss = 3.63518003\n",
      "Iteration 910, loss = 3.63517660\n",
      "Iteration 911, loss = 3.63517694\n",
      "Iteration 912, loss = 3.63517527\n",
      "Iteration 913, loss = 3.63517399\n",
      "Iteration 914, loss = 3.63517400\n",
      "Iteration 915, loss = 3.63517212\n",
      "Iteration 916, loss = 3.63516967\n",
      "Iteration 917, loss = 3.63516886\n",
      "Iteration 918, loss = 3.63516814\n",
      "Iteration 919, loss = 3.63516627\n",
      "Iteration 920, loss = 3.63516592\n",
      "Iteration 921, loss = 3.63516422\n",
      "Iteration 922, loss = 3.63516180\n",
      "Iteration 923, loss = 3.63515939\n",
      "Iteration 924, loss = 3.63516049\n",
      "Iteration 925, loss = 3.63515805\n",
      "Iteration 926, loss = 3.63515653\n",
      "Iteration 927, loss = 3.63515448\n",
      "Iteration 928, loss = 3.63515331\n",
      "Iteration 929, loss = 3.63515369\n",
      "Iteration 930, loss = 3.63515051\n",
      "Iteration 931, loss = 3.63515039\n",
      "Iteration 932, loss = 3.63514836\n",
      "Iteration 933, loss = 3.63514972\n",
      "Iteration 934, loss = 3.63514373\n",
      "Iteration 935, loss = 3.63514410\n",
      "Iteration 936, loss = 3.63514305\n",
      "Iteration 937, loss = 3.63514246\n",
      "Iteration 938, loss = 3.63514148\n",
      "Iteration 939, loss = 3.63513925\n",
      "Iteration 940, loss = 3.63513728\n",
      "Iteration 941, loss = 3.63513874\n",
      "Iteration 942, loss = 3.63513724\n",
      "Iteration 943, loss = 3.63513505\n",
      "Iteration 944, loss = 3.63513349\n",
      "Iteration 945, loss = 3.63513419\n",
      "Iteration 946, loss = 3.63513296\n",
      "Iteration 947, loss = 3.63513068\n",
      "Iteration 948, loss = 3.63513072\n",
      "Iteration 949, loss = 3.63513025\n",
      "Iteration 950, loss = 3.63512884\n",
      "Iteration 951, loss = 3.63512585\n",
      "Iteration 952, loss = 3.63512570\n",
      "Iteration 953, loss = 3.63512375\n",
      "Iteration 954, loss = 3.63512091\n",
      "Iteration 955, loss = 3.63512210\n",
      "Iteration 956, loss = 3.63512155\n",
      "Iteration 957, loss = 3.63512247\n",
      "Iteration 958, loss = 3.63511869\n",
      "Iteration 959, loss = 3.63511861\n",
      "Iteration 960, loss = 3.63511807\n",
      "Iteration 961, loss = 3.63511671\n",
      "Iteration 962, loss = 3.63511769\n",
      "Iteration 963, loss = 3.63511427\n",
      "Iteration 964, loss = 3.63511447\n",
      "Iteration 965, loss = 3.63511426\n",
      "Iteration 966, loss = 3.63511024\n",
      "Iteration 967, loss = 3.63511128\n",
      "Iteration 968, loss = 3.63510951\n",
      "Iteration 969, loss = 3.63510842\n",
      "Iteration 970, loss = 3.63510901\n",
      "Iteration 971, loss = 3.63510895\n",
      "Iteration 972, loss = 3.63510755\n",
      "Iteration 973, loss = 3.63510509\n",
      "Iteration 974, loss = 3.63510443\n",
      "Iteration 975, loss = 3.63510228\n",
      "Iteration 976, loss = 3.63510610\n",
      "Iteration 977, loss = 3.63510210\n",
      "Iteration 978, loss = 3.63510345\n",
      "Iteration 979, loss = 3.63510141\n",
      "Iteration 980, loss = 3.63509931\n",
      "Iteration 981, loss = 3.63510069\n",
      "Iteration 982, loss = 3.63509737\n",
      "Iteration 983, loss = 3.63509726\n",
      "Iteration 984, loss = 3.63509589\n",
      "Iteration 985, loss = 3.63509500\n",
      "Iteration 986, loss = 3.63509454\n",
      "Iteration 987, loss = 3.63509444\n",
      "Iteration 988, loss = 3.63509288\n",
      "Iteration 989, loss = 3.63509255\n",
      "Iteration 990, loss = 3.63509200\n",
      "Iteration 991, loss = 3.63509194\n",
      "Iteration 992, loss = 3.63508985\n",
      "Iteration 993, loss = 3.63508997\n",
      "Iteration 994, loss = 3.63508974\n",
      "Iteration 995, loss = 3.63508731\n",
      "Iteration 996, loss = 3.63508762\n",
      "Iteration 997, loss = 3.63508483\n",
      "Iteration 998, loss = 3.63508814\n",
      "Iteration 999, loss = 3.63508461\n",
      "Iteration 1000, loss = 3.63508404\n",
      "Iteration 1001, loss = 3.63508282\n",
      "Iteration 1002, loss = 3.63508066\n",
      "Iteration 1003, loss = 3.63508224\n",
      "Iteration 1004, loss = 3.63508108\n",
      "Iteration 1005, loss = 3.63507877\n",
      "Iteration 1006, loss = 3.63507728\n",
      "Iteration 1007, loss = 3.63507679\n",
      "Iteration 1008, loss = 3.63507610\n",
      "Iteration 1009, loss = 3.63507535\n",
      "Iteration 1010, loss = 3.63507616\n",
      "Iteration 1011, loss = 3.63507420\n",
      "Iteration 1012, loss = 3.63507297\n",
      "Iteration 1013, loss = 3.63507279\n",
      "Iteration 1014, loss = 3.63507182\n",
      "Iteration 1015, loss = 3.63507018\n",
      "Iteration 1016, loss = 3.63506923\n",
      "Iteration 1017, loss = 3.63506974\n",
      "Iteration 1018, loss = 3.63506750\n",
      "Iteration 1019, loss = 3.63506884\n",
      "Iteration 1020, loss = 3.63506671\n",
      "Iteration 1021, loss = 3.63506697\n",
      "Iteration 1022, loss = 3.63506457\n",
      "Iteration 1023, loss = 3.63506396\n",
      "Iteration 1024, loss = 3.63506468\n",
      "Iteration 1025, loss = 3.63506432\n",
      "Iteration 1026, loss = 3.63506390\n",
      "Iteration 1027, loss = 3.63506393\n",
      "Iteration 1028, loss = 3.63506056\n",
      "Iteration 1029, loss = 3.63506086\n",
      "Iteration 1030, loss = 3.63505904\n",
      "Iteration 1031, loss = 3.63505888\n",
      "Iteration 1032, loss = 3.63505763\n",
      "Iteration 1033, loss = 3.63505636\n",
      "Iteration 1034, loss = 3.63505570\n",
      "Iteration 1035, loss = 3.63505481\n",
      "Iteration 1036, loss = 3.63505349\n",
      "Iteration 1037, loss = 3.63505352\n",
      "Iteration 1038, loss = 3.63505196\n",
      "Iteration 1039, loss = 3.63505195\n",
      "Iteration 1040, loss = 3.63505439\n",
      "Iteration 1041, loss = 3.63505039\n",
      "Iteration 1042, loss = 3.63505044\n",
      "Iteration 1043, loss = 3.63504810\n",
      "Iteration 1044, loss = 3.63505102\n",
      "Iteration 1045, loss = 3.63504760\n",
      "Iteration 1046, loss = 3.63504812\n",
      "Iteration 1047, loss = 3.63504762\n",
      "Iteration 1048, loss = 3.63504543\n",
      "Iteration 1049, loss = 3.63504606\n",
      "Iteration 1050, loss = 3.63504428\n",
      "Iteration 1051, loss = 3.63504523\n",
      "Iteration 1052, loss = 3.63504475\n",
      "Iteration 1053, loss = 3.63504294\n",
      "Iteration 1054, loss = 3.63504140\n",
      "Iteration 1055, loss = 3.63504196\n",
      "Iteration 1056, loss = 3.63504187\n",
      "Iteration 1057, loss = 3.63503923\n",
      "Iteration 1058, loss = 3.63503936\n",
      "Iteration 1059, loss = 3.63504031\n",
      "Iteration 1060, loss = 3.63504068\n",
      "Iteration 1061, loss = 3.63503728\n",
      "Iteration 1062, loss = 3.63503633\n",
      "Iteration 1063, loss = 3.63503628\n",
      "Iteration 1064, loss = 3.63503613\n",
      "Iteration 1065, loss = 3.63503475\n",
      "Iteration 1066, loss = 3.63503508\n",
      "Iteration 1067, loss = 3.63503439\n",
      "Iteration 1068, loss = 3.63503465\n",
      "Iteration 1069, loss = 3.63503183\n",
      "Iteration 1070, loss = 3.63503202\n",
      "Iteration 1071, loss = 3.63502934\n",
      "Iteration 1072, loss = 3.63502888\n",
      "Iteration 1073, loss = 3.63503065\n",
      "Iteration 1074, loss = 3.63502938\n",
      "Iteration 1075, loss = 3.63502706\n",
      "Iteration 1076, loss = 3.63502884\n",
      "Iteration 1077, loss = 3.63502696\n",
      "Iteration 1078, loss = 3.63502611\n",
      "Iteration 1079, loss = 3.63502475\n",
      "Iteration 1080, loss = 3.63502579\n",
      "Iteration 1081, loss = 3.63502443\n",
      "Iteration 1082, loss = 3.63502443\n",
      "Iteration 1083, loss = 3.63502418\n",
      "Iteration 1084, loss = 3.63502156\n",
      "Iteration 1085, loss = 3.63502272\n",
      "Iteration 1086, loss = 3.63502076\n",
      "Iteration 1087, loss = 3.63502319\n",
      "Iteration 1088, loss = 3.63502145\n",
      "Iteration 1089, loss = 3.63501832\n",
      "Iteration 1090, loss = 3.63502014\n",
      "Iteration 1091, loss = 3.63501862\n",
      "Iteration 1092, loss = 3.63501800\n",
      "Iteration 1093, loss = 3.63501670\n",
      "Iteration 1094, loss = 3.63501676\n",
      "Iteration 1095, loss = 3.63501666\n",
      "Iteration 1096, loss = 3.63501767\n",
      "Iteration 1097, loss = 3.63501519\n",
      "Iteration 1098, loss = 3.63501403\n",
      "Iteration 1099, loss = 3.63501531\n",
      "Iteration 1100, loss = 3.63501366\n",
      "Iteration 1101, loss = 3.63501381\n",
      "Iteration 1102, loss = 3.63501382\n",
      "Iteration 1103, loss = 3.63501283\n",
      "Iteration 1104, loss = 3.63501025\n",
      "Iteration 1105, loss = 3.63500987\n",
      "Iteration 1106, loss = 3.63500988\n",
      "Iteration 1107, loss = 3.63500947\n",
      "Iteration 1108, loss = 3.63500884\n",
      "Iteration 1109, loss = 3.63500815\n",
      "Iteration 1110, loss = 3.63500785\n",
      "Iteration 1111, loss = 3.63500743\n",
      "Iteration 1112, loss = 3.63500466\n",
      "Iteration 1113, loss = 3.63500704\n",
      "Iteration 1114, loss = 3.63500691\n",
      "Iteration 1115, loss = 3.63500433\n",
      "Iteration 1116, loss = 3.63500512\n",
      "Iteration 1117, loss = 3.63500462\n",
      "Iteration 1118, loss = 3.63500475\n",
      "Iteration 1119, loss = 3.63500412\n",
      "Iteration 1120, loss = 3.63500449\n",
      "Iteration 1121, loss = 3.63500318\n",
      "Iteration 1122, loss = 3.63500146\n",
      "Iteration 1123, loss = 3.63500262\n",
      "Iteration 1124, loss = 3.63500265\n",
      "Iteration 1125, loss = 3.63500231\n",
      "Iteration 1126, loss = 3.63500044\n",
      "Iteration 1127, loss = 3.63499956\n",
      "Iteration 1128, loss = 3.63499995\n",
      "Iteration 1129, loss = 3.63500024\n",
      "Iteration 1130, loss = 3.63499868\n",
      "Iteration 1131, loss = 3.63499875\n",
      "Iteration 1132, loss = 3.63499818\n",
      "Iteration 1133, loss = 3.63499820\n",
      "Iteration 1134, loss = 3.63499734\n",
      "Iteration 1135, loss = 3.63499546\n",
      "Iteration 1136, loss = 3.63499577\n",
      "Iteration 1137, loss = 3.63499608\n",
      "Iteration 1138, loss = 3.63499490\n",
      "Iteration 1139, loss = 3.63499356\n",
      "Iteration 1140, loss = 3.63499267\n",
      "Iteration 1141, loss = 3.63499275\n",
      "Iteration 1142, loss = 3.63499341\n",
      "Iteration 1143, loss = 3.63499274\n",
      "Iteration 1144, loss = 3.63499295\n",
      "Iteration 1145, loss = 3.63499185\n",
      "Iteration 1146, loss = 3.63499135\n",
      "Iteration 1147, loss = 3.63499014\n",
      "Iteration 1148, loss = 3.63499211\n",
      "Iteration 1149, loss = 3.63498911\n",
      "Iteration 1150, loss = 3.63499064\n",
      "Iteration 1151, loss = 3.63498786\n",
      "Iteration 1152, loss = 3.63498805\n",
      "Iteration 1153, loss = 3.63498876\n",
      "Iteration 1154, loss = 3.63498737\n",
      "Iteration 1155, loss = 3.63498830\n",
      "Iteration 1156, loss = 3.63498724\n",
      "Iteration 1157, loss = 3.63498676\n",
      "Iteration 1158, loss = 3.63498527\n",
      "Iteration 1159, loss = 3.63498804\n",
      "Iteration 1160, loss = 3.63498879\n",
      "Iteration 1161, loss = 3.63498518\n",
      "Iteration 1162, loss = 3.63498538\n",
      "Iteration 1163, loss = 3.63498291\n",
      "Iteration 1164, loss = 3.63498437\n",
      "Iteration 1165, loss = 3.63498392\n",
      "Iteration 1166, loss = 3.63498244\n",
      "Iteration 1167, loss = 3.63498374\n",
      "Iteration 1168, loss = 3.63498404\n",
      "Iteration 1169, loss = 3.63498141\n",
      "Iteration 1170, loss = 3.63498084\n",
      "Iteration 1171, loss = 3.63498105\n",
      "Iteration 1172, loss = 3.63497882\n",
      "Iteration 1173, loss = 3.63497863\n",
      "Iteration 1174, loss = 3.63498163\n",
      "Iteration 1175, loss = 3.63497783\n",
      "Iteration 1176, loss = 3.63497862\n",
      "Iteration 1177, loss = 3.63497892\n",
      "Iteration 1178, loss = 3.63498126\n",
      "Iteration 1179, loss = 3.63497769\n",
      "Iteration 1180, loss = 3.63497619\n",
      "Iteration 1181, loss = 3.63497613\n",
      "Iteration 1182, loss = 3.63497724\n",
      "Iteration 1183, loss = 3.63497599\n",
      "Iteration 1184, loss = 3.63497563\n",
      "Iteration 1185, loss = 3.63497501\n",
      "Iteration 1186, loss = 3.63497635\n",
      "Iteration 1187, loss = 3.63497351\n",
      "Iteration 1188, loss = 3.63497394\n",
      "Iteration 1189, loss = 3.63497463\n",
      "Iteration 1190, loss = 3.63497368\n",
      "Iteration 1191, loss = 3.63497497\n",
      "Iteration 1192, loss = 3.63497340\n",
      "Iteration 1193, loss = 3.63497322\n",
      "Iteration 1194, loss = 3.63497423\n",
      "Iteration 1195, loss = 3.63497096\n",
      "Iteration 1196, loss = 3.63497096\n",
      "Iteration 1197, loss = 3.63497195\n",
      "Iteration 1198, loss = 3.63497249\n",
      "Iteration 1199, loss = 3.63497123\n",
      "Iteration 1200, loss = 3.63496990\n",
      "Iteration 1201, loss = 3.63496930\n",
      "Iteration 1202, loss = 3.63496980\n",
      "Iteration 1203, loss = 3.63496981\n",
      "Iteration 1204, loss = 3.63496964\n",
      "Iteration 1205, loss = 3.63497085\n",
      "Iteration 1206, loss = 3.63496870\n",
      "Iteration 1207, loss = 3.63496845\n",
      "Iteration 1208, loss = 3.63496919\n",
      "Iteration 1209, loss = 3.63496866\n",
      "Iteration 1210, loss = 3.63496887\n",
      "Iteration 1211, loss = 3.63496805\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.45278824\n",
      "Iteration 2, loss = 6.72385942\n",
      "Iteration 3, loss = 4.89284673\n",
      "Iteration 4, loss = 4.05246925\n",
      "Iteration 5, loss = 3.65206853\n",
      "Iteration 6, loss = 3.45444873\n",
      "Iteration 7, loss = 3.33861676\n",
      "Iteration 8, loss = 3.26715199\n",
      "Iteration 9, loss = 3.21540188\n",
      "Iteration 10, loss = 3.17711430\n",
      "Iteration 11, loss = 3.14459009\n",
      "Iteration 12, loss = 3.11486145\n",
      "Iteration 13, loss = 3.08879318\n",
      "Iteration 14, loss = 3.06376690\n",
      "Iteration 15, loss = 3.04323468\n",
      "Iteration 16, loss = 3.02414627\n",
      "Iteration 17, loss = 3.00683993\n",
      "Iteration 18, loss = 2.98936273\n",
      "Iteration 19, loss = 2.97616651\n",
      "Iteration 20, loss = 2.96214355\n",
      "Iteration 21, loss = 2.95107497\n",
      "Iteration 22, loss = 2.93896968\n",
      "Iteration 23, loss = 2.92837350\n",
      "Iteration 24, loss = 2.92009683\n",
      "Iteration 25, loss = 2.91522510\n",
      "Iteration 26, loss = 2.90453324\n",
      "Iteration 27, loss = 2.89851570\n",
      "Iteration 28, loss = 2.89171502\n",
      "Iteration 29, loss = 2.88523091\n",
      "Iteration 30, loss = 2.87932095\n",
      "Iteration 31, loss = 2.87436653\n",
      "Iteration 32, loss = 2.86891615\n",
      "Iteration 33, loss = 2.86517963\n",
      "Iteration 34, loss = 2.86046160\n",
      "Iteration 35, loss = 2.85699195\n",
      "Iteration 36, loss = 2.85158604\n",
      "Iteration 37, loss = 2.84934998\n",
      "Iteration 38, loss = 2.84520078\n",
      "Iteration 39, loss = 2.84188571\n",
      "Iteration 40, loss = 2.83926317\n",
      "Iteration 41, loss = 2.83280194\n",
      "Iteration 42, loss = 2.82970506\n",
      "Iteration 43, loss = 2.82560199\n",
      "Iteration 44, loss = 2.82261372\n",
      "Iteration 45, loss = 2.82360480\n",
      "Iteration 46, loss = 2.81727584\n",
      "Iteration 47, loss = 2.81656878\n",
      "Iteration 48, loss = 2.81377199\n",
      "Iteration 49, loss = 2.81116077\n",
      "Iteration 50, loss = 2.80880448\n",
      "Iteration 51, loss = 2.80712548\n",
      "Iteration 52, loss = 2.80531906\n",
      "Iteration 53, loss = 2.80316938\n",
      "Iteration 54, loss = 2.80004469\n",
      "Iteration 55, loss = 2.79974234\n",
      "Iteration 56, loss = 2.79651066\n",
      "Iteration 57, loss = 2.79387460\n",
      "Iteration 58, loss = 2.79247790\n",
      "Iteration 59, loss = 2.79481345\n",
      "Iteration 60, loss = 2.79073653\n",
      "Iteration 61, loss = 2.78827993\n",
      "Iteration 62, loss = 2.78778733\n",
      "Iteration 63, loss = 2.78301368\n",
      "Iteration 64, loss = 2.78284839\n",
      "Iteration 65, loss = 2.78169804\n",
      "Iteration 66, loss = 2.77980769\n",
      "Iteration 67, loss = 2.77695775\n",
      "Iteration 68, loss = 2.77642707\n",
      "Iteration 69, loss = 2.77419305\n",
      "Iteration 70, loss = 2.77633310\n",
      "Iteration 71, loss = 2.77137378\n",
      "Iteration 72, loss = 2.77174855\n",
      "Iteration 73, loss = 2.77252852\n",
      "Iteration 74, loss = 2.76728697\n",
      "Iteration 75, loss = 2.76760008\n",
      "Iteration 76, loss = 2.76550198\n",
      "Iteration 77, loss = 2.76536507\n",
      "Iteration 78, loss = 2.76254910\n",
      "Iteration 79, loss = 2.76397741\n",
      "Iteration 80, loss = 2.76218714\n",
      "Iteration 81, loss = 2.76021225\n",
      "Iteration 82, loss = 2.75809488\n",
      "Iteration 83, loss = 2.75750751\n",
      "Iteration 84, loss = 2.75693898\n",
      "Iteration 85, loss = 2.75826342\n",
      "Iteration 86, loss = 2.75558254\n",
      "Iteration 87, loss = 2.75490940\n",
      "Iteration 88, loss = 2.75482399\n",
      "Iteration 89, loss = 2.75378138\n",
      "Iteration 90, loss = 2.75189665\n",
      "Iteration 91, loss = 2.75249454\n",
      "Iteration 92, loss = 2.75168146\n",
      "Iteration 93, loss = 2.75056621\n",
      "Iteration 94, loss = 2.75073419\n",
      "Iteration 95, loss = 2.74796851\n",
      "Iteration 96, loss = 2.74895126\n",
      "Iteration 97, loss = 2.74809524\n",
      "Iteration 98, loss = 2.74529713\n",
      "Iteration 99, loss = 2.74667902\n",
      "Iteration 100, loss = 2.74554222\n",
      "Iteration 101, loss = 2.74347308\n",
      "Iteration 102, loss = 2.74434264\n",
      "Iteration 103, loss = 2.74357293\n",
      "Iteration 104, loss = 2.74280259\n",
      "Iteration 105, loss = 2.74299133\n",
      "Iteration 106, loss = 2.74249117\n",
      "Iteration 107, loss = 2.74121425\n",
      "Iteration 108, loss = 2.73959962\n",
      "Iteration 109, loss = 2.74006602\n",
      "Iteration 110, loss = 2.74071375\n",
      "Iteration 111, loss = 2.73998201\n",
      "Iteration 112, loss = 2.73757301\n",
      "Iteration 113, loss = 2.73728152\n",
      "Iteration 114, loss = 2.73480348\n",
      "Iteration 115, loss = 2.73434760\n",
      "Iteration 116, loss = 2.73509349\n",
      "Iteration 117, loss = 2.73602416\n",
      "Iteration 118, loss = 2.73254284\n",
      "Iteration 119, loss = 2.73277327\n",
      "Iteration 120, loss = 2.73310301\n",
      "Iteration 121, loss = 2.73046034\n",
      "Iteration 122, loss = 2.73088824\n",
      "Iteration 123, loss = 2.72978240\n",
      "Iteration 124, loss = 2.73006592\n",
      "Iteration 125, loss = 2.72952367\n",
      "Iteration 126, loss = 2.72742417\n",
      "Iteration 127, loss = 2.72823515\n",
      "Iteration 128, loss = 2.72636271\n",
      "Iteration 129, loss = 2.72667528\n",
      "Iteration 130, loss = 2.72746501\n",
      "Iteration 131, loss = 2.72542375\n",
      "Iteration 132, loss = 2.72926494\n",
      "Iteration 133, loss = 2.72481933\n",
      "Iteration 134, loss = 2.72487229\n",
      "Iteration 135, loss = 2.72422083\n",
      "Iteration 136, loss = 2.72222040\n",
      "Iteration 137, loss = 2.72459386\n",
      "Iteration 138, loss = 2.72298464\n",
      "Iteration 139, loss = 2.72427876\n",
      "Iteration 140, loss = 2.72209746\n",
      "Iteration 141, loss = 2.72205910\n",
      "Iteration 142, loss = 2.71953055\n",
      "Iteration 143, loss = 2.72144610\n",
      "Iteration 144, loss = 2.71973480\n",
      "Iteration 145, loss = 2.71835054\n",
      "Iteration 146, loss = 2.71781876\n",
      "Iteration 147, loss = 2.71842784\n",
      "Iteration 148, loss = 2.71731763\n",
      "Iteration 149, loss = 2.71653414\n",
      "Iteration 150, loss = 2.71781700\n",
      "Iteration 151, loss = 2.71594515\n",
      "Iteration 152, loss = 2.71577711\n",
      "Iteration 153, loss = 2.71664379\n",
      "Iteration 154, loss = 2.71364355\n",
      "Iteration 155, loss = 2.71458962\n",
      "Iteration 156, loss = 2.71185717\n",
      "Iteration 157, loss = 2.71163876\n",
      "Iteration 158, loss = 2.71179595\n",
      "Iteration 159, loss = 2.71035874\n",
      "Iteration 160, loss = 2.71125027\n",
      "Iteration 161, loss = 2.71330176\n",
      "Iteration 162, loss = 2.71278147\n",
      "Iteration 163, loss = 2.71060138\n",
      "Iteration 164, loss = 2.70811744\n",
      "Iteration 165, loss = 2.70845144\n",
      "Iteration 166, loss = 2.70971455\n",
      "Iteration 167, loss = 2.70842742\n",
      "Iteration 168, loss = 2.70714022\n",
      "Iteration 169, loss = 2.70877918\n",
      "Iteration 170, loss = 2.70714451\n",
      "Iteration 171, loss = 2.70553759\n",
      "Iteration 172, loss = 2.70545704\n",
      "Iteration 173, loss = 2.70677628\n",
      "Iteration 174, loss = 2.70615501\n",
      "Iteration 175, loss = 2.70549351\n",
      "Iteration 176, loss = 2.70417099\n",
      "Iteration 177, loss = 2.70628271\n",
      "Iteration 178, loss = 2.70331318\n",
      "Iteration 179, loss = 2.70496647\n",
      "Iteration 180, loss = 2.70271570\n",
      "Iteration 181, loss = 2.70161641\n",
      "Iteration 182, loss = 2.70117208\n",
      "Iteration 183, loss = 2.70079872\n",
      "Iteration 184, loss = 2.70099002\n",
      "Iteration 185, loss = 2.69953174\n",
      "Iteration 186, loss = 2.70201683\n",
      "Iteration 187, loss = 2.70130712\n",
      "Iteration 188, loss = 2.69937583\n",
      "Iteration 189, loss = 2.70013609\n",
      "Iteration 190, loss = 2.69898497\n",
      "Iteration 191, loss = 2.69809673\n",
      "Iteration 192, loss = 2.69881316\n",
      "Iteration 193, loss = 2.69803374\n",
      "Iteration 194, loss = 2.69746024\n",
      "Iteration 195, loss = 2.69764319\n",
      "Iteration 196, loss = 2.69618057\n",
      "Iteration 197, loss = 2.69820377\n",
      "Iteration 198, loss = 2.69948840\n",
      "Iteration 199, loss = 2.69622374\n",
      "Iteration 200, loss = 2.69532176\n",
      "Iteration 201, loss = 2.69576013\n",
      "Iteration 202, loss = 2.69455926\n",
      "Iteration 203, loss = 2.69583511\n",
      "Iteration 204, loss = 2.69348274\n",
      "Iteration 205, loss = 2.69628301\n",
      "Iteration 206, loss = 2.69134321\n",
      "Iteration 207, loss = 2.69283542\n",
      "Iteration 208, loss = 2.69247375\n",
      "Iteration 209, loss = 2.69084364\n",
      "Iteration 210, loss = 2.69221796\n",
      "Iteration 211, loss = 2.69092730\n",
      "Iteration 212, loss = 2.69111346\n",
      "Iteration 213, loss = 2.69013974\n",
      "Iteration 214, loss = 2.69143661\n",
      "Iteration 215, loss = 2.68996530\n",
      "Iteration 216, loss = 2.69017450\n",
      "Iteration 217, loss = 2.68960298\n",
      "Iteration 218, loss = 2.68791928\n",
      "Iteration 219, loss = 2.69089839\n",
      "Iteration 220, loss = 2.68982806\n",
      "Iteration 221, loss = 2.69017626\n",
      "Iteration 222, loss = 2.68811126\n",
      "Iteration 223, loss = 2.68640778\n",
      "Iteration 224, loss = 2.68706594\n",
      "Iteration 225, loss = 2.68515510\n",
      "Iteration 226, loss = 2.68649680\n",
      "Iteration 227, loss = 2.68736601\n",
      "Iteration 228, loss = 2.68617008\n",
      "Iteration 229, loss = 2.68428614\n",
      "Iteration 230, loss = 2.68560623\n",
      "Iteration 231, loss = 2.68417152\n",
      "Iteration 232, loss = 2.68606301\n",
      "Iteration 233, loss = 2.68438055\n",
      "Iteration 234, loss = 2.68278045\n",
      "Iteration 235, loss = 2.68371482\n",
      "Iteration 236, loss = 2.68442599\n",
      "Iteration 237, loss = 2.68488883\n",
      "Iteration 238, loss = 2.68299964\n",
      "Iteration 239, loss = 2.68261650\n",
      "Iteration 240, loss = 2.68057869\n",
      "Iteration 241, loss = 2.68092184\n",
      "Iteration 242, loss = 2.68138366\n",
      "Iteration 243, loss = 2.68029379\n",
      "Iteration 244, loss = 2.68010612\n",
      "Iteration 245, loss = 2.68397489\n",
      "Iteration 246, loss = 2.68121211\n",
      "Iteration 247, loss = 2.67971317\n",
      "Iteration 248, loss = 2.67990256\n",
      "Iteration 249, loss = 2.67724547\n",
      "Iteration 250, loss = 2.67942809\n",
      "Iteration 251, loss = 2.67869648\n",
      "Iteration 252, loss = 2.67955445\n",
      "Iteration 253, loss = 2.67793700\n",
      "Iteration 254, loss = 2.67866254\n",
      "Iteration 255, loss = 2.67872092\n",
      "Iteration 256, loss = 2.67797640\n",
      "Iteration 257, loss = 2.67542572\n",
      "Iteration 258, loss = 2.67391002\n",
      "Iteration 259, loss = 2.67400029\n",
      "Iteration 260, loss = 2.67783368\n",
      "Iteration 261, loss = 2.67600443\n",
      "Iteration 262, loss = 2.67403534\n",
      "Iteration 263, loss = 2.67534339\n",
      "Iteration 264, loss = 2.67612559\n",
      "Iteration 265, loss = 2.67341031\n",
      "Iteration 266, loss = 2.67454114\n",
      "Iteration 267, loss = 2.67401028\n",
      "Iteration 268, loss = 2.67375795\n",
      "Iteration 269, loss = 2.67345640\n",
      "Iteration 270, loss = 2.67241108\n",
      "Iteration 271, loss = 2.67181810\n",
      "Iteration 272, loss = 2.67080631\n",
      "Iteration 273, loss = 2.67193126\n",
      "Iteration 274, loss = 2.67319001\n",
      "Iteration 275, loss = 2.67138975\n",
      "Iteration 276, loss = 2.67184616\n",
      "Iteration 277, loss = 2.67051008\n",
      "Iteration 278, loss = 2.67040179\n",
      "Iteration 279, loss = 2.67075722\n",
      "Iteration 280, loss = 2.67130752\n",
      "Iteration 281, loss = 2.67133209\n",
      "Iteration 282, loss = 2.66755931\n",
      "Iteration 283, loss = 2.66792530\n",
      "Iteration 284, loss = 2.66652721\n",
      "Iteration 285, loss = 2.66809386\n",
      "Iteration 286, loss = 2.66796727\n",
      "Iteration 287, loss = 2.66787400\n",
      "Iteration 288, loss = 2.66820790\n",
      "Iteration 289, loss = 2.66738925\n",
      "Iteration 290, loss = 2.66870198\n",
      "Iteration 291, loss = 2.66634350\n",
      "Iteration 292, loss = 2.66630141\n",
      "Iteration 293, loss = 2.66791605\n",
      "Iteration 294, loss = 2.66860570\n",
      "Iteration 295, loss = 2.66477752\n",
      "Iteration 296, loss = 2.66619930\n",
      "Iteration 297, loss = 2.66515699\n",
      "Iteration 298, loss = 2.66357579\n",
      "Iteration 299, loss = 2.66518243\n",
      "Iteration 300, loss = 2.66440132\n",
      "Iteration 301, loss = 2.66257363\n",
      "Iteration 302, loss = 2.66538398\n",
      "Iteration 303, loss = 2.66392239\n",
      "Iteration 304, loss = 2.66298199\n",
      "Iteration 305, loss = 2.66108581\n",
      "Iteration 306, loss = 2.66123287\n",
      "Iteration 307, loss = 2.66527667\n",
      "Iteration 308, loss = 2.66115587\n",
      "Iteration 309, loss = 2.66204631\n",
      "Iteration 310, loss = 2.66216131\n",
      "Iteration 311, loss = 2.66317472\n",
      "Iteration 312, loss = 2.66113645\n",
      "Iteration 313, loss = 2.66172329\n",
      "Iteration 314, loss = 2.66114195\n",
      "Iteration 315, loss = 2.65989364\n",
      "Iteration 316, loss = 2.66168918\n",
      "Iteration 317, loss = 2.65922890\n",
      "Iteration 318, loss = 2.65905016\n",
      "Iteration 319, loss = 2.65845736\n",
      "Iteration 320, loss = 2.66162472\n",
      "Iteration 321, loss = 2.65935617\n",
      "Iteration 322, loss = 2.65670677\n",
      "Iteration 323, loss = 2.65709256\n",
      "Iteration 324, loss = 2.65807848\n",
      "Iteration 325, loss = 2.65660829\n",
      "Iteration 326, loss = 2.65632122\n",
      "Iteration 327, loss = 2.65806251\n",
      "Iteration 328, loss = 2.65491447\n",
      "Iteration 329, loss = 2.65690090\n",
      "Iteration 330, loss = 2.65755402\n",
      "Iteration 331, loss = 2.65790831\n",
      "Iteration 332, loss = 2.65700673\n",
      "Iteration 333, loss = 2.65586995\n",
      "Iteration 334, loss = 2.65940944\n",
      "Iteration 335, loss = 2.65333923\n",
      "Iteration 336, loss = 2.65384337\n",
      "Iteration 337, loss = 2.65294211\n",
      "Iteration 338, loss = 2.65374707\n",
      "Iteration 339, loss = 2.65370357\n",
      "Iteration 340, loss = 2.65386352\n",
      "Iteration 341, loss = 2.65353721\n",
      "Iteration 342, loss = 2.65325841\n",
      "Iteration 343, loss = 2.65264899\n",
      "Iteration 344, loss = 2.65233473\n",
      "Iteration 345, loss = 2.65569088\n",
      "Iteration 346, loss = 2.65243966\n",
      "Iteration 347, loss = 2.65154702\n",
      "Iteration 348, loss = 2.65095919\n",
      "Iteration 349, loss = 2.65294406\n",
      "Iteration 350, loss = 2.65060689\n",
      "Iteration 351, loss = 2.65048164\n",
      "Iteration 352, loss = 2.64951020\n",
      "Iteration 353, loss = 2.64928390\n",
      "Iteration 354, loss = 2.64814999\n",
      "Iteration 355, loss = 2.65019012\n",
      "Iteration 356, loss = 2.65030943\n",
      "Iteration 357, loss = 2.65115965\n",
      "Iteration 358, loss = 2.64738929\n",
      "Iteration 359, loss = 2.65074757\n",
      "Iteration 360, loss = 2.64781130\n",
      "Iteration 361, loss = 2.65111380\n",
      "Iteration 362, loss = 2.64848018\n",
      "Iteration 363, loss = 2.64919804\n",
      "Iteration 364, loss = 2.64874184\n",
      "Iteration 365, loss = 2.64690427\n",
      "Iteration 366, loss = 2.64643538\n",
      "Iteration 367, loss = 2.64821448\n",
      "Iteration 368, loss = 2.64481455\n",
      "Iteration 369, loss = 2.64792223\n",
      "Iteration 370, loss = 2.64453312\n",
      "Iteration 371, loss = 2.64529025\n",
      "Iteration 372, loss = 2.64714568\n",
      "Iteration 373, loss = 2.64624870\n",
      "Iteration 374, loss = 2.64823694\n",
      "Iteration 375, loss = 2.64851838\n",
      "Iteration 376, loss = 2.64759220\n",
      "Iteration 377, loss = 2.64398522\n",
      "Iteration 378, loss = 2.64533146\n",
      "Iteration 379, loss = 2.64652098\n",
      "Iteration 380, loss = 2.64447390\n",
      "Iteration 381, loss = 2.64551519\n",
      "Iteration 382, loss = 2.64445405\n",
      "Iteration 383, loss = 2.64317952\n",
      "Iteration 384, loss = 2.64471722\n",
      "Iteration 385, loss = 2.64212168\n",
      "Iteration 386, loss = 2.64228836\n",
      "Iteration 387, loss = 2.64069274\n",
      "Iteration 388, loss = 2.64253658\n",
      "Iteration 389, loss = 2.64325190\n",
      "Iteration 390, loss = 2.64357310\n",
      "Iteration 391, loss = 2.64235425\n",
      "Iteration 392, loss = 2.64290149\n",
      "Iteration 393, loss = 2.64326189\n",
      "Iteration 394, loss = 2.63971029\n",
      "Iteration 395, loss = 2.64293501\n",
      "Iteration 396, loss = 2.63871464\n",
      "Iteration 397, loss = 2.64201133\n",
      "Iteration 398, loss = 2.63985262\n",
      "Iteration 399, loss = 2.64158755\n",
      "Iteration 400, loss = 2.63981009\n",
      "Iteration 401, loss = 2.63721571\n",
      "Iteration 402, loss = 2.63750568\n",
      "Iteration 403, loss = 2.63693205\n",
      "Iteration 404, loss = 2.64144640\n",
      "Iteration 405, loss = 2.63949380\n",
      "Iteration 406, loss = 2.64007480\n",
      "Iteration 407, loss = 2.63948912\n",
      "Iteration 408, loss = 2.64077239\n",
      "Iteration 409, loss = 2.63664324\n",
      "Iteration 410, loss = 2.63694515\n",
      "Iteration 411, loss = 2.63842676\n",
      "Iteration 412, loss = 2.63525998\n",
      "Iteration 413, loss = 2.63817016\n",
      "Iteration 414, loss = 2.63451310\n",
      "Iteration 415, loss = 2.63563106\n",
      "Iteration 416, loss = 2.63606893\n",
      "Iteration 417, loss = 2.63619559\n",
      "Iteration 418, loss = 2.63540135\n",
      "Iteration 419, loss = 2.63774359\n",
      "Iteration 420, loss = 2.63321139\n",
      "Iteration 421, loss = 2.63473781\n",
      "Iteration 422, loss = 2.63271369\n",
      "Iteration 423, loss = 2.63542679\n",
      "Iteration 424, loss = 2.63542592\n",
      "Iteration 425, loss = 2.63544041\n",
      "Iteration 426, loss = 2.63313581\n",
      "Iteration 427, loss = 2.63414017\n",
      "Iteration 428, loss = 2.63471666\n",
      "Iteration 429, loss = 2.63238272\n",
      "Iteration 430, loss = 2.63211303\n",
      "Iteration 431, loss = 2.63422570\n",
      "Iteration 432, loss = 2.63351972\n",
      "Iteration 433, loss = 2.63662057\n",
      "Iteration 434, loss = 2.63335038\n",
      "Iteration 435, loss = 2.63168682\n",
      "Iteration 436, loss = 2.63389264\n",
      "Iteration 437, loss = 2.63102526\n",
      "Iteration 438, loss = 2.63273614\n",
      "Iteration 439, loss = 2.63167046\n",
      "Iteration 440, loss = 2.63321743\n",
      "Iteration 441, loss = 2.63306276\n",
      "Iteration 442, loss = 2.62891816\n",
      "Iteration 443, loss = 2.62970511\n",
      "Iteration 444, loss = 2.62901322\n",
      "Iteration 445, loss = 2.63129717\n",
      "Iteration 446, loss = 2.62943684\n",
      "Iteration 447, loss = 2.63396774\n",
      "Iteration 448, loss = 2.63129245\n",
      "Iteration 449, loss = 2.63038977\n",
      "Iteration 450, loss = 2.62957116\n",
      "Iteration 451, loss = 2.63252804\n",
      "Iteration 452, loss = 2.63110887\n",
      "Iteration 453, loss = 2.63004556\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "data,LR,SVM,MLP,LR_pca,SVM_pca,MLP_pca =classifier(_X,_y,\"allFace.csv\",30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataname =  allFace.csv \n",
      " LR =  [0.54633472 0.47994467 0.54633472] \n",
      " SVM=  [[0.96957123 0.01798064 0.96403873]\n",
      " [0.96957123 0.01798064 0.96403873]] \n",
      " MLP=  [[0.02074689 0.07883817]\n",
      " [0.01798064 0.01798064]] \n",
      "--------------\n",
      " LR_pca=  [0.08713693 0.07607192 0.0857538 ] \n",
      " SVM_pca=  [[0.10511757 0.22544952 0.11341632]\n",
      " [0.10511757 0.22544952 0.11341632]] \n",
      " MLP_pca=  [[0.18810512 0.08990318]\n",
      " [0.17427386 0.06639004]]\n"
     ]
    }
   ],
   "source": [
    "print(\"dataname = \",data,\"\\n\",\"LR = \",LR,\"\\n\",\"SVM= \",SVM,\"\\n\",\"MLP= \",MLP,\\\n",
    "      \"\\n--------------\\n\",\\\n",
    "      \"LR_pca= \",LR_pca,\"\\n\",\"SVM_pca= \",SVM_pca,\"\\n\",\"MLP_pca= \",MLP_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>實驗結果觀察</h3>\n",
    "根據以上實驗結果, 只有SVM能夠達到較高的正確率,就連羅集思迴歸都只有0.5的正確率,且經過pca的資料集正確率相當低,因此對於此資料集,非常不建議進行pca,且只建議使用SVM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
