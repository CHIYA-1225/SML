{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>使用資料: wine.csv</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('E:\\\\淺度機器學習\\\\data\\\\wine.csv')\n",
    "X = np.array(df.iloc[:, :-1]) # 排 除 最 後 一 欄 標 籤\n",
    "y = np.array(df.iloc[:, -1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>邏吉斯迴歸</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing data 7:3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.30) \n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.15%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.96      0.98        25\n",
      "           2       0.94      1.00      0.97        17\n",
      "           3       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.99      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "solver = 'lbfgs' # 'lbfgs' is the default\n",
    "# solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "clf_original = LogisticRegression(solver = solver, **opts)\n",
    "\n",
    "clf_original.fit(X_train_, y_train)\n",
    "y_pred = clf_original.predict(X_test_)\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "#print(f\"{clf_original.score(X_test_, y_test):.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>多加一步PCA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.74%\n",
      "\n",
      "90.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_)\n",
    "Z_test = pca.transform(X_test_)\n",
    "\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "solver = 'lbfgs' # 'lbfgs' is the default\n",
    "# solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "clf_PCA = LogisticRegression(solver = solver, **opts)\n",
    "\n",
    "clf_PCA.fit(Z_train, y_train)\n",
    "y_pred = clf_PCA.predict(Z_test)\n",
    "print(f\"{clf_PCA.score(Z_test, y_test):.2%}\\n\")\n",
    "#print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LogisticRegressionCV</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.44%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.88      0.94        25\n",
      "           2       0.85      1.00      0.92        17\n",
      "           3       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.96      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "Cs = np.logspace(-5, 5, 20)\n",
    "clf_original = LogisticRegressionCV(solver = solver, \\\n",
    "Cs=Cs, **opts)\n",
    "\n",
    "clf_original.fit(X_train_, y_train)\n",
    "y_pred = clf_original.predict(X_test_)\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "#print(f\"{clf_original.score(X_test_, y_test):.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>支援向量機 SVM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        25\n",
      "           2       0.94      0.88      0.91        17\n",
      "           3       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.93      0.94        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n",
      "0.9444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "C = 1 # SVM regularization parameter\n",
    "opts = dict(C = C, tol = 1e-6, max_iter = int(1e6))\n",
    "# opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "# tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel=\"linear\", **opts)\n",
    "# clf_svm = SVC(kernel=\"rbf\", gamma=0.2, **opts)\n",
    "# clf_svm = SVC(kernel=\"poly\", degree=3, gamma=\"auto\", **opts)\n",
    "# clf_svm = LinearSVC(**opts) # one vs the rest\n",
    "\n",
    "\n",
    "clf_svm.fit(X_train, y_train)\n",
    "predictions = clf_svm.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06119647\n",
      "Iteration 2, loss = 1.05227285\n",
      "Iteration 3, loss = 1.04361727\n",
      "Iteration 4, loss = 1.03555977\n",
      "Iteration 5, loss = 1.02809891\n",
      "Iteration 6, loss = 1.02110684\n",
      "Iteration 7, loss = 1.01463753\n",
      "Iteration 8, loss = 1.00902588\n",
      "Iteration 9, loss = 1.00476229\n",
      "Iteration 10, loss = 1.00197527\n",
      "Iteration 11, loss = 1.00018917\n",
      "Iteration 12, loss = 0.99890023\n",
      "Iteration 13, loss = 0.99788390\n",
      "Iteration 14, loss = 0.99708114\n",
      "Iteration 15, loss = 0.99639886\n",
      "Iteration 16, loss = 0.99565484\n",
      "Iteration 17, loss = 0.99477295\n",
      "Iteration 18, loss = 0.99388828\n",
      "Iteration 19, loss = 0.99310644\n",
      "Iteration 20, loss = 0.99222553\n",
      "Iteration 21, loss = 0.99092081\n",
      "Iteration 22, loss = 0.98917886\n",
      "Iteration 23, loss = 0.98725581\n",
      "Iteration 24, loss = 0.98543714\n",
      "Iteration 25, loss = 0.98388701\n",
      "Iteration 26, loss = 0.98260789\n",
      "Iteration 27, loss = 0.98148792\n",
      "Iteration 28, loss = 0.98038608\n",
      "Iteration 29, loss = 0.97919957\n",
      "Iteration 30, loss = 0.97788872\n",
      "Iteration 31, loss = 0.97647238\n",
      "Iteration 32, loss = 0.97501230\n",
      "Iteration 33, loss = 0.97359376\n",
      "Iteration 34, loss = 0.97230143\n",
      "Iteration 35, loss = 0.97118597\n",
      "Iteration 36, loss = 0.97022645\n",
      "Iteration 37, loss = 0.96931608\n",
      "Iteration 38, loss = 0.96830510\n",
      "Iteration 39, loss = 0.96708485\n",
      "Iteration 40, loss = 0.96563825\n",
      "Iteration 41, loss = 0.96401080\n",
      "Iteration 42, loss = 0.96223768\n",
      "Iteration 43, loss = 0.96029291\n",
      "Iteration 44, loss = 0.95809403\n",
      "Iteration 45, loss = 0.95554990\n",
      "Iteration 46, loss = 0.95263752\n",
      "Iteration 47, loss = 0.94955008\n",
      "Iteration 48, loss = 0.94687514\n",
      "Iteration 49, loss = 0.94532621\n",
      "Iteration 50, loss = 0.94476238\n",
      "Iteration 51, loss = 0.94428893\n",
      "Iteration 52, loss = 0.94341055\n",
      "Iteration 53, loss = 0.94209877\n",
      "Iteration 54, loss = 0.94048029\n",
      "Iteration 55, loss = 0.93870596\n",
      "Iteration 56, loss = 0.93692547\n",
      "Iteration 57, loss = 0.93526702\n",
      "Iteration 58, loss = 0.93380879\n",
      "Iteration 59, loss = 0.93255883\n",
      "Iteration 60, loss = 0.93145170\n",
      "Iteration 61, loss = 0.93037480\n",
      "Iteration 62, loss = 0.92922648\n",
      "Iteration 63, loss = 0.92796434\n",
      "Iteration 64, loss = 0.92661070\n",
      "Iteration 65, loss = 0.92522363\n",
      "Iteration 66, loss = 0.92386286\n",
      "Iteration 67, loss = 0.92256606\n",
      "Iteration 68, loss = 0.92134032\n",
      "Iteration 69, loss = 0.92016238\n",
      "Iteration 70, loss = 0.91898919\n",
      "Iteration 71, loss = 0.91778388\n",
      "Iteration 72, loss = 0.91653189\n",
      "Iteration 73, loss = 0.91524092\n",
      "Iteration 74, loss = 0.91392736\n",
      "Iteration 75, loss = 0.91260747\n",
      "Iteration 76, loss = 0.91129723\n",
      "Iteration 77, loss = 0.91000620\n",
      "Iteration 78, loss = 0.90874795\n",
      "Iteration 79, loss = 0.90749635\n",
      "Iteration 80, loss = 0.90624141\n",
      "Iteration 81, loss = 0.90494542\n",
      "Iteration 82, loss = 0.90361997\n",
      "Iteration 83, loss = 0.90224390\n",
      "Iteration 84, loss = 0.90084435\n",
      "Iteration 85, loss = 0.89942441\n",
      "Iteration 86, loss = 0.89797207\n",
      "Iteration 87, loss = 0.89650667\n",
      "Iteration 88, loss = 0.89498749\n",
      "Iteration 89, loss = 0.89340960\n",
      "Iteration 90, loss = 0.89176841\n",
      "Iteration 91, loss = 0.89005106\n",
      "Iteration 92, loss = 0.88819899\n",
      "Iteration 93, loss = 0.88618386\n",
      "Iteration 94, loss = 0.88403893\n",
      "Iteration 95, loss = 0.88164384\n",
      "Iteration 96, loss = 0.87911373\n",
      "Iteration 97, loss = 0.87731207\n",
      "Iteration 98, loss = 0.87476225\n",
      "Iteration 99, loss = 0.87211085\n",
      "Iteration 100, loss = 0.86976262\n",
      "Iteration 101, loss = 0.86733634\n",
      "Iteration 102, loss = 0.86481031\n",
      "Iteration 103, loss = 0.86216045\n",
      "Iteration 104, loss = 0.85940766\n",
      "Iteration 105, loss = 0.85685509\n",
      "Iteration 106, loss = 0.85437787\n",
      "Iteration 107, loss = 0.85176837\n",
      "Iteration 108, loss = 0.84908353\n",
      "Iteration 109, loss = 0.84652878\n",
      "Iteration 110, loss = 0.84406790\n",
      "Iteration 111, loss = 0.84154802\n",
      "Iteration 112, loss = 0.83898212\n",
      "Iteration 113, loss = 0.83650531\n",
      "Iteration 114, loss = 0.83415557\n",
      "Iteration 115, loss = 0.83172905\n",
      "Iteration 116, loss = 0.82928663\n",
      "Iteration 117, loss = 0.82694955\n",
      "Iteration 118, loss = 0.82466275\n",
      "Iteration 119, loss = 0.82232403\n",
      "Iteration 120, loss = 0.82003028\n",
      "Iteration 121, loss = 0.81779767\n",
      "Iteration 122, loss = 0.81558430\n",
      "Iteration 123, loss = 0.81336715\n",
      "Iteration 124, loss = 0.81116968\n",
      "Iteration 125, loss = 0.80903475\n",
      "Iteration 126, loss = 0.80689942\n",
      "Iteration 127, loss = 0.80476197\n",
      "Iteration 128, loss = 0.80267001\n",
      "Iteration 129, loss = 0.80060148\n",
      "Iteration 130, loss = 0.79853366\n",
      "Iteration 131, loss = 0.79647179\n",
      "Iteration 132, loss = 0.79444563\n",
      "Iteration 133, loss = 0.79239949\n",
      "Iteration 134, loss = 0.79019984\n",
      "Iteration 135, loss = 0.78712840\n",
      "Iteration 136, loss = 0.78152276\n",
      "Iteration 137, loss = 0.77705626\n",
      "Iteration 138, loss = 0.77706872\n",
      "Iteration 139, loss = 0.77331562\n",
      "Iteration 140, loss = 0.76932788\n",
      "Iteration 141, loss = 0.76774503\n",
      "Iteration 142, loss = 0.76566411\n",
      "Iteration 143, loss = 0.76221348\n",
      "Iteration 144, loss = 0.75892016\n",
      "Iteration 145, loss = 0.75684262\n",
      "Iteration 146, loss = 0.75475336\n",
      "Iteration 147, loss = 0.75181881\n",
      "Iteration 148, loss = 0.74884563\n",
      "Iteration 149, loss = 0.74658685\n",
      "Iteration 150, loss = 0.74459743\n",
      "Iteration 151, loss = 0.74203516\n",
      "Iteration 152, loss = 0.73923502\n",
      "Iteration 153, loss = 0.73692505\n",
      "Iteration 154, loss = 0.73487432\n",
      "Iteration 155, loss = 0.73251988\n",
      "Iteration 156, loss = 0.72990830\n",
      "Iteration 157, loss = 0.72751066\n",
      "Iteration 158, loss = 0.72535898\n",
      "Iteration 159, loss = 0.72308817\n",
      "Iteration 160, loss = 0.72064498\n",
      "Iteration 161, loss = 0.71824007\n",
      "Iteration 162, loss = 0.71597636\n",
      "Iteration 163, loss = 0.71375989\n",
      "Iteration 164, loss = 0.71145460\n",
      "Iteration 165, loss = 0.70908419\n",
      "Iteration 166, loss = 0.70679186\n",
      "Iteration 167, loss = 0.70459947\n",
      "Iteration 168, loss = 0.70236595\n",
      "Iteration 169, loss = 0.70005027\n",
      "Iteration 170, loss = 0.69776821\n",
      "Iteration 171, loss = 0.69554737\n",
      "Iteration 172, loss = 0.69330034\n",
      "Iteration 173, loss = 0.69099264\n",
      "Iteration 174, loss = 0.68865372\n",
      "Iteration 175, loss = 0.68632825\n",
      "Iteration 176, loss = 0.68399734\n",
      "Iteration 177, loss = 0.68159781\n",
      "Iteration 178, loss = 0.67915921\n",
      "Iteration 179, loss = 0.67673725\n",
      "Iteration 180, loss = 0.67424575\n",
      "Iteration 181, loss = 0.67174582\n",
      "Iteration 182, loss = 0.66919647\n",
      "Iteration 183, loss = 0.66667247\n",
      "Iteration 184, loss = 0.66408496\n",
      "Iteration 185, loss = 0.66152868\n",
      "Iteration 186, loss = 0.65894594\n",
      "Iteration 187, loss = 0.65634856\n",
      "Iteration 188, loss = 0.65378871\n",
      "Iteration 189, loss = 0.65121375\n",
      "Iteration 190, loss = 0.64867092\n",
      "Iteration 191, loss = 0.64610943\n",
      "Iteration 192, loss = 0.64358405\n",
      "Iteration 193, loss = 0.64106802\n",
      "Iteration 194, loss = 0.63853473\n",
      "Iteration 195, loss = 0.63597264\n",
      "Iteration 196, loss = 0.63328804\n",
      "Iteration 197, loss = 0.63035274\n",
      "Iteration 198, loss = 0.62695849\n",
      "Iteration 199, loss = 0.62337127\n",
      "Iteration 200, loss = 0.62080213\n",
      "Iteration 201, loss = 0.61843399\n",
      "Iteration 202, loss = 0.61525652\n",
      "Iteration 203, loss = 0.61218349\n",
      "Iteration 204, loss = 0.60949769\n",
      "Iteration 205, loss = 0.60691970\n",
      "Iteration 206, loss = 0.60401157\n",
      "Iteration 207, loss = 0.60088571\n",
      "Iteration 208, loss = 0.59787370\n",
      "Iteration 209, loss = 0.59522330\n",
      "Iteration 210, loss = 0.59236849\n",
      "Iteration 211, loss = 0.58933805\n",
      "Iteration 212, loss = 0.58630667\n",
      "Iteration 213, loss = 0.58306267\n",
      "Iteration 214, loss = 0.57917379\n",
      "Iteration 215, loss = 0.57417064\n",
      "Iteration 216, loss = 0.57239222\n",
      "Iteration 217, loss = 0.56790303\n",
      "Iteration 218, loss = 0.56404622\n",
      "Iteration 219, loss = 0.56135166\n",
      "Iteration 220, loss = 0.55828236\n",
      "Iteration 221, loss = 0.55449934\n",
      "Iteration 222, loss = 0.55068239\n",
      "Iteration 223, loss = 0.54741197\n",
      "Iteration 224, loss = 0.54415402\n",
      "Iteration 225, loss = 0.54047413\n",
      "Iteration 226, loss = 0.53716514\n",
      "Iteration 227, loss = 0.53399617\n",
      "Iteration 228, loss = 0.53057870\n",
      "Iteration 229, loss = 0.52720818\n",
      "Iteration 230, loss = 0.52400462\n",
      "Iteration 231, loss = 0.52086707\n",
      "Iteration 232, loss = 0.51765703\n",
      "Iteration 233, loss = 0.51447031\n",
      "Iteration 234, loss = 0.51138557\n",
      "Iteration 235, loss = 0.50835544\n",
      "Iteration 236, loss = 0.50524045\n",
      "Iteration 237, loss = 0.50213084\n",
      "Iteration 238, loss = 0.49904195\n",
      "Iteration 239, loss = 0.49575720\n",
      "Iteration 240, loss = 0.49453194\n",
      "Iteration 241, loss = 0.49005411\n",
      "Iteration 242, loss = 0.48713785\n",
      "Iteration 243, loss = 0.48411006\n",
      "Iteration 244, loss = 0.48127382\n",
      "Iteration 245, loss = 0.47854364\n",
      "Iteration 246, loss = 0.47557595\n",
      "Iteration 247, loss = 0.47258231\n",
      "Iteration 248, loss = 0.46963202\n",
      "Iteration 249, loss = 0.46618646\n",
      "Iteration 250, loss = 0.46215769\n",
      "Iteration 251, loss = 0.45758099\n",
      "Iteration 252, loss = 0.45286331\n",
      "Iteration 253, loss = 0.45164079\n",
      "Iteration 254, loss = 0.44796633\n",
      "Iteration 255, loss = 0.44377892\n",
      "Iteration 256, loss = 0.44048692\n",
      "Iteration 257, loss = 0.43760973\n",
      "Iteration 258, loss = 0.43438935\n",
      "Iteration 259, loss = 0.43111021\n",
      "Iteration 260, loss = 0.42707059\n",
      "Iteration 261, loss = 0.42229057\n",
      "Iteration 262, loss = 0.41867217\n",
      "Iteration 263, loss = 0.41667800\n",
      "Iteration 264, loss = 0.41275537\n",
      "Iteration 265, loss = 0.40913621\n",
      "Iteration 266, loss = 0.40558717\n",
      "Iteration 267, loss = 0.40288508\n",
      "Iteration 268, loss = 0.39934008\n",
      "Iteration 269, loss = 0.39603825\n",
      "Iteration 270, loss = 0.39270088\n",
      "Iteration 271, loss = 0.38938065\n",
      "Iteration 272, loss = 0.38654139\n",
      "Iteration 273, loss = 0.38344162\n",
      "Iteration 274, loss = 0.38027642\n",
      "Iteration 275, loss = 0.37706438\n",
      "Iteration 276, loss = 0.37413185\n",
      "Iteration 277, loss = 0.37132034\n",
      "Iteration 278, loss = 0.36833409\n",
      "Iteration 279, loss = 0.36543891\n",
      "Iteration 280, loss = 0.36255231\n",
      "Iteration 281, loss = 0.35955027\n",
      "Iteration 282, loss = 0.35694890\n",
      "Iteration 283, loss = 0.35414349\n",
      "Iteration 284, loss = 0.35144986\n",
      "Iteration 285, loss = 0.34875171\n",
      "Iteration 286, loss = 0.34604945\n",
      "Iteration 287, loss = 0.34341089\n",
      "Iteration 288, loss = 0.34081041\n",
      "Iteration 289, loss = 0.33833127\n",
      "Iteration 290, loss = 0.33572905\n",
      "Iteration 291, loss = 0.33321875\n",
      "Iteration 292, loss = 0.33072973\n",
      "Iteration 293, loss = 0.32828498\n",
      "Iteration 294, loss = 0.32587455\n",
      "Iteration 295, loss = 0.32343077\n",
      "Iteration 296, loss = 0.32101807\n",
      "Iteration 297, loss = 0.31876596\n",
      "Iteration 298, loss = 0.31633613\n",
      "Iteration 299, loss = 0.31407212\n",
      "Iteration 300, loss = 0.31175392\n",
      "Iteration 301, loss = 0.30937305\n",
      "Iteration 302, loss = 0.30705532\n",
      "Iteration 303, loss = 0.30479135\n",
      "Iteration 304, loss = 0.30245129\n",
      "Iteration 305, loss = 0.30000680\n",
      "Iteration 306, loss = 0.29776687\n",
      "Iteration 307, loss = 0.29531738\n",
      "Iteration 308, loss = 0.29287580\n",
      "Iteration 309, loss = 0.29061474\n",
      "Iteration 310, loss = 0.28828896\n",
      "Iteration 311, loss = 0.28584202\n",
      "Iteration 312, loss = 0.28354493\n",
      "Iteration 313, loss = 0.28119499\n",
      "Iteration 314, loss = 0.27871922\n",
      "Iteration 315, loss = 0.27628857\n",
      "Iteration 316, loss = 0.27378701\n",
      "Iteration 317, loss = 0.27140416\n",
      "Iteration 318, loss = 0.26918295\n",
      "Iteration 319, loss = 0.26691143\n",
      "Iteration 320, loss = 0.26481613\n",
      "Iteration 321, loss = 0.26257876\n",
      "Iteration 322, loss = 0.26016586\n",
      "Iteration 323, loss = 0.25784918\n",
      "Iteration 324, loss = 0.25568750\n",
      "Iteration 325, loss = 0.25373186\n",
      "Iteration 326, loss = 0.25169691\n",
      "Iteration 327, loss = 0.24958747\n",
      "Iteration 328, loss = 0.24739199\n",
      "Iteration 329, loss = 0.24525291\n",
      "Iteration 330, loss = 0.24328926\n",
      "Iteration 331, loss = 0.24139698\n",
      "Iteration 332, loss = 0.23941882\n",
      "Iteration 333, loss = 0.23737741\n",
      "Iteration 334, loss = 0.23532285\n",
      "Iteration 335, loss = 0.23330833\n",
      "Iteration 336, loss = 0.23136603\n",
      "Iteration 337, loss = 0.22949887\n",
      "Iteration 338, loss = 0.22764841\n",
      "Iteration 339, loss = 0.22575913\n",
      "Iteration 340, loss = 0.22385235\n",
      "Iteration 341, loss = 0.22188336\n",
      "Iteration 342, loss = 0.21993972\n",
      "Iteration 343, loss = 0.21805063\n",
      "Iteration 344, loss = 0.21623113\n",
      "Iteration 345, loss = 0.21445849\n",
      "Iteration 346, loss = 0.21272677\n",
      "Iteration 347, loss = 0.21105314\n",
      "Iteration 348, loss = 0.20936333\n",
      "Iteration 349, loss = 0.20767831\n",
      "Iteration 350, loss = 0.20589337\n",
      "Iteration 351, loss = 0.20409892\n",
      "Iteration 352, loss = 0.20238298\n",
      "Iteration 353, loss = 0.20077350\n",
      "Iteration 354, loss = 0.19931544\n",
      "Iteration 355, loss = 0.19762955\n",
      "Iteration 356, loss = 0.19607492\n",
      "Iteration 357, loss = 0.19456746\n",
      "Iteration 358, loss = 0.19307664\n",
      "Iteration 359, loss = 0.19158965\n",
      "Iteration 360, loss = 0.19009895\n",
      "Iteration 361, loss = 0.18866879\n",
      "Iteration 362, loss = 0.18715551\n",
      "Iteration 363, loss = 0.18529707\n",
      "Iteration 364, loss = 0.18407722\n",
      "Iteration 365, loss = 0.18317784\n",
      "Iteration 366, loss = 0.18161887\n",
      "Iteration 367, loss = 0.18006001\n",
      "Iteration 368, loss = 0.17805657\n",
      "Iteration 369, loss = 0.17715443\n",
      "Iteration 370, loss = 0.17595217\n",
      "Iteration 371, loss = 0.17453498\n",
      "Iteration 372, loss = 0.17300410\n",
      "Iteration 373, loss = 0.17181891\n",
      "Iteration 374, loss = 0.17083210\n",
      "Iteration 375, loss = 0.16939017\n",
      "Iteration 376, loss = 0.16822781\n",
      "Iteration 377, loss = 0.16709635\n",
      "Iteration 378, loss = 0.16608921\n",
      "Iteration 379, loss = 0.16478722\n",
      "Iteration 380, loss = 0.16378667\n",
      "Iteration 381, loss = 0.16269000\n",
      "Iteration 382, loss = 0.16169482\n",
      "Iteration 383, loss = 0.16054418\n",
      "Iteration 384, loss = 0.15961024\n",
      "Iteration 385, loss = 0.15860134\n",
      "Iteration 386, loss = 0.15759743\n",
      "Iteration 387, loss = 0.15658138\n",
      "Iteration 388, loss = 0.15568654\n",
      "Iteration 389, loss = 0.15473191\n",
      "Iteration 390, loss = 0.15377012\n",
      "Iteration 391, loss = 0.15284567\n",
      "Iteration 392, loss = 0.15195903\n",
      "Iteration 393, loss = 0.15107455\n",
      "Iteration 394, loss = 0.15015702\n",
      "Iteration 395, loss = 0.14929063\n",
      "Iteration 396, loss = 0.14843444\n",
      "Iteration 397, loss = 0.14759890\n",
      "Iteration 398, loss = 0.14673033\n",
      "Iteration 399, loss = 0.14590405\n",
      "Iteration 400, loss = 0.14507453\n",
      "Iteration 401, loss = 0.14428165\n",
      "Iteration 402, loss = 0.14346677\n",
      "Iteration 403, loss = 0.14266874\n",
      "Iteration 404, loss = 0.14187612\n",
      "Iteration 405, loss = 0.14110513\n",
      "Iteration 406, loss = 0.14033991\n",
      "Iteration 407, loss = 0.13957920\n",
      "Iteration 408, loss = 0.13882506\n",
      "Iteration 409, loss = 0.13807191\n",
      "Iteration 410, loss = 0.13733883\n",
      "Iteration 411, loss = 0.13660679\n",
      "Iteration 412, loss = 0.13589239\n",
      "Iteration 413, loss = 0.13518051\n",
      "Iteration 414, loss = 0.13448010\n",
      "Iteration 415, loss = 0.13378507\n",
      "Iteration 416, loss = 0.13309834\n",
      "Iteration 417, loss = 0.13242058\n",
      "Iteration 418, loss = 0.13174847\n",
      "Iteration 419, loss = 0.13108988\n",
      "Iteration 420, loss = 0.13043926\n",
      "Iteration 421, loss = 0.12980242\n",
      "Iteration 422, loss = 0.12917904\n",
      "Iteration 423, loss = 0.12856652\n",
      "Iteration 424, loss = 0.12796424\n",
      "Iteration 425, loss = 0.12734575\n",
      "Iteration 426, loss = 0.12671181\n",
      "Iteration 427, loss = 0.12605744\n",
      "Iteration 428, loss = 0.12541605\n",
      "Iteration 429, loss = 0.12480523\n",
      "Iteration 430, loss = 0.12422646\n",
      "Iteration 431, loss = 0.12365791\n",
      "Iteration 432, loss = 0.12309069\n",
      "Iteration 433, loss = 0.12252101\n",
      "Iteration 434, loss = 0.12192845\n",
      "Iteration 435, loss = 0.12133072\n",
      "Iteration 436, loss = 0.12073640\n",
      "Iteration 437, loss = 0.12016352\n",
      "Iteration 438, loss = 0.11961020\n",
      "Iteration 439, loss = 0.11906853\n",
      "Iteration 440, loss = 0.11853167\n",
      "Iteration 441, loss = 0.11797815\n",
      "Iteration 442, loss = 0.11742095\n",
      "Iteration 443, loss = 0.11687212\n",
      "Iteration 444, loss = 0.11632002\n",
      "Iteration 445, loss = 0.11577338\n",
      "Iteration 446, loss = 0.11519276\n",
      "Iteration 447, loss = 0.11462220\n",
      "Iteration 448, loss = 0.11409976\n",
      "Iteration 449, loss = 0.11358789\n",
      "Iteration 450, loss = 0.11311432\n",
      "Iteration 451, loss = 0.11262933\n",
      "Iteration 452, loss = 0.11207590\n",
      "Iteration 453, loss = 0.11149572\n",
      "Iteration 454, loss = 0.11094895\n",
      "Iteration 455, loss = 0.11041179\n",
      "Iteration 456, loss = 0.10990198\n",
      "Iteration 457, loss = 0.10940189\n",
      "Iteration 458, loss = 0.10890200\n",
      "Iteration 459, loss = 0.10842190\n",
      "Iteration 460, loss = 0.10795064\n",
      "Iteration 461, loss = 0.10749280\n",
      "Iteration 462, loss = 0.10702155\n",
      "Iteration 463, loss = 0.10650923\n",
      "Iteration 464, loss = 0.10598933\n",
      "Iteration 465, loss = 0.10547708\n",
      "Iteration 466, loss = 0.10498183\n",
      "Iteration 467, loss = 0.10450398\n",
      "Iteration 468, loss = 0.10404078\n",
      "Iteration 469, loss = 0.10358865\n",
      "Iteration 470, loss = 0.10314849\n",
      "Iteration 471, loss = 0.10272836\n",
      "Iteration 472, loss = 0.10231203\n",
      "Iteration 473, loss = 0.10189835\n",
      "Iteration 474, loss = 0.10143484\n",
      "Iteration 475, loss = 0.10095084\n",
      "Iteration 476, loss = 0.10045150\n",
      "Iteration 477, loss = 0.09997765\n",
      "Iteration 478, loss = 0.09953779\n",
      "Iteration 479, loss = 0.09912966\n",
      "Iteration 480, loss = 0.09874058\n",
      "Iteration 481, loss = 0.09835028\n",
      "Iteration 482, loss = 0.09795864\n",
      "Iteration 483, loss = 0.09752551\n",
      "Iteration 484, loss = 0.09707392\n",
      "Iteration 485, loss = 0.09661561\n",
      "Iteration 486, loss = 0.09617883\n",
      "Iteration 487, loss = 0.09576243\n",
      "Iteration 488, loss = 0.09536365\n",
      "Iteration 489, loss = 0.09498400\n",
      "Iteration 490, loss = 0.09461851\n",
      "Iteration 491, loss = 0.09428395\n",
      "Iteration 492, loss = 0.09395012\n",
      "Iteration 493, loss = 0.09363953\n",
      "Iteration 494, loss = 0.09320575\n",
      "Iteration 495, loss = 0.09270822\n",
      "Iteration 496, loss = 0.09222027\n",
      "Iteration 497, loss = 0.09184277\n",
      "Iteration 498, loss = 0.09155866\n",
      "Iteration 499, loss = 0.09129321\n",
      "Iteration 500, loss = 0.09099610\n",
      "Iteration 501, loss = 0.09051676\n",
      "Iteration 502, loss = 0.09000371\n",
      "Iteration 503, loss = 0.08962535\n",
      "Iteration 504, loss = 0.08939812\n",
      "Iteration 505, loss = 0.08919270\n",
      "Iteration 506, loss = 0.08887251\n",
      "Iteration 507, loss = 0.08836568\n",
      "Iteration 508, loss = 0.08785616\n",
      "Iteration 509, loss = 0.08756016\n",
      "Iteration 510, loss = 0.08736559\n",
      "Iteration 511, loss = 0.08704885\n",
      "Iteration 512, loss = 0.08660900\n",
      "Iteration 513, loss = 0.08616110\n",
      "Iteration 514, loss = 0.08585459\n",
      "Iteration 515, loss = 0.08561332\n",
      "Iteration 516, loss = 0.08527399\n",
      "Iteration 517, loss = 0.08487116\n",
      "Iteration 518, loss = 0.08450969\n",
      "Iteration 519, loss = 0.08422521\n",
      "Iteration 520, loss = 0.08395537\n",
      "Iteration 521, loss = 0.08362788\n",
      "Iteration 522, loss = 0.08326450\n",
      "Iteration 523, loss = 0.08292325\n",
      "Iteration 524, loss = 0.08263286\n",
      "Iteration 525, loss = 0.08236257\n",
      "Iteration 526, loss = 0.08206398\n",
      "Iteration 527, loss = 0.08174065\n",
      "Iteration 528, loss = 0.08142349\n",
      "Iteration 529, loss = 0.08108416\n",
      "Iteration 530, loss = 0.08077550\n",
      "Iteration 531, loss = 0.08048040\n",
      "Iteration 532, loss = 0.08019510\n",
      "Iteration 533, loss = 0.07991273\n",
      "Iteration 534, loss = 0.07963182\n",
      "Iteration 535, loss = 0.07934455\n",
      "Iteration 536, loss = 0.07905134\n",
      "Iteration 537, loss = 0.07875365\n",
      "Iteration 538, loss = 0.07845679\n",
      "Iteration 539, loss = 0.07815707\n",
      "Iteration 540, loss = 0.07785691\n",
      "Iteration 541, loss = 0.07761759\n",
      "Iteration 542, loss = 0.07734133\n",
      "Iteration 543, loss = 0.07714383\n",
      "Iteration 544, loss = 0.07701878\n",
      "Iteration 545, loss = 0.07683053\n",
      "Iteration 546, loss = 0.07657708\n",
      "Iteration 547, loss = 0.07609814\n",
      "Iteration 548, loss = 0.07566139\n",
      "Iteration 549, loss = 0.07541530\n",
      "Iteration 550, loss = 0.07532667\n",
      "Iteration 551, loss = 0.07527482\n",
      "Iteration 552, loss = 0.07501984\n",
      "Iteration 553, loss = 0.07453544\n",
      "Iteration 554, loss = 0.07411404\n",
      "Iteration 555, loss = 0.07400345\n",
      "Iteration 556, loss = 0.07391560\n",
      "Iteration 557, loss = 0.07355187\n",
      "Iteration 558, loss = 0.07312352\n",
      "Iteration 559, loss = 0.07287146\n",
      "Iteration 560, loss = 0.07274526\n",
      "Iteration 561, loss = 0.07254736\n",
      "Iteration 562, loss = 0.07219610\n",
      "Iteration 563, loss = 0.07188205\n",
      "Iteration 564, loss = 0.07170490\n",
      "Iteration 565, loss = 0.07153122\n",
      "Iteration 566, loss = 0.07126062\n",
      "Iteration 567, loss = 0.07094963\n",
      "Iteration 568, loss = 0.07070223\n",
      "Iteration 569, loss = 0.07051565\n",
      "Iteration 570, loss = 0.07031094\n",
      "Iteration 571, loss = 0.07004873\n",
      "Iteration 572, loss = 0.06977691\n",
      "Iteration 573, loss = 0.06955231\n",
      "Iteration 574, loss = 0.06936038\n",
      "Iteration 575, loss = 0.06915320\n",
      "Iteration 576, loss = 0.06891130\n",
      "Iteration 577, loss = 0.06865498\n",
      "Iteration 578, loss = 0.06842231\n",
      "Iteration 579, loss = 0.06821702\n",
      "Iteration 580, loss = 0.06801542\n",
      "Iteration 581, loss = 0.06780148\n",
      "Iteration 582, loss = 0.06757145\n",
      "Iteration 583, loss = 0.06733698\n",
      "Iteration 584, loss = 0.06710988\n",
      "Iteration 585, loss = 0.06689647\n",
      "Iteration 586, loss = 0.06669125\n",
      "Iteration 587, loss = 0.06648739\n",
      "Iteration 588, loss = 0.06628067\n",
      "Iteration 589, loss = 0.06606825\n",
      "Iteration 590, loss = 0.06585353\n",
      "Iteration 591, loss = 0.06563583\n",
      "Iteration 592, loss = 0.06541853\n",
      "Iteration 593, loss = 0.06520454\n",
      "Iteration 594, loss = 0.06499456\n",
      "Iteration 595, loss = 0.06478704\n",
      "Iteration 596, loss = 0.06458188\n",
      "Iteration 597, loss = 0.06437966\n",
      "Iteration 598, loss = 0.06417946\n",
      "Iteration 599, loss = 0.06398361\n",
      "Iteration 600, loss = 0.06379554\n",
      "Iteration 601, loss = 0.06362679\n",
      "Iteration 602, loss = 0.06347985\n",
      "Iteration 603, loss = 0.06338602\n",
      "Iteration 604, loss = 0.06327602\n",
      "Iteration 605, loss = 0.06316742\n",
      "Iteration 606, loss = 0.06286047\n",
      "Iteration 607, loss = 0.06249578\n",
      "Iteration 608, loss = 0.06218289\n",
      "Iteration 609, loss = 0.06202594\n",
      "Iteration 610, loss = 0.06196080\n",
      "Iteration 611, loss = 0.06183801\n",
      "Iteration 612, loss = 0.06160495\n",
      "Iteration 613, loss = 0.06126462\n",
      "Iteration 614, loss = 0.06101952\n",
      "Iteration 615, loss = 0.06102696\n",
      "Iteration 616, loss = 0.06111357\n",
      "Iteration 617, loss = 0.06093358\n",
      "Iteration 618, loss = 0.06039590\n",
      "Iteration 619, loss = 0.06010101\n",
      "Iteration 620, loss = 0.06013440\n",
      "Iteration 621, loss = 0.06007983\n",
      "Iteration 622, loss = 0.05976870\n",
      "Iteration 623, loss = 0.05936424\n",
      "Iteration 624, loss = 0.05924369\n",
      "Iteration 625, loss = 0.05922202\n",
      "Iteration 626, loss = 0.05895786\n",
      "Iteration 627, loss = 0.05864011\n",
      "Iteration 628, loss = 0.05851587\n",
      "Iteration 629, loss = 0.05844255\n",
      "Iteration 630, loss = 0.05823409\n",
      "Iteration 631, loss = 0.05795131\n",
      "Iteration 632, loss = 0.05778877\n",
      "Iteration 633, loss = 0.05769118\n",
      "Iteration 634, loss = 0.05749879\n",
      "Iteration 635, loss = 0.05725900\n",
      "Iteration 636, loss = 0.05708887\n",
      "Iteration 637, loss = 0.05696983\n",
      "Iteration 638, loss = 0.05679725\n",
      "Iteration 639, loss = 0.05657795\n",
      "Iteration 640, loss = 0.05639731\n",
      "Iteration 641, loss = 0.05625923\n",
      "Iteration 642, loss = 0.05610250\n",
      "Iteration 643, loss = 0.05591281\n",
      "Iteration 644, loss = 0.05573049\n",
      "Iteration 645, loss = 0.05557490\n",
      "Iteration 646, loss = 0.05542554\n",
      "Iteration 647, loss = 0.05525584\n",
      "Iteration 648, loss = 0.05507594\n",
      "Iteration 649, loss = 0.05490976\n",
      "Iteration 650, loss = 0.05475836\n",
      "Iteration 651, loss = 0.05459913\n",
      "Iteration 652, loss = 0.05442937\n",
      "Iteration 653, loss = 0.05426155\n",
      "Iteration 654, loss = 0.05410192\n",
      "Iteration 655, loss = 0.05394624\n",
      "Iteration 656, loss = 0.05379089\n",
      "Iteration 657, loss = 0.05362994\n",
      "Iteration 658, loss = 0.05346498\n",
      "Iteration 659, loss = 0.05330353\n",
      "Iteration 660, loss = 0.05314884\n",
      "Iteration 661, loss = 0.05299757\n",
      "Iteration 662, loss = 0.05284121\n",
      "Iteration 663, loss = 0.05268191\n",
      "Iteration 664, loss = 0.05252353\n",
      "Iteration 665, loss = 0.05236496\n",
      "Iteration 666, loss = 0.05220788\n",
      "Iteration 667, loss = 0.05205622\n",
      "Iteration 668, loss = 0.05190488\n",
      "Iteration 669, loss = 0.05175265\n",
      "Iteration 670, loss = 0.05160081\n",
      "Iteration 671, loss = 0.05144579\n",
      "Iteration 672, loss = 0.05128906\n",
      "Iteration 673, loss = 0.05113540\n",
      "Iteration 674, loss = 0.05098058\n",
      "Iteration 675, loss = 0.05082815\n",
      "Iteration 676, loss = 0.05067519\n",
      "Iteration 677, loss = 0.05052278\n",
      "Iteration 678, loss = 0.05037155\n",
      "Iteration 679, loss = 0.05022060\n",
      "Iteration 680, loss = 0.05007206\n",
      "Iteration 681, loss = 0.04992518\n",
      "Iteration 682, loss = 0.04978412\n",
      "Iteration 683, loss = 0.04965593\n",
      "Iteration 684, loss = 0.04956069\n",
      "Iteration 685, loss = 0.04949152\n",
      "Iteration 686, loss = 0.04946272\n",
      "Iteration 687, loss = 0.04939195\n",
      "Iteration 688, loss = 0.04929400\n",
      "Iteration 689, loss = 0.04899887\n",
      "Iteration 690, loss = 0.04866187\n",
      "Iteration 691, loss = 0.04844569\n",
      "Iteration 692, loss = 0.04840012\n",
      "Iteration 693, loss = 0.04840961\n",
      "Iteration 694, loss = 0.04829919\n",
      "Iteration 695, loss = 0.04805397\n",
      "Iteration 696, loss = 0.04776890\n",
      "Iteration 697, loss = 0.04762049\n",
      "Iteration 698, loss = 0.04758647\n",
      "Iteration 699, loss = 0.04751440\n",
      "Iteration 700, loss = 0.04733692\n",
      "Iteration 701, loss = 0.04709735\n",
      "Iteration 702, loss = 0.04692002\n",
      "Iteration 703, loss = 0.04683510\n",
      "Iteration 704, loss = 0.04675905\n",
      "Iteration 705, loss = 0.04661479\n",
      "Iteration 706, loss = 0.04641110\n",
      "Iteration 707, loss = 0.04624018\n",
      "Iteration 708, loss = 0.04613178\n",
      "Iteration 709, loss = 0.04603708\n",
      "Iteration 710, loss = 0.04590544\n",
      "Iteration 711, loss = 0.04573760\n",
      "Iteration 712, loss = 0.04557671\n",
      "Iteration 713, loss = 0.04545000\n",
      "Iteration 714, loss = 0.04534196\n",
      "Iteration 715, loss = 0.04522099\n",
      "Iteration 716, loss = 0.04507658\n",
      "Iteration 717, loss = 0.04492696\n",
      "Iteration 718, loss = 0.04479050\n",
      "Iteration 719, loss = 0.04466983\n",
      "Iteration 720, loss = 0.04455283\n",
      "Iteration 721, loss = 0.04442630\n",
      "Iteration 722, loss = 0.04428990\n",
      "Iteration 723, loss = 0.04415183\n",
      "Iteration 724, loss = 0.04402066\n",
      "Iteration 725, loss = 0.04389736\n",
      "Iteration 726, loss = 0.04377731\n",
      "Iteration 727, loss = 0.04365498\n",
      "Iteration 728, loss = 0.04352766\n",
      "Iteration 729, loss = 0.04339755\n",
      "Iteration 730, loss = 0.04326807\n",
      "Iteration 731, loss = 0.04314163\n",
      "Iteration 732, loss = 0.04301870\n",
      "Iteration 733, loss = 0.04289813\n",
      "Iteration 734, loss = 0.04277790\n",
      "Iteration 735, loss = 0.04265668\n",
      "Iteration 736, loss = 0.04253435\n",
      "Iteration 737, loss = 0.04241093\n",
      "Iteration 738, loss = 0.04228750\n",
      "Iteration 739, loss = 0.04216479\n",
      "Iteration 740, loss = 0.04204313\n",
      "Iteration 741, loss = 0.04192272\n",
      "Iteration 742, loss = 0.04180348\n",
      "Iteration 743, loss = 0.04168513\n",
      "Iteration 744, loss = 0.04156753\n",
      "Iteration 745, loss = 0.04145060\n",
      "Iteration 746, loss = 0.04133426\n",
      "Iteration 747, loss = 0.04121861\n",
      "Iteration 748, loss = 0.04110370\n",
      "Iteration 749, loss = 0.04098994\n",
      "Iteration 750, loss = 0.04087744\n",
      "Iteration 751, loss = 0.04076724\n",
      "Iteration 752, loss = 0.04065925\n",
      "Iteration 753, loss = 0.04055577\n",
      "Iteration 754, loss = 0.04045611\n",
      "Iteration 755, loss = 0.04036489\n",
      "Iteration 756, loss = 0.04027628\n",
      "Iteration 757, loss = 0.04019547\n",
      "Iteration 758, loss = 0.04010267\n",
      "Iteration 759, loss = 0.04000187\n",
      "Iteration 760, loss = 0.03986866\n",
      "Iteration 761, loss = 0.03972009\n",
      "Iteration 762, loss = 0.03956280\n",
      "Iteration 763, loss = 0.03942180\n",
      "Iteration 764, loss = 0.03930631\n",
      "Iteration 765, loss = 0.03921336\n",
      "Iteration 766, loss = 0.03913148\n",
      "Iteration 767, loss = 0.03904608\n",
      "Iteration 768, loss = 0.03894980\n",
      "Iteration 769, loss = 0.03883545\n",
      "Iteration 770, loss = 0.03871141\n",
      "Iteration 771, loss = 0.03858502\n",
      "Iteration 772, loss = 0.03846716\n",
      "Iteration 773, loss = 0.03836160\n",
      "Iteration 774, loss = 0.03826624\n",
      "Iteration 775, loss = 0.03817557\n",
      "Iteration 776, loss = 0.03808331\n",
      "Iteration 777, loss = 0.03798640\n",
      "Iteration 778, loss = 0.03788247\n",
      "Iteration 779, loss = 0.03777443\n",
      "Iteration 780, loss = 0.03766467\n",
      "Iteration 781, loss = 0.03755718\n",
      "Iteration 782, loss = 0.03745382\n",
      "Iteration 783, loss = 0.03735493\n",
      "Iteration 784, loss = 0.03725948\n",
      "Iteration 785, loss = 0.03716586\n",
      "Iteration 786, loss = 0.03707264\n",
      "Iteration 787, loss = 0.03697852\n",
      "Iteration 788, loss = 0.03688333\n",
      "Iteration 789, loss = 0.03678661\n",
      "Iteration 790, loss = 0.03668915\n",
      "Iteration 791, loss = 0.03659109\n",
      "Iteration 792, loss = 0.03649329\n",
      "Iteration 793, loss = 0.03639596\n",
      "Iteration 794, loss = 0.03629953\n",
      "Iteration 795, loss = 0.03620405\n",
      "Iteration 796, loss = 0.03610955\n",
      "Iteration 797, loss = 0.03601595\n",
      "Iteration 798, loss = 0.03592314\n",
      "Iteration 799, loss = 0.03583101\n",
      "Iteration 800, loss = 0.03573949\n",
      "Iteration 801, loss = 0.03564854\n",
      "Iteration 802, loss = 0.03555814\n",
      "Iteration 803, loss = 0.03546836\n",
      "Iteration 804, loss = 0.03537928\n",
      "Iteration 805, loss = 0.03529112\n",
      "Iteration 806, loss = 0.03520407\n",
      "Iteration 807, loss = 0.03511873\n",
      "Iteration 808, loss = 0.03503553\n",
      "Iteration 809, loss = 0.03495602\n",
      "Iteration 810, loss = 0.03488077\n",
      "Iteration 811, loss = 0.03481332\n",
      "Iteration 812, loss = 0.03475250\n",
      "Iteration 813, loss = 0.03470374\n",
      "Iteration 814, loss = 0.03465468\n",
      "Iteration 815, loss = 0.03460638\n",
      "Iteration 816, loss = 0.03452471\n",
      "Iteration 817, loss = 0.03441305\n",
      "Iteration 818, loss = 0.03426178\n",
      "Iteration 819, loss = 0.03411085\n",
      "Iteration 820, loss = 0.03398944\n",
      "Iteration 821, loss = 0.03391083\n",
      "Iteration 822, loss = 0.03386130\n",
      "Iteration 823, loss = 0.03381373\n",
      "Iteration 824, loss = 0.03374760\n",
      "Iteration 825, loss = 0.03365024\n",
      "Iteration 826, loss = 0.03353588\n",
      "Iteration 827, loss = 0.03342384\n",
      "Iteration 828, loss = 0.03333217\n",
      "Iteration 829, loss = 0.03326207\n",
      "Iteration 830, loss = 0.03320136\n",
      "Iteration 831, loss = 0.03313543\n",
      "Iteration 832, loss = 0.03305458\n",
      "Iteration 833, loss = 0.03296219\n",
      "Iteration 834, loss = 0.03286673\n",
      "Iteration 835, loss = 0.03277847\n",
      "Iteration 836, loss = 0.03270106\n",
      "Iteration 837, loss = 0.03263132\n",
      "Iteration 838, loss = 0.03256262\n",
      "Iteration 839, loss = 0.03248902\n",
      "Iteration 840, loss = 0.03240928\n",
      "Iteration 841, loss = 0.03232536\n",
      "Iteration 842, loss = 0.03224189\n",
      "Iteration 843, loss = 0.03216224\n",
      "Iteration 844, loss = 0.03208731\n",
      "Iteration 845, loss = 0.03201557\n",
      "Iteration 846, loss = 0.03194437\n",
      "Iteration 847, loss = 0.03187167\n",
      "Iteration 848, loss = 0.03179651\n",
      "Iteration 849, loss = 0.03171966\n",
      "Iteration 850, loss = 0.03164237\n",
      "Iteration 851, loss = 0.03156609\n",
      "Iteration 852, loss = 0.03149158\n",
      "Iteration 853, loss = 0.03141886\n",
      "Iteration 854, loss = 0.03134737\n",
      "Iteration 855, loss = 0.03127632\n",
      "Iteration 856, loss = 0.03120510\n",
      "Iteration 857, loss = 0.03113332\n",
      "Iteration 858, loss = 0.03106106\n",
      "Iteration 859, loss = 0.03098850\n",
      "Iteration 860, loss = 0.03091602\n",
      "Iteration 861, loss = 0.03084391\n",
      "Iteration 862, loss = 0.03077240\n",
      "Iteration 863, loss = 0.03070154\n",
      "Iteration 864, loss = 0.03063130\n",
      "Iteration 865, loss = 0.03056156\n",
      "Iteration 866, loss = 0.03049219\n",
      "Iteration 867, loss = 0.03042309\n",
      "Iteration 868, loss = 0.03035417\n",
      "Iteration 869, loss = 0.03028539\n",
      "Iteration 870, loss = 0.03021673\n",
      "Iteration 871, loss = 0.03014822\n",
      "Iteration 872, loss = 0.03007985\n",
      "Iteration 873, loss = 0.03001168\n",
      "Iteration 874, loss = 0.02994372\n",
      "Iteration 875, loss = 0.02987599\n",
      "Iteration 876, loss = 0.02980850\n",
      "Iteration 877, loss = 0.02974128\n",
      "Iteration 878, loss = 0.02967432\n",
      "Iteration 879, loss = 0.02960766\n",
      "Iteration 880, loss = 0.02954130\n",
      "Iteration 881, loss = 0.02947529\n",
      "Iteration 882, loss = 0.02940963\n",
      "Iteration 883, loss = 0.02934443\n",
      "Iteration 884, loss = 0.02927973\n",
      "Iteration 885, loss = 0.02921571\n",
      "Iteration 886, loss = 0.02915253\n",
      "Iteration 887, loss = 0.02909059\n",
      "Iteration 888, loss = 0.02903020\n",
      "Iteration 889, loss = 0.02897235\n",
      "Iteration 890, loss = 0.02891756\n",
      "Iteration 891, loss = 0.02886796\n",
      "Iteration 892, loss = 0.02882366\n",
      "Iteration 893, loss = 0.02878823\n",
      "Iteration 894, loss = 0.02875735\n",
      "Iteration 895, loss = 0.02873299\n",
      "Iteration 896, loss = 0.02869634\n",
      "Iteration 897, loss = 0.02864368\n",
      "Iteration 898, loss = 0.02855100\n",
      "Iteration 899, loss = 0.02843284\n",
      "Iteration 900, loss = 0.02830563\n",
      "Iteration 901, loss = 0.02820232\n",
      "Iteration 902, loss = 0.02813625\n",
      "Iteration 903, loss = 0.02809992\n",
      "Iteration 904, loss = 0.02807317\n",
      "Iteration 905, loss = 0.02803369\n",
      "Iteration 906, loss = 0.02797155\n",
      "Iteration 907, loss = 0.02788627\n",
      "Iteration 908, loss = 0.02779486\n",
      "Iteration 909, loss = 0.02771389\n",
      "Iteration 910, loss = 0.02765235\n",
      "Iteration 911, loss = 0.02760607\n",
      "Iteration 912, loss = 0.02756312\n",
      "Iteration 913, loss = 0.02751278\n",
      "Iteration 914, loss = 0.02744975\n",
      "Iteration 915, loss = 0.02737811\n",
      "Iteration 916, loss = 0.02730528\n",
      "Iteration 917, loss = 0.02723880\n",
      "Iteration 918, loss = 0.02718114\n",
      "Iteration 919, loss = 0.02712964\n",
      "Iteration 920, loss = 0.02707912\n",
      "Iteration 921, loss = 0.02702491\n",
      "Iteration 922, loss = 0.02696560\n",
      "Iteration 923, loss = 0.02690240\n",
      "Iteration 924, loss = 0.02683872\n",
      "Iteration 925, loss = 0.02677743\n",
      "Iteration 926, loss = 0.02671981\n",
      "Iteration 927, loss = 0.02666523\n",
      "Iteration 928, loss = 0.02661188\n",
      "Iteration 929, loss = 0.02655793\n",
      "Iteration 930, loss = 0.02650220\n",
      "Iteration 931, loss = 0.02644474\n",
      "Iteration 932, loss = 0.02638621\n",
      "Iteration 933, loss = 0.02632775\n",
      "Iteration 934, loss = 0.02627020\n",
      "Iteration 935, loss = 0.02621399\n",
      "Iteration 936, loss = 0.02615901\n",
      "Iteration 937, loss = 0.02610482\n",
      "Iteration 938, loss = 0.02605087\n",
      "Iteration 939, loss = 0.02599671\n",
      "Iteration 940, loss = 0.02594213\n",
      "Iteration 941, loss = 0.02588712\n",
      "Iteration 942, loss = 0.02583184\n",
      "Iteration 943, loss = 0.02577651\n",
      "Iteration 944, loss = 0.02572136\n",
      "Iteration 945, loss = 0.02566655\n",
      "Iteration 946, loss = 0.02561216\n",
      "Iteration 947, loss = 0.02555818\n",
      "Iteration 948, loss = 0.02550454\n",
      "Iteration 949, loss = 0.02545118\n",
      "Iteration 950, loss = 0.02539802\n",
      "Iteration 951, loss = 0.02534499\n",
      "Iteration 952, loss = 0.02529204\n",
      "Iteration 953, loss = 0.02523917\n",
      "Iteration 954, loss = 0.02518635\n",
      "Iteration 955, loss = 0.02513362\n",
      "Iteration 956, loss = 0.02508097\n",
      "Iteration 957, loss = 0.02502842\n",
      "Iteration 958, loss = 0.02497599\n",
      "Iteration 959, loss = 0.02492370\n",
      "Iteration 960, loss = 0.02487154\n",
      "Iteration 961, loss = 0.02481955\n",
      "Iteration 962, loss = 0.02476773\n",
      "Iteration 963, loss = 0.02471611\n",
      "Iteration 964, loss = 0.02466470\n",
      "Iteration 965, loss = 0.02461355\n",
      "Iteration 966, loss = 0.02456269\n",
      "Iteration 967, loss = 0.02451222\n",
      "Iteration 968, loss = 0.02446222\n",
      "Iteration 969, loss = 0.02441287\n",
      "Iteration 970, loss = 0.02436437\n",
      "Iteration 971, loss = 0.02431715\n",
      "Iteration 972, loss = 0.02427157\n",
      "Iteration 973, loss = 0.02422859\n",
      "Iteration 974, loss = 0.02418884\n",
      "Iteration 975, loss = 0.02415414\n",
      "Iteration 976, loss = 0.02412481\n",
      "Iteration 977, loss = 0.02410324\n",
      "Iteration 978, loss = 0.02408592\n",
      "Iteration 979, loss = 0.02407206\n",
      "Iteration 980, loss = 0.02404665\n",
      "Iteration 981, loss = 0.02400257\n",
      "Iteration 982, loss = 0.02392322\n",
      "Iteration 983, loss = 0.02381863\n",
      "Iteration 984, loss = 0.02370669\n",
      "Iteration 985, loss = 0.02361597\n",
      "Iteration 986, loss = 0.02355971\n",
      "Iteration 987, loss = 0.02353232\n",
      "Iteration 988, loss = 0.02351612\n",
      "Iteration 989, loss = 0.02349128\n",
      "Iteration 990, loss = 0.02344621\n",
      "Iteration 991, loss = 0.02337888\n",
      "Iteration 992, loss = 0.02330145\n",
      "Iteration 993, loss = 0.02322936\n",
      "Iteration 994, loss = 0.02317361\n",
      "Iteration 995, loss = 0.02313403\n",
      "Iteration 996, loss = 0.02310188\n",
      "Iteration 997, loss = 0.02306659\n",
      "Iteration 998, loss = 0.02302111\n",
      "Iteration 999, loss = 0.02296558\n",
      "Iteration 1000, loss = 0.02290508\n",
      "Iteration 1001, loss = 0.02284695\n",
      "Iteration 1002, loss = 0.02279586\n",
      "Iteration 1003, loss = 0.02275206\n",
      "Iteration 1004, loss = 0.02271211\n",
      "Iteration 1005, loss = 0.02267144\n",
      "Iteration 1006, loss = 0.02262691\n",
      "Iteration 1007, loss = 0.02257784\n",
      "Iteration 1008, loss = 0.02252609\n",
      "Iteration 1009, loss = 0.02247440\n",
      "Iteration 1010, loss = 0.02242506\n",
      "Iteration 1011, loss = 0.02237887\n",
      "Iteration 1012, loss = 0.02233511\n",
      "Iteration 1013, loss = 0.02229221\n",
      "Iteration 1014, loss = 0.02224862\n",
      "Iteration 1015, loss = 0.02220351\n",
      "Iteration 1016, loss = 0.02215684\n",
      "Iteration 1017, loss = 0.02210927\n",
      "Iteration 1018, loss = 0.02206166\n",
      "Iteration 1019, loss = 0.02201477\n",
      "Iteration 1020, loss = 0.02196897\n",
      "Iteration 1021, loss = 0.02192423\n",
      "Iteration 1022, loss = 0.02188016\n",
      "Iteration 1023, loss = 0.02183631\n",
      "Iteration 1024, loss = 0.02179228\n",
      "Iteration 1025, loss = 0.02174787\n",
      "Iteration 1026, loss = 0.02170305\n",
      "Iteration 1027, loss = 0.02165794\n",
      "Iteration 1028, loss = 0.02161272\n",
      "Iteration 1029, loss = 0.02156760\n",
      "Iteration 1030, loss = 0.02152274\n",
      "Iteration 1031, loss = 0.02147821\n",
      "Iteration 1032, loss = 0.02143401\n",
      "Iteration 1033, loss = 0.02139009\n",
      "Iteration 1034, loss = 0.02134638\n",
      "Iteration 1035, loss = 0.02130280\n",
      "Iteration 1036, loss = 0.02125930\n",
      "Iteration 1037, loss = 0.02121583\n",
      "Iteration 1038, loss = 0.02117236\n",
      "Iteration 1039, loss = 0.02112891\n",
      "Iteration 1040, loss = 0.02108546\n",
      "Iteration 1041, loss = 0.02104205\n",
      "Iteration 1042, loss = 0.02099869\n",
      "Iteration 1043, loss = 0.02095539\n",
      "Iteration 1044, loss = 0.02091217\n",
      "Iteration 1045, loss = 0.02086903\n",
      "Iteration 1046, loss = 0.02082599\n",
      "Iteration 1047, loss = 0.02078304\n",
      "Iteration 1048, loss = 0.02074018\n",
      "Iteration 1049, loss = 0.02069743\n",
      "Iteration 1050, loss = 0.02065477\n",
      "Iteration 1051, loss = 0.02061220\n",
      "Iteration 1052, loss = 0.02056973\n",
      "Iteration 1053, loss = 0.02052736\n",
      "Iteration 1054, loss = 0.02048509\n",
      "Iteration 1055, loss = 0.02044293\n",
      "Iteration 1056, loss = 0.02040087\n",
      "Iteration 1057, loss = 0.02035894\n",
      "Iteration 1058, loss = 0.02031715\n",
      "Iteration 1059, loss = 0.02027553\n",
      "Iteration 1060, loss = 0.02023412\n",
      "Iteration 1061, loss = 0.02019299\n",
      "Iteration 1062, loss = 0.02015222\n",
      "Iteration 1063, loss = 0.02011196\n",
      "Iteration 1064, loss = 0.02007245\n",
      "Iteration 1065, loss = 0.02003403\n",
      "Iteration 1066, loss = 0.01999722\n",
      "Iteration 1067, loss = 0.01996288\n",
      "Iteration 1068, loss = 0.01993211\n",
      "Iteration 1069, loss = 0.01990672\n",
      "Iteration 1070, loss = 0.01988851\n",
      "Iteration 1071, loss = 0.01988009\n",
      "Iteration 1072, loss = 0.01988142\n",
      "Iteration 1073, loss = 0.01989064\n",
      "Iteration 1074, loss = 0.01989505\n",
      "Iteration 1075, loss = 0.01987676\n",
      "Iteration 1076, loss = 0.01981297\n",
      "Iteration 1077, loss = 0.01970165\n",
      "Iteration 1078, loss = 0.01957136\n",
      "Iteration 1079, loss = 0.01946778\n",
      "Iteration 1080, loss = 0.01941808\n",
      "Iteration 1081, loss = 0.01941328\n",
      "Iteration 1082, loss = 0.01942077\n",
      "Iteration 1083, loss = 0.01940686\n",
      "Iteration 1084, loss = 0.01935456\n",
      "Iteration 1085, loss = 0.01927403\n",
      "Iteration 1086, loss = 0.01919373\n",
      "Iteration 1087, loss = 0.01913886\n",
      "Iteration 1088, loss = 0.01911272\n",
      "Iteration 1089, loss = 0.01909851\n",
      "Iteration 1090, loss = 0.01907476\n",
      "Iteration 1091, loss = 0.01903036\n",
      "Iteration 1092, loss = 0.01897103\n",
      "Iteration 1093, loss = 0.01891222\n",
      "Iteration 1094, loss = 0.01886667\n",
      "Iteration 1095, loss = 0.01883539\n",
      "Iteration 1096, loss = 0.01880931\n",
      "Iteration 1097, loss = 0.01877778\n",
      "Iteration 1098, loss = 0.01873600\n",
      "Iteration 1099, loss = 0.01868736\n",
      "Iteration 1100, loss = 0.01863943\n",
      "Iteration 1101, loss = 0.01859812\n",
      "Iteration 1102, loss = 0.01856373\n",
      "Iteration 1103, loss = 0.01853181\n",
      "Iteration 1104, loss = 0.01849725\n",
      "Iteration 1105, loss = 0.01845782\n",
      "Iteration 1106, loss = 0.01841512\n",
      "Iteration 1107, loss = 0.01837262\n",
      "Iteration 1108, loss = 0.01833310\n",
      "Iteration 1109, loss = 0.01829693\n",
      "Iteration 1110, loss = 0.01826231\n",
      "Iteration 1111, loss = 0.01822689\n",
      "Iteration 1112, loss = 0.01818935\n",
      "Iteration 1113, loss = 0.01815004\n",
      "Iteration 1114, loss = 0.01811034\n",
      "Iteration 1115, loss = 0.01807168\n",
      "Iteration 1116, loss = 0.01803465\n",
      "Iteration 1117, loss = 0.01799877\n",
      "Iteration 1118, loss = 0.01796306\n",
      "Iteration 1119, loss = 0.01792668\n",
      "Iteration 1120, loss = 0.01788942\n",
      "Iteration 1121, loss = 0.01785160\n",
      "Iteration 1122, loss = 0.01781387\n",
      "Iteration 1123, loss = 0.01777672\n",
      "Iteration 1124, loss = 0.01774028\n",
      "Iteration 1125, loss = 0.01770433\n",
      "Iteration 1126, loss = 0.01766844\n",
      "Iteration 1127, loss = 0.01763233\n",
      "Iteration 1128, loss = 0.01759589\n",
      "Iteration 1129, loss = 0.01755924\n",
      "Iteration 1130, loss = 0.01752262\n",
      "Iteration 1131, loss = 0.01748624\n",
      "Iteration 1132, loss = 0.01745018\n",
      "Iteration 1133, loss = 0.01741440\n",
      "Iteration 1134, loss = 0.01737875\n",
      "Iteration 1135, loss = 0.01734312\n",
      "Iteration 1136, loss = 0.01730741\n",
      "Iteration 1137, loss = 0.01727162\n",
      "Iteration 1138, loss = 0.01723582\n",
      "Iteration 1139, loss = 0.01720009\n",
      "Iteration 1140, loss = 0.01716449\n",
      "Iteration 1141, loss = 0.01712905\n",
      "Iteration 1142, loss = 0.01709376\n",
      "Iteration 1143, loss = 0.01705857\n",
      "Iteration 1144, loss = 0.01702343\n",
      "Iteration 1145, loss = 0.01698832\n",
      "Iteration 1146, loss = 0.01695322\n",
      "Iteration 1147, loss = 0.01691815\n",
      "Iteration 1148, loss = 0.01688313\n",
      "Iteration 1149, loss = 0.01684818\n",
      "Iteration 1150, loss = 0.01681332\n",
      "Iteration 1151, loss = 0.01677855\n",
      "Iteration 1152, loss = 0.01674388\n",
      "Iteration 1153, loss = 0.01670929\n",
      "Iteration 1154, loss = 0.01667476\n",
      "Iteration 1155, loss = 0.01664029\n",
      "Iteration 1156, loss = 0.01660587\n",
      "Iteration 1157, loss = 0.01657150\n",
      "Iteration 1158, loss = 0.01653719\n",
      "Iteration 1159, loss = 0.01650294\n",
      "Iteration 1160, loss = 0.01646876\n",
      "Iteration 1161, loss = 0.01643464\n",
      "Iteration 1162, loss = 0.01640060\n",
      "Iteration 1163, loss = 0.01636664\n",
      "Iteration 1164, loss = 0.01633274\n",
      "Iteration 1165, loss = 0.01629891\n",
      "Iteration 1166, loss = 0.01626515\n",
      "Iteration 1167, loss = 0.01623145\n",
      "Iteration 1168, loss = 0.01619782\n",
      "Iteration 1169, loss = 0.01616424\n",
      "Iteration 1170, loss = 0.01613073\n",
      "Iteration 1171, loss = 0.01609728\n",
      "Iteration 1172, loss = 0.01606390\n",
      "Iteration 1173, loss = 0.01603058\n",
      "Iteration 1174, loss = 0.01599733\n",
      "Iteration 1175, loss = 0.01596414\n",
      "Iteration 1176, loss = 0.01593102\n",
      "Iteration 1177, loss = 0.01589796\n",
      "Iteration 1178, loss = 0.01586498\n",
      "Iteration 1179, loss = 0.01583205\n",
      "Iteration 1180, loss = 0.01579920\n",
      "Iteration 1181, loss = 0.01576640\n",
      "Iteration 1182, loss = 0.01573368\n",
      "Iteration 1183, loss = 0.01570101\n",
      "Iteration 1184, loss = 0.01566842\n",
      "Iteration 1185, loss = 0.01563588\n",
      "Iteration 1186, loss = 0.01560341\n",
      "Iteration 1187, loss = 0.01557101\n",
      "Iteration 1188, loss = 0.01553867\n",
      "Iteration 1189, loss = 0.01550639\n",
      "Iteration 1190, loss = 0.01547418\n",
      "Iteration 1191, loss = 0.01544204\n",
      "Iteration 1192, loss = 0.01540996\n",
      "Iteration 1193, loss = 0.01537794\n",
      "Iteration 1194, loss = 0.01534599\n",
      "Iteration 1195, loss = 0.01531410\n",
      "Iteration 1196, loss = 0.01528227\n",
      "Iteration 1197, loss = 0.01525052\n",
      "Iteration 1198, loss = 0.01521882\n",
      "Iteration 1199, loss = 0.01518719\n",
      "Iteration 1200, loss = 0.01515563\n",
      "Iteration 1201, loss = 0.01512413\n",
      "Iteration 1202, loss = 0.01509269\n",
      "Iteration 1203, loss = 0.01506132\n",
      "Iteration 1204, loss = 0.01503001\n",
      "Iteration 1205, loss = 0.01499877\n",
      "Iteration 1206, loss = 0.01496759\n",
      "Iteration 1207, loss = 0.01493647\n",
      "Iteration 1208, loss = 0.01490542\n",
      "Iteration 1209, loss = 0.01487444\n",
      "Iteration 1210, loss = 0.01484352\n",
      "Iteration 1211, loss = 0.01481266\n",
      "Iteration 1212, loss = 0.01478187\n",
      "Iteration 1213, loss = 0.01475114\n",
      "Iteration 1214, loss = 0.01472048\n",
      "Iteration 1215, loss = 0.01468988\n",
      "Iteration 1216, loss = 0.01465935\n",
      "Iteration 1217, loss = 0.01462888\n",
      "Iteration 1218, loss = 0.01459848\n",
      "Iteration 1219, loss = 0.01456815\n",
      "Iteration 1220, loss = 0.01453789\n",
      "Iteration 1221, loss = 0.01450770\n",
      "Iteration 1222, loss = 0.01447758\n",
      "Iteration 1223, loss = 0.01444754\n",
      "Iteration 1224, loss = 0.01441758\n",
      "Iteration 1225, loss = 0.01438771\n",
      "Iteration 1226, loss = 0.01435795\n",
      "Iteration 1227, loss = 0.01432830\n",
      "Iteration 1228, loss = 0.01429879\n",
      "Iteration 1229, loss = 0.01426946\n",
      "Iteration 1230, loss = 0.01424035\n",
      "Iteration 1231, loss = 0.01421153\n",
      "Iteration 1232, loss = 0.01418311\n",
      "Iteration 1233, loss = 0.01415522\n",
      "Iteration 1234, loss = 0.01412807\n",
      "Iteration 1235, loss = 0.01410197\n",
      "Iteration 1236, loss = 0.01407731\n",
      "Iteration 1237, loss = 0.01405466\n",
      "Iteration 1238, loss = 0.01403467\n",
      "Iteration 1239, loss = 0.01401818\n",
      "Iteration 1240, loss = 0.01400587\n",
      "Iteration 1241, loss = 0.01399806\n",
      "Iteration 1242, loss = 0.01399377\n",
      "Iteration 1243, loss = 0.01398994\n",
      "Iteration 1244, loss = 0.01398077\n",
      "Iteration 1245, loss = 0.01395761\n",
      "Iteration 1246, loss = 0.01391443\n",
      "Iteration 1247, loss = 0.01385011\n",
      "Iteration 1248, loss = 0.01377605\n",
      "Iteration 1249, loss = 0.01370849\n",
      "Iteration 1250, loss = 0.01366139\n",
      "Iteration 1251, loss = 0.01363735\n",
      "Iteration 1252, loss = 0.01362820\n",
      "Iteration 1253, loss = 0.01362078\n",
      "Iteration 1254, loss = 0.01360327\n",
      "Iteration 1255, loss = 0.01357084\n",
      "Iteration 1256, loss = 0.01352637\n",
      "Iteration 1257, loss = 0.01347941\n",
      "Iteration 1258, loss = 0.01343942\n",
      "Iteration 1259, loss = 0.01341078\n",
      "Iteration 1260, loss = 0.01339092\n",
      "Iteration 1261, loss = 0.01337311\n",
      "Iteration 1262, loss = 0.01335091\n",
      "Iteration 1263, loss = 0.01332141\n",
      "Iteration 1264, loss = 0.01328657\n",
      "Iteration 1265, loss = 0.01325101\n",
      "Iteration 1266, loss = 0.01321915\n",
      "Iteration 1267, loss = 0.01319252\n",
      "Iteration 1268, loss = 0.01316944\n",
      "Iteration 1269, loss = 0.01314662\n",
      "Iteration 1270, loss = 0.01312129\n",
      "Iteration 1271, loss = 0.01309272\n",
      "Iteration 1272, loss = 0.01306223\n",
      "Iteration 1273, loss = 0.01303211\n",
      "Iteration 1274, loss = 0.01300406\n",
      "Iteration 1275, loss = 0.01297831\n",
      "Iteration 1276, loss = 0.01295377\n",
      "Iteration 1277, loss = 0.01292892\n",
      "Iteration 1278, loss = 0.01290274\n",
      "Iteration 1279, loss = 0.01287525\n",
      "Iteration 1280, loss = 0.01284724\n",
      "Iteration 1281, loss = 0.01281973\n",
      "Iteration 1282, loss = 0.01279328\n",
      "Iteration 1283, loss = 0.01276779\n",
      "Iteration 1284, loss = 0.01274266\n",
      "Iteration 1285, loss = 0.01271726\n",
      "Iteration 1286, loss = 0.01269126\n",
      "Iteration 1287, loss = 0.01266478\n",
      "Iteration 1288, loss = 0.01263823\n",
      "Iteration 1289, loss = 0.01261203\n",
      "Iteration 1290, loss = 0.01258634\n",
      "Iteration 1291, loss = 0.01256105\n",
      "Iteration 1292, loss = 0.01253589\n",
      "Iteration 1293, loss = 0.01251058\n",
      "Iteration 1294, loss = 0.01248505\n",
      "Iteration 1295, loss = 0.01245937\n",
      "Iteration 1296, loss = 0.01243373\n",
      "Iteration 1297, loss = 0.01240830\n",
      "Iteration 1298, loss = 0.01238313\n",
      "Iteration 1299, loss = 0.01235814\n",
      "Iteration 1300, loss = 0.01233322\n",
      "Iteration 1301, loss = 0.01230826\n",
      "Iteration 1302, loss = 0.01228323\n",
      "Iteration 1303, loss = 0.01225818\n",
      "Iteration 1304, loss = 0.01223319\n",
      "Iteration 1305, loss = 0.01220834\n",
      "Iteration 1306, loss = 0.01218362\n",
      "Iteration 1307, loss = 0.01215901\n",
      "Iteration 1308, loss = 0.01213446\n",
      "Iteration 1309, loss = 0.01210992\n",
      "Iteration 1310, loss = 0.01208538\n",
      "Iteration 1311, loss = 0.01206087\n",
      "Iteration 1312, loss = 0.01203642\n",
      "Iteration 1313, loss = 0.01201206\n",
      "Iteration 1314, loss = 0.01198779\n",
      "Iteration 1315, loss = 0.01196360\n",
      "Iteration 1316, loss = 0.01193946\n",
      "Iteration 1317, loss = 0.01191536\n",
      "Iteration 1318, loss = 0.01189129\n",
      "Iteration 1319, loss = 0.01186727\n",
      "Iteration 1320, loss = 0.01184331\n",
      "Iteration 1321, loss = 0.01181941\n",
      "Iteration 1322, loss = 0.01179559\n",
      "Iteration 1323, loss = 0.01177183\n",
      "Iteration 1324, loss = 0.01174813\n",
      "Iteration 1325, loss = 0.01172447\n",
      "Iteration 1326, loss = 0.01170086\n",
      "Iteration 1327, loss = 0.01167730\n",
      "Iteration 1328, loss = 0.01165380\n",
      "Iteration 1329, loss = 0.01163035\n",
      "Iteration 1330, loss = 0.01160697\n",
      "Iteration 1331, loss = 0.01158365\n",
      "Iteration 1332, loss = 0.01156038\n",
      "Iteration 1333, loss = 0.01153717\n",
      "Iteration 1334, loss = 0.01151400\n",
      "Iteration 1335, loss = 0.01149089\n",
      "Iteration 1336, loss = 0.01146783\n",
      "Iteration 1337, loss = 0.01144483\n",
      "Iteration 1338, loss = 0.01142189\n",
      "Iteration 1339, loss = 0.01139900\n",
      "Iteration 1340, loss = 0.01137617\n",
      "Iteration 1341, loss = 0.01135339\n",
      "Iteration 1342, loss = 0.01133067\n",
      "Iteration 1343, loss = 0.01130799\n",
      "Iteration 1344, loss = 0.01128537\n",
      "Iteration 1345, loss = 0.01126281\n",
      "Iteration 1346, loss = 0.01124030\n",
      "Iteration 1347, loss = 0.01121784\n",
      "Iteration 1348, loss = 0.01119544\n",
      "Iteration 1349, loss = 0.01117309\n",
      "Iteration 1350, loss = 0.01115080\n",
      "Iteration 1351, loss = 0.01112855\n",
      "Iteration 1352, loss = 0.01110636\n",
      "Iteration 1353, loss = 0.01108423\n",
      "Iteration 1354, loss = 0.01106214\n",
      "Iteration 1355, loss = 0.01104011\n",
      "Iteration 1356, loss = 0.01101814\n",
      "Iteration 1357, loss = 0.01099621\n",
      "Iteration 1358, loss = 0.01097434\n",
      "Iteration 1359, loss = 0.01095252\n",
      "Iteration 1360, loss = 0.01093076\n",
      "Iteration 1361, loss = 0.01090904\n",
      "Iteration 1362, loss = 0.01088738\n",
      "Iteration 1363, loss = 0.01086577\n",
      "Iteration 1364, loss = 0.01084421\n",
      "Iteration 1365, loss = 0.01082271\n",
      "Iteration 1366, loss = 0.01080125\n",
      "Iteration 1367, loss = 0.01077985\n",
      "Iteration 1368, loss = 0.01075850\n",
      "Iteration 1369, loss = 0.01073720\n",
      "Iteration 1370, loss = 0.01071595\n",
      "Iteration 1371, loss = 0.01069476\n",
      "Iteration 1372, loss = 0.01067361\n",
      "Iteration 1373, loss = 0.01065252\n",
      "Iteration 1374, loss = 0.01063148\n",
      "Iteration 1375, loss = 0.01061048\n",
      "Iteration 1376, loss = 0.01058954\n",
      "Iteration 1377, loss = 0.01056865\n",
      "Iteration 1378, loss = 0.01054781\n",
      "Iteration 1379, loss = 0.01052703\n",
      "Iteration 1380, loss = 0.01050629\n",
      "Iteration 1381, loss = 0.01048560\n",
      "Iteration 1382, loss = 0.01046496\n",
      "Iteration 1383, loss = 0.01044437\n",
      "Iteration 1384, loss = 0.01042384\n",
      "Iteration 1385, loss = 0.01040335\n",
      "Iteration 1386, loss = 0.01038291\n",
      "Iteration 1387, loss = 0.01036252\n",
      "Iteration 1388, loss = 0.01034218\n",
      "Iteration 1389, loss = 0.01032189\n",
      "Iteration 1390, loss = 0.01030165\n",
      "Iteration 1391, loss = 0.01028146\n",
      "Iteration 1392, loss = 0.01026132\n",
      "Iteration 1393, loss = 0.01024123\n",
      "Iteration 1394, loss = 0.01022119\n",
      "Iteration 1395, loss = 0.01020119\n",
      "Iteration 1396, loss = 0.01018125\n",
      "Iteration 1397, loss = 0.01016135\n",
      "Iteration 1398, loss = 0.01014150\n",
      "Iteration 1399, loss = 0.01012170\n",
      "Iteration 1400, loss = 0.01010195\n",
      "Iteration 1401, loss = 0.01008225\n",
      "Iteration 1402, loss = 0.01006259\n",
      "Iteration 1403, loss = 0.01004298\n",
      "Iteration 1404, loss = 0.01002343\n",
      "Iteration 1405, loss = 0.01000391\n",
      "Iteration 1406, loss = 0.00998445\n",
      "Iteration 1407, loss = 0.00996503\n",
      "Iteration 1408, loss = 0.00994567\n",
      "Iteration 1409, loss = 0.00992634\n",
      "Iteration 1410, loss = 0.00990707\n",
      "Iteration 1411, loss = 0.00988784\n",
      "Iteration 1412, loss = 0.00986866\n",
      "Iteration 1413, loss = 0.00984953\n",
      "Iteration 1414, loss = 0.00983044\n",
      "Iteration 1415, loss = 0.00981141\n",
      "Iteration 1416, loss = 0.00979241\n",
      "Iteration 1417, loss = 0.00977347\n",
      "Iteration 1418, loss = 0.00975457\n",
      "Iteration 1419, loss = 0.00973571\n",
      "Iteration 1420, loss = 0.00971691\n",
      "Iteration 1421, loss = 0.00969815\n",
      "Iteration 1422, loss = 0.00967943\n",
      "Iteration 1423, loss = 0.00966076\n",
      "Iteration 1424, loss = 0.00964214\n",
      "Iteration 1425, loss = 0.00962356\n",
      "Iteration 1426, loss = 0.00960503\n",
      "Iteration 1427, loss = 0.00958654\n",
      "Iteration 1428, loss = 0.00956810\n",
      "Iteration 1429, loss = 0.00954971\n",
      "Iteration 1430, loss = 0.00953135\n",
      "Iteration 1431, loss = 0.00951305\n",
      "Iteration 1432, loss = 0.00949479\n",
      "Iteration 1433, loss = 0.00947657\n",
      "Iteration 1434, loss = 0.00945840\n",
      "Iteration 1435, loss = 0.00944027\n",
      "Iteration 1436, loss = 0.00942219\n",
      "Iteration 1437, loss = 0.00940415\n",
      "Iteration 1438, loss = 0.00938616\n",
      "Iteration 1439, loss = 0.00936821\n",
      "Iteration 1440, loss = 0.00935031\n",
      "Iteration 1441, loss = 0.00933244\n",
      "Iteration 1442, loss = 0.00931463\n",
      "Iteration 1443, loss = 0.00929685\n",
      "Iteration 1444, loss = 0.00927912\n",
      "Iteration 1445, loss = 0.00926144\n",
      "Iteration 1446, loss = 0.00924379\n",
      "Iteration 1447, loss = 0.00922619\n",
      "Iteration 1448, loss = 0.00920864\n",
      "Iteration 1449, loss = 0.00919112\n",
      "Iteration 1450, loss = 0.00917365\n",
      "Iteration 1451, loss = 0.00915623\n",
      "Iteration 1452, loss = 0.00913884\n",
      "Iteration 1453, loss = 0.00912150\n",
      "Iteration 1454, loss = 0.00910420\n",
      "Iteration 1455, loss = 0.00908694\n",
      "Iteration 1456, loss = 0.00906973\n",
      "Iteration 1457, loss = 0.00905256\n",
      "Iteration 1458, loss = 0.00903543\n",
      "Iteration 1459, loss = 0.00901834\n",
      "Iteration 1460, loss = 0.00900129\n",
      "Iteration 1461, loss = 0.00898429\n",
      "Iteration 1462, loss = 0.00896733\n",
      "Iteration 1463, loss = 0.00895041\n",
      "Iteration 1464, loss = 0.00893353\n",
      "Iteration 1465, loss = 0.00891669\n",
      "Iteration 1466, loss = 0.00889989\n",
      "Iteration 1467, loss = 0.00888314\n",
      "Iteration 1468, loss = 0.00886643\n",
      "Iteration 1469, loss = 0.00884975\n",
      "Iteration 1470, loss = 0.00883312\n",
      "Iteration 1471, loss = 0.00881653\n",
      "Iteration 1472, loss = 0.00879998\n",
      "Iteration 1473, loss = 0.00878347\n",
      "Iteration 1474, loss = 0.00876700\n",
      "Iteration 1475, loss = 0.00875057\n",
      "Iteration 1476, loss = 0.00873418\n",
      "Iteration 1477, loss = 0.00871783\n",
      "Iteration 1478, loss = 0.00870152\n",
      "Iteration 1479, loss = 0.00868526\n",
      "Iteration 1480, loss = 0.00866903\n",
      "Iteration 1481, loss = 0.00865284\n",
      "Iteration 1482, loss = 0.00863669\n",
      "Iteration 1483, loss = 0.00862058\n",
      "Iteration 1484, loss = 0.00860451\n",
      "Iteration 1485, loss = 0.00858848\n",
      "Iteration 1486, loss = 0.00857249\n",
      "Iteration 1487, loss = 0.00855654\n",
      "Iteration 1488, loss = 0.00854062\n",
      "Iteration 1489, loss = 0.00852475\n",
      "Iteration 1490, loss = 0.00850891\n",
      "Iteration 1491, loss = 0.00849312\n",
      "Iteration 1492, loss = 0.00847736\n",
      "Iteration 1493, loss = 0.00846164\n",
      "Iteration 1494, loss = 0.00844596\n",
      "Iteration 1495, loss = 0.00843032\n",
      "Iteration 1496, loss = 0.00841471\n",
      "Iteration 1497, loss = 0.00839915\n",
      "Iteration 1498, loss = 0.00838362\n",
      "Iteration 1499, loss = 0.00836813\n",
      "Iteration 1500, loss = 0.00835267\n",
      "Iteration 1501, loss = 0.00833726\n",
      "Iteration 1502, loss = 0.00832188\n",
      "Iteration 1503, loss = 0.00830654\n",
      "Iteration 1504, loss = 0.00829124\n",
      "Iteration 1505, loss = 0.00827598\n",
      "Iteration 1506, loss = 0.00826075\n",
      "Iteration 1507, loss = 0.00824556\n",
      "Iteration 1508, loss = 0.00823041\n",
      "Iteration 1509, loss = 0.00821529\n",
      "Iteration 1510, loss = 0.00820021\n",
      "Iteration 1511, loss = 0.00818517\n",
      "Iteration 1512, loss = 0.00817016\n",
      "Iteration 1513, loss = 0.00815519\n",
      "Iteration 1514, loss = 0.00814026\n",
      "Iteration 1515, loss = 0.00812537\n",
      "Iteration 1516, loss = 0.00811051\n",
      "Iteration 1517, loss = 0.00809568\n",
      "Iteration 1518, loss = 0.00808089\n",
      "Iteration 1519, loss = 0.00806614\n",
      "Iteration 1520, loss = 0.00805143\n",
      "Iteration 1521, loss = 0.00803675\n",
      "Iteration 1522, loss = 0.00802210\n",
      "Iteration 1523, loss = 0.00800749\n",
      "Iteration 1524, loss = 0.00799292\n",
      "Iteration 1525, loss = 0.00797838\n",
      "Iteration 1526, loss = 0.00796388\n",
      "Iteration 1527, loss = 0.00794941\n",
      "Iteration 1528, loss = 0.00793498\n",
      "Iteration 1529, loss = 0.00792058\n",
      "Iteration 1530, loss = 0.00790622\n",
      "Iteration 1531, loss = 0.00789189\n",
      "Iteration 1532, loss = 0.00787760\n",
      "Iteration 1533, loss = 0.00786334\n",
      "Iteration 1534, loss = 0.00784912\n",
      "Iteration 1535, loss = 0.00783493\n",
      "Iteration 1536, loss = 0.00782078\n",
      "Iteration 1537, loss = 0.00780666\n",
      "Iteration 1538, loss = 0.00779257\n",
      "Iteration 1539, loss = 0.00777852\n",
      "Iteration 1540, loss = 0.00776450\n",
      "Iteration 1541, loss = 0.00775052\n",
      "Iteration 1542, loss = 0.00773657\n",
      "Iteration 1543, loss = 0.00772265\n",
      "Iteration 1544, loss = 0.00770877\n",
      "Iteration 1545, loss = 0.00769492\n",
      "Iteration 1546, loss = 0.00768110\n",
      "Iteration 1547, loss = 0.00766732\n",
      "Iteration 1548, loss = 0.00765357\n",
      "Iteration 1549, loss = 0.00763985\n",
      "Iteration 1550, loss = 0.00762617\n",
      "Iteration 1551, loss = 0.00761252\n",
      "Iteration 1552, loss = 0.00759890\n",
      "Iteration 1553, loss = 0.00758532\n",
      "Iteration 1554, loss = 0.00757177\n",
      "Iteration 1555, loss = 0.00755825\n",
      "Iteration 1556, loss = 0.00754476\n",
      "Iteration 1557, loss = 0.00753131\n",
      "Iteration 1558, loss = 0.00751789\n",
      "Iteration 1559, loss = 0.00750450\n",
      "Iteration 1560, loss = 0.00749114\n",
      "Iteration 1561, loss = 0.00747782\n",
      "Iteration 1562, loss = 0.00746452\n",
      "Iteration 1563, loss = 0.00745126\n",
      "Iteration 1564, loss = 0.00743803\n",
      "Iteration 1565, loss = 0.00742484\n",
      "Iteration 1566, loss = 0.00741167\n",
      "Iteration 1567, loss = 0.00739854\n",
      "Iteration 1568, loss = 0.00738543\n",
      "Iteration 1569, loss = 0.00737236\n",
      "Iteration 1570, loss = 0.00735932\n",
      "Iteration 1571, loss = 0.00734631\n",
      "Iteration 1572, loss = 0.00733334\n",
      "Iteration 1573, loss = 0.00732039\n",
      "Iteration 1574, loss = 0.00730747\n",
      "Iteration 1575, loss = 0.00729459\n",
      "Iteration 1576, loss = 0.00728174\n",
      "Iteration 1577, loss = 0.00726891\n",
      "Iteration 1578, loss = 0.00725612\n",
      "Iteration 1579, loss = 0.00724336\n",
      "Iteration 1580, loss = 0.00723063\n",
      "Iteration 1581, loss = 0.00721793\n",
      "Iteration 1582, loss = 0.00720526\n",
      "Iteration 1583, loss = 0.00719262\n",
      "Iteration 1584, loss = 0.00718000\n",
      "Iteration 1585, loss = 0.00716742\n",
      "Iteration 1586, loss = 0.00715487\n",
      "Iteration 1587, loss = 0.00714235\n",
      "Iteration 1588, loss = 0.00712986\n",
      "Iteration 1589, loss = 0.00711740\n",
      "Iteration 1590, loss = 0.00710497\n",
      "Iteration 1591, loss = 0.00709257\n",
      "Iteration 1592, loss = 0.00708020\n",
      "Iteration 1593, loss = 0.00706785\n",
      "Iteration 1594, loss = 0.00705554\n",
      "Iteration 1595, loss = 0.00704326\n",
      "Iteration 1596, loss = 0.00703100\n",
      "Iteration 1597, loss = 0.00701877\n",
      "Iteration 1598, loss = 0.00700658\n",
      "Iteration 1599, loss = 0.00699441\n",
      "Iteration 1600, loss = 0.00698227\n",
      "Iteration 1601, loss = 0.00697016\n",
      "Iteration 1602, loss = 0.00695808\n",
      "Iteration 1603, loss = 0.00694602\n",
      "Iteration 1604, loss = 0.00693400\n",
      "Iteration 1605, loss = 0.00692200\n",
      "Iteration 1606, loss = 0.00691003\n",
      "Iteration 1607, loss = 0.00689809\n",
      "Iteration 1608, loss = 0.00688618\n",
      "Iteration 1609, loss = 0.00687430\n",
      "Iteration 1610, loss = 0.00686244\n",
      "Iteration 1611, loss = 0.00685061\n",
      "Iteration 1612, loss = 0.00683881\n",
      "Iteration 1613, loss = 0.00682704\n",
      "Iteration 1614, loss = 0.00681530\n",
      "Iteration 1615, loss = 0.00680358\n",
      "Iteration 1616, loss = 0.00679189\n",
      "Iteration 1617, loss = 0.00678023\n",
      "Iteration 1618, loss = 0.00676860\n",
      "Iteration 1619, loss = 0.00675699\n",
      "Iteration 1620, loss = 0.00674541\n",
      "Iteration 1621, loss = 0.00673386\n",
      "Iteration 1622, loss = 0.00672233\n",
      "Iteration 1623, loss = 0.00671083\n",
      "Iteration 1624, loss = 0.00669936\n",
      "Iteration 1625, loss = 0.00668792\n",
      "Iteration 1626, loss = 0.00667650\n",
      "Iteration 1627, loss = 0.00666511\n",
      "Iteration 1628, loss = 0.00665374\n",
      "Iteration 1629, loss = 0.00664241\n",
      "Iteration 1630, loss = 0.00663110\n",
      "Iteration 1631, loss = 0.00661981\n",
      "Iteration 1632, loss = 0.00660855\n",
      "Iteration 1633, loss = 0.00659732\n",
      "Iteration 1634, loss = 0.00658611\n",
      "Iteration 1635, loss = 0.00657494\n",
      "Iteration 1636, loss = 0.00656378\n",
      "Iteration 1637, loss = 0.00655265\n",
      "Iteration 1638, loss = 0.00654155\n",
      "Iteration 1639, loss = 0.00653048\n",
      "Iteration 1640, loss = 0.00651943\n",
      "Iteration 1641, loss = 0.00650840\n",
      "Iteration 1642, loss = 0.00649740\n",
      "Iteration 1643, loss = 0.00648643\n",
      "Iteration 1644, loss = 0.00647548\n",
      "Iteration 1645, loss = 0.00646456\n",
      "Iteration 1646, loss = 0.00645366\n",
      "Iteration 1647, loss = 0.00644279\n",
      "Iteration 1648, loss = 0.00643195\n",
      "Iteration 1649, loss = 0.00642112\n",
      "Iteration 1650, loss = 0.00641033\n",
      "Iteration 1651, loss = 0.00639956\n",
      "Iteration 1652, loss = 0.00638881\n",
      "Iteration 1653, loss = 0.00637809\n",
      "Iteration 1654, loss = 0.00636739\n",
      "Iteration 1655, loss = 0.00635672\n",
      "Iteration 1656, loss = 0.00634607\n",
      "Iteration 1657, loss = 0.00633545\n",
      "Iteration 1658, loss = 0.00632485\n",
      "Iteration 1659, loss = 0.00631428\n",
      "Iteration 1660, loss = 0.00630373\n",
      "Iteration 1661, loss = 0.00629320\n",
      "Iteration 1662, loss = 0.00628270\n",
      "Iteration 1663, loss = 0.00627223\n",
      "Iteration 1664, loss = 0.00626178\n",
      "Iteration 1665, loss = 0.00625135\n",
      "Iteration 1666, loss = 0.00624094\n",
      "Iteration 1667, loss = 0.00623056\n",
      "Iteration 1668, loss = 0.00622021\n",
      "Iteration 1669, loss = 0.00620987\n",
      "Iteration 1670, loss = 0.00619956\n",
      "Iteration 1671, loss = 0.00618928\n",
      "Iteration 1672, loss = 0.00617902\n",
      "Iteration 1673, loss = 0.00616878\n",
      "Iteration 1674, loss = 0.00615856\n",
      "Iteration 1675, loss = 0.00614837\n",
      "Iteration 1676, loss = 0.00613820\n",
      "Iteration 1677, loss = 0.00612806\n",
      "Iteration 1678, loss = 0.00611794\n",
      "Iteration 1679, loss = 0.00610784\n",
      "Iteration 1680, loss = 0.00609776\n",
      "Iteration 1681, loss = 0.00608771\n",
      "Iteration 1682, loss = 0.00607768\n",
      "Iteration 1683, loss = 0.00606767\n",
      "Iteration 1684, loss = 0.00605769\n",
      "Iteration 1685, loss = 0.00604773\n",
      "Iteration 1686, loss = 0.00603779\n",
      "Iteration 1687, loss = 0.00602787\n",
      "Iteration 1688, loss = 0.00601798\n",
      "Iteration 1689, loss = 0.00600811\n",
      "Iteration 1690, loss = 0.00599826\n",
      "Iteration 1691, loss = 0.00598844\n",
      "Iteration 1692, loss = 0.00597863\n",
      "Iteration 1693, loss = 0.00596885\n",
      "Iteration 1694, loss = 0.00595909\n",
      "Iteration 1695, loss = 0.00594935\n",
      "Iteration 1696, loss = 0.00593964\n",
      "Iteration 1697, loss = 0.00592995\n",
      "Iteration 1698, loss = 0.00592027\n",
      "Iteration 1699, loss = 0.00591063\n",
      "Iteration 1700, loss = 0.00590100\n",
      "Iteration 1701, loss = 0.00589139\n",
      "Iteration 1702, loss = 0.00588181\n",
      "Iteration 1703, loss = 0.00587225\n",
      "Iteration 1704, loss = 0.00586271\n",
      "Iteration 1705, loss = 0.00585319\n",
      "Iteration 1706, loss = 0.00584369\n",
      "Iteration 1707, loss = 0.00583421\n",
      "Iteration 1708, loss = 0.00582476\n",
      "Iteration 1709, loss = 0.00581532\n",
      "Iteration 1710, loss = 0.00580591\n",
      "Iteration 1711, loss = 0.00579652\n",
      "Iteration 1712, loss = 0.00578715\n",
      "Iteration 1713, loss = 0.00577780\n",
      "Iteration 1714, loss = 0.00576847\n",
      "Iteration 1715, loss = 0.00575917\n",
      "Iteration 1716, loss = 0.00574988\n",
      "Iteration 1717, loss = 0.00574061\n",
      "Iteration 1718, loss = 0.00573137\n",
      "Iteration 1719, loss = 0.00572214\n",
      "Iteration 1720, loss = 0.00571294\n",
      "Iteration 1721, loss = 0.00570376\n",
      "Iteration 1722, loss = 0.00569460\n",
      "Iteration 1723, loss = 0.00568545\n",
      "Iteration 1724, loss = 0.00567633\n",
      "Iteration 1725, loss = 0.00566723\n",
      "Iteration 1726, loss = 0.00565815\n",
      "Iteration 1727, loss = 0.00564909\n",
      "Iteration 1728, loss = 0.00564005\n",
      "Iteration 1729, loss = 0.00563103\n",
      "Iteration 1730, loss = 0.00562203\n",
      "Iteration 1731, loss = 0.00561305\n",
      "Iteration 1732, loss = 0.00560409\n",
      "Iteration 1733, loss = 0.00559515\n",
      "Iteration 1734, loss = 0.00558623\n",
      "Iteration 1735, loss = 0.00557733\n",
      "Iteration 1736, loss = 0.00556845\n",
      "Iteration 1737, loss = 0.00555959\n",
      "Iteration 1738, loss = 0.00555075\n",
      "Iteration 1739, loss = 0.00554193\n",
      "Iteration 1740, loss = 0.00553313\n",
      "Iteration 1741, loss = 0.00552434\n",
      "Iteration 1742, loss = 0.00551558\n",
      "Iteration 1743, loss = 0.00550684\n",
      "Iteration 1744, loss = 0.00549811\n",
      "Iteration 1745, loss = 0.00548941\n",
      "Iteration 1746, loss = 0.00548072\n",
      "Iteration 1747, loss = 0.00547205\n",
      "Iteration 1748, loss = 0.00546341\n",
      "Iteration 1749, loss = 0.00545478\n",
      "Iteration 1750, loss = 0.00544617\n",
      "Iteration 1751, loss = 0.00543758\n",
      "Iteration 1752, loss = 0.00542901\n",
      "Iteration 1753, loss = 0.00542045\n",
      "Iteration 1754, loss = 0.00541192\n",
      "Iteration 1755, loss = 0.00540341\n",
      "Iteration 1756, loss = 0.00539491\n",
      "Iteration 1757, loss = 0.00538643\n",
      "Iteration 1758, loss = 0.00537797\n",
      "Iteration 1759, loss = 0.00536953\n",
      "Iteration 1760, loss = 0.00536111\n",
      "Iteration 1761, loss = 0.00535271\n",
      "Iteration 1762, loss = 0.00534432\n",
      "Iteration 1763, loss = 0.00533596\n",
      "Iteration 1764, loss = 0.00532761\n",
      "Iteration 1765, loss = 0.00531928\n",
      "Iteration 1766, loss = 0.00531096\n",
      "Iteration 1767, loss = 0.00530267\n",
      "Iteration 1768, loss = 0.00529440\n",
      "Iteration 1769, loss = 0.00528614\n",
      "Iteration 1770, loss = 0.00527790\n",
      "Iteration 1771, loss = 0.00526968\n",
      "Iteration 1772, loss = 0.00526147\n",
      "Iteration 1773, loss = 0.00525329\n",
      "Iteration 1774, loss = 0.00524512\n",
      "Iteration 1775, loss = 0.00523697\n",
      "Iteration 1776, loss = 0.00522884\n",
      "Iteration 1777, loss = 0.00522072\n",
      "Iteration 1778, loss = 0.00521262\n",
      "Iteration 1779, loss = 0.00520455\n",
      "Iteration 1780, loss = 0.00519648\n",
      "Iteration 1781, loss = 0.00518844\n",
      "Iteration 1782, loss = 0.00518041\n",
      "Iteration 1783, loss = 0.00517240\n",
      "Iteration 1784, loss = 0.00516441\n",
      "Iteration 1785, loss = 0.00515643\n",
      "Iteration 1786, loss = 0.00514848\n",
      "Iteration 1787, loss = 0.00514054\n",
      "Iteration 1788, loss = 0.00513261\n",
      "Iteration 1789, loss = 0.00512471\n",
      "Iteration 1790, loss = 0.00511682\n",
      "Iteration 1791, loss = 0.00510895\n",
      "Iteration 1792, loss = 0.00510109\n",
      "Iteration 1793, loss = 0.00509325\n",
      "Iteration 1794, loss = 0.00508543\n",
      "Iteration 1795, loss = 0.00507763\n",
      "Iteration 1796, loss = 0.00506984\n",
      "Iteration 1797, loss = 0.00506207\n",
      "Iteration 1798, loss = 0.00505431\n",
      "Iteration 1799, loss = 0.00504658\n",
      "Iteration 1800, loss = 0.00503885\n",
      "Iteration 1801, loss = 0.00503115\n",
      "Iteration 1802, loss = 0.00502346\n",
      "Iteration 1803, loss = 0.00501579\n",
      "Iteration 1804, loss = 0.00500814\n",
      "Iteration 1805, loss = 0.00500050\n",
      "Iteration 1806, loss = 0.00499288\n",
      "Iteration 1807, loss = 0.00498527\n",
      "Iteration 1808, loss = 0.00497768\n",
      "Iteration 1809, loss = 0.00497011\n",
      "Iteration 1810, loss = 0.00496255\n",
      "Iteration 1811, loss = 0.00495501\n",
      "Iteration 1812, loss = 0.00494748\n",
      "Iteration 1813, loss = 0.00493997\n",
      "Iteration 1814, loss = 0.00493248\n",
      "Iteration 1815, loss = 0.00492501\n",
      "Iteration 1816, loss = 0.00491754\n",
      "Iteration 1817, loss = 0.00491010\n",
      "Iteration 1818, loss = 0.00490267\n",
      "Iteration 1819, loss = 0.00489526\n",
      "Iteration 1820, loss = 0.00488786\n",
      "Iteration 1821, loss = 0.00488048\n",
      "Iteration 1822, loss = 0.00487311\n",
      "Iteration 1823, loss = 0.00486576\n",
      "Iteration 1824, loss = 0.00485843\n",
      "Iteration 1825, loss = 0.00485111\n",
      "Iteration 1826, loss = 0.00484380\n",
      "Iteration 1827, loss = 0.00483651\n",
      "Iteration 1828, loss = 0.00482924\n",
      "Iteration 1829, loss = 0.00482198\n",
      "Iteration 1830, loss = 0.00481474\n",
      "Iteration 1831, loss = 0.00480751\n",
      "Iteration 1832, loss = 0.00480030\n",
      "Iteration 1833, loss = 0.00479311\n",
      "Iteration 1834, loss = 0.00478592\n",
      "Iteration 1835, loss = 0.00477876\n",
      "Iteration 1836, loss = 0.00477161\n",
      "Iteration 1837, loss = 0.00476447\n",
      "Iteration 1838, loss = 0.00475735\n",
      "Iteration 1839, loss = 0.00475024\n",
      "Iteration 1840, loss = 0.00474315\n",
      "Iteration 1841, loss = 0.00473608\n",
      "Iteration 1842, loss = 0.00472902\n",
      "Iteration 1843, loss = 0.00472197\n",
      "Iteration 1844, loss = 0.00471494\n",
      "Iteration 1845, loss = 0.00470792\n",
      "Iteration 1846, loss = 0.00470092\n",
      "Iteration 1847, loss = 0.00469393\n",
      "Iteration 1848, loss = 0.00468696\n",
      "Iteration 1849, loss = 0.00468000\n",
      "Iteration 1850, loss = 0.00467306\n",
      "Iteration 1851, loss = 0.00466613\n",
      "Iteration 1852, loss = 0.00465921\n",
      "Iteration 1853, loss = 0.00465231\n",
      "Iteration 1854, loss = 0.00464543\n",
      "Iteration 1855, loss = 0.00463856\n",
      "Iteration 1856, loss = 0.00463170\n",
      "Iteration 1857, loss = 0.00462486\n",
      "Iteration 1858, loss = 0.00461803\n",
      "Iteration 1859, loss = 0.00461121\n",
      "Iteration 1860, loss = 0.00460441\n",
      "Iteration 1861, loss = 0.00459763\n",
      "Iteration 1862, loss = 0.00459085\n",
      "Iteration 1863, loss = 0.00458410\n",
      "Iteration 1864, loss = 0.00457735\n",
      "Iteration 1865, loss = 0.00457062\n",
      "Iteration 1866, loss = 0.00456391\n",
      "Iteration 1867, loss = 0.00455721\n",
      "Iteration 1868, loss = 0.00455052\n",
      "Iteration 1869, loss = 0.00454384\n",
      "Iteration 1870, loss = 0.00453718\n",
      "Iteration 1871, loss = 0.00453054\n",
      "Iteration 1872, loss = 0.00452390\n",
      "Iteration 1873, loss = 0.00451729\n",
      "Iteration 1874, loss = 0.00451068\n",
      "Iteration 1875, loss = 0.00450409\n",
      "Iteration 1876, loss = 0.00449751\n",
      "Iteration 1877, loss = 0.00449095\n",
      "Iteration 1878, loss = 0.00448440\n",
      "Iteration 1879, loss = 0.00447786\n",
      "Iteration 1880, loss = 0.00447133\n",
      "Iteration 1881, loss = 0.00446482\n",
      "Iteration 1882, loss = 0.00445833\n",
      "Iteration 1883, loss = 0.00445184\n",
      "Iteration 1884, loss = 0.00444537\n",
      "Iteration 1885, loss = 0.00443892\n",
      "Iteration 1886, loss = 0.00443247\n",
      "Iteration 1887, loss = 0.00442604\n",
      "Iteration 1888, loss = 0.00441962\n",
      "Iteration 1889, loss = 0.00441322\n",
      "Iteration 1890, loss = 0.00440683\n",
      "Iteration 1891, loss = 0.00440045\n",
      "Iteration 1892, loss = 0.00439408\n",
      "Iteration 1893, loss = 0.00438773\n",
      "Iteration 1894, loss = 0.00438139\n",
      "Iteration 1895, loss = 0.00437507\n",
      "Iteration 1896, loss = 0.00436875\n",
      "Iteration 1897, loss = 0.00436245\n",
      "Iteration 1898, loss = 0.00435617\n",
      "Iteration 1899, loss = 0.00434989\n",
      "Iteration 1900, loss = 0.00434363\n",
      "Iteration 1901, loss = 0.00433738\n",
      "Iteration 1902, loss = 0.00433115\n",
      "Iteration 1903, loss = 0.00432492\n",
      "Iteration 1904, loss = 0.00431871\n",
      "Iteration 1905, loss = 0.00431251\n",
      "Iteration 1906, loss = 0.00430633\n",
      "Iteration 1907, loss = 0.00430015\n",
      "Iteration 1908, loss = 0.00429399\n",
      "Iteration 1909, loss = 0.00428785\n",
      "Iteration 1910, loss = 0.00428171\n",
      "Iteration 1911, loss = 0.00427559\n",
      "Iteration 1912, loss = 0.00426948\n",
      "Iteration 1913, loss = 0.00426338\n",
      "Iteration 1914, loss = 0.00425729\n",
      "Iteration 1915, loss = 0.00425122\n",
      "Iteration 1916, loss = 0.00424516\n",
      "Iteration 1917, loss = 0.00423911\n",
      "Iteration 1918, loss = 0.00423307\n",
      "Iteration 1919, loss = 0.00422705\n",
      "Iteration 1920, loss = 0.00422103\n",
      "Iteration 1921, loss = 0.00421503\n",
      "Iteration 1922, loss = 0.00420904\n",
      "Iteration 1923, loss = 0.00420307\n",
      "Iteration 1924, loss = 0.00419710\n",
      "Iteration 1925, loss = 0.00419115\n",
      "Iteration 1926, loss = 0.00418521\n",
      "Iteration 1927, loss = 0.00417928\n",
      "Iteration 1928, loss = 0.00417337\n",
      "Iteration 1929, loss = 0.00416746\n",
      "Iteration 1930, loss = 0.00416157\n",
      "Iteration 1931, loss = 0.00415569\n",
      "Iteration 1932, loss = 0.00414982\n",
      "Iteration 1933, loss = 0.00414396\n",
      "Iteration 1934, loss = 0.00413812\n",
      "Iteration 1935, loss = 0.00413228\n",
      "Iteration 1936, loss = 0.00412646\n",
      "Iteration 1937, loss = 0.00412065\n",
      "Iteration 1938, loss = 0.00411485\n",
      "Iteration 1939, loss = 0.00410906\n",
      "Iteration 1940, loss = 0.00410329\n",
      "Iteration 1941, loss = 0.00409752\n",
      "Iteration 1942, loss = 0.00409177\n",
      "Iteration 1943, loss = 0.00408603\n",
      "Iteration 1944, loss = 0.00408030\n",
      "Iteration 1945, loss = 0.00407458\n",
      "Iteration 1946, loss = 0.00406888\n",
      "Iteration 1947, loss = 0.00406318\n",
      "Iteration 1948, loss = 0.00405750\n",
      "Iteration 1949, loss = 0.00405182\n",
      "Iteration 1950, loss = 0.00404616\n",
      "Iteration 1951, loss = 0.00404051\n",
      "Iteration 1952, loss = 0.00403487\n",
      "Iteration 1953, loss = 0.00402924\n",
      "Iteration 1954, loss = 0.00402363\n",
      "Iteration 1955, loss = 0.00401802\n",
      "Iteration 1956, loss = 0.00401242\n",
      "Iteration 1957, loss = 0.00400684\n",
      "Iteration 1958, loss = 0.00400127\n",
      "Iteration 1959, loss = 0.00399571\n",
      "Iteration 1960, loss = 0.00399016\n",
      "Iteration 1961, loss = 0.00398462\n",
      "Iteration 1962, loss = 0.00397909\n",
      "Iteration 1963, loss = 0.00397357\n",
      "Iteration 1964, loss = 0.00396806\n",
      "Iteration 1965, loss = 0.00396257\n",
      "Iteration 1966, loss = 0.00395708\n",
      "Iteration 1967, loss = 0.00395161\n",
      "Iteration 1968, loss = 0.00394614\n",
      "Iteration 1969, loss = 0.00394069\n",
      "Iteration 1970, loss = 0.00393525\n",
      "Iteration 1971, loss = 0.00392982\n",
      "Iteration 1972, loss = 0.00392440\n",
      "Iteration 1973, loss = 0.00391899\n",
      "Iteration 1974, loss = 0.00391359\n",
      "Iteration 1975, loss = 0.00390820\n",
      "Iteration 1976, loss = 0.00390282\n",
      "Iteration 1977, loss = 0.00389745\n",
      "Iteration 1978, loss = 0.00389209\n",
      "Iteration 1979, loss = 0.00388675\n",
      "Iteration 1980, loss = 0.00388141\n",
      "Iteration 1981, loss = 0.00387609\n",
      "Iteration 1982, loss = 0.00387077\n",
      "Iteration 1983, loss = 0.00386547\n",
      "Iteration 1984, loss = 0.00386017\n",
      "Iteration 1985, loss = 0.00385489\n",
      "Iteration 1986, loss = 0.00384961\n",
      "Iteration 1987, loss = 0.00384435\n",
      "Iteration 1988, loss = 0.00383909\n",
      "Iteration 1989, loss = 0.00383385\n",
      "Iteration 1990, loss = 0.00382862\n",
      "Iteration 1991, loss = 0.00382339\n",
      "Iteration 1992, loss = 0.00381818\n",
      "Iteration 1993, loss = 0.00381298\n",
      "Iteration 1994, loss = 0.00380779\n",
      "Iteration 1995, loss = 0.00380260\n",
      "Iteration 1996, loss = 0.00379743\n",
      "Iteration 1997, loss = 0.00379227\n",
      "Iteration 1998, loss = 0.00378712\n",
      "Iteration 1999, loss = 0.00378197\n",
      "Iteration 2000, loss = 0.00377684\n",
      "Iteration 2001, loss = 0.00377172\n",
      "Iteration 2002, loss = 0.00376661\n",
      "Iteration 2003, loss = 0.00376150\n",
      "Iteration 2004, loss = 0.00375641\n",
      "Iteration 2005, loss = 0.00375133\n",
      "Iteration 2006, loss = 0.00374625\n",
      "Iteration 2007, loss = 0.00374119\n",
      "Iteration 2008, loss = 0.00373614\n",
      "Iteration 2009, loss = 0.00373109\n",
      "Iteration 2010, loss = 0.00372606\n",
      "Iteration 2011, loss = 0.00372104\n",
      "Iteration 2012, loss = 0.00371602\n",
      "Iteration 2013, loss = 0.00371102\n",
      "Iteration 2014, loss = 0.00370602\n",
      "Iteration 2015, loss = 0.00370103\n",
      "Iteration 2016, loss = 0.00369606\n",
      "Iteration 2017, loss = 0.00369109\n",
      "Iteration 2018, loss = 0.00368613\n",
      "Iteration 2019, loss = 0.00368119\n",
      "Iteration 2020, loss = 0.00367625\n",
      "Iteration 2021, loss = 0.00367132\n",
      "Iteration 2022, loss = 0.00366640\n",
      "Iteration 2023, loss = 0.00366149\n",
      "Iteration 2024, loss = 0.00365659\n",
      "Iteration 2025, loss = 0.00365170\n",
      "Iteration 2026, loss = 0.00364681\n",
      "Iteration 2027, loss = 0.00364194\n",
      "Iteration 2028, loss = 0.00363708\n",
      "Iteration 2029, loss = 0.00363222\n",
      "Iteration 2030, loss = 0.00362738\n",
      "Iteration 2031, loss = 0.00362254\n",
      "Iteration 2032, loss = 0.00361772\n",
      "Iteration 2033, loss = 0.00361290\n",
      "Iteration 2034, loss = 0.00360809\n",
      "Iteration 2035, loss = 0.00360329\n",
      "Iteration 2036, loss = 0.00359850\n",
      "Iteration 2037, loss = 0.00359372\n",
      "Iteration 2038, loss = 0.00358895\n",
      "Iteration 2039, loss = 0.00358419\n",
      "Iteration 2040, loss = 0.00357944\n",
      "Iteration 2041, loss = 0.00357469\n",
      "Iteration 2042, loss = 0.00356996\n",
      "Iteration 2043, loss = 0.00356523\n",
      "Iteration 2044, loss = 0.00356051\n",
      "Iteration 2045, loss = 0.00355580\n",
      "Iteration 2046, loss = 0.00355110\n",
      "Iteration 2047, loss = 0.00354641\n",
      "Iteration 2048, loss = 0.00354173\n",
      "Iteration 2049, loss = 0.00353706\n",
      "Iteration 2050, loss = 0.00353239\n",
      "Iteration 2051, loss = 0.00352774\n",
      "Iteration 2052, loss = 0.00352309\n",
      "Iteration 2053, loss = 0.00351845\n",
      "Iteration 2054, loss = 0.00351383\n",
      "Iteration 2055, loss = 0.00350920\n",
      "Iteration 2056, loss = 0.00350459\n",
      "Iteration 2057, loss = 0.00349999\n",
      "Iteration 2058, loss = 0.00349540\n",
      "Iteration 2059, loss = 0.00349081\n",
      "Iteration 2060, loss = 0.00348623\n",
      "Iteration 2061, loss = 0.00348166\n",
      "Iteration 2062, loss = 0.00347710\n",
      "Iteration 2063, loss = 0.00347255\n",
      "Iteration 2064, loss = 0.00346801\n",
      "Iteration 2065, loss = 0.00346348\n",
      "Iteration 2066, loss = 0.00345895\n",
      "Iteration 2067, loss = 0.00345443\n",
      "Iteration 2068, loss = 0.00344992\n",
      "Iteration 2069, loss = 0.00344542\n",
      "Iteration 2070, loss = 0.00344093\n",
      "Iteration 2071, loss = 0.00343645\n",
      "Iteration 2072, loss = 0.00343197\n",
      "Iteration 2073, loss = 0.00342750\n",
      "Iteration 2074, loss = 0.00342305\n",
      "Iteration 2075, loss = 0.00341859\n",
      "Iteration 2076, loss = 0.00341415\n",
      "Iteration 2077, loss = 0.00340972\n",
      "Iteration 2078, loss = 0.00340529\n",
      "Iteration 2079, loss = 0.00340088\n",
      "Iteration 2080, loss = 0.00339647\n",
      "Iteration 2081, loss = 0.00339207\n",
      "Iteration 2082, loss = 0.00338767\n",
      "Iteration 2083, loss = 0.00338329\n",
      "Iteration 2084, loss = 0.00337891\n",
      "Iteration 2085, loss = 0.00337454\n",
      "Iteration 2086, loss = 0.00337018\n",
      "Iteration 2087, loss = 0.00336583\n",
      "Iteration 2088, loss = 0.00336149\n",
      "Iteration 2089, loss = 0.00335715\n",
      "Iteration 2090, loss = 0.00335282\n",
      "Iteration 2091, loss = 0.00334850\n",
      "Iteration 2092, loss = 0.00334419\n",
      "Iteration 2093, loss = 0.00333989\n",
      "Iteration 2094, loss = 0.00333559\n",
      "Iteration 2095, loss = 0.00333130\n",
      "Iteration 2096, loss = 0.00332702\n",
      "Iteration 2097, loss = 0.00332275\n",
      "Iteration 2098, loss = 0.00331848\n",
      "Iteration 2099, loss = 0.00331423\n",
      "Iteration 2100, loss = 0.00330998\n",
      "Iteration 2101, loss = 0.00330574\n",
      "Iteration 2102, loss = 0.00330150\n",
      "Iteration 2103, loss = 0.00329728\n",
      "Iteration 2104, loss = 0.00329306\n",
      "Iteration 2105, loss = 0.00328885\n",
      "Iteration 2106, loss = 0.00328465\n",
      "Iteration 2107, loss = 0.00328045\n",
      "Iteration 2108, loss = 0.00327627\n",
      "Iteration 2109, loss = 0.00327209\n",
      "Iteration 2110, loss = 0.00326792\n",
      "Iteration 2111, loss = 0.00326375\n",
      "Iteration 2112, loss = 0.00325960\n",
      "Iteration 2113, loss = 0.00325545\n",
      "Iteration 2114, loss = 0.00325131\n",
      "Iteration 2115, loss = 0.00324717\n",
      "Iteration 2116, loss = 0.00324305\n",
      "Iteration 2117, loss = 0.00323893\n",
      "Iteration 2118, loss = 0.00323482\n",
      "Iteration 2119, loss = 0.00323071\n",
      "Iteration 2120, loss = 0.00322662\n",
      "Iteration 2121, loss = 0.00322253\n",
      "Iteration 2122, loss = 0.00321845\n",
      "Iteration 2123, loss = 0.00321437\n",
      "Iteration 2124, loss = 0.00321031\n",
      "Iteration 2125, loss = 0.00320625\n",
      "Iteration 2126, loss = 0.00320220\n",
      "Iteration 2127, loss = 0.00319815\n",
      "Iteration 2128, loss = 0.00319412\n",
      "Iteration 2129, loss = 0.00319009\n",
      "Iteration 2130, loss = 0.00318606\n",
      "Iteration 2131, loss = 0.00318205\n",
      "Iteration 2132, loss = 0.00317804\n",
      "Iteration 2133, loss = 0.00317404\n",
      "Iteration 2134, loss = 0.00317005\n",
      "Iteration 2135, loss = 0.00316606\n",
      "Iteration 2136, loss = 0.00316208\n",
      "Iteration 2137, loss = 0.00315811\n",
      "Iteration 2138, loss = 0.00315415\n",
      "Iteration 2139, loss = 0.00315019\n",
      "Iteration 2140, loss = 0.00314624\n",
      "Iteration 2141, loss = 0.00314230\n",
      "Iteration 2142, loss = 0.00313836\n",
      "Iteration 2143, loss = 0.00313443\n",
      "Iteration 2144, loss = 0.00313051\n",
      "Iteration 2145, loss = 0.00312660\n",
      "Iteration 2146, loss = 0.00312269\n",
      "Iteration 2147, loss = 0.00311879\n",
      "Iteration 2148, loss = 0.00311489\n",
      "Iteration 2149, loss = 0.00311101\n",
      "Iteration 2150, loss = 0.00310713\n",
      "Iteration 2151, loss = 0.00310326\n",
      "Iteration 2152, loss = 0.00309939\n",
      "Iteration 2153, loss = 0.00309553\n",
      "Iteration 2154, loss = 0.00309168\n",
      "Iteration 2155, loss = 0.00308783\n",
      "Iteration 2156, loss = 0.00308400\n",
      "Iteration 2157, loss = 0.00308016\n",
      "Iteration 2158, loss = 0.00307634\n",
      "Iteration 2159, loss = 0.00307252\n",
      "Iteration 2160, loss = 0.00306871\n",
      "Iteration 2161, loss = 0.00306491\n",
      "Iteration 2162, loss = 0.00306111\n",
      "Iteration 2163, loss = 0.00305732\n",
      "Iteration 2164, loss = 0.00305354\n",
      "Iteration 2165, loss = 0.00304976\n",
      "Iteration 2166, loss = 0.00304599\n",
      "Iteration 2167, loss = 0.00304223\n",
      "Iteration 2168, loss = 0.00303847\n",
      "Iteration 2169, loss = 0.00303472\n",
      "Iteration 2170, loss = 0.00303098\n",
      "Iteration 2171, loss = 0.00302724\n",
      "Iteration 2172, loss = 0.00302351\n",
      "Iteration 2173, loss = 0.00301979\n",
      "Iteration 2174, loss = 0.00301607\n",
      "Iteration 2175, loss = 0.00301236\n",
      "Iteration 2176, loss = 0.00300866\n",
      "Iteration 2177, loss = 0.00300496\n",
      "Iteration 2178, loss = 0.00300127\n",
      "Iteration 2179, loss = 0.00299758\n",
      "Iteration 2180, loss = 0.00299391\n",
      "Iteration 2181, loss = 0.00299023\n",
      "Iteration 2182, loss = 0.00298657\n",
      "Iteration 2183, loss = 0.00298291\n",
      "Iteration 2184, loss = 0.00297926\n",
      "Iteration 2185, loss = 0.00297562\n",
      "Iteration 2186, loss = 0.00297198\n",
      "Iteration 2187, loss = 0.00296834\n",
      "Iteration 2188, loss = 0.00296472\n",
      "Iteration 2189, loss = 0.00296110\n",
      "Iteration 2190, loss = 0.00295749\n",
      "Iteration 2191, loss = 0.00295388\n",
      "Iteration 2192, loss = 0.00295028\n",
      "Iteration 2193, loss = 0.00294668\n",
      "Iteration 2194, loss = 0.00294310\n",
      "Iteration 2195, loss = 0.00293951\n",
      "Iteration 2196, loss = 0.00293594\n",
      "Iteration 2197, loss = 0.00293237\n",
      "Iteration 2198, loss = 0.00292881\n",
      "Iteration 2199, loss = 0.00292525\n",
      "Iteration 2200, loss = 0.00292170\n",
      "Iteration 2201, loss = 0.00291816\n",
      "Iteration 2202, loss = 0.00291462\n",
      "Iteration 2203, loss = 0.00291109\n",
      "Iteration 2204, loss = 0.00290756\n",
      "Iteration 2205, loss = 0.00290404\n",
      "Iteration 2206, loss = 0.00290053\n",
      "Iteration 2207, loss = 0.00289702\n",
      "Iteration 2208, loss = 0.00289352\n",
      "Iteration 2209, loss = 0.00289003\n",
      "Iteration 2210, loss = 0.00288654\n",
      "Iteration 2211, loss = 0.00288305\n",
      "Iteration 2212, loss = 0.00287958\n",
      "Iteration 2213, loss = 0.00287611\n",
      "Iteration 2214, loss = 0.00287264\n",
      "Iteration 2215, loss = 0.00286918\n",
      "Iteration 2216, loss = 0.00286573\n",
      "Iteration 2217, loss = 0.00286228\n",
      "Iteration 2218, loss = 0.00285884\n",
      "Iteration 2219, loss = 0.00285541\n",
      "Iteration 2220, loss = 0.00285198\n",
      "Iteration 2221, loss = 0.00284856\n",
      "Iteration 2222, loss = 0.00284514\n",
      "Iteration 2223, loss = 0.00284173\n",
      "Iteration 2224, loss = 0.00283832\n",
      "Iteration 2225, loss = 0.00283493\n",
      "Iteration 2226, loss = 0.00283153\n",
      "Iteration 2227, loss = 0.00282815\n",
      "Iteration 2228, loss = 0.00282476\n",
      "Iteration 2229, loss = 0.00282139\n",
      "Iteration 2230, loss = 0.00281802\n",
      "Iteration 2231, loss = 0.00281465\n",
      "Iteration 2232, loss = 0.00281130\n",
      "Iteration 2233, loss = 0.00280794\n",
      "Iteration 2234, loss = 0.00280460\n",
      "Iteration 2235, loss = 0.00280126\n",
      "Iteration 2236, loss = 0.00279792\n",
      "Iteration 2237, loss = 0.00279459\n",
      "Iteration 2238, loss = 0.00279127\n",
      "Iteration 2239, loss = 0.00278795\n",
      "Iteration 2240, loss = 0.00278464\n",
      "Iteration 2241, loss = 0.00278133\n",
      "Iteration 2242, loss = 0.00277803\n",
      "Iteration 2243, loss = 0.00277473\n",
      "Iteration 2244, loss = 0.00277145\n",
      "Iteration 2245, loss = 0.00276816\n",
      "Iteration 2246, loss = 0.00276488\n",
      "Iteration 2247, loss = 0.00276161\n",
      "Iteration 2248, loss = 0.00275834\n",
      "Iteration 2249, loss = 0.00275508\n",
      "Iteration 2250, loss = 0.00275183\n",
      "Iteration 2251, loss = 0.00274857\n",
      "Iteration 2252, loss = 0.00274533\n",
      "Iteration 2253, loss = 0.00274209\n",
      "Iteration 2254, loss = 0.00273886\n",
      "Iteration 2255, loss = 0.00273563\n",
      "Iteration 2256, loss = 0.00273241\n",
      "Iteration 2257, loss = 0.00272919\n",
      "Iteration 2258, loss = 0.00272598\n",
      "Iteration 2259, loss = 0.00272277\n",
      "Iteration 2260, loss = 0.00271957\n",
      "Iteration 2261, loss = 0.00271637\n",
      "Iteration 2262, loss = 0.00271318\n",
      "Iteration 2263, loss = 0.00271000\n",
      "Iteration 2264, loss = 0.00270682\n",
      "Iteration 2265, loss = 0.00270365\n",
      "Iteration 2266, loss = 0.00270048\n",
      "Iteration 2267, loss = 0.00269731\n",
      "Iteration 2268, loss = 0.00269416\n",
      "Iteration 2269, loss = 0.00269100\n",
      "Iteration 2270, loss = 0.00268786\n",
      "Iteration 2271, loss = 0.00268472\n",
      "Iteration 2272, loss = 0.00268158\n",
      "Iteration 2273, loss = 0.00267845\n",
      "Iteration 2274, loss = 0.00267532\n",
      "Iteration 2275, loss = 0.00267220\n",
      "Iteration 2276, loss = 0.00266909\n",
      "Iteration 2277, loss = 0.00266598\n",
      "Iteration 2278, loss = 0.00266287\n",
      "Iteration 2279, loss = 0.00265977\n",
      "Iteration 2280, loss = 0.00265668\n",
      "Iteration 2281, loss = 0.00265359\n",
      "Iteration 2282, loss = 0.00265050\n",
      "Iteration 2283, loss = 0.00264743\n",
      "Iteration 2284, loss = 0.00264435\n",
      "Iteration 2285, loss = 0.00264128\n",
      "Iteration 2286, loss = 0.00263822\n",
      "Iteration 2287, loss = 0.00263516\n",
      "Iteration 2288, loss = 0.00263211\n",
      "Iteration 2289, loss = 0.00262906\n",
      "Iteration 2290, loss = 0.00262602\n",
      "Iteration 2291, loss = 0.00262298\n",
      "Iteration 2292, loss = 0.00261995\n",
      "Iteration 2293, loss = 0.00261692\n",
      "Iteration 2294, loss = 0.00261390\n",
      "Iteration 2295, loss = 0.00261088\n",
      "Iteration 2296, loss = 0.00260787\n",
      "Iteration 2297, loss = 0.00260486\n",
      "Iteration 2298, loss = 0.00260186\n",
      "Iteration 2299, loss = 0.00259886\n",
      "Iteration 2300, loss = 0.00259587\n",
      "Iteration 2301, loss = 0.00259288\n",
      "Iteration 2302, loss = 0.00258990\n",
      "Iteration 2303, loss = 0.00258692\n",
      "Iteration 2304, loss = 0.00258395\n",
      "Iteration 2305, loss = 0.00258098\n",
      "Iteration 2306, loss = 0.00257802\n",
      "Iteration 2307, loss = 0.00257506\n",
      "Iteration 2308, loss = 0.00257211\n",
      "Iteration 2309, loss = 0.00256916\n",
      "Iteration 2310, loss = 0.00256622\n",
      "Iteration 2311, loss = 0.00256328\n",
      "Iteration 2312, loss = 0.00256035\n",
      "Iteration 2313, loss = 0.00255742\n",
      "Iteration 2314, loss = 0.00255450\n",
      "Iteration 2315, loss = 0.00255158\n",
      "Iteration 2316, loss = 0.00254866\n",
      "Iteration 2317, loss = 0.00254576\n",
      "Iteration 2318, loss = 0.00254285\n",
      "Iteration 2319, loss = 0.00253995\n",
      "Iteration 2320, loss = 0.00253706\n",
      "Iteration 2321, loss = 0.00253417\n",
      "Iteration 2322, loss = 0.00253128\n",
      "Iteration 2323, loss = 0.00252840\n",
      "Iteration 2324, loss = 0.00252553\n",
      "Iteration 2325, loss = 0.00252266\n",
      "Iteration 2326, loss = 0.00251979\n",
      "Iteration 2327, loss = 0.00251693\n",
      "Iteration 2328, loss = 0.00251407\n",
      "Iteration 2329, loss = 0.00251122\n",
      "Iteration 2330, loss = 0.00250837\n",
      "Iteration 2331, loss = 0.00250553\n",
      "Iteration 2332, loss = 0.00250269\n",
      "Iteration 2333, loss = 0.00249986\n",
      "Iteration 2334, loss = 0.00249703\n",
      "Iteration 2335, loss = 0.00249421\n",
      "Iteration 2336, loss = 0.00249139\n",
      "Iteration 2337, loss = 0.00248857\n",
      "Iteration 2338, loss = 0.00248576\n",
      "Iteration 2339, loss = 0.00248296\n",
      "Iteration 2340, loss = 0.00248016\n",
      "Iteration 2341, loss = 0.00247736\n",
      "Iteration 2342, loss = 0.00247457\n",
      "Iteration 2343, loss = 0.00247178\n",
      "Iteration 2344, loss = 0.00246900\n",
      "Iteration 2345, loss = 0.00246622\n",
      "Iteration 2346, loss = 0.00246345\n",
      "Iteration 2347, loss = 0.00246068\n",
      "Iteration 2348, loss = 0.00245792\n",
      "Iteration 2349, loss = 0.00245516\n",
      "Iteration 2350, loss = 0.00245240\n",
      "Iteration 2351, loss = 0.00244965\n",
      "Iteration 2352, loss = 0.00244690\n",
      "Iteration 2353, loss = 0.00244416\n",
      "Iteration 2354, loss = 0.00244142\n",
      "Iteration 2355, loss = 0.00243869\n",
      "Iteration 2356, loss = 0.00243596\n",
      "Iteration 2357, loss = 0.00243324\n",
      "Iteration 2358, loss = 0.00243052\n",
      "Iteration 2359, loss = 0.00242780\n",
      "Iteration 2360, loss = 0.00242509\n",
      "Iteration 2361, loss = 0.00242239\n",
      "Iteration 2362, loss = 0.00241968\n",
      "Iteration 2363, loss = 0.00241699\n",
      "Iteration 2364, loss = 0.00241429\n",
      "Iteration 2365, loss = 0.00241160\n",
      "Iteration 2366, loss = 0.00240892\n",
      "Iteration 2367, loss = 0.00240624\n",
      "Iteration 2368, loss = 0.00240356\n",
      "Iteration 2369, loss = 0.00240089\n",
      "Iteration 2370, loss = 0.00239822\n",
      "Iteration 2371, loss = 0.00239556\n",
      "Iteration 2372, loss = 0.00239290\n",
      "Iteration 2373, loss = 0.00239025\n",
      "Iteration 2374, loss = 0.00238760\n",
      "Iteration 2375, loss = 0.00238495\n",
      "Iteration 2376, loss = 0.00238231\n",
      "Iteration 2377, loss = 0.00237967\n",
      "Iteration 2378, loss = 0.00237704\n",
      "Iteration 2379, loss = 0.00237441\n",
      "Iteration 2380, loss = 0.00237178\n",
      "Iteration 2381, loss = 0.00236916\n",
      "Iteration 2382, loss = 0.00236655\n",
      "Iteration 2383, loss = 0.00236394\n",
      "Iteration 2384, loss = 0.00236133\n",
      "Iteration 2385, loss = 0.00235872\n",
      "Iteration 2386, loss = 0.00235612\n",
      "Iteration 2387, loss = 0.00235353\n",
      "Iteration 2388, loss = 0.00235094\n",
      "Iteration 2389, loss = 0.00234835\n",
      "Iteration 2390, loss = 0.00234577\n",
      "Iteration 2391, loss = 0.00234319\n",
      "Iteration 2392, loss = 0.00234061\n",
      "Iteration 2393, loss = 0.00233804\n",
      "Iteration 2394, loss = 0.00233548\n",
      "Iteration 2395, loss = 0.00233291\n",
      "Iteration 2396, loss = 0.00233036\n",
      "Iteration 2397, loss = 0.00232780\n",
      "Iteration 2398, loss = 0.00232525\n",
      "Iteration 2399, loss = 0.00232271\n",
      "Iteration 2400, loss = 0.00232016\n",
      "Iteration 2401, loss = 0.00231762\n",
      "Iteration 2402, loss = 0.00231509\n",
      "Iteration 2403, loss = 0.00231256\n",
      "Iteration 2404, loss = 0.00231003\n",
      "Iteration 2405, loss = 0.00230751\n",
      "Iteration 2406, loss = 0.00230499\n",
      "Iteration 2407, loss = 0.00230248\n",
      "Iteration 2408, loss = 0.00229997\n",
      "Iteration 2409, loss = 0.00229747\n",
      "Iteration 2410, loss = 0.00229496\n",
      "Iteration 2411, loss = 0.00229246\n",
      "Iteration 2412, loss = 0.00228997\n",
      "Iteration 2413, loss = 0.00228748\n",
      "Iteration 2414, loss = 0.00228499\n",
      "Iteration 2415, loss = 0.00228251\n",
      "Iteration 2416, loss = 0.00228003\n",
      "Iteration 2417, loss = 0.00227756\n",
      "Iteration 2418, loss = 0.00227509\n",
      "Iteration 2419, loss = 0.00227262\n",
      "Iteration 2420, loss = 0.00227016\n",
      "Iteration 2421, loss = 0.00226770\n",
      "Iteration 2422, loss = 0.00226525\n",
      "Iteration 2423, loss = 0.00226279\n",
      "Iteration 2424, loss = 0.00226035\n",
      "Iteration 2425, loss = 0.00225790\n",
      "Iteration 2426, loss = 0.00225546\n",
      "Iteration 2427, loss = 0.00225303\n",
      "Iteration 2428, loss = 0.00225060\n",
      "Iteration 2429, loss = 0.00224817\n",
      "Iteration 2430, loss = 0.00224574\n",
      "Iteration 2431, loss = 0.00224332\n",
      "Iteration 2432, loss = 0.00224091\n",
      "Iteration 2433, loss = 0.00223849\n",
      "Iteration 2434, loss = 0.00223608\n",
      "Iteration 2435, loss = 0.00223368\n",
      "Iteration 2436, loss = 0.00223128\n",
      "Iteration 2437, loss = 0.00222888\n",
      "Iteration 2438, loss = 0.00222649\n",
      "Iteration 2439, loss = 0.00222410\n",
      "Iteration 2440, loss = 0.00222171\n",
      "Iteration 2441, loss = 0.00221933\n",
      "Iteration 2442, loss = 0.00221695\n",
      "Iteration 2443, loss = 0.00221457\n",
      "Iteration 2444, loss = 0.00221220\n",
      "Iteration 2445, loss = 0.00220983\n",
      "Iteration 2446, loss = 0.00220747\n",
      "Iteration 2447, loss = 0.00220511\n",
      "Iteration 2448, loss = 0.00220275\n",
      "Iteration 2449, loss = 0.00220040\n",
      "Iteration 2450, loss = 0.00219805\n",
      "Iteration 2451, loss = 0.00219570\n",
      "Iteration 2452, loss = 0.00219336\n",
      "Iteration 2453, loss = 0.00219102\n",
      "Iteration 2454, loss = 0.00218869\n",
      "Iteration 2455, loss = 0.00218635\n",
      "Iteration 2456, loss = 0.00218403\n",
      "Iteration 2457, loss = 0.00218170\n",
      "Iteration 2458, loss = 0.00217938\n",
      "Iteration 2459, loss = 0.00217706\n",
      "Iteration 2460, loss = 0.00217475\n",
      "Iteration 2461, loss = 0.00217244\n",
      "Iteration 2462, loss = 0.00217013\n",
      "Iteration 2463, loss = 0.00216783\n",
      "Iteration 2464, loss = 0.00216553\n",
      "Iteration 2465, loss = 0.00216324\n",
      "Iteration 2466, loss = 0.00216095\n",
      "Iteration 2467, loss = 0.00215866\n",
      "Iteration 2468, loss = 0.00215637\n",
      "Iteration 2469, loss = 0.00215409\n",
      "Iteration 2470, loss = 0.00215181\n",
      "Iteration 2471, loss = 0.00214954\n",
      "Iteration 2472, loss = 0.00214727\n",
      "Iteration 2473, loss = 0.00214500\n",
      "Iteration 2474, loss = 0.00214274\n",
      "Iteration 2475, loss = 0.00214048\n",
      "Iteration 2476, loss = 0.00213822\n",
      "Iteration 2477, loss = 0.00213596\n",
      "Iteration 2478, loss = 0.00213371\n",
      "Iteration 2479, loss = 0.00213147\n",
      "Iteration 2480, loss = 0.00212923\n",
      "Iteration 2481, loss = 0.00212699\n",
      "Iteration 2482, loss = 0.00212475\n",
      "Iteration 2483, loss = 0.00212252\n",
      "Iteration 2484, loss = 0.00212029\n",
      "Iteration 2485, loss = 0.00211806\n",
      "Iteration 2486, loss = 0.00211584\n",
      "Iteration 2487, loss = 0.00211362\n",
      "Iteration 2488, loss = 0.00211140\n",
      "Iteration 2489, loss = 0.00210919\n",
      "Iteration 2490, loss = 0.00210698\n",
      "Iteration 2491, loss = 0.00210478\n",
      "Iteration 2492, loss = 0.00210257\n",
      "Iteration 2493, loss = 0.00210038\n",
      "Iteration 2494, loss = 0.00209818\n",
      "Iteration 2495, loss = 0.00209599\n",
      "Iteration 2496, loss = 0.00209380\n",
      "Iteration 2497, loss = 0.00209161\n",
      "Iteration 2498, loss = 0.00208943\n",
      "Iteration 2499, loss = 0.00208725\n",
      "Iteration 2500, loss = 0.00208508\n",
      "Iteration 2501, loss = 0.00208290\n",
      "Iteration 2502, loss = 0.00208074\n",
      "Iteration 2503, loss = 0.00207857\n",
      "Iteration 2504, loss = 0.00207641\n",
      "Iteration 2505, loss = 0.00207425\n",
      "Iteration 2506, loss = 0.00207209\n",
      "Iteration 2507, loss = 0.00206994\n",
      "Iteration 2508, loss = 0.00206779\n",
      "Iteration 2509, loss = 0.00206565\n",
      "Iteration 2510, loss = 0.00206350\n",
      "Iteration 2511, loss = 0.00206136\n",
      "Iteration 2512, loss = 0.00205923\n",
      "Iteration 2513, loss = 0.00205709\n",
      "Iteration 2514, loss = 0.00205496\n",
      "Iteration 2515, loss = 0.00205284\n",
      "Iteration 2516, loss = 0.00205071\n",
      "Iteration 2517, loss = 0.00204859\n",
      "Iteration 2518, loss = 0.00204648\n",
      "Iteration 2519, loss = 0.00204436\n",
      "Iteration 2520, loss = 0.00204225\n",
      "Iteration 2521, loss = 0.00204015\n",
      "Iteration 2522, loss = 0.00203804\n",
      "Iteration 2523, loss = 0.00203594\n",
      "Iteration 2524, loss = 0.00203384\n",
      "Iteration 2525, loss = 0.00203175\n",
      "Iteration 2526, loss = 0.00202966\n",
      "Iteration 2527, loss = 0.00202757\n",
      "Iteration 2528, loss = 0.00202548\n",
      "Iteration 2529, loss = 0.00202340\n",
      "Iteration 2530, loss = 0.00202132\n",
      "Iteration 2531, loss = 0.00201924\n",
      "Iteration 2532, loss = 0.00201717\n",
      "Iteration 2533, loss = 0.00201510\n",
      "Iteration 2534, loss = 0.00201304\n",
      "Iteration 2535, loss = 0.00201097\n",
      "Iteration 2536, loss = 0.00200891\n",
      "Iteration 2537, loss = 0.00200685\n",
      "Iteration 2538, loss = 0.00200480\n",
      "Iteration 2539, loss = 0.00200275\n",
      "Iteration 2540, loss = 0.00200070\n",
      "Iteration 2541, loss = 0.00199866\n",
      "Iteration 2542, loss = 0.00199661\n",
      "Iteration 2543, loss = 0.00199458\n",
      "Iteration 2544, loss = 0.00199254\n",
      "Iteration 2545, loss = 0.00199051\n",
      "Iteration 2546, loss = 0.00198848\n",
      "Iteration 2547, loss = 0.00198645\n",
      "Iteration 2548, loss = 0.00198443\n",
      "Iteration 2549, loss = 0.00198241\n",
      "Iteration 2550, loss = 0.00198039\n",
      "Iteration 2551, loss = 0.00197837\n",
      "Iteration 2552, loss = 0.00197636\n",
      "Iteration 2553, loss = 0.00197435\n",
      "Iteration 2554, loss = 0.00197235\n",
      "Iteration 2555, loss = 0.00197034\n",
      "Iteration 2556, loss = 0.00196834\n",
      "Iteration 2557, loss = 0.00196635\n",
      "Iteration 2558, loss = 0.00196435\n",
      "Iteration 2559, loss = 0.00196236\n",
      "Iteration 2560, loss = 0.00196037\n",
      "Iteration 2561, loss = 0.00195839\n",
      "Iteration 2562, loss = 0.00195641\n",
      "Iteration 2563, loss = 0.00195443\n",
      "Iteration 2564, loss = 0.00195245\n",
      "Iteration 2565, loss = 0.00195048\n",
      "Iteration 2566, loss = 0.00194851\n",
      "Iteration 2567, loss = 0.00194654\n",
      "Iteration 2568, loss = 0.00194458\n",
      "Iteration 2569, loss = 0.00194261\n",
      "Iteration 2570, loss = 0.00194066\n",
      "Iteration 2571, loss = 0.00193870\n",
      "Iteration 2572, loss = 0.00193675\n",
      "Iteration 2573, loss = 0.00193480\n",
      "Iteration 2574, loss = 0.00193285\n",
      "Iteration 2575, loss = 0.00193091\n",
      "Iteration 2576, loss = 0.00192896\n",
      "Iteration 2577, loss = 0.00192703\n",
      "Iteration 2578, loss = 0.00192509\n",
      "Iteration 2579, loss = 0.00192316\n",
      "Iteration 2580, loss = 0.00192123\n",
      "Iteration 2581, loss = 0.00191930\n",
      "Iteration 2582, loss = 0.00191737\n",
      "Iteration 2583, loss = 0.00191545\n",
      "Iteration 2584, loss = 0.00191353\n",
      "Iteration 2585, loss = 0.00191162\n",
      "Iteration 2586, loss = 0.00190971\n",
      "Iteration 2587, loss = 0.00190780\n",
      "Iteration 2588, loss = 0.00190589\n",
      "Iteration 2589, loss = 0.00190398\n",
      "Iteration 2590, loss = 0.00190208\n",
      "Iteration 2591, loss = 0.00190018\n",
      "Iteration 2592, loss = 0.00189829\n",
      "Iteration 2593, loss = 0.00189639\n",
      "Iteration 2594, loss = 0.00189450\n",
      "Iteration 2595, loss = 0.00189261\n",
      "Iteration 2596, loss = 0.00189073\n",
      "Iteration 2597, loss = 0.00188884\n",
      "Iteration 2598, loss = 0.00188696\n",
      "Iteration 2599, loss = 0.00188509\n",
      "Iteration 2600, loss = 0.00188321\n",
      "Iteration 2601, loss = 0.00188134\n",
      "Iteration 2602, loss = 0.00187947\n",
      "Iteration 2603, loss = 0.00187761\n",
      "Iteration 2604, loss = 0.00187574\n",
      "Iteration 2605, loss = 0.00187388\n",
      "Iteration 2606, loss = 0.00187202\n",
      "Iteration 2607, loss = 0.00187017\n",
      "Iteration 2608, loss = 0.00186832\n",
      "Iteration 2609, loss = 0.00186647\n",
      "Iteration 2610, loss = 0.00186462\n",
      "Iteration 2611, loss = 0.00186277\n",
      "Iteration 2612, loss = 0.00186093\n",
      "Iteration 2613, loss = 0.00185909\n",
      "Iteration 2614, loss = 0.00185726\n",
      "Iteration 2615, loss = 0.00185542\n",
      "Iteration 2616, loss = 0.00185359\n",
      "Iteration 2617, loss = 0.00185176\n",
      "Iteration 2618, loss = 0.00184994\n",
      "Iteration 2619, loss = 0.00184811\n",
      "Iteration 2620, loss = 0.00184629\n",
      "Iteration 2621, loss = 0.00184447\n",
      "Iteration 2622, loss = 0.00184266\n",
      "Iteration 2623, loss = 0.00184085\n",
      "Iteration 2624, loss = 0.00183904\n",
      "Iteration 2625, loss = 0.00183723\n",
      "Iteration 2626, loss = 0.00183542\n",
      "Iteration 2627, loss = 0.00183362\n",
      "Iteration 2628, loss = 0.00183182\n",
      "Iteration 2629, loss = 0.00183002\n",
      "Iteration 2630, loss = 0.00182823\n",
      "Iteration 2631, loss = 0.00182644\n",
      "Iteration 2632, loss = 0.00182465\n",
      "Iteration 2633, loss = 0.00182286\n",
      "Iteration 2634, loss = 0.00182108\n",
      "Iteration 2635, loss = 0.00181929\n",
      "Iteration 2636, loss = 0.00181751\n",
      "Iteration 2637, loss = 0.00181574\n",
      "Iteration 2638, loss = 0.00181396\n",
      "Iteration 2639, loss = 0.00181219\n",
      "Iteration 2640, loss = 0.00181042\n",
      "Iteration 2641, loss = 0.00180866\n",
      "Iteration 2642, loss = 0.00180689\n",
      "Iteration 2643, loss = 0.00180513\n",
      "Iteration 2644, loss = 0.00180337\n",
      "Iteration 2645, loss = 0.00180162\n",
      "Iteration 2646, loss = 0.00179986\n",
      "Iteration 2647, loss = 0.00179811\n",
      "Iteration 2648, loss = 0.00179636\n",
      "Iteration 2649, loss = 0.00179462\n",
      "Iteration 2650, loss = 0.00179287\n",
      "Iteration 2651, loss = 0.00179113\n",
      "Iteration 2652, loss = 0.00178939\n",
      "Iteration 2653, loss = 0.00178765\n",
      "Iteration 2654, loss = 0.00178592\n",
      "Iteration 2655, loss = 0.00178419\n",
      "Iteration 2656, loss = 0.00178246\n",
      "Iteration 2657, loss = 0.00178073\n",
      "Iteration 2658, loss = 0.00177901\n",
      "Iteration 2659, loss = 0.00177729\n",
      "Iteration 2660, loss = 0.00177557\n",
      "Iteration 2661, loss = 0.00177385\n",
      "Iteration 2662, loss = 0.00177214\n",
      "Iteration 2663, loss = 0.00177043\n",
      "Iteration 2664, loss = 0.00176872\n",
      "Iteration 2665, loss = 0.00176701\n",
      "Iteration 2666, loss = 0.00176530\n",
      "Iteration 2667, loss = 0.00176360\n",
      "Iteration 2668, loss = 0.00176190\n",
      "Iteration 2669, loss = 0.00176020\n",
      "Iteration 2670, loss = 0.00175851\n",
      "Iteration 2671, loss = 0.00175682\n",
      "Iteration 2672, loss = 0.00175513\n",
      "Iteration 2673, loss = 0.00175344\n",
      "Iteration 2674, loss = 0.00175175\n",
      "Iteration 2675, loss = 0.00175007\n",
      "Iteration 2676, loss = 0.00174839\n",
      "Iteration 2677, loss = 0.00174671\n",
      "Iteration 2678, loss = 0.00174504\n",
      "Iteration 2679, loss = 0.00174336\n",
      "Iteration 2680, loss = 0.00174169\n",
      "Iteration 2681, loss = 0.00174002\n",
      "Iteration 2682, loss = 0.00173836\n",
      "Iteration 2683, loss = 0.00173669\n",
      "Iteration 2684, loss = 0.00173503\n",
      "Iteration 2685, loss = 0.00173337\n",
      "Iteration 2686, loss = 0.00173171\n",
      "Iteration 2687, loss = 0.00173006\n",
      "Iteration 2688, loss = 0.00172841\n",
      "Iteration 2689, loss = 0.00172676\n",
      "Iteration 2690, loss = 0.00172511\n",
      "Iteration 2691, loss = 0.00172346\n",
      "Iteration 2692, loss = 0.00172182\n",
      "Iteration 2693, loss = 0.00172018\n",
      "Iteration 2694, loss = 0.00171854\n",
      "Iteration 2695, loss = 0.00171690\n",
      "Iteration 2696, loss = 0.00171527\n",
      "Iteration 2697, loss = 0.00171364\n",
      "Iteration 2698, loss = 0.00171201\n",
      "Iteration 2699, loss = 0.00171038\n",
      "Iteration 2700, loss = 0.00170876\n",
      "Iteration 2701, loss = 0.00170713\n",
      "Iteration 2702, loss = 0.00170551\n",
      "Iteration 2703, loss = 0.00170389\n",
      "Iteration 2704, loss = 0.00170228\n",
      "Iteration 2705, loss = 0.00170067\n",
      "Iteration 2706, loss = 0.00169905\n",
      "Iteration 2707, loss = 0.00169744\n",
      "Iteration 2708, loss = 0.00169584\n",
      "Iteration 2709, loss = 0.00169423\n",
      "Iteration 2710, loss = 0.00169263\n",
      "Iteration 2711, loss = 0.00169103\n",
      "Iteration 2712, loss = 0.00168943\n",
      "Iteration 2713, loss = 0.00168784\n",
      "Iteration 2714, loss = 0.00168624\n",
      "Iteration 2715, loss = 0.00168465\n",
      "Iteration 2716, loss = 0.00168306\n",
      "Iteration 2717, loss = 0.00168148\n",
      "Iteration 2718, loss = 0.00167989\n",
      "Iteration 2719, loss = 0.00167831\n",
      "Iteration 2720, loss = 0.00167673\n",
      "Iteration 2721, loss = 0.00167515\n",
      "Iteration 2722, loss = 0.00167358\n",
      "Iteration 2723, loss = 0.00167200\n",
      "Iteration 2724, loss = 0.00167043\n",
      "Iteration 2725, loss = 0.00166886\n",
      "Iteration 2726, loss = 0.00166729\n",
      "Iteration 2727, loss = 0.00166573\n",
      "Iteration 2728, loss = 0.00166417\n",
      "Iteration 2729, loss = 0.00166261\n",
      "Iteration 2730, loss = 0.00166105\n",
      "Iteration 2731, loss = 0.00165949\n",
      "Iteration 2732, loss = 0.00165794\n",
      "Iteration 2733, loss = 0.00165638\n",
      "Iteration 2734, loss = 0.00165483\n",
      "Iteration 2735, loss = 0.00165329\n",
      "Iteration 2736, loss = 0.00165174\n",
      "Iteration 2737, loss = 0.00165020\n",
      "Iteration 2738, loss = 0.00164866\n",
      "Iteration 2739, loss = 0.00164712\n",
      "Iteration 2740, loss = 0.00164558\n",
      "Iteration 2741, loss = 0.00164404\n",
      "Iteration 2742, loss = 0.00164251\n",
      "Iteration 2743, loss = 0.00164098\n",
      "Iteration 2744, loss = 0.00163945\n",
      "Iteration 2745, loss = 0.00163793\n",
      "Iteration 2746, loss = 0.00163640\n",
      "Iteration 2747, loss = 0.00163488\n",
      "Iteration 2748, loss = 0.00163336\n",
      "Iteration 2749, loss = 0.00163184\n",
      "Iteration 2750, loss = 0.00163032\n",
      "Iteration 2751, loss = 0.00162881\n",
      "Iteration 2752, loss = 0.00162730\n",
      "Iteration 2753, loss = 0.00162579\n",
      "Iteration 2754, loss = 0.00162428\n",
      "Iteration 2755, loss = 0.00162277\n",
      "Iteration 2756, loss = 0.00162127\n",
      "Iteration 2757, loss = 0.00161977\n",
      "Iteration 2758, loss = 0.00161827\n",
      "Iteration 2759, loss = 0.00161677\n",
      "Iteration 2760, loss = 0.00161528\n",
      "Iteration 2761, loss = 0.00161378\n",
      "Iteration 2762, loss = 0.00161229\n",
      "Iteration 2763, loss = 0.00161080\n",
      "Iteration 2764, loss = 0.00160931\n",
      "Iteration 2765, loss = 0.00160783\n",
      "Iteration 2766, loss = 0.00160634\n",
      "Iteration 2767, loss = 0.00160486\n",
      "Iteration 2768, loss = 0.00160338\n",
      "Iteration 2769, loss = 0.00160191\n",
      "Iteration 2770, loss = 0.00160043\n",
      "Iteration 2771, loss = 0.00159896\n",
      "Iteration 2772, loss = 0.00159749\n",
      "Iteration 2773, loss = 0.00159602\n",
      "Iteration 2774, loss = 0.00159455\n",
      "Iteration 2775, loss = 0.00159309\n",
      "Iteration 2776, loss = 0.00159162\n",
      "Iteration 2777, loss = 0.00159016\n",
      "Iteration 2778, loss = 0.00158870\n",
      "Iteration 2779, loss = 0.00158724\n",
      "Iteration 2780, loss = 0.00158579\n",
      "Iteration 2781, loss = 0.00158433\n",
      "Iteration 2782, loss = 0.00158288\n",
      "Iteration 2783, loss = 0.00158143\n",
      "Iteration 2784, loss = 0.00157999\n",
      "Iteration 2785, loss = 0.00157854\n",
      "Iteration 2786, loss = 0.00157710\n",
      "Iteration 2787, loss = 0.00157566\n",
      "Iteration 2788, loss = 0.00157422\n",
      "Iteration 2789, loss = 0.00157278\n",
      "Iteration 2790, loss = 0.00157134\n",
      "Iteration 2791, loss = 0.00156991\n",
      "Iteration 2792, loss = 0.00156848\n",
      "Iteration 2793, loss = 0.00156705\n",
      "Iteration 2794, loss = 0.00156562\n",
      "Iteration 2795, loss = 0.00156419\n",
      "Iteration 2796, loss = 0.00156277\n",
      "Iteration 2797, loss = 0.00156135\n",
      "Iteration 2798, loss = 0.00155992\n",
      "Iteration 2799, loss = 0.00155851\n",
      "Iteration 2800, loss = 0.00155709\n",
      "Iteration 2801, loss = 0.00155567\n",
      "Iteration 2802, loss = 0.00155426\n",
      "Iteration 2803, loss = 0.00155285\n",
      "Iteration 2804, loss = 0.00155144\n",
      "Iteration 2805, loss = 0.00155004\n",
      "Iteration 2806, loss = 0.00154863\n",
      "Iteration 2807, loss = 0.00154723\n",
      "Iteration 2808, loss = 0.00154582\n",
      "Iteration 2809, loss = 0.00154443\n",
      "Iteration 2810, loss = 0.00154303\n",
      "Iteration 2811, loss = 0.00154163\n",
      "Iteration 2812, loss = 0.00154024\n",
      "Iteration 2813, loss = 0.00153885\n",
      "Iteration 2814, loss = 0.00153746\n",
      "Iteration 2815, loss = 0.00153607\n",
      "Iteration 2816, loss = 0.00153468\n",
      "Iteration 2817, loss = 0.00153330\n",
      "Iteration 2818, loss = 0.00153191\n",
      "Iteration 2819, loss = 0.00153053\n",
      "Iteration 2820, loss = 0.00152915\n",
      "Iteration 2821, loss = 0.00152778\n",
      "Iteration 2822, loss = 0.00152640\n",
      "Iteration 2823, loss = 0.00152503\n",
      "Iteration 2824, loss = 0.00152365\n",
      "Iteration 2825, loss = 0.00152228\n",
      "Iteration 2826, loss = 0.00152092\n",
      "Iteration 2827, loss = 0.00151955\n",
      "Iteration 2828, loss = 0.00151819\n",
      "Iteration 2829, loss = 0.00151682\n",
      "Iteration 2830, loss = 0.00151546\n",
      "Iteration 2831, loss = 0.00151410\n",
      "Iteration 2832, loss = 0.00151275\n",
      "Iteration 2833, loss = 0.00151139\n",
      "Iteration 2834, loss = 0.00151004\n",
      "Iteration 2835, loss = 0.00150868\n",
      "Iteration 2836, loss = 0.00150733\n",
      "Iteration 2837, loss = 0.00150599\n",
      "Iteration 2838, loss = 0.00150464\n",
      "Iteration 2839, loss = 0.00150329\n",
      "Iteration 2840, loss = 0.00150195\n",
      "Iteration 2841, loss = 0.00150061\n",
      "Iteration 2842, loss = 0.00149927\n",
      "Iteration 2843, loss = 0.00149793\n",
      "Iteration 2844, loss = 0.00149660\n",
      "Iteration 2845, loss = 0.00149526\n",
      "Iteration 2846, loss = 0.00149393\n",
      "Iteration 2847, loss = 0.00149260\n",
      "Iteration 2848, loss = 0.00149127\n",
      "Iteration 2849, loss = 0.00148994\n",
      "Iteration 2850, loss = 0.00148862\n",
      "Iteration 2851, loss = 0.00148730\n",
      "Iteration 2852, loss = 0.00148597\n",
      "Iteration 2853, loss = 0.00148465\n",
      "Iteration 2854, loss = 0.00148334\n",
      "Iteration 2855, loss = 0.00148202\n",
      "Iteration 2856, loss = 0.00148070\n",
      "Iteration 2857, loss = 0.00147939\n",
      "Iteration 2858, loss = 0.00147808\n",
      "Iteration 2859, loss = 0.00147677\n",
      "Iteration 2860, loss = 0.00147546\n",
      "Iteration 2861, loss = 0.00147416\n",
      "Iteration 2862, loss = 0.00147285\n",
      "Iteration 2863, loss = 0.00147155\n",
      "Iteration 2864, loss = 0.00147025\n",
      "Iteration 2865, loss = 0.00146895\n",
      "Iteration 2866, loss = 0.00146765\n",
      "Iteration 2867, loss = 0.00146635\n",
      "Iteration 2868, loss = 0.00146506\n",
      "Iteration 2869, loss = 0.00146377\n",
      "Iteration 2870, loss = 0.00146248\n",
      "Iteration 2871, loss = 0.00146119\n",
      "Iteration 2872, loss = 0.00145990\n",
      "Iteration 2873, loss = 0.00145861\n",
      "Iteration 2874, loss = 0.00145733\n",
      "Iteration 2875, loss = 0.00145605\n",
      "Iteration 2876, loss = 0.00145477\n",
      "Iteration 2877, loss = 0.00145349\n",
      "Iteration 2878, loss = 0.00145221\n",
      "Iteration 2879, loss = 0.00145093\n",
      "Iteration 2880, loss = 0.00144966\n",
      "Iteration 2881, loss = 0.00144839\n",
      "Iteration 2882, loss = 0.00144712\n",
      "Iteration 2883, loss = 0.00144585\n",
      "Iteration 2884, loss = 0.00144458\n",
      "Iteration 2885, loss = 0.00144331\n",
      "Iteration 2886, loss = 0.00144205\n",
      "Iteration 2887, loss = 0.00144079\n",
      "Iteration 2888, loss = 0.00143952\n",
      "Iteration 2889, loss = 0.00143827\n",
      "Iteration 2890, loss = 0.00143701\n",
      "Iteration 2891, loss = 0.00143575\n",
      "Iteration 2892, loss = 0.00143450\n",
      "Iteration 2893, loss = 0.00143324\n",
      "Iteration 2894, loss = 0.00143199\n",
      "Iteration 2895, loss = 0.00143074\n",
      "Iteration 2896, loss = 0.00142950\n",
      "Iteration 2897, loss = 0.00142825\n",
      "Iteration 2898, loss = 0.00142700\n",
      "Iteration 2899, loss = 0.00142576\n",
      "Iteration 2900, loss = 0.00142452\n",
      "Iteration 2901, loss = 0.00142328\n",
      "Iteration 2902, loss = 0.00142204\n",
      "Iteration 2903, loss = 0.00142081\n",
      "Iteration 2904, loss = 0.00141957\n",
      "Iteration 2905, loss = 0.00141834\n",
      "Iteration 2906, loss = 0.00141711\n",
      "Iteration 2907, loss = 0.00141587\n",
      "Iteration 2908, loss = 0.00141465\n",
      "Iteration 2909, loss = 0.00141342\n",
      "Iteration 2910, loss = 0.00141219\n",
      "Iteration 2911, loss = 0.00141097\n",
      "Iteration 2912, loss = 0.00140975\n",
      "Iteration 2913, loss = 0.00140853\n",
      "Iteration 2914, loss = 0.00140731\n",
      "Iteration 2915, loss = 0.00140609\n",
      "Iteration 2916, loss = 0.00140487\n",
      "Iteration 2917, loss = 0.00140366\n",
      "Iteration 2918, loss = 0.00140245\n",
      "Iteration 2919, loss = 0.00140123\n",
      "Iteration 2920, loss = 0.00140002\n",
      "Iteration 2921, loss = 0.00139882\n",
      "Iteration 2922, loss = 0.00139761\n",
      "Iteration 2923, loss = 0.00139640\n",
      "Iteration 2924, loss = 0.00139520\n",
      "Iteration 2925, loss = 0.00139400\n",
      "Iteration 2926, loss = 0.00139280\n",
      "Iteration 2927, loss = 0.00139160\n",
      "Iteration 2928, loss = 0.00139040\n",
      "Iteration 2929, loss = 0.00138921\n",
      "Iteration 2930, loss = 0.00138801\n",
      "Iteration 2931, loss = 0.00138682\n",
      "Iteration 2932, loss = 0.00138563\n",
      "Iteration 2933, loss = 0.00138444\n",
      "Iteration 2934, loss = 0.00138325\n",
      "Iteration 2935, loss = 0.00138206\n",
      "Iteration 2936, loss = 0.00138088\n",
      "Iteration 2937, loss = 0.00137969\n",
      "Iteration 2938, loss = 0.00137851\n",
      "Iteration 2939, loss = 0.00137733\n",
      "Iteration 2940, loss = 0.00137615\n",
      "Iteration 2941, loss = 0.00137497\n",
      "Iteration 2942, loss = 0.00137380\n",
      "Iteration 2943, loss = 0.00137262\n",
      "Iteration 2944, loss = 0.00137145\n",
      "Iteration 2945, loss = 0.00137028\n",
      "Iteration 2946, loss = 0.00136911\n",
      "Iteration 2947, loss = 0.00136794\n",
      "Iteration 2948, loss = 0.00136677\n",
      "Iteration 2949, loss = 0.00136561\n",
      "Iteration 2950, loss = 0.00136444\n",
      "Iteration 2951, loss = 0.00136328\n",
      "Iteration 2952, loss = 0.00136212\n",
      "Iteration 2953, loss = 0.00136096\n",
      "Iteration 2954, loss = 0.00135980\n",
      "Iteration 2955, loss = 0.00135864\n",
      "Iteration 2956, loss = 0.00135749\n",
      "Iteration 2957, loss = 0.00135633\n",
      "Iteration 2958, loss = 0.00135518\n",
      "Iteration 2959, loss = 0.00135403\n",
      "Iteration 2960, loss = 0.00135288\n",
      "Iteration 2961, loss = 0.00135173\n",
      "Iteration 2962, loss = 0.00135058\n",
      "Iteration 2963, loss = 0.00134944\n",
      "Iteration 2964, loss = 0.00134830\n",
      "Iteration 2965, loss = 0.00134715\n",
      "Iteration 2966, loss = 0.00134601\n",
      "Iteration 2967, loss = 0.00134487\n",
      "Iteration 2968, loss = 0.00134374\n",
      "Iteration 2969, loss = 0.00134260\n",
      "Iteration 2970, loss = 0.00134146\n",
      "Iteration 2971, loss = 0.00134033\n",
      "Iteration 2972, loss = 0.00133920\n",
      "Iteration 2973, loss = 0.00133807\n",
      "Iteration 2974, loss = 0.00133694\n",
      "Iteration 2975, loss = 0.00133581\n",
      "Iteration 2976, loss = 0.00133468\n",
      "Iteration 2977, loss = 0.00133356\n",
      "Iteration 2978, loss = 0.00133243\n",
      "Iteration 2979, loss = 0.00133131\n",
      "Iteration 2980, loss = 0.00133019\n",
      "Iteration 2981, loss = 0.00132907\n",
      "Iteration 2982, loss = 0.00132795\n",
      "Iteration 2983, loss = 0.00132684\n",
      "Iteration 2984, loss = 0.00132572\n",
      "Iteration 2985, loss = 0.00132461\n",
      "Iteration 2986, loss = 0.00132349\n",
      "Iteration 2987, loss = 0.00132238\n",
      "Iteration 2988, loss = 0.00132127\n",
      "Iteration 2989, loss = 0.00132017\n",
      "Iteration 2990, loss = 0.00131906\n",
      "Iteration 2991, loss = 0.00131795\n",
      "Iteration 2992, loss = 0.00131685\n",
      "Iteration 2993, loss = 0.00131575\n",
      "Iteration 2994, loss = 0.00131464\n",
      "Iteration 2995, loss = 0.00131354\n",
      "Iteration 2996, loss = 0.00131245\n",
      "Iteration 2997, loss = 0.00131135\n",
      "Iteration 2998, loss = 0.00131025\n",
      "Iteration 2999, loss = 0.00130916\n",
      "Iteration 3000, loss = 0.00130806\n",
      "Iteration 3001, loss = 0.00130697\n",
      "Iteration 3002, loss = 0.00130588\n",
      "Iteration 3003, loss = 0.00130479\n",
      "Iteration 3004, loss = 0.00130371\n",
      "Iteration 3005, loss = 0.00130262\n",
      "Iteration 3006, loss = 0.00130153\n",
      "Iteration 3007, loss = 0.00130045\n",
      "Iteration 3008, loss = 0.00129937\n",
      "Iteration 3009, loss = 0.00129829\n",
      "Iteration 3010, loss = 0.00129721\n",
      "Iteration 3011, loss = 0.00129613\n",
      "Iteration 3012, loss = 0.00129505\n",
      "Iteration 3013, loss = 0.00129398\n",
      "Iteration 3014, loss = 0.00129290\n",
      "Iteration 3015, loss = 0.00129183\n",
      "Iteration 3016, loss = 0.00129076\n",
      "Iteration 3017, loss = 0.00128969\n",
      "Iteration 3018, loss = 0.00128862\n",
      "Iteration 3019, loss = 0.00128755\n",
      "Iteration 3020, loss = 0.00128649\n",
      "Iteration 3021, loss = 0.00128542\n",
      "Iteration 3022, loss = 0.00128436\n",
      "Iteration 3023, loss = 0.00128329\n",
      "Iteration 3024, loss = 0.00128223\n",
      "Iteration 3025, loss = 0.00128117\n",
      "Iteration 3026, loss = 0.00128012\n",
      "Iteration 3027, loss = 0.00127906\n",
      "Iteration 3028, loss = 0.00127800\n",
      "Iteration 3029, loss = 0.00127695\n",
      "Iteration 3030, loss = 0.00127589\n",
      "Iteration 3031, loss = 0.00127484\n",
      "Iteration 3032, loss = 0.00127379\n",
      "Iteration 3033, loss = 0.00127274\n",
      "Iteration 3034, loss = 0.00127170\n",
      "Iteration 3035, loss = 0.00127065\n",
      "Iteration 3036, loss = 0.00126960\n",
      "Iteration 3037, loss = 0.00126856\n",
      "Iteration 3038, loss = 0.00126752\n",
      "Iteration 3039, loss = 0.00126647\n",
      "Iteration 3040, loss = 0.00126543\n",
      "Iteration 3041, loss = 0.00126440\n",
      "Iteration 3042, loss = 0.00126336\n",
      "Iteration 3043, loss = 0.00126232\n",
      "Iteration 3044, loss = 0.00126129\n",
      "Iteration 3045, loss = 0.00126025\n",
      "Iteration 3046, loss = 0.00125922\n",
      "Iteration 3047, loss = 0.00125819\n",
      "Iteration 3048, loss = 0.00125716\n",
      "Iteration 3049, loss = 0.00125613\n",
      "Iteration 3050, loss = 0.00125510\n",
      "Iteration 3051, loss = 0.00125408\n",
      "Iteration 3052, loss = 0.00125305\n",
      "Iteration 3053, loss = 0.00125203\n",
      "Iteration 3054, loss = 0.00125100\n",
      "Iteration 3055, loss = 0.00124998\n",
      "Iteration 3056, loss = 0.00124896\n",
      "Iteration 3057, loss = 0.00124794\n",
      "Iteration 3058, loss = 0.00124693\n",
      "Iteration 3059, loss = 0.00124591\n",
      "Iteration 3060, loss = 0.00124490\n",
      "Iteration 3061, loss = 0.00124388\n",
      "Iteration 3062, loss = 0.00124287\n",
      "Iteration 3063, loss = 0.00124186\n",
      "Iteration 3064, loss = 0.00124085\n",
      "Iteration 3065, loss = 0.00123984\n",
      "Iteration 3066, loss = 0.00123883\n",
      "Iteration 3067, loss = 0.00123783\n",
      "Iteration 3068, loss = 0.00123682\n",
      "Iteration 3069, loss = 0.00123582\n",
      "Iteration 3070, loss = 0.00123481\n",
      "Iteration 3071, loss = 0.00123381\n",
      "Iteration 3072, loss = 0.00123281\n",
      "Iteration 3073, loss = 0.00123181\n",
      "Iteration 3074, loss = 0.00123082\n",
      "Iteration 3075, loss = 0.00122982\n",
      "Iteration 3076, loss = 0.00122882\n",
      "Iteration 3077, loss = 0.00122783\n",
      "Iteration 3078, loss = 0.00122684\n",
      "Iteration 3079, loss = 0.00122585\n",
      "Iteration 3080, loss = 0.00122485\n",
      "Iteration 3081, loss = 0.00122387\n",
      "Iteration 3082, loss = 0.00122288\n",
      "Iteration 3083, loss = 0.00122189\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        25\n",
      "           2       1.00      0.94      0.97        17\n",
      "           3       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.97      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# hidden_layers = (512,) # one hidden layer\n",
    "# activation = 'relu' # the default\n",
    "hidden_layers = (30,) #(輸入,輸出),輸出讓型自行決定\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "# solver = 'sgd' # not efficient, need more tuning\n",
    "# solver = 'lbfgs' # not suitable here\n",
    "solver = 'adam' # default solver\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "\n",
    "\n",
    "clf_MLP.fit(X_train, y_train)\n",
    "predictions = clf_MLP.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "score = 100*clf_MLP.score(X_test, y_test)\n",
    "title = 'Testing score ={:.2f}%'.format(score)\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "clf_MLP,\n",
    "X_test,\n",
    "y_test,\n",
    "xticks_rotation=45, #'vertical',\n",
    "# display_labels=class_names,\n",
    "cmap=plt.cm.Blues,\n",
    "normalize='true',\n",
    "ax = ax\n",
    ")\n",
    "\n",
    "disp.ax_.set_title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
