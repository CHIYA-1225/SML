{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>使用資料: wine.csv</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>函式說明: </h3>\n",
    "將資料集分割成訓練集和測試集，比例為 7:3。<br>\n",
    "使用 StandardScaler() 進行資料標準化，並使用PCA降維至 2個維度<br>\n",
    "使用 LogisticRegression 進行模型訓練，其中使用了三種不同的 solver，並計算其在測試集上的準確率。<br>\n",
    "使用 LogisticRegressionCV 進行模型訓練，同樣使用了三種不同的 solver 和不同的正則化程度 (Cs)，並計算其在測試集上的準確率。<br>\n",
    "使用 SVM 進行模型訓練，其中使用了不同的核函數 (linear、rbf、poly) 搭配正則化程度 (C=1)，並計算其在測試集上的準確率。<br>\n",
    "使用 MLPClassifier 進行模型訓練，其中使用了隱藏層大小 (h=30)、不同的激活函數 (logistic、relu) 和求解器 (adam、sgd)，並計算其在測試集上的準確率。<br>\n",
    "同樣的步驟，但是使用PCA降維後的資料集。<br>\n",
    "最後，函式會回傳原始的資料集、各模型在原始資料集上的準確率、各模型在PCA後資料集上的準確率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  classifier (X,y,dataname,h):\n",
    "    data=dataname\n",
    "    # Split data into training and testing data 7:3\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_ = scaler.fit_transform(X_train)\n",
    "    X_test_ = scaler.fit_transform(X_test)\n",
    "\n",
    "#PCA\n",
    "    pca = PCA(n_components = 2).fit(X_train_)\n",
    "    Z_train = pca.transform(X_train_)\n",
    "    Z_test = pca.transform(X_test_)\n",
    "\n",
    "#LogisticRegression\n",
    "    LR=np.zeros(3)\n",
    "    LR_CV=np.zeros(3)\n",
    "    LR_pca=np.zeros(3)\n",
    "    LR_CV_pca=np.zeros(3)\n",
    "\n",
    "    Cs = np.logspace(-5, 5, 20)\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "    solver = ['lbfgs','liblinear','newton-cg'] \n",
    "    for s in range(3):\n",
    "        clf_original = LogisticRegression(solver = solver[s], **opts)\n",
    "        clf_original.fit(X_train_, y_train)\n",
    "        y_pred = clf_original.predict(X_test_)\n",
    "        # 測 試 資 料 之 準 確 率 回 報\n",
    "        LR[s]= accuracy_score(y_test, y_pred)\n",
    "\n",
    "        clf_PCA = LogisticRegression(solver = solver[s], **opts)\n",
    "        clf_PCA.fit(Z_train, y_train)\n",
    "        LR_pca[s]=clf_PCA.score(Z_test, y_test)\n",
    "\n",
    "#LogisticRegressionCV\n",
    "        clf_original = LogisticRegressionCV(solver = solver[s], Cs=Cs, **opts)\n",
    "        clf_original.fit(X_train_, y_train)\n",
    "        y_pred = clf_original.predict(X_test_)\n",
    "        # 測 試 資 料 之 準 確 率 回 報\n",
    "        LR_CV[s]= accuracy_score(y_test, y_pred)\n",
    "\n",
    "        clf_PCA = LogisticRegressionCV(solver = solver[s], Cs=Cs, **opts)\n",
    "        clf_PCA.fit(Z_train, y_train)\n",
    "        LR_CV_pca[s]=clf_PCA.score(Z_test, y_test)    \n",
    "\n",
    " # SVM\n",
    "    SVM=np.zeros((2,3))\n",
    "    SVM_pca=np.zeros((2,3))\n",
    "    C = 1 # SVM regularization parameter\n",
    "    opts = [dict(C = C, tol = 1e-6, max_iter = int(1e6)),dict(C = C, decision_function_shape = 'ovo', tol = 1e-6, max_iter = int(1e6))]\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            clf_svm = [SVC(kernel=\"linear\", **opts[i]),\\\n",
    "            SVC(kernel=\"rbf\", gamma=0.2, **opts[i]),\\\n",
    "             SVC(kernel=\"poly\", degree=3, gamma=\"auto\", **opts[i])]\n",
    "             #LinearSVC(**opts[i]) ]\n",
    "\n",
    "            clf_svm[j].fit(X_train, y_train)\n",
    "            predictions = clf_svm[j].predict(X_test)\n",
    "            SVM[i][j]= accuracy_score(y_test, predictions)\n",
    "            \n",
    "            clf_svm[j].fit(Z_train, y_train) #pca\n",
    "            predictions = clf_svm[j].predict(Z_test)\n",
    "            SVM_pca[i][j]= accuracy_score(y_test, predictions)\n",
    "\n",
    "#MLPClassifier\n",
    "    MLP= np.zeros((2,2))\n",
    "    MLP_pca = np.zeros((2,2))\n",
    "    hidden_layers = (h,)\n",
    "    activation = ['logistic','relu']\n",
    "    solver = ['adam','sgd']\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "            activation = activation[i], tol = 1e-6, max_iter = int(1e6))\n",
    "            clf_MLP = MLPClassifier(solver = solver[j], **opts)\n",
    "            clf_MLP.fit(X_train, y_train)\n",
    "            predictions_mlp = clf_MLP.predict(X_test)\n",
    "            MLP[i][j]= accuracy_score(y_test, predictions_mlp)\n",
    "\n",
    "            clf_MLP.fit(Z_train, y_train) #pca\n",
    "            predictions_mlp = clf_MLP.predict(Z_test)\n",
    "            MLP_pca[i][j]= accuracy_score(y_test, predictions_mlp)\n",
    "\n",
    "    return data,LR,LR_CV,SVM,MLP,LR_pca,LR_CV_pca,SVM_pca,MLP_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.27625767\n",
      "Iteration 2, loss = 1.26876266\n",
      "Iteration 3, loss = 1.26140735\n",
      "Iteration 4, loss = 1.25418597\n",
      "Iteration 5, loss = 1.24709967\n",
      "Iteration 6, loss = 1.24015161\n",
      "Iteration 7, loss = 1.23334497\n",
      "Iteration 8, loss = 1.22668204\n",
      "Iteration 9, loss = 1.22016354\n",
      "Iteration 10, loss = 1.21378770\n",
      "Iteration 11, loss = 1.20754899\n",
      "Iteration 12, loss = 1.20143615\n",
      "Iteration 13, loss = 1.19542911\n",
      "Iteration 14, loss = 1.18949428\n",
      "Iteration 15, loss = 1.18357731\n",
      "Iteration 16, loss = 1.17759200\n",
      "Iteration 17, loss = 1.17140381\n",
      "Iteration 18, loss = 1.16480618\n",
      "Iteration 19, loss = 1.15749067\n",
      "Iteration 20, loss = 1.14902482\n",
      "Iteration 21, loss = 1.13893241\n",
      "Iteration 22, loss = 1.12722519\n",
      "Iteration 23, loss = 1.11604196\n",
      "Iteration 24, loss = 1.11142378\n",
      "Iteration 25, loss = 1.11211819\n",
      "Iteration 26, loss = 1.11085390\n",
      "Iteration 27, loss = 1.10736055\n",
      "Iteration 28, loss = 1.10194035\n",
      "Iteration 29, loss = 1.09505073\n",
      "Iteration 30, loss = 1.08855617\n",
      "Iteration 31, loss = 1.08472192\n",
      "Iteration 32, loss = 1.08299128\n",
      "Iteration 33, loss = 1.08118036\n",
      "Iteration 34, loss = 1.07823605\n",
      "Iteration 35, loss = 1.07440500\n",
      "Iteration 36, loss = 1.07040222\n",
      "Iteration 37, loss = 1.06662465\n",
      "Iteration 38, loss = 1.06401537\n",
      "Iteration 39, loss = 1.06270870\n",
      "Iteration 40, loss = 1.06153676\n",
      "Iteration 41, loss = 1.05947920\n",
      "Iteration 42, loss = 1.05663809\n",
      "Iteration 43, loss = 1.05384841\n",
      "Iteration 44, loss = 1.05173849\n",
      "Iteration 45, loss = 1.05023408\n",
      "Iteration 46, loss = 1.04884333\n",
      "Iteration 47, loss = 1.04711520\n",
      "Iteration 48, loss = 1.04489864\n",
      "Iteration 49, loss = 1.04243205\n",
      "Iteration 50, loss = 1.04015263\n",
      "Iteration 51, loss = 1.03833808\n",
      "Iteration 52, loss = 1.03689003\n",
      "Iteration 53, loss = 1.03548231\n",
      "Iteration 54, loss = 1.03393858\n",
      "Iteration 55, loss = 1.03232239\n",
      "Iteration 56, loss = 1.03073060\n",
      "Iteration 57, loss = 1.02900396\n",
      "Iteration 58, loss = 1.02673369\n",
      "Iteration 59, loss = 1.02369235\n",
      "Iteration 60, loss = 1.02022249\n",
      "Iteration 61, loss = 1.01700825\n",
      "Iteration 62, loss = 1.01457729\n",
      "Iteration 63, loss = 1.01305791\n",
      "Iteration 64, loss = 1.01220140\n",
      "Iteration 65, loss = 1.01136045\n",
      "Iteration 66, loss = 1.01010686\n",
      "Iteration 67, loss = 1.00849422\n",
      "Iteration 68, loss = 1.00681712\n",
      "Iteration 69, loss = 1.00541088\n",
      "Iteration 70, loss = 1.00415613\n",
      "Iteration 71, loss = 1.00270855\n",
      "Iteration 72, loss = 1.00091357\n",
      "Iteration 73, loss = 0.99863660\n",
      "Iteration 74, loss = 0.99575585\n",
      "Iteration 75, loss = 0.99213276\n",
      "Iteration 76, loss = 0.98750592\n",
      "Iteration 77, loss = 0.98155639\n",
      "Iteration 78, loss = 0.97460314\n",
      "Iteration 79, loss = 0.96950870\n",
      "Iteration 80, loss = 0.97081635\n",
      "Iteration 81, loss = 0.96826301\n",
      "Iteration 82, loss = 0.96185199\n",
      "Iteration 83, loss = 0.95562794\n",
      "Iteration 84, loss = 0.95130874\n",
      "Iteration 85, loss = 0.94863906\n",
      "Iteration 86, loss = 0.94663731\n",
      "Iteration 87, loss = 0.94417971\n",
      "Iteration 88, loss = 0.94096786\n",
      "Iteration 89, loss = 0.93718657\n",
      "Iteration 90, loss = 0.93324540\n",
      "Iteration 91, loss = 0.92961033\n",
      "Iteration 92, loss = 0.92647639\n",
      "Iteration 93, loss = 0.92370452\n",
      "Iteration 94, loss = 0.92086170\n",
      "Iteration 95, loss = 0.91759349\n",
      "Iteration 96, loss = 0.91402455\n",
      "Iteration 97, loss = 0.91045024\n",
      "Iteration 98, loss = 0.90705945\n",
      "Iteration 99, loss = 0.90378156\n",
      "Iteration 100, loss = 0.90043007\n",
      "Iteration 101, loss = 0.89688839\n",
      "Iteration 102, loss = 0.89328337\n",
      "Iteration 103, loss = 0.88978049\n",
      "Iteration 104, loss = 0.88712464\n",
      "Iteration 105, loss = 0.88499711\n",
      "Iteration 106, loss = 0.88190666\n",
      "Iteration 107, loss = 0.87855097\n",
      "Iteration 108, loss = 0.87557851\n",
      "Iteration 109, loss = 0.87304115\n",
      "Iteration 110, loss = 0.87062516\n",
      "Iteration 111, loss = 0.86814688\n",
      "Iteration 112, loss = 0.86556262\n",
      "Iteration 113, loss = 0.86287167\n",
      "Iteration 114, loss = 0.86009109\n",
      "Iteration 115, loss = 0.85729840\n",
      "Iteration 116, loss = 0.85467179\n",
      "Iteration 117, loss = 0.85230793\n",
      "Iteration 118, loss = 0.84993265\n",
      "Iteration 119, loss = 0.84721717\n",
      "Iteration 120, loss = 0.84421312\n",
      "Iteration 121, loss = 0.84118071\n",
      "Iteration 122, loss = 0.83820658\n",
      "Iteration 123, loss = 0.83485694\n",
      "Iteration 124, loss = 0.83035369\n",
      "Iteration 125, loss = 0.82402357\n",
      "Iteration 126, loss = 0.81674335\n",
      "Iteration 127, loss = 0.81718333\n",
      "Iteration 128, loss = 0.81350172\n",
      "Iteration 129, loss = 0.80825841\n",
      "Iteration 130, loss = 0.80546930\n",
      "Iteration 131, loss = 0.80411765\n",
      "Iteration 132, loss = 0.80227092\n",
      "Iteration 133, loss = 0.79900352\n",
      "Iteration 134, loss = 0.79498752\n",
      "Iteration 135, loss = 0.79018200\n",
      "Iteration 136, loss = 0.78525268\n",
      "Iteration 137, loss = 0.78109624\n",
      "Iteration 138, loss = 0.77760084\n",
      "Iteration 139, loss = 0.77674539\n",
      "Iteration 140, loss = 0.77237799\n",
      "Iteration 141, loss = 0.76859405\n",
      "Iteration 142, loss = 0.76621213\n",
      "Iteration 143, loss = 0.76354413\n",
      "Iteration 144, loss = 0.76105973\n",
      "Iteration 145, loss = 0.75889965\n",
      "Iteration 146, loss = 0.75558196\n",
      "Iteration 147, loss = 0.75219637\n",
      "Iteration 148, loss = 0.74957429\n",
      "Iteration 149, loss = 0.74754962\n",
      "Iteration 150, loss = 0.74555908\n",
      "Iteration 151, loss = 0.74301776\n",
      "Iteration 152, loss = 0.74003597\n",
      "Iteration 153, loss = 0.73726407\n",
      "Iteration 154, loss = 0.73508128\n",
      "Iteration 155, loss = 0.73318446\n",
      "Iteration 156, loss = 0.73097806\n",
      "Iteration 157, loss = 0.72847257\n",
      "Iteration 158, loss = 0.72603458\n",
      "Iteration 159, loss = 0.72382514\n",
      "Iteration 160, loss = 0.72179817\n",
      "Iteration 161, loss = 0.71977765\n",
      "Iteration 162, loss = 0.71760398\n",
      "Iteration 163, loss = 0.71530787\n",
      "Iteration 164, loss = 0.71302373\n",
      "Iteration 165, loss = 0.71086364\n",
      "Iteration 166, loss = 0.70913980\n",
      "Iteration 167, loss = 0.70689301\n",
      "Iteration 168, loss = 0.70473973\n",
      "Iteration 169, loss = 0.70273379\n",
      "Iteration 170, loss = 0.70076809\n",
      "Iteration 171, loss = 0.69876471\n",
      "Iteration 172, loss = 0.69664882\n",
      "Iteration 173, loss = 0.69452773\n",
      "Iteration 174, loss = 0.69258614\n",
      "Iteration 175, loss = 0.69056884\n",
      "Iteration 176, loss = 0.68847678\n",
      "Iteration 177, loss = 0.68648321\n",
      "Iteration 178, loss = 0.68451659\n",
      "Iteration 179, loss = 0.68247788\n",
      "Iteration 180, loss = 0.68041926\n",
      "Iteration 181, loss = 0.67845171\n",
      "Iteration 182, loss = 0.67651483\n",
      "Iteration 183, loss = 0.67450860\n",
      "Iteration 184, loss = 0.67250765\n",
      "Iteration 185, loss = 0.67052900\n",
      "Iteration 186, loss = 0.66856547\n",
      "Iteration 187, loss = 0.66657960\n",
      "Iteration 188, loss = 0.66463311\n",
      "Iteration 189, loss = 0.66264581\n",
      "Iteration 190, loss = 0.66064425\n",
      "Iteration 191, loss = 0.65863413\n",
      "Iteration 192, loss = 0.65664777\n",
      "Iteration 193, loss = 0.65460495\n",
      "Iteration 194, loss = 0.65255719\n",
      "Iteration 195, loss = 0.65045671\n",
      "Iteration 196, loss = 0.64839227\n",
      "Iteration 197, loss = 0.64640035\n",
      "Iteration 198, loss = 0.64426484\n",
      "Iteration 199, loss = 0.64217376\n",
      "Iteration 200, loss = 0.64010695\n",
      "Iteration 201, loss = 0.63804362\n",
      "Iteration 202, loss = 0.63595367\n",
      "Iteration 203, loss = 0.63381742\n",
      "Iteration 204, loss = 0.63165324\n",
      "Iteration 205, loss = 0.62944608\n",
      "Iteration 206, loss = 0.62715471\n",
      "Iteration 207, loss = 0.62466958\n",
      "Iteration 208, loss = 0.62189582\n",
      "Iteration 209, loss = 0.61891323\n",
      "Iteration 210, loss = 0.61629963\n",
      "Iteration 211, loss = 0.61457394\n",
      "Iteration 212, loss = 0.61197356\n",
      "Iteration 213, loss = 0.60901351\n",
      "Iteration 214, loss = 0.60603974\n",
      "Iteration 215, loss = 0.60304973\n",
      "Iteration 216, loss = 0.60069932\n",
      "Iteration 217, loss = 0.59688891\n",
      "Iteration 218, loss = 0.59442581\n",
      "Iteration 219, loss = 0.59090460\n",
      "Iteration 220, loss = 0.58709497\n",
      "Iteration 221, loss = 0.58447084\n",
      "Iteration 222, loss = 0.58072388\n",
      "Iteration 223, loss = 0.57780854\n",
      "Iteration 224, loss = 0.57478004\n",
      "Iteration 225, loss = 0.57094574\n",
      "Iteration 226, loss = 0.56810969\n",
      "Iteration 227, loss = 0.56470852\n",
      "Iteration 228, loss = 0.56109537\n",
      "Iteration 229, loss = 0.55759099\n",
      "Iteration 230, loss = 0.55309495\n",
      "Iteration 231, loss = 0.54900810\n",
      "Iteration 232, loss = 0.54373397\n",
      "Iteration 233, loss = 0.53873498\n",
      "Iteration 234, loss = 0.53529037\n",
      "Iteration 235, loss = 0.53100901\n",
      "Iteration 236, loss = 0.52693590\n",
      "Iteration 237, loss = 0.52263314\n",
      "Iteration 238, loss = 0.51944896\n",
      "Iteration 239, loss = 0.51535076\n",
      "Iteration 240, loss = 0.51114089\n",
      "Iteration 241, loss = 0.50778454\n",
      "Iteration 242, loss = 0.50377361\n",
      "Iteration 243, loss = 0.50047139\n",
      "Iteration 244, loss = 0.49696596\n",
      "Iteration 245, loss = 0.49304480\n",
      "Iteration 246, loss = 0.48990770\n",
      "Iteration 247, loss = 0.48628066\n",
      "Iteration 248, loss = 0.48256936\n",
      "Iteration 249, loss = 0.47933251\n",
      "Iteration 250, loss = 0.47543001\n",
      "Iteration 251, loss = 0.47161672\n",
      "Iteration 252, loss = 0.46732483\n",
      "Iteration 253, loss = 0.46303295\n",
      "Iteration 254, loss = 0.46059919\n",
      "Iteration 255, loss = 0.45747728\n",
      "Iteration 256, loss = 0.45409638\n",
      "Iteration 257, loss = 0.45096126\n",
      "Iteration 258, loss = 0.44766030\n",
      "Iteration 259, loss = 0.44422120\n",
      "Iteration 260, loss = 0.44052166\n",
      "Iteration 261, loss = 0.43730611\n",
      "Iteration 262, loss = 0.43474959\n",
      "Iteration 263, loss = 0.43159363\n",
      "Iteration 264, loss = 0.42834624\n",
      "Iteration 265, loss = 0.42510612\n",
      "Iteration 266, loss = 0.42236983\n",
      "Iteration 267, loss = 0.41943541\n",
      "Iteration 268, loss = 0.41649477\n",
      "Iteration 269, loss = 0.41342242\n",
      "Iteration 270, loss = 0.41035395\n",
      "Iteration 271, loss = 0.40758620\n",
      "Iteration 272, loss = 0.40470333\n",
      "Iteration 273, loss = 0.40188253\n",
      "Iteration 274, loss = 0.39888899\n",
      "Iteration 275, loss = 0.39606464\n",
      "Iteration 276, loss = 0.39325691\n",
      "Iteration 277, loss = 0.39055324\n",
      "Iteration 278, loss = 0.38775575\n",
      "Iteration 279, loss = 0.38498868\n",
      "Iteration 280, loss = 0.38224565\n",
      "Iteration 281, loss = 0.37959409\n",
      "Iteration 282, loss = 0.37694298\n",
      "Iteration 283, loss = 0.37432315\n",
      "Iteration 284, loss = 0.37167900\n",
      "Iteration 285, loss = 0.36908995\n",
      "Iteration 286, loss = 0.36652492\n",
      "Iteration 287, loss = 0.36401649\n",
      "Iteration 288, loss = 0.36149716\n",
      "Iteration 289, loss = 0.35900120\n",
      "Iteration 290, loss = 0.35651585\n",
      "Iteration 291, loss = 0.35407699\n",
      "Iteration 292, loss = 0.35166479\n",
      "Iteration 293, loss = 0.34926676\n",
      "Iteration 294, loss = 0.34686888\n",
      "Iteration 295, loss = 0.34447624\n",
      "Iteration 296, loss = 0.34202099\n",
      "Iteration 297, loss = 0.33938583\n",
      "Iteration 298, loss = 0.33663850\n",
      "Iteration 299, loss = 0.33503494\n",
      "Iteration 300, loss = 0.33243104\n",
      "Iteration 301, loss = 0.32994568\n",
      "Iteration 302, loss = 0.32786966\n",
      "Iteration 303, loss = 0.32531802\n",
      "Iteration 304, loss = 0.32286448\n",
      "Iteration 305, loss = 0.32084393\n",
      "Iteration 306, loss = 0.31828165\n",
      "Iteration 307, loss = 0.31623607\n",
      "Iteration 308, loss = 0.31410330\n",
      "Iteration 309, loss = 0.31192570\n",
      "Iteration 310, loss = 0.30958970\n",
      "Iteration 311, loss = 0.30745130\n",
      "Iteration 312, loss = 0.30524436\n",
      "Iteration 313, loss = 0.30311160\n",
      "Iteration 314, loss = 0.30097362\n",
      "Iteration 315, loss = 0.29891358\n",
      "Iteration 316, loss = 0.29681000\n",
      "Iteration 317, loss = 0.29472731\n",
      "Iteration 318, loss = 0.29269205\n",
      "Iteration 319, loss = 0.29071242\n",
      "Iteration 320, loss = 0.28871213\n",
      "Iteration 321, loss = 0.28673303\n",
      "Iteration 322, loss = 0.28482919\n",
      "Iteration 323, loss = 0.28281347\n",
      "Iteration 324, loss = 0.28086272\n",
      "Iteration 325, loss = 0.27892602\n",
      "Iteration 326, loss = 0.27701790\n",
      "Iteration 327, loss = 0.27522221\n",
      "Iteration 328, loss = 0.27338865\n",
      "Iteration 329, loss = 0.27165036\n",
      "Iteration 330, loss = 0.26988834\n",
      "Iteration 331, loss = 0.26782943\n",
      "Iteration 332, loss = 0.26586245\n",
      "Iteration 333, loss = 0.26410621\n",
      "Iteration 334, loss = 0.26252156\n",
      "Iteration 335, loss = 0.26071844\n",
      "Iteration 336, loss = 0.25885352\n",
      "Iteration 337, loss = 0.25697504\n",
      "Iteration 338, loss = 0.25519540\n",
      "Iteration 339, loss = 0.25354867\n",
      "Iteration 340, loss = 0.25186796\n",
      "Iteration 341, loss = 0.25004468\n",
      "Iteration 342, loss = 0.24824587\n",
      "Iteration 343, loss = 0.24664441\n",
      "Iteration 344, loss = 0.24499348\n",
      "Iteration 345, loss = 0.24327578\n",
      "Iteration 346, loss = 0.24164171\n",
      "Iteration 347, loss = 0.23991215\n",
      "Iteration 348, loss = 0.23829649\n",
      "Iteration 349, loss = 0.23669321\n",
      "Iteration 350, loss = 0.23505006\n",
      "Iteration 351, loss = 0.23348386\n",
      "Iteration 352, loss = 0.23187414\n",
      "Iteration 353, loss = 0.23031465\n",
      "Iteration 354, loss = 0.22876015\n",
      "Iteration 355, loss = 0.22720429\n",
      "Iteration 356, loss = 0.22565191\n",
      "Iteration 357, loss = 0.22414439\n",
      "Iteration 358, loss = 0.22263309\n",
      "Iteration 359, loss = 0.22117847\n",
      "Iteration 360, loss = 0.21970543\n",
      "Iteration 361, loss = 0.21825909\n",
      "Iteration 362, loss = 0.21683415\n",
      "Iteration 363, loss = 0.21541948\n",
      "Iteration 364, loss = 0.21398335\n",
      "Iteration 365, loss = 0.21249938\n",
      "Iteration 366, loss = 0.21101623\n",
      "Iteration 367, loss = 0.20958079\n",
      "Iteration 368, loss = 0.20822446\n",
      "Iteration 369, loss = 0.20690033\n",
      "Iteration 370, loss = 0.20560728\n",
      "Iteration 371, loss = 0.20432690\n",
      "Iteration 372, loss = 0.20298541\n",
      "Iteration 373, loss = 0.20163526\n",
      "Iteration 374, loss = 0.20026520\n",
      "Iteration 375, loss = 0.19891312\n",
      "Iteration 376, loss = 0.19762280\n",
      "Iteration 377, loss = 0.19636392\n",
      "Iteration 378, loss = 0.19514013\n",
      "Iteration 379, loss = 0.19394433\n",
      "Iteration 380, loss = 0.19276379\n",
      "Iteration 381, loss = 0.19157498\n",
      "Iteration 382, loss = 0.19037015\n",
      "Iteration 383, loss = 0.18910542\n",
      "Iteration 384, loss = 0.18785525\n",
      "Iteration 385, loss = 0.18662763\n",
      "Iteration 386, loss = 0.18545314\n",
      "Iteration 387, loss = 0.18432451\n",
      "Iteration 388, loss = 0.18321834\n",
      "Iteration 389, loss = 0.18213512\n",
      "Iteration 390, loss = 0.18104164\n",
      "Iteration 391, loss = 0.17995091\n",
      "Iteration 392, loss = 0.17881431\n",
      "Iteration 393, loss = 0.17766949\n",
      "Iteration 394, loss = 0.17653736\n",
      "Iteration 395, loss = 0.17544385\n",
      "Iteration 396, loss = 0.17438804\n",
      "Iteration 397, loss = 0.17336336\n",
      "Iteration 398, loss = 0.17235372\n",
      "Iteration 399, loss = 0.17134700\n",
      "Iteration 400, loss = 0.17034429\n",
      "Iteration 401, loss = 0.16931886\n",
      "Iteration 402, loss = 0.16828606\n",
      "Iteration 403, loss = 0.16724171\n",
      "Iteration 404, loss = 0.16621656\n",
      "Iteration 405, loss = 0.16521799\n",
      "Iteration 406, loss = 0.16424612\n",
      "Iteration 407, loss = 0.16329492\n",
      "Iteration 408, loss = 0.16235392\n",
      "Iteration 409, loss = 0.16142151\n",
      "Iteration 410, loss = 0.16048570\n",
      "Iteration 411, loss = 0.15955189\n",
      "Iteration 412, loss = 0.15860545\n",
      "Iteration 413, loss = 0.15765983\n",
      "Iteration 414, loss = 0.15671844\n",
      "Iteration 415, loss = 0.15579292\n",
      "Iteration 416, loss = 0.15488679\n",
      "Iteration 417, loss = 0.15399935\n",
      "Iteration 418, loss = 0.15312735\n",
      "Iteration 419, loss = 0.15226687\n",
      "Iteration 420, loss = 0.15141617\n",
      "Iteration 421, loss = 0.15057010\n",
      "Iteration 422, loss = 0.14973017\n",
      "Iteration 423, loss = 0.14888912\n",
      "Iteration 424, loss = 0.14805293\n",
      "Iteration 425, loss = 0.14721738\n",
      "Iteration 426, loss = 0.14638353\n",
      "Iteration 427, loss = 0.14555486\n",
      "Iteration 428, loss = 0.14480793\n",
      "Iteration 429, loss = 0.14394698\n",
      "Iteration 430, loss = 0.14322040\n",
      "Iteration 431, loss = 0.14243880\n",
      "Iteration 432, loss = 0.14141329\n",
      "Iteration 433, loss = 0.14193447\n",
      "Iteration 434, loss = 0.14051325\n",
      "Iteration 435, loss = 0.13896428\n",
      "Iteration 436, loss = 0.13962990\n",
      "Iteration 437, loss = 0.13793314\n",
      "Iteration 438, loss = 0.13772180\n",
      "Iteration 439, loss = 0.13610780\n",
      "Iteration 440, loss = 0.13598247\n",
      "Iteration 441, loss = 0.13471458\n",
      "Iteration 442, loss = 0.13373425\n",
      "Iteration 443, loss = 0.13279565\n",
      "Iteration 444, loss = 0.13249417\n",
      "Iteration 445, loss = 0.13160454\n",
      "Iteration 446, loss = 0.13069228\n",
      "Iteration 447, loss = 0.12995842\n",
      "Iteration 448, loss = 0.12919988\n",
      "Iteration 449, loss = 0.12855874\n",
      "Iteration 450, loss = 0.12772502\n",
      "Iteration 451, loss = 0.12722243\n",
      "Iteration 452, loss = 0.12631819\n",
      "Iteration 453, loss = 0.12569198\n",
      "Iteration 454, loss = 0.12484050\n",
      "Iteration 455, loss = 0.12434582\n",
      "Iteration 456, loss = 0.12362879\n",
      "Iteration 457, loss = 0.12302325\n",
      "Iteration 458, loss = 0.12232889\n",
      "Iteration 459, loss = 0.12164842\n",
      "Iteration 460, loss = 0.12091261\n",
      "Iteration 461, loss = 0.12034957\n",
      "Iteration 462, loss = 0.11968341\n",
      "Iteration 463, loss = 0.11902205\n",
      "Iteration 464, loss = 0.11840221\n",
      "Iteration 465, loss = 0.11777732\n",
      "Iteration 466, loss = 0.11712080\n",
      "Iteration 467, loss = 0.11646224\n",
      "Iteration 468, loss = 0.11587528\n",
      "Iteration 469, loss = 0.11524505\n",
      "Iteration 470, loss = 0.11462329\n",
      "Iteration 471, loss = 0.11399336\n",
      "Iteration 472, loss = 0.11341101\n",
      "Iteration 473, loss = 0.11278517\n",
      "Iteration 474, loss = 0.11218766\n",
      "Iteration 475, loss = 0.11160843\n",
      "Iteration 476, loss = 0.11105178\n",
      "Iteration 477, loss = 0.11051166\n",
      "Iteration 478, loss = 0.11005337\n",
      "Iteration 479, loss = 0.10951508\n",
      "Iteration 480, loss = 0.10883133\n",
      "Iteration 481, loss = 0.10808410\n",
      "Iteration 482, loss = 0.10751002\n",
      "Iteration 483, loss = 0.10707060\n",
      "Iteration 484, loss = 0.10661632\n",
      "Iteration 485, loss = 0.10607430\n",
      "Iteration 486, loss = 0.10533680\n",
      "Iteration 487, loss = 0.10469807\n",
      "Iteration 488, loss = 0.10425312\n",
      "Iteration 489, loss = 0.10383174\n",
      "Iteration 490, loss = 0.10329708\n",
      "Iteration 491, loss = 0.10261732\n",
      "Iteration 492, loss = 0.10199927\n",
      "Iteration 493, loss = 0.10154131\n",
      "Iteration 494, loss = 0.10111386\n",
      "Iteration 495, loss = 0.10058586\n",
      "Iteration 496, loss = 0.09994001\n",
      "Iteration 497, loss = 0.09936913\n",
      "Iteration 498, loss = 0.09890541\n",
      "Iteration 499, loss = 0.09846880\n",
      "Iteration 500, loss = 0.09798539\n",
      "Iteration 501, loss = 0.09740349\n",
      "Iteration 502, loss = 0.09681557\n",
      "Iteration 503, loss = 0.09631817\n",
      "Iteration 504, loss = 0.09590297\n",
      "Iteration 505, loss = 0.09548172\n",
      "Iteration 506, loss = 0.09495005\n",
      "Iteration 507, loss = 0.09437001\n",
      "Iteration 508, loss = 0.09381663\n",
      "Iteration 509, loss = 0.09335813\n",
      "Iteration 510, loss = 0.09295002\n",
      "Iteration 511, loss = 0.09252443\n",
      "Iteration 512, loss = 0.09207304\n",
      "Iteration 513, loss = 0.09154455\n",
      "Iteration 514, loss = 0.09098399\n",
      "Iteration 515, loss = 0.09048530\n",
      "Iteration 516, loss = 0.09009733\n",
      "Iteration 517, loss = 0.08976087\n",
      "Iteration 518, loss = 0.08935943\n",
      "Iteration 519, loss = 0.08889390\n",
      "Iteration 520, loss = 0.08830963\n",
      "Iteration 521, loss = 0.08778271\n",
      "Iteration 522, loss = 0.08739219\n",
      "Iteration 523, loss = 0.08708909\n",
      "Iteration 524, loss = 0.08676781\n",
      "Iteration 525, loss = 0.08628280\n",
      "Iteration 526, loss = 0.08571299\n",
      "Iteration 527, loss = 0.08521160\n",
      "Iteration 528, loss = 0.08487832\n",
      "Iteration 529, loss = 0.08458854\n",
      "Iteration 530, loss = 0.08417159\n",
      "Iteration 531, loss = 0.08367126\n",
      "Iteration 532, loss = 0.08317434\n",
      "Iteration 533, loss = 0.08278107\n",
      "Iteration 534, loss = 0.08244963\n",
      "Iteration 535, loss = 0.08208875\n",
      "Iteration 536, loss = 0.08166989\n",
      "Iteration 537, loss = 0.08121374\n",
      "Iteration 538, loss = 0.08080263\n",
      "Iteration 539, loss = 0.08044791\n",
      "Iteration 540, loss = 0.08010300\n",
      "Iteration 541, loss = 0.07973395\n",
      "Iteration 542, loss = 0.07933728\n",
      "Iteration 543, loss = 0.07893808\n",
      "Iteration 544, loss = 0.07855668\n",
      "Iteration 545, loss = 0.07820024\n",
      "Iteration 546, loss = 0.07785595\n",
      "Iteration 547, loss = 0.07751047\n",
      "Iteration 548, loss = 0.07716028\n",
      "Iteration 549, loss = 0.07679994\n",
      "Iteration 550, loss = 0.07643865\n",
      "Iteration 551, loss = 0.07607859\n",
      "Iteration 552, loss = 0.07572393\n",
      "Iteration 553, loss = 0.07537675\n",
      "Iteration 554, loss = 0.07503791\n",
      "Iteration 555, loss = 0.07470444\n",
      "Iteration 556, loss = 0.07437661\n",
      "Iteration 557, loss = 0.07405604\n",
      "Iteration 558, loss = 0.07374115\n",
      "Iteration 559, loss = 0.07343941\n",
      "Iteration 560, loss = 0.07314848\n",
      "Iteration 561, loss = 0.07288681\n",
      "Iteration 562, loss = 0.07261744\n",
      "Iteration 563, loss = 0.07235473\n",
      "Iteration 564, loss = 0.07199958\n",
      "Iteration 565, loss = 0.07161114\n",
      "Iteration 566, loss = 0.07120623\n",
      "Iteration 567, loss = 0.07086076\n",
      "Iteration 568, loss = 0.07058259\n",
      "Iteration 569, loss = 0.07033982\n",
      "Iteration 570, loss = 0.07009586\n",
      "Iteration 571, loss = 0.06979702\n",
      "Iteration 572, loss = 0.06946180\n",
      "Iteration 573, loss = 0.06910996\n",
      "Iteration 574, loss = 0.06879285\n",
      "Iteration 575, loss = 0.06852025\n",
      "Iteration 576, loss = 0.06827194\n",
      "Iteration 577, loss = 0.06802240\n",
      "Iteration 578, loss = 0.06774279\n",
      "Iteration 579, loss = 0.06744165\n",
      "Iteration 580, loss = 0.06712951\n",
      "Iteration 581, loss = 0.06683401\n",
      "Iteration 582, loss = 0.06656353\n",
      "Iteration 583, loss = 0.06631127\n",
      "Iteration 584, loss = 0.06606458\n",
      "Iteration 585, loss = 0.06580894\n",
      "Iteration 586, loss = 0.06554306\n",
      "Iteration 587, loss = 0.06526454\n",
      "Iteration 588, loss = 0.06498583\n",
      "Iteration 589, loss = 0.06471397\n",
      "Iteration 590, loss = 0.06445348\n",
      "Iteration 591, loss = 0.06420295\n",
      "Iteration 592, loss = 0.06395869\n",
      "Iteration 593, loss = 0.06371741\n",
      "Iteration 594, loss = 0.06347476\n",
      "Iteration 595, loss = 0.06323081\n",
      "Iteration 596, loss = 0.06298294\n",
      "Iteration 597, loss = 0.06273453\n",
      "Iteration 598, loss = 0.06248442\n",
      "Iteration 599, loss = 0.06223337\n",
      "Iteration 600, loss = 0.06193837\n",
      "Iteration 601, loss = 0.06150141\n",
      "Iteration 602, loss = 0.06100160\n",
      "Iteration 603, loss = 0.06054726\n",
      "Iteration 604, loss = 0.06040583\n",
      "Iteration 605, loss = 0.06025524\n",
      "Iteration 606, loss = 0.06005286\n",
      "Iteration 607, loss = 0.05980769\n",
      "Iteration 608, loss = 0.05957702\n",
      "Iteration 609, loss = 0.05930018\n",
      "Iteration 610, loss = 0.05897278\n",
      "Iteration 611, loss = 0.05868895\n",
      "Iteration 612, loss = 0.05851724\n",
      "Iteration 613, loss = 0.05837479\n",
      "Iteration 614, loss = 0.05820746\n",
      "Iteration 615, loss = 0.05798992\n",
      "Iteration 616, loss = 0.05767344\n",
      "Iteration 617, loss = 0.05738564\n",
      "Iteration 618, loss = 0.05714166\n",
      "Iteration 619, loss = 0.05689675\n",
      "Iteration 620, loss = 0.05667086\n",
      "Iteration 621, loss = 0.05646024\n",
      "Iteration 622, loss = 0.05626494\n",
      "Iteration 623, loss = 0.05608516\n",
      "Iteration 624, loss = 0.05589128\n",
      "Iteration 625, loss = 0.05564323\n",
      "Iteration 626, loss = 0.05539535\n",
      "Iteration 627, loss = 0.05517986\n",
      "Iteration 628, loss = 0.05498989\n",
      "Iteration 629, loss = 0.05480471\n",
      "Iteration 630, loss = 0.05461658\n",
      "Iteration 631, loss = 0.05441917\n",
      "Iteration 632, loss = 0.05421484\n",
      "Iteration 633, loss = 0.05400880\n",
      "Iteration 634, loss = 0.05379646\n",
      "Iteration 635, loss = 0.05359490\n",
      "Iteration 636, loss = 0.05341198\n",
      "Iteration 637, loss = 0.05323696\n",
      "Iteration 638, loss = 0.05305304\n",
      "Iteration 639, loss = 0.05286189\n",
      "Iteration 640, loss = 0.05267100\n",
      "Iteration 641, loss = 0.05248068\n",
      "Iteration 642, loss = 0.05229170\n",
      "Iteration 643, loss = 0.05210173\n",
      "Iteration 644, loss = 0.05191698\n",
      "Iteration 645, loss = 0.05174027\n",
      "Iteration 646, loss = 0.05156632\n",
      "Iteration 647, loss = 0.05138884\n",
      "Iteration 648, loss = 0.05121059\n",
      "Iteration 649, loss = 0.05103493\n",
      "Iteration 650, loss = 0.05085887\n",
      "Iteration 651, loss = 0.05068182\n",
      "Iteration 652, loss = 0.05050416\n",
      "Iteration 653, loss = 0.05032852\n",
      "Iteration 654, loss = 0.05015558\n",
      "Iteration 655, loss = 0.04998376\n",
      "Iteration 656, loss = 0.04981216\n",
      "Iteration 657, loss = 0.04964214\n",
      "Iteration 658, loss = 0.04947471\n",
      "Iteration 659, loss = 0.04930774\n",
      "Iteration 660, loss = 0.04914106\n",
      "Iteration 661, loss = 0.04897579\n",
      "Iteration 662, loss = 0.04881216\n",
      "Iteration 663, loss = 0.04864954\n",
      "Iteration 664, loss = 0.04848769\n",
      "Iteration 665, loss = 0.04832731\n",
      "Iteration 666, loss = 0.04816922\n",
      "Iteration 667, loss = 0.04801371\n",
      "Iteration 668, loss = 0.04786134\n",
      "Iteration 669, loss = 0.04771250\n",
      "Iteration 670, loss = 0.04757045\n",
      "Iteration 671, loss = 0.04743343\n",
      "Iteration 672, loss = 0.04730489\n",
      "Iteration 673, loss = 0.04717539\n",
      "Iteration 674, loss = 0.04704693\n",
      "Iteration 675, loss = 0.04689489\n",
      "Iteration 676, loss = 0.04672321\n",
      "Iteration 677, loss = 0.04652221\n",
      "Iteration 678, loss = 0.04631977\n",
      "Iteration 679, loss = 0.04613488\n",
      "Iteration 680, loss = 0.04598026\n",
      "Iteration 681, loss = 0.04585020\n",
      "Iteration 682, loss = 0.04572808\n",
      "Iteration 683, loss = 0.04559847\n",
      "Iteration 684, loss = 0.04544868\n",
      "Iteration 685, loss = 0.04528381\n",
      "Iteration 686, loss = 0.04511286\n",
      "Iteration 687, loss = 0.04495099\n",
      "Iteration 688, loss = 0.04480456\n",
      "Iteration 689, loss = 0.04467067\n",
      "Iteration 690, loss = 0.04454094\n",
      "Iteration 691, loss = 0.04440638\n",
      "Iteration 692, loss = 0.04426369\n",
      "Iteration 693, loss = 0.04411335\n",
      "Iteration 694, loss = 0.04396175\n",
      "Iteration 695, loss = 0.04381420\n",
      "Iteration 696, loss = 0.04367364\n",
      "Iteration 697, loss = 0.04353910\n",
      "Iteration 698, loss = 0.04340719\n",
      "Iteration 699, loss = 0.04327417\n",
      "Iteration 700, loss = 0.04313410\n",
      "Iteration 701, loss = 0.04297686\n",
      "Iteration 702, loss = 0.04275678\n",
      "Iteration 703, loss = 0.04256596\n",
      "Iteration 704, loss = 0.04252159\n",
      "Iteration 705, loss = 0.04248623\n",
      "Iteration 706, loss = 0.04246696\n",
      "Iteration 707, loss = 0.04244511\n",
      "Iteration 708, loss = 0.04217246\n",
      "Iteration 709, loss = 0.04181757\n",
      "Iteration 710, loss = 0.04155382\n",
      "Iteration 711, loss = 0.04142891\n",
      "Iteration 712, loss = 0.04144451\n",
      "Iteration 713, loss = 0.04131817\n",
      "Iteration 714, loss = 0.04107608\n",
      "Iteration 715, loss = 0.04084466\n",
      "Iteration 716, loss = 0.04071008\n",
      "Iteration 717, loss = 0.04066898\n",
      "Iteration 718, loss = 0.04054081\n",
      "Iteration 719, loss = 0.04034921\n",
      "Iteration 720, loss = 0.04016307\n",
      "Iteration 721, loss = 0.04005023\n",
      "Iteration 722, loss = 0.03998064\n",
      "Iteration 723, loss = 0.03983500\n",
      "Iteration 724, loss = 0.03966633\n",
      "Iteration 725, loss = 0.03951933\n",
      "Iteration 726, loss = 0.03942048\n",
      "Iteration 727, loss = 0.03931861\n",
      "Iteration 728, loss = 0.03917547\n",
      "Iteration 729, loss = 0.03903000\n",
      "Iteration 730, loss = 0.03890051\n",
      "Iteration 731, loss = 0.03880083\n",
      "Iteration 732, loss = 0.03868623\n",
      "Iteration 733, loss = 0.03855014\n",
      "Iteration 734, loss = 0.03841834\n",
      "Iteration 735, loss = 0.03830161\n",
      "Iteration 736, loss = 0.03819659\n",
      "Iteration 737, loss = 0.03807739\n",
      "Iteration 738, loss = 0.03795202\n",
      "Iteration 739, loss = 0.03782845\n",
      "Iteration 740, loss = 0.03771646\n",
      "Iteration 741, loss = 0.03760832\n",
      "Iteration 742, loss = 0.03749220\n",
      "Iteration 743, loss = 0.03737315\n",
      "Iteration 744, loss = 0.03725558\n",
      "Iteration 745, loss = 0.03714619\n",
      "Iteration 746, loss = 0.03703722\n",
      "Iteration 747, loss = 0.03692518\n",
      "Iteration 748, loss = 0.03681053\n",
      "Iteration 749, loss = 0.03669727\n",
      "Iteration 750, loss = 0.03658922\n",
      "Iteration 751, loss = 0.03648159\n",
      "Iteration 752, loss = 0.03637267\n",
      "Iteration 753, loss = 0.03626162\n",
      "Iteration 754, loss = 0.03615217\n",
      "Iteration 755, loss = 0.03604523\n",
      "Iteration 756, loss = 0.03593932\n",
      "Iteration 757, loss = 0.03583324\n",
      "Iteration 758, loss = 0.03572573\n",
      "Iteration 759, loss = 0.03561909\n",
      "Iteration 760, loss = 0.03551378\n",
      "Iteration 761, loss = 0.03540983\n",
      "Iteration 762, loss = 0.03530606\n",
      "Iteration 763, loss = 0.03520188\n",
      "Iteration 764, loss = 0.03509789\n",
      "Iteration 765, loss = 0.03499450\n",
      "Iteration 766, loss = 0.03489244\n",
      "Iteration 767, loss = 0.03479088\n",
      "Iteration 768, loss = 0.03468948\n",
      "Iteration 769, loss = 0.03458812\n",
      "Iteration 770, loss = 0.03448708\n",
      "Iteration 771, loss = 0.03438686\n",
      "Iteration 772, loss = 0.03428729\n",
      "Iteration 773, loss = 0.03418829\n",
      "Iteration 774, loss = 0.03408947\n",
      "Iteration 775, loss = 0.03399092\n",
      "Iteration 776, loss = 0.03389278\n",
      "Iteration 777, loss = 0.03379518\n",
      "Iteration 778, loss = 0.03369825\n",
      "Iteration 779, loss = 0.03360173\n",
      "Iteration 780, loss = 0.03350559\n",
      "Iteration 781, loss = 0.03340976\n",
      "Iteration 782, loss = 0.03331432\n",
      "Iteration 783, loss = 0.03321936\n",
      "Iteration 784, loss = 0.03312490\n",
      "Iteration 785, loss = 0.03303093\n",
      "Iteration 786, loss = 0.03293735\n",
      "Iteration 787, loss = 0.03284415\n",
      "Iteration 788, loss = 0.03275132\n",
      "Iteration 789, loss = 0.03265889\n",
      "Iteration 790, loss = 0.03256692\n",
      "Iteration 791, loss = 0.03247537\n",
      "Iteration 792, loss = 0.03238426\n",
      "Iteration 793, loss = 0.03229354\n",
      "Iteration 794, loss = 0.03220321\n",
      "Iteration 795, loss = 0.03211325\n",
      "Iteration 796, loss = 0.03202367\n",
      "Iteration 797, loss = 0.03193449\n",
      "Iteration 798, loss = 0.03184571\n",
      "Iteration 799, loss = 0.03175733\n",
      "Iteration 800, loss = 0.03166934\n",
      "Iteration 801, loss = 0.03158172\n",
      "Iteration 802, loss = 0.03149446\n",
      "Iteration 803, loss = 0.03140757\n",
      "Iteration 804, loss = 0.03132105\n",
      "Iteration 805, loss = 0.03123489\n",
      "Iteration 806, loss = 0.03114911\n",
      "Iteration 807, loss = 0.03106369\n",
      "Iteration 808, loss = 0.03097863\n",
      "Iteration 809, loss = 0.03089393\n",
      "Iteration 810, loss = 0.03080958\n",
      "Iteration 811, loss = 0.03072558\n",
      "Iteration 812, loss = 0.03064192\n",
      "Iteration 813, loss = 0.03055861\n",
      "Iteration 814, loss = 0.03047564\n",
      "Iteration 815, loss = 0.03039302\n",
      "Iteration 816, loss = 0.03031073\n",
      "Iteration 817, loss = 0.03022879\n",
      "Iteration 818, loss = 0.03014717\n",
      "Iteration 819, loss = 0.03006590\n",
      "Iteration 820, loss = 0.02998495\n",
      "Iteration 821, loss = 0.02990433\n",
      "Iteration 822, loss = 0.02982403\n",
      "Iteration 823, loss = 0.02974406\n",
      "Iteration 824, loss = 0.02966441\n",
      "Iteration 825, loss = 0.02958508\n",
      "Iteration 826, loss = 0.02950607\n",
      "Iteration 827, loss = 0.02942737\n",
      "Iteration 828, loss = 0.02934898\n",
      "Iteration 829, loss = 0.02927091\n",
      "Iteration 830, loss = 0.02919314\n",
      "Iteration 831, loss = 0.02911569\n",
      "Iteration 832, loss = 0.02903854\n",
      "Iteration 833, loss = 0.02896169\n",
      "Iteration 834, loss = 0.02888514\n",
      "Iteration 835, loss = 0.02880890\n",
      "Iteration 836, loss = 0.02873295\n",
      "Iteration 837, loss = 0.02865730\n",
      "Iteration 838, loss = 0.02858194\n",
      "Iteration 839, loss = 0.02850687\n",
      "Iteration 840, loss = 0.02843210\n",
      "Iteration 841, loss = 0.02835761\n",
      "Iteration 842, loss = 0.02828341\n",
      "Iteration 843, loss = 0.02820950\n",
      "Iteration 844, loss = 0.02813586\n",
      "Iteration 845, loss = 0.02806252\n",
      "Iteration 846, loss = 0.02798945\n",
      "Iteration 847, loss = 0.02791666\n",
      "Iteration 848, loss = 0.02784414\n",
      "Iteration 849, loss = 0.02777191\n",
      "Iteration 850, loss = 0.02769995\n",
      "Iteration 851, loss = 0.02762826\n",
      "Iteration 852, loss = 0.02755685\n",
      "Iteration 853, loss = 0.02748571\n",
      "Iteration 854, loss = 0.02741485\n",
      "Iteration 855, loss = 0.02734428\n",
      "Iteration 856, loss = 0.02727399\n",
      "Iteration 857, loss = 0.02720401\n",
      "Iteration 858, loss = 0.02713437\n",
      "Iteration 859, loss = 0.02706510\n",
      "Iteration 860, loss = 0.02699628\n",
      "Iteration 861, loss = 0.02692804\n",
      "Iteration 862, loss = 0.02686058\n",
      "Iteration 863, loss = 0.02679432\n",
      "Iteration 864, loss = 0.02672980\n",
      "Iteration 865, loss = 0.02666824\n",
      "Iteration 866, loss = 0.02661106\n",
      "Iteration 867, loss = 0.02656160\n",
      "Iteration 868, loss = 0.02652219\n",
      "Iteration 869, loss = 0.02649969\n",
      "Iteration 870, loss = 0.02648944\n",
      "Iteration 871, loss = 0.02649370\n",
      "Iteration 872, loss = 0.02647010\n",
      "Iteration 873, loss = 0.02640530\n",
      "Iteration 874, loss = 0.02625556\n",
      "Iteration 875, loss = 0.02607702\n",
      "Iteration 876, loss = 0.02593258\n",
      "Iteration 877, loss = 0.02586933\n",
      "Iteration 878, loss = 0.02586386\n",
      "Iteration 879, loss = 0.02585157\n",
      "Iteration 880, loss = 0.02578863\n",
      "Iteration 881, loss = 0.02567124\n",
      "Iteration 882, loss = 0.02555347\n",
      "Iteration 883, loss = 0.02547880\n",
      "Iteration 884, loss = 0.02544562\n",
      "Iteration 885, loss = 0.02541504\n",
      "Iteration 886, loss = 0.02535149\n",
      "Iteration 887, loss = 0.02525997\n",
      "Iteration 888, loss = 0.02516993\n",
      "Iteration 889, loss = 0.02510642\n",
      "Iteration 890, loss = 0.02506379\n",
      "Iteration 891, loss = 0.02501714\n",
      "Iteration 892, loss = 0.02495135\n",
      "Iteration 893, loss = 0.02487233\n",
      "Iteration 894, loss = 0.02479945\n",
      "Iteration 895, loss = 0.02474229\n",
      "Iteration 896, loss = 0.02469338\n",
      "Iteration 897, loss = 0.02463905\n",
      "Iteration 898, loss = 0.02457352\n",
      "Iteration 899, loss = 0.02450391\n",
      "Iteration 900, loss = 0.02444003\n",
      "Iteration 901, loss = 0.02438459\n",
      "Iteration 902, loss = 0.02433178\n",
      "Iteration 903, loss = 0.02427449\n",
      "Iteration 904, loss = 0.02421179\n",
      "Iteration 905, loss = 0.02414823\n",
      "Iteration 906, loss = 0.02408868\n",
      "Iteration 907, loss = 0.02403342\n",
      "Iteration 908, loss = 0.02397877\n",
      "Iteration 909, loss = 0.02392146\n",
      "Iteration 910, loss = 0.02386152\n",
      "Iteration 911, loss = 0.02380173\n",
      "Iteration 912, loss = 0.02374436\n",
      "Iteration 913, loss = 0.02368919\n",
      "Iteration 914, loss = 0.02363421\n",
      "Iteration 915, loss = 0.02357781\n",
      "Iteration 916, loss = 0.02352020\n",
      "Iteration 917, loss = 0.02346281\n",
      "Iteration 918, loss = 0.02340679\n",
      "Iteration 919, loss = 0.02335201\n",
      "Iteration 920, loss = 0.02329744\n",
      "Iteration 921, loss = 0.02324222\n",
      "Iteration 922, loss = 0.02318642\n",
      "Iteration 923, loss = 0.02313077\n",
      "Iteration 924, loss = 0.02307587\n",
      "Iteration 925, loss = 0.02302174\n",
      "Iteration 926, loss = 0.02296788\n",
      "Iteration 927, loss = 0.02291382\n",
      "Iteration 928, loss = 0.02285951\n",
      "Iteration 929, loss = 0.02280527\n",
      "Iteration 930, loss = 0.02275144\n",
      "Iteration 931, loss = 0.02269811\n",
      "Iteration 932, loss = 0.02264509\n",
      "Iteration 933, loss = 0.02259211\n",
      "Iteration 934, loss = 0.02253907\n",
      "Iteration 935, loss = 0.02248607\n",
      "Iteration 936, loss = 0.02243330\n",
      "Iteration 937, loss = 0.02238087\n",
      "Iteration 938, loss = 0.02232873\n",
      "Iteration 939, loss = 0.02227675\n",
      "Iteration 940, loss = 0.02222483\n",
      "Iteration 941, loss = 0.02217298\n",
      "Iteration 942, loss = 0.02212127\n",
      "Iteration 943, loss = 0.02206979\n",
      "Iteration 944, loss = 0.02201855\n",
      "Iteration 945, loss = 0.02196751\n",
      "Iteration 946, loss = 0.02191662\n",
      "Iteration 947, loss = 0.02186582\n",
      "Iteration 948, loss = 0.02181515\n",
      "Iteration 949, loss = 0.02176464\n",
      "Iteration 950, loss = 0.02171432\n",
      "Iteration 951, loss = 0.02166420\n",
      "Iteration 952, loss = 0.02161425\n",
      "Iteration 953, loss = 0.02156445\n",
      "Iteration 954, loss = 0.02151477\n",
      "Iteration 955, loss = 0.02146523\n",
      "Iteration 956, loss = 0.02141585\n",
      "Iteration 957, loss = 0.02136664\n",
      "Iteration 958, loss = 0.02131760\n",
      "Iteration 959, loss = 0.02126872\n",
      "Iteration 960, loss = 0.02121999\n",
      "Iteration 961, loss = 0.02117139\n",
      "Iteration 962, loss = 0.02112294\n",
      "Iteration 963, loss = 0.02107464\n",
      "Iteration 964, loss = 0.02102649\n",
      "Iteration 965, loss = 0.02097851\n",
      "Iteration 966, loss = 0.02093067\n",
      "Iteration 967, loss = 0.02088298\n",
      "Iteration 968, loss = 0.02083544\n",
      "Iteration 969, loss = 0.02078804\n",
      "Iteration 970, loss = 0.02074078\n",
      "Iteration 971, loss = 0.02069366\n",
      "Iteration 972, loss = 0.02064670\n",
      "Iteration 973, loss = 0.02059988\n",
      "Iteration 974, loss = 0.02055321\n",
      "Iteration 975, loss = 0.02050668\n",
      "Iteration 976, loss = 0.02046029\n",
      "Iteration 977, loss = 0.02041404\n",
      "Iteration 978, loss = 0.02036793\n",
      "Iteration 979, loss = 0.02032196\n",
      "Iteration 980, loss = 0.02027613\n",
      "Iteration 981, loss = 0.02023044\n",
      "Iteration 982, loss = 0.02018490\n",
      "Iteration 983, loss = 0.02013949\n",
      "Iteration 984, loss = 0.02009421\n",
      "Iteration 985, loss = 0.02004908\n",
      "Iteration 986, loss = 0.02000408\n",
      "Iteration 987, loss = 0.01995921\n",
      "Iteration 988, loss = 0.01991448\n",
      "Iteration 989, loss = 0.01986989\n",
      "Iteration 990, loss = 0.01982543\n",
      "Iteration 991, loss = 0.01978110\n",
      "Iteration 992, loss = 0.01973691\n",
      "Iteration 993, loss = 0.01969285\n",
      "Iteration 994, loss = 0.01964892\n",
      "Iteration 995, loss = 0.01960512\n",
      "Iteration 996, loss = 0.01956145\n",
      "Iteration 997, loss = 0.01951792\n",
      "Iteration 998, loss = 0.01947451\n",
      "Iteration 999, loss = 0.01943123\n",
      "Iteration 1000, loss = 0.01938808\n",
      "Iteration 1001, loss = 0.01934506\n",
      "Iteration 1002, loss = 0.01930217\n",
      "Iteration 1003, loss = 0.01925940\n",
      "Iteration 1004, loss = 0.01921676\n",
      "Iteration 1005, loss = 0.01917425\n",
      "Iteration 1006, loss = 0.01913186\n",
      "Iteration 1007, loss = 0.01908960\n",
      "Iteration 1008, loss = 0.01904746\n",
      "Iteration 1009, loss = 0.01900545\n",
      "Iteration 1010, loss = 0.01896356\n",
      "Iteration 1011, loss = 0.01892179\n",
      "Iteration 1012, loss = 0.01888015\n",
      "Iteration 1013, loss = 0.01883862\n",
      "Iteration 1014, loss = 0.01879722\n",
      "Iteration 1015, loss = 0.01875594\n",
      "Iteration 1016, loss = 0.01871478\n",
      "Iteration 1017, loss = 0.01867374\n",
      "Iteration 1018, loss = 0.01863282\n",
      "Iteration 1019, loss = 0.01859202\n",
      "Iteration 1020, loss = 0.01855134\n",
      "Iteration 1021, loss = 0.01851078\n",
      "Iteration 1022, loss = 0.01847033\n",
      "Iteration 1023, loss = 0.01843000\n",
      "Iteration 1024, loss = 0.01838979\n",
      "Iteration 1025, loss = 0.01834969\n",
      "Iteration 1026, loss = 0.01830971\n",
      "Iteration 1027, loss = 0.01826985\n",
      "Iteration 1028, loss = 0.01823010\n",
      "Iteration 1029, loss = 0.01819046\n",
      "Iteration 1030, loss = 0.01815094\n",
      "Iteration 1031, loss = 0.01811153\n",
      "Iteration 1032, loss = 0.01807224\n",
      "Iteration 1033, loss = 0.01803306\n",
      "Iteration 1034, loss = 0.01799399\n",
      "Iteration 1035, loss = 0.01795503\n",
      "Iteration 1036, loss = 0.01791619\n",
      "Iteration 1037, loss = 0.01787745\n",
      "Iteration 1038, loss = 0.01783883\n",
      "Iteration 1039, loss = 0.01780031\n",
      "Iteration 1040, loss = 0.01776191\n",
      "Iteration 1041, loss = 0.01772361\n",
      "Iteration 1042, loss = 0.01768543\n",
      "Iteration 1043, loss = 0.01764735\n",
      "Iteration 1044, loss = 0.01760938\n",
      "Iteration 1045, loss = 0.01757152\n",
      "Iteration 1046, loss = 0.01753376\n",
      "Iteration 1047, loss = 0.01749611\n",
      "Iteration 1048, loss = 0.01745857\n",
      "Iteration 1049, loss = 0.01742114\n",
      "Iteration 1050, loss = 0.01738381\n",
      "Iteration 1051, loss = 0.01734658\n",
      "Iteration 1052, loss = 0.01730946\n",
      "Iteration 1053, loss = 0.01727245\n",
      "Iteration 1054, loss = 0.01723554\n",
      "Iteration 1055, loss = 0.01719873\n",
      "Iteration 1056, loss = 0.01716202\n",
      "Iteration 1057, loss = 0.01712542\n",
      "Iteration 1058, loss = 0.01708892\n",
      "Iteration 1059, loss = 0.01705253\n",
      "Iteration 1060, loss = 0.01701623\n",
      "Iteration 1061, loss = 0.01698004\n",
      "Iteration 1062, loss = 0.01694394\n",
      "Iteration 1063, loss = 0.01690795\n",
      "Iteration 1064, loss = 0.01687206\n",
      "Iteration 1065, loss = 0.01683627\n",
      "Iteration 1066, loss = 0.01680058\n",
      "Iteration 1067, loss = 0.01676498\n",
      "Iteration 1068, loss = 0.01672949\n",
      "Iteration 1069, loss = 0.01669410\n",
      "Iteration 1070, loss = 0.01665880\n",
      "Iteration 1071, loss = 0.01662360\n",
      "Iteration 1072, loss = 0.01658850\n",
      "Iteration 1073, loss = 0.01655349\n",
      "Iteration 1074, loss = 0.01651858\n",
      "Iteration 1075, loss = 0.01648377\n",
      "Iteration 1076, loss = 0.01644906\n",
      "Iteration 1077, loss = 0.01641444\n",
      "Iteration 1078, loss = 0.01637991\n",
      "Iteration 1079, loss = 0.01634548\n",
      "Iteration 1080, loss = 0.01631115\n",
      "Iteration 1081, loss = 0.01627691\n",
      "Iteration 1082, loss = 0.01624276\n",
      "Iteration 1083, loss = 0.01620871\n",
      "Iteration 1084, loss = 0.01617475\n",
      "Iteration 1085, loss = 0.01614089\n",
      "Iteration 1086, loss = 0.01610711\n",
      "Iteration 1087, loss = 0.01607343\n",
      "Iteration 1088, loss = 0.01603985\n",
      "Iteration 1089, loss = 0.01600635\n",
      "Iteration 1090, loss = 0.01597294\n",
      "Iteration 1091, loss = 0.01593963\n",
      "Iteration 1092, loss = 0.01590641\n",
      "Iteration 1093, loss = 0.01587328\n",
      "Iteration 1094, loss = 0.01584024\n",
      "Iteration 1095, loss = 0.01580729\n",
      "Iteration 1096, loss = 0.01577443\n",
      "Iteration 1097, loss = 0.01574165\n",
      "Iteration 1098, loss = 0.01570897\n",
      "Iteration 1099, loss = 0.01567638\n",
      "Iteration 1100, loss = 0.01564387\n",
      "Iteration 1101, loss = 0.01561146\n",
      "Iteration 1102, loss = 0.01557913\n",
      "Iteration 1103, loss = 0.01554689\n",
      "Iteration 1104, loss = 0.01551474\n",
      "Iteration 1105, loss = 0.01548267\n",
      "Iteration 1106, loss = 0.01545069\n",
      "Iteration 1107, loss = 0.01541880\n",
      "Iteration 1108, loss = 0.01538699\n",
      "Iteration 1109, loss = 0.01535528\n",
      "Iteration 1110, loss = 0.01532364\n",
      "Iteration 1111, loss = 0.01529209\n",
      "Iteration 1112, loss = 0.01526063\n",
      "Iteration 1113, loss = 0.01522925\n",
      "Iteration 1114, loss = 0.01519796\n",
      "Iteration 1115, loss = 0.01516675\n",
      "Iteration 1116, loss = 0.01513563\n",
      "Iteration 1117, loss = 0.01510459\n",
      "Iteration 1118, loss = 0.01507363\n",
      "Iteration 1119, loss = 0.01504276\n",
      "Iteration 1120, loss = 0.01501197\n",
      "Iteration 1121, loss = 0.01498127\n",
      "Iteration 1122, loss = 0.01495064\n",
      "Iteration 1123, loss = 0.01492010\n",
      "Iteration 1124, loss = 0.01488965\n",
      "Iteration 1125, loss = 0.01485927\n",
      "Iteration 1126, loss = 0.01482898\n",
      "Iteration 1127, loss = 0.01479876\n",
      "Iteration 1128, loss = 0.01476863\n",
      "Iteration 1129, loss = 0.01473858\n",
      "Iteration 1130, loss = 0.01470861\n",
      "Iteration 1131, loss = 0.01467873\n",
      "Iteration 1132, loss = 0.01464892\n",
      "Iteration 1133, loss = 0.01461920\n",
      "Iteration 1134, loss = 0.01458955\n",
      "Iteration 1135, loss = 0.01455999\n",
      "Iteration 1136, loss = 0.01453052\n",
      "Iteration 1137, loss = 0.01450113\n",
      "Iteration 1138, loss = 0.01447182\n",
      "Iteration 1139, loss = 0.01444261\n",
      "Iteration 1140, loss = 0.01441350\n",
      "Iteration 1141, loss = 0.01438450\n",
      "Iteration 1142, loss = 0.01435563\n",
      "Iteration 1143, loss = 0.01432691\n",
      "Iteration 1144, loss = 0.01429838\n",
      "Iteration 1145, loss = 0.01427010\n",
      "Iteration 1146, loss = 0.01424217\n",
      "Iteration 1147, loss = 0.01421472\n",
      "Iteration 1148, loss = 0.01418797\n",
      "Iteration 1149, loss = 0.01416222\n",
      "Iteration 1150, loss = 0.01413797\n",
      "Iteration 1151, loss = 0.01411585\n",
      "Iteration 1152, loss = 0.01409679\n",
      "Iteration 1153, loss = 0.01408172\n",
      "Iteration 1154, loss = 0.01407177\n",
      "Iteration 1155, loss = 0.01406671\n",
      "Iteration 1156, loss = 0.01406515\n",
      "Iteration 1157, loss = 0.01406077\n",
      "Iteration 1158, loss = 0.01404515\n",
      "Iteration 1159, loss = 0.01400719\n",
      "Iteration 1160, loss = 0.01394534\n",
      "Iteration 1161, loss = 0.01387042\n",
      "Iteration 1162, loss = 0.01380450\n",
      "Iteration 1163, loss = 0.01376441\n",
      "Iteration 1164, loss = 0.01375019\n",
      "Iteration 1165, loss = 0.01374689\n",
      "Iteration 1166, loss = 0.01373582\n",
      "Iteration 1167, loss = 0.01370619\n",
      "Iteration 1168, loss = 0.01366056\n",
      "Iteration 1169, loss = 0.01361249\n",
      "Iteration 1170, loss = 0.01357567\n",
      "Iteration 1171, loss = 0.01355377\n",
      "Iteration 1172, loss = 0.01353920\n",
      "Iteration 1173, loss = 0.01352067\n",
      "Iteration 1174, loss = 0.01349194\n",
      "Iteration 1175, loss = 0.01345552\n",
      "Iteration 1176, loss = 0.01341951\n",
      "Iteration 1177, loss = 0.01339045\n",
      "Iteration 1178, loss = 0.01336849\n",
      "Iteration 1179, loss = 0.01334839\n",
      "Iteration 1180, loss = 0.01332458\n",
      "Iteration 1181, loss = 0.01329551\n",
      "Iteration 1182, loss = 0.01326411\n",
      "Iteration 1183, loss = 0.01323474\n",
      "Iteration 1184, loss = 0.01320945\n",
      "Iteration 1185, loss = 0.01318677\n",
      "Iteration 1186, loss = 0.01316352\n",
      "Iteration 1187, loss = 0.01313768\n",
      "Iteration 1188, loss = 0.01310972\n",
      "Iteration 1189, loss = 0.01308181\n",
      "Iteration 1190, loss = 0.01305574\n",
      "Iteration 1191, loss = 0.01303158\n",
      "Iteration 1192, loss = 0.01300792\n",
      "Iteration 1193, loss = 0.01298331\n",
      "Iteration 1194, loss = 0.01295738\n",
      "Iteration 1195, loss = 0.01293096\n",
      "Iteration 1196, loss = 0.01290519\n",
      "Iteration 1197, loss = 0.01288052\n",
      "Iteration 1198, loss = 0.01285651\n",
      "Iteration 1199, loss = 0.01283234\n",
      "Iteration 1200, loss = 0.01280752\n",
      "Iteration 1201, loss = 0.01278225\n",
      "Iteration 1202, loss = 0.01275709\n",
      "Iteration 1203, loss = 0.01273249\n",
      "Iteration 1204, loss = 0.01270839\n",
      "Iteration 1205, loss = 0.01268443\n",
      "Iteration 1206, loss = 0.01266025\n",
      "Iteration 1207, loss = 0.01263578\n",
      "Iteration 1208, loss = 0.01261126\n",
      "Iteration 1209, loss = 0.01258698\n",
      "Iteration 1210, loss = 0.01256302\n",
      "Iteration 1211, loss = 0.01253926\n",
      "Iteration 1212, loss = 0.01251549\n",
      "Iteration 1213, loss = 0.01249160\n",
      "Iteration 1214, loss = 0.01246764\n",
      "Iteration 1215, loss = 0.01244378\n",
      "Iteration 1216, loss = 0.01242010\n",
      "Iteration 1217, loss = 0.01239659\n",
      "Iteration 1218, loss = 0.01237316\n",
      "Iteration 1219, loss = 0.01234971\n",
      "Iteration 1220, loss = 0.01232623\n",
      "Iteration 1221, loss = 0.01230279\n",
      "Iteration 1222, loss = 0.01227946\n",
      "Iteration 1223, loss = 0.01225625\n",
      "Iteration 1224, loss = 0.01223314\n",
      "Iteration 1225, loss = 0.01221006\n",
      "Iteration 1226, loss = 0.01218700\n",
      "Iteration 1227, loss = 0.01216397\n",
      "Iteration 1228, loss = 0.01214101\n",
      "Iteration 1229, loss = 0.01211814\n",
      "Iteration 1230, loss = 0.01209535\n",
      "Iteration 1231, loss = 0.01207263\n",
      "Iteration 1232, loss = 0.01204994\n",
      "Iteration 1233, loss = 0.01202729\n",
      "Iteration 1234, loss = 0.01200470\n",
      "Iteration 1235, loss = 0.01198217\n",
      "Iteration 1236, loss = 0.01195972\n",
      "Iteration 1237, loss = 0.01193734\n",
      "Iteration 1238, loss = 0.01191500\n",
      "Iteration 1239, loss = 0.01189272\n",
      "Iteration 1240, loss = 0.01187048\n",
      "Iteration 1241, loss = 0.01184830\n",
      "Iteration 1242, loss = 0.01182619\n",
      "Iteration 1243, loss = 0.01180414\n",
      "Iteration 1244, loss = 0.01178215\n",
      "Iteration 1245, loss = 0.01176021\n",
      "Iteration 1246, loss = 0.01173832\n",
      "Iteration 1247, loss = 0.01171649\n",
      "Iteration 1248, loss = 0.01169471\n",
      "Iteration 1249, loss = 0.01167300\n",
      "Iteration 1250, loss = 0.01165134\n",
      "Iteration 1251, loss = 0.01162974\n",
      "Iteration 1252, loss = 0.01160819\n",
      "Iteration 1253, loss = 0.01158669\n",
      "Iteration 1254, loss = 0.01156525\n",
      "Iteration 1255, loss = 0.01154387\n",
      "Iteration 1256, loss = 0.01152254\n",
      "Iteration 1257, loss = 0.01150127\n",
      "Iteration 1258, loss = 0.01148005\n",
      "Iteration 1259, loss = 0.01145888\n",
      "Iteration 1260, loss = 0.01143777\n",
      "Iteration 1261, loss = 0.01141671\n",
      "Iteration 1262, loss = 0.01139570\n",
      "Iteration 1263, loss = 0.01137475\n",
      "Iteration 1264, loss = 0.01135386\n",
      "Iteration 1265, loss = 0.01133301\n",
      "Iteration 1266, loss = 0.01131222\n",
      "Iteration 1267, loss = 0.01129148\n",
      "Iteration 1268, loss = 0.01127079\n",
      "Iteration 1269, loss = 0.01125016\n",
      "Iteration 1270, loss = 0.01122958\n",
      "Iteration 1271, loss = 0.01120905\n",
      "Iteration 1272, loss = 0.01118858\n",
      "Iteration 1273, loss = 0.01116815\n",
      "Iteration 1274, loss = 0.01114778\n",
      "Iteration 1275, loss = 0.01112746\n",
      "Iteration 1276, loss = 0.01110719\n",
      "Iteration 1277, loss = 0.01108697\n",
      "Iteration 1278, loss = 0.01106680\n",
      "Iteration 1279, loss = 0.01104669\n",
      "Iteration 1280, loss = 0.01102662\n",
      "Iteration 1281, loss = 0.01100661\n",
      "Iteration 1282, loss = 0.01098664\n",
      "Iteration 1283, loss = 0.01096673\n",
      "Iteration 1284, loss = 0.01094686\n",
      "Iteration 1285, loss = 0.01092705\n",
      "Iteration 1286, loss = 0.01090729\n",
      "Iteration 1287, loss = 0.01088757\n",
      "Iteration 1288, loss = 0.01086791\n",
      "Iteration 1289, loss = 0.01084829\n",
      "Iteration 1290, loss = 0.01082873\n",
      "Iteration 1291, loss = 0.01080921\n",
      "Iteration 1292, loss = 0.01078974\n",
      "Iteration 1293, loss = 0.01077032\n",
      "Iteration 1294, loss = 0.01075095\n",
      "Iteration 1295, loss = 0.01073163\n",
      "Iteration 1296, loss = 0.01071236\n",
      "Iteration 1297, loss = 0.01069313\n",
      "Iteration 1298, loss = 0.01067395\n",
      "Iteration 1299, loss = 0.01065482\n",
      "Iteration 1300, loss = 0.01063574\n",
      "Iteration 1301, loss = 0.01061671\n",
      "Iteration 1302, loss = 0.01059772\n",
      "Iteration 1303, loss = 0.01057878\n",
      "Iteration 1304, loss = 0.01055989\n",
      "Iteration 1305, loss = 0.01054104\n",
      "Iteration 1306, loss = 0.01052225\n",
      "Iteration 1307, loss = 0.01050349\n",
      "Iteration 1308, loss = 0.01048479\n",
      "Iteration 1309, loss = 0.01046613\n",
      "Iteration 1310, loss = 0.01044752\n",
      "Iteration 1311, loss = 0.01042895\n",
      "Iteration 1312, loss = 0.01041043\n",
      "Iteration 1313, loss = 0.01039196\n",
      "Iteration 1314, loss = 0.01037353\n",
      "Iteration 1315, loss = 0.01035515\n",
      "Iteration 1316, loss = 0.01033681\n",
      "Iteration 1317, loss = 0.01031852\n",
      "Iteration 1318, loss = 0.01030027\n",
      "Iteration 1319, loss = 0.01028207\n",
      "Iteration 1320, loss = 0.01026391\n",
      "Iteration 1321, loss = 0.01024580\n",
      "Iteration 1322, loss = 0.01022773\n",
      "Iteration 1323, loss = 0.01020971\n",
      "Iteration 1324, loss = 0.01019173\n",
      "Iteration 1325, loss = 0.01017380\n",
      "Iteration 1326, loss = 0.01015591\n",
      "Iteration 1327, loss = 0.01013806\n",
      "Iteration 1328, loss = 0.01012026\n",
      "Iteration 1329, loss = 0.01010250\n",
      "Iteration 1330, loss = 0.01008478\n",
      "Iteration 1331, loss = 0.01006711\n",
      "Iteration 1332, loss = 0.01004948\n",
      "Iteration 1333, loss = 0.01003190\n",
      "Iteration 1334, loss = 0.01001435\n",
      "Iteration 1335, loss = 0.00999685\n",
      "Iteration 1336, loss = 0.00997940\n",
      "Iteration 1337, loss = 0.00996198\n",
      "Iteration 1338, loss = 0.00994461\n",
      "Iteration 1339, loss = 0.00992728\n",
      "Iteration 1340, loss = 0.00990999\n",
      "Iteration 1341, loss = 0.00989275\n",
      "Iteration 1342, loss = 0.00987555\n",
      "Iteration 1343, loss = 0.00985838\n",
      "Iteration 1344, loss = 0.00984126\n",
      "Iteration 1345, loss = 0.00982419\n",
      "Iteration 1346, loss = 0.00980715\n",
      "Iteration 1347, loss = 0.00979015\n",
      "Iteration 1348, loss = 0.00977320\n",
      "Iteration 1349, loss = 0.00975629\n",
      "Iteration 1350, loss = 0.00973941\n",
      "Iteration 1351, loss = 0.00972258\n",
      "Iteration 1352, loss = 0.00970579\n",
      "Iteration 1353, loss = 0.00968904\n",
      "Iteration 1354, loss = 0.00967233\n",
      "Iteration 1355, loss = 0.00965566\n",
      "Iteration 1356, loss = 0.00963903\n",
      "Iteration 1357, loss = 0.00962244\n",
      "Iteration 1358, loss = 0.00960590\n",
      "Iteration 1359, loss = 0.00958939\n",
      "Iteration 1360, loss = 0.00957292\n",
      "Iteration 1361, loss = 0.00955649\n",
      "Iteration 1362, loss = 0.00954010\n",
      "Iteration 1363, loss = 0.00952374\n",
      "Iteration 1364, loss = 0.00950743\n",
      "Iteration 1365, loss = 0.00949116\n",
      "Iteration 1366, loss = 0.00947493\n",
      "Iteration 1367, loss = 0.00945873\n",
      "Iteration 1368, loss = 0.00944257\n",
      "Iteration 1369, loss = 0.00942646\n",
      "Iteration 1370, loss = 0.00941038\n",
      "Iteration 1371, loss = 0.00939434\n",
      "Iteration 1372, loss = 0.00937833\n",
      "Iteration 1373, loss = 0.00936237\n",
      "Iteration 1374, loss = 0.00934644\n",
      "Iteration 1375, loss = 0.00933056\n",
      "Iteration 1376, loss = 0.00931471\n",
      "Iteration 1377, loss = 0.00929889\n",
      "Iteration 1378, loss = 0.00928312\n",
      "Iteration 1379, loss = 0.00926738\n",
      "Iteration 1380, loss = 0.00925168\n",
      "Iteration 1381, loss = 0.00923602\n",
      "Iteration 1382, loss = 0.00922039\n",
      "Iteration 1383, loss = 0.00920480\n",
      "Iteration 1384, loss = 0.00918925\n",
      "Iteration 1385, loss = 0.00917374\n",
      "Iteration 1386, loss = 0.00915826\n",
      "Iteration 1387, loss = 0.00914282\n",
      "Iteration 1388, loss = 0.00912741\n",
      "Iteration 1389, loss = 0.00911204\n",
      "Iteration 1390, loss = 0.00909671\n",
      "Iteration 1391, loss = 0.00908141\n",
      "Iteration 1392, loss = 0.00906615\n",
      "Iteration 1393, loss = 0.00905093\n",
      "Iteration 1394, loss = 0.00903574\n",
      "Iteration 1395, loss = 0.00902059\n",
      "Iteration 1396, loss = 0.00900547\n",
      "Iteration 1397, loss = 0.00899039\n",
      "Iteration 1398, loss = 0.00897534\n",
      "Iteration 1399, loss = 0.00896033\n",
      "Iteration 1400, loss = 0.00894536\n",
      "Iteration 1401, loss = 0.00893042\n",
      "Iteration 1402, loss = 0.00891551\n",
      "Iteration 1403, loss = 0.00890064\n",
      "Iteration 1404, loss = 0.00888580\n",
      "Iteration 1405, loss = 0.00887100\n",
      "Iteration 1406, loss = 0.00885624\n",
      "Iteration 1407, loss = 0.00884150\n",
      "Iteration 1408, loss = 0.00882681\n",
      "Iteration 1409, loss = 0.00881214\n",
      "Iteration 1410, loss = 0.00879751\n",
      "Iteration 1411, loss = 0.00878292\n",
      "Iteration 1412, loss = 0.00876836\n",
      "Iteration 1413, loss = 0.00875383\n",
      "Iteration 1414, loss = 0.00873933\n",
      "Iteration 1415, loss = 0.00872487\n",
      "Iteration 1416, loss = 0.00871045\n",
      "Iteration 1417, loss = 0.00869605\n",
      "Iteration 1418, loss = 0.00868169\n",
      "Iteration 1419, loss = 0.00866737\n",
      "Iteration 1420, loss = 0.00865308\n",
      "Iteration 1421, loss = 0.00863882\n",
      "Iteration 1422, loss = 0.00862459\n",
      "Iteration 1423, loss = 0.00861039\n",
      "Iteration 1424, loss = 0.00859623\n",
      "Iteration 1425, loss = 0.00858210\n",
      "Iteration 1426, loss = 0.00856801\n",
      "Iteration 1427, loss = 0.00855394\n",
      "Iteration 1428, loss = 0.00853991\n",
      "Iteration 1429, loss = 0.00852591\n",
      "Iteration 1430, loss = 0.00851195\n",
      "Iteration 1431, loss = 0.00849801\n",
      "Iteration 1432, loss = 0.00848411\n",
      "Iteration 1433, loss = 0.00847024\n",
      "Iteration 1434, loss = 0.00845640\n",
      "Iteration 1435, loss = 0.00844259\n",
      "Iteration 1436, loss = 0.00842882\n",
      "Iteration 1437, loss = 0.00841507\n",
      "Iteration 1438, loss = 0.00840136\n",
      "Iteration 1439, loss = 0.00838768\n",
      "Iteration 1440, loss = 0.00837403\n",
      "Iteration 1441, loss = 0.00836041\n",
      "Iteration 1442, loss = 0.00834682\n",
      "Iteration 1443, loss = 0.00833327\n",
      "Iteration 1444, loss = 0.00831974\n",
      "Iteration 1445, loss = 0.00830625\n",
      "Iteration 1446, loss = 0.00829278\n",
      "Iteration 1447, loss = 0.00827935\n",
      "Iteration 1448, loss = 0.00826595\n",
      "Iteration 1449, loss = 0.00825257\n",
      "Iteration 1450, loss = 0.00823923\n",
      "Iteration 1451, loss = 0.00822592\n",
      "Iteration 1452, loss = 0.00821264\n",
      "Iteration 1453, loss = 0.00819939\n",
      "Iteration 1454, loss = 0.00818616\n",
      "Iteration 1455, loss = 0.00817297\n",
      "Iteration 1456, loss = 0.00815981\n",
      "Iteration 1457, loss = 0.00814668\n",
      "Iteration 1458, loss = 0.00813358\n",
      "Iteration 1459, loss = 0.00812051\n",
      "Iteration 1460, loss = 0.00810746\n",
      "Iteration 1461, loss = 0.00809445\n",
      "Iteration 1462, loss = 0.00808146\n",
      "Iteration 1463, loss = 0.00806851\n",
      "Iteration 1464, loss = 0.00805558\n",
      "Iteration 1465, loss = 0.00804269\n",
      "Iteration 1466, loss = 0.00802982\n",
      "Iteration 1467, loss = 0.00801698\n",
      "Iteration 1468, loss = 0.00800417\n",
      "Iteration 1469, loss = 0.00799139\n",
      "Iteration 1470, loss = 0.00797864\n",
      "Iteration 1471, loss = 0.00796591\n",
      "Iteration 1472, loss = 0.00795322\n",
      "Iteration 1473, loss = 0.00794055\n",
      "Iteration 1474, loss = 0.00792791\n",
      "Iteration 1475, loss = 0.00791530\n",
      "Iteration 1476, loss = 0.00790272\n",
      "Iteration 1477, loss = 0.00789017\n",
      "Iteration 1478, loss = 0.00787764\n",
      "Iteration 1479, loss = 0.00786514\n",
      "Iteration 1480, loss = 0.00785267\n",
      "Iteration 1481, loss = 0.00784023\n",
      "Iteration 1482, loss = 0.00782781\n",
      "Iteration 1483, loss = 0.00781543\n",
      "Iteration 1484, loss = 0.00780307\n",
      "Iteration 1485, loss = 0.00779074\n",
      "Iteration 1486, loss = 0.00777843\n",
      "Iteration 1487, loss = 0.00776615\n",
      "Iteration 1488, loss = 0.00775390\n",
      "Iteration 1489, loss = 0.00774168\n",
      "Iteration 1490, loss = 0.00772948\n",
      "Iteration 1491, loss = 0.00771732\n",
      "Iteration 1492, loss = 0.00770517\n",
      "Iteration 1493, loss = 0.00769306\n",
      "Iteration 1494, loss = 0.00768097\n",
      "Iteration 1495, loss = 0.00766891\n",
      "Iteration 1496, loss = 0.00765687\n",
      "Iteration 1497, loss = 0.00764487\n",
      "Iteration 1498, loss = 0.00763288\n",
      "Iteration 1499, loss = 0.00762093\n",
      "Iteration 1500, loss = 0.00760900\n",
      "Iteration 1501, loss = 0.00759710\n",
      "Iteration 1502, loss = 0.00758522\n",
      "Iteration 1503, loss = 0.00757337\n",
      "Iteration 1504, loss = 0.00756154\n",
      "Iteration 1505, loss = 0.00754975\n",
      "Iteration 1506, loss = 0.00753797\n",
      "Iteration 1507, loss = 0.00752623\n",
      "Iteration 1508, loss = 0.00751450\n",
      "Iteration 1509, loss = 0.00750281\n",
      "Iteration 1510, loss = 0.00749114\n",
      "Iteration 1511, loss = 0.00747949\n",
      "Iteration 1512, loss = 0.00746788\n",
      "Iteration 1513, loss = 0.00745628\n",
      "Iteration 1514, loss = 0.00744471\n",
      "Iteration 1515, loss = 0.00743317\n",
      "Iteration 1516, loss = 0.00742165\n",
      "Iteration 1517, loss = 0.00741016\n",
      "Iteration 1518, loss = 0.00739869\n",
      "Iteration 1519, loss = 0.00738725\n",
      "Iteration 1520, loss = 0.00737583\n",
      "Iteration 1521, loss = 0.00736444\n",
      "Iteration 1522, loss = 0.00735307\n",
      "Iteration 1523, loss = 0.00734173\n",
      "Iteration 1524, loss = 0.00733041\n",
      "Iteration 1525, loss = 0.00731911\n",
      "Iteration 1526, loss = 0.00730784\n",
      "Iteration 1527, loss = 0.00729660\n",
      "Iteration 1528, loss = 0.00728538\n",
      "Iteration 1529, loss = 0.00727418\n",
      "Iteration 1530, loss = 0.00726301\n",
      "Iteration 1531, loss = 0.00725186\n",
      "Iteration 1532, loss = 0.00724073\n",
      "Iteration 1533, loss = 0.00722963\n",
      "Iteration 1534, loss = 0.00721856\n",
      "Iteration 1535, loss = 0.00720750\n",
      "Iteration 1536, loss = 0.00719648\n",
      "Iteration 1537, loss = 0.00718547\n",
      "Iteration 1538, loss = 0.00717449\n",
      "Iteration 1539, loss = 0.00716353\n",
      "Iteration 1540, loss = 0.00715260\n",
      "Iteration 1541, loss = 0.00714169\n",
      "Iteration 1542, loss = 0.00713080\n",
      "Iteration 1543, loss = 0.00711993\n",
      "Iteration 1544, loss = 0.00710909\n",
      "Iteration 1545, loss = 0.00709828\n",
      "Iteration 1546, loss = 0.00708748\n",
      "Iteration 1547, loss = 0.00707671\n",
      "Iteration 1548, loss = 0.00706596\n",
      "Iteration 1549, loss = 0.00705524\n",
      "Iteration 1550, loss = 0.00704453\n",
      "Iteration 1551, loss = 0.00703385\n",
      "Iteration 1552, loss = 0.00702320\n",
      "Iteration 1553, loss = 0.00701256\n",
      "Iteration 1554, loss = 0.00700195\n",
      "Iteration 1555, loss = 0.00699136\n",
      "Iteration 1556, loss = 0.00698080\n",
      "Iteration 1557, loss = 0.00697025\n",
      "Iteration 1558, loss = 0.00695973\n",
      "Iteration 1559, loss = 0.00694923\n",
      "Iteration 1560, loss = 0.00693876\n",
      "Iteration 1561, loss = 0.00692830\n",
      "Iteration 1562, loss = 0.00691787\n",
      "Iteration 1563, loss = 0.00690746\n",
      "Iteration 1564, loss = 0.00689707\n",
      "Iteration 1565, loss = 0.00688671\n",
      "Iteration 1566, loss = 0.00687636\n",
      "Iteration 1567, loss = 0.00686604\n",
      "Iteration 1568, loss = 0.00685574\n",
      "Iteration 1569, loss = 0.00684546\n",
      "Iteration 1570, loss = 0.00683520\n",
      "Iteration 1571, loss = 0.00682497\n",
      "Iteration 1572, loss = 0.00681475\n",
      "Iteration 1573, loss = 0.00680456\n",
      "Iteration 1574, loss = 0.00679439\n",
      "Iteration 1575, loss = 0.00678424\n",
      "Iteration 1576, loss = 0.00677411\n",
      "Iteration 1577, loss = 0.00676400\n",
      "Iteration 1578, loss = 0.00675392\n",
      "Iteration 1579, loss = 0.00674385\n",
      "Iteration 1580, loss = 0.00673381\n",
      "Iteration 1581, loss = 0.00672379\n",
      "Iteration 1582, loss = 0.00671379\n",
      "Iteration 1583, loss = 0.00670381\n",
      "Iteration 1584, loss = 0.00669385\n",
      "Iteration 1585, loss = 0.00668391\n",
      "Iteration 1586, loss = 0.00667399\n",
      "Iteration 1587, loss = 0.00666409\n",
      "Iteration 1588, loss = 0.00665422\n",
      "Iteration 1589, loss = 0.00664436\n",
      "Iteration 1590, loss = 0.00663453\n",
      "Iteration 1591, loss = 0.00662471\n",
      "Iteration 1592, loss = 0.00661492\n",
      "Iteration 1593, loss = 0.00660514\n",
      "Iteration 1594, loss = 0.00659539\n",
      "Iteration 1595, loss = 0.00658566\n",
      "Iteration 1596, loss = 0.00657594\n",
      "Iteration 1597, loss = 0.00656625\n",
      "Iteration 1598, loss = 0.00655658\n",
      "Iteration 1599, loss = 0.00654692\n",
      "Iteration 1600, loss = 0.00653729\n",
      "Iteration 1601, loss = 0.00652768\n",
      "Iteration 1602, loss = 0.00651808\n",
      "Iteration 1603, loss = 0.00650851\n",
      "Iteration 1604, loss = 0.00649896\n",
      "Iteration 1605, loss = 0.00648942\n",
      "Iteration 1606, loss = 0.00647991\n",
      "Iteration 1607, loss = 0.00647042\n",
      "Iteration 1608, loss = 0.00646094\n",
      "Iteration 1609, loss = 0.00645149\n",
      "Iteration 1610, loss = 0.00644205\n",
      "Iteration 1611, loss = 0.00643263\n",
      "Iteration 1612, loss = 0.00642324\n",
      "Iteration 1613, loss = 0.00641386\n",
      "Iteration 1614, loss = 0.00640450\n",
      "Iteration 1615, loss = 0.00639516\n",
      "Iteration 1616, loss = 0.00638584\n",
      "Iteration 1617, loss = 0.00637654\n",
      "Iteration 1618, loss = 0.00636726\n",
      "Iteration 1619, loss = 0.00635800\n",
      "Iteration 1620, loss = 0.00634875\n",
      "Iteration 1621, loss = 0.00633953\n",
      "Iteration 1622, loss = 0.00633032\n",
      "Iteration 1623, loss = 0.00632114\n",
      "Iteration 1624, loss = 0.00631197\n",
      "Iteration 1625, loss = 0.00630282\n",
      "Iteration 1626, loss = 0.00629369\n",
      "Iteration 1627, loss = 0.00628458\n",
      "Iteration 1628, loss = 0.00627548\n",
      "Iteration 1629, loss = 0.00626641\n",
      "Iteration 1630, loss = 0.00625735\n",
      "Iteration 1631, loss = 0.00624831\n",
      "Iteration 1632, loss = 0.00623929\n",
      "Iteration 1633, loss = 0.00623029\n",
      "Iteration 1634, loss = 0.00622131\n",
      "Iteration 1635, loss = 0.00621234\n",
      "Iteration 1636, loss = 0.00620340\n",
      "Iteration 1637, loss = 0.00619447\n",
      "Iteration 1638, loss = 0.00618556\n",
      "Iteration 1639, loss = 0.00617667\n",
      "Iteration 1640, loss = 0.00616779\n",
      "Iteration 1641, loss = 0.00615894\n",
      "Iteration 1642, loss = 0.00615010\n",
      "Iteration 1643, loss = 0.00614128\n",
      "Iteration 1644, loss = 0.00613247\n",
      "Iteration 1645, loss = 0.00612369\n",
      "Iteration 1646, loss = 0.00611492\n",
      "Iteration 1647, loss = 0.00610617\n",
      "Iteration 1648, loss = 0.00609744\n",
      "Iteration 1649, loss = 0.00608873\n",
      "Iteration 1650, loss = 0.00608003\n",
      "Iteration 1651, loss = 0.00607135\n",
      "Iteration 1652, loss = 0.00606269\n",
      "Iteration 1653, loss = 0.00605404\n",
      "Iteration 1654, loss = 0.00604542\n",
      "Iteration 1655, loss = 0.00603681\n",
      "Iteration 1656, loss = 0.00602821\n",
      "Iteration 1657, loss = 0.00601964\n",
      "Iteration 1658, loss = 0.00601108\n",
      "Iteration 1659, loss = 0.00600254\n",
      "Iteration 1660, loss = 0.00599402\n",
      "Iteration 1661, loss = 0.00598551\n",
      "Iteration 1662, loss = 0.00597702\n",
      "Iteration 1663, loss = 0.00596855\n",
      "Iteration 1664, loss = 0.00596009\n",
      "Iteration 1665, loss = 0.00595165\n",
      "Iteration 1666, loss = 0.00594323\n",
      "Iteration 1667, loss = 0.00593482\n",
      "Iteration 1668, loss = 0.00592644\n",
      "Iteration 1669, loss = 0.00591806\n",
      "Iteration 1670, loss = 0.00590971\n",
      "Iteration 1671, loss = 0.00590137\n",
      "Iteration 1672, loss = 0.00589305\n",
      "Iteration 1673, loss = 0.00588474\n",
      "Iteration 1674, loss = 0.00587645\n",
      "Iteration 1675, loss = 0.00586818\n",
      "Iteration 1676, loss = 0.00585992\n",
      "Iteration 1677, loss = 0.00585168\n",
      "Iteration 1678, loss = 0.00584346\n",
      "Iteration 1679, loss = 0.00583525\n",
      "Iteration 1680, loss = 0.00582706\n",
      "Iteration 1681, loss = 0.00581889\n",
      "Iteration 1682, loss = 0.00581073\n",
      "Iteration 1683, loss = 0.00580259\n",
      "Iteration 1684, loss = 0.00579446\n",
      "Iteration 1685, loss = 0.00578635\n",
      "Iteration 1686, loss = 0.00577826\n",
      "Iteration 1687, loss = 0.00577018\n",
      "Iteration 1688, loss = 0.00576212\n",
      "Iteration 1689, loss = 0.00575407\n",
      "Iteration 1690, loss = 0.00574604\n",
      "Iteration 1691, loss = 0.00573802\n",
      "Iteration 1692, loss = 0.00573002\n",
      "Iteration 1693, loss = 0.00572204\n",
      "Iteration 1694, loss = 0.00571407\n",
      "Iteration 1695, loss = 0.00570612\n",
      "Iteration 1696, loss = 0.00569818\n",
      "Iteration 1697, loss = 0.00569026\n",
      "Iteration 1698, loss = 0.00568236\n",
      "Iteration 1699, loss = 0.00567447\n",
      "Iteration 1700, loss = 0.00566659\n",
      "Iteration 1701, loss = 0.00565873\n",
      "Iteration 1702, loss = 0.00565089\n",
      "Iteration 1703, loss = 0.00564306\n",
      "Iteration 1704, loss = 0.00563525\n",
      "Iteration 1705, loss = 0.00562745\n",
      "Iteration 1706, loss = 0.00561967\n",
      "Iteration 1707, loss = 0.00561190\n",
      "Iteration 1708, loss = 0.00560415\n",
      "Iteration 1709, loss = 0.00559641\n",
      "Iteration 1710, loss = 0.00558869\n",
      "Iteration 1711, loss = 0.00558098\n",
      "Iteration 1712, loss = 0.00557329\n",
      "Iteration 1713, loss = 0.00556561\n",
      "Iteration 1714, loss = 0.00555795\n",
      "Iteration 1715, loss = 0.00555030\n",
      "Iteration 1716, loss = 0.00554267\n",
      "Iteration 1717, loss = 0.00553505\n",
      "Iteration 1718, loss = 0.00552745\n",
      "Iteration 1719, loss = 0.00551986\n",
      "Iteration 1720, loss = 0.00551229\n",
      "Iteration 1721, loss = 0.00550473\n",
      "Iteration 1722, loss = 0.00549718\n",
      "Iteration 1723, loss = 0.00548965\n",
      "Iteration 1724, loss = 0.00548214\n",
      "Iteration 1725, loss = 0.00547464\n",
      "Iteration 1726, loss = 0.00546715\n",
      "Iteration 1727, loss = 0.00545968\n",
      "Iteration 1728, loss = 0.00545222\n",
      "Iteration 1729, loss = 0.00544478\n",
      "Iteration 1730, loss = 0.00543735\n",
      "Iteration 1731, loss = 0.00542993\n",
      "Iteration 1732, loss = 0.00542253\n",
      "Iteration 1733, loss = 0.00541515\n",
      "Iteration 1734, loss = 0.00540777\n",
      "Iteration 1735, loss = 0.00540042\n",
      "Iteration 1736, loss = 0.00539307\n",
      "Iteration 1737, loss = 0.00538574\n",
      "Iteration 1738, loss = 0.00537843\n",
      "Iteration 1739, loss = 0.00537113\n",
      "Iteration 1740, loss = 0.00536384\n",
      "Iteration 1741, loss = 0.00535656\n",
      "Iteration 1742, loss = 0.00534930\n",
      "Iteration 1743, loss = 0.00534206\n",
      "Iteration 1744, loss = 0.00533483\n",
      "Iteration 1745, loss = 0.00532761\n",
      "Iteration 1746, loss = 0.00532040\n",
      "Iteration 1747, loss = 0.00531321\n",
      "Iteration 1748, loss = 0.00530604\n",
      "Iteration 1749, loss = 0.00529887\n",
      "Iteration 1750, loss = 0.00529172\n",
      "Iteration 1751, loss = 0.00528459\n",
      "Iteration 1752, loss = 0.00527746\n",
      "Iteration 1753, loss = 0.00527036\n",
      "Iteration 1754, loss = 0.00526326\n",
      "Iteration 1755, loss = 0.00525618\n",
      "Iteration 1756, loss = 0.00524911\n",
      "Iteration 1757, loss = 0.00524205\n",
      "Iteration 1758, loss = 0.00523501\n",
      "Iteration 1759, loss = 0.00522798\n",
      "Iteration 1760, loss = 0.00522097\n",
      "Iteration 1761, loss = 0.00521397\n",
      "Iteration 1762, loss = 0.00520698\n",
      "Iteration 1763, loss = 0.00520000\n",
      "Iteration 1764, loss = 0.00519304\n",
      "Iteration 1765, loss = 0.00518609\n",
      "Iteration 1766, loss = 0.00517916\n",
      "Iteration 1767, loss = 0.00517223\n",
      "Iteration 1768, loss = 0.00516532\n",
      "Iteration 1769, loss = 0.00515843\n",
      "Iteration 1770, loss = 0.00515154\n",
      "Iteration 1771, loss = 0.00514467\n",
      "Iteration 1772, loss = 0.00513781\n",
      "Iteration 1773, loss = 0.00513097\n",
      "Iteration 1774, loss = 0.00512414\n",
      "Iteration 1775, loss = 0.00511732\n",
      "Iteration 1776, loss = 0.00511051\n",
      "Iteration 1777, loss = 0.00510372\n",
      "Iteration 1778, loss = 0.00509693\n",
      "Iteration 1779, loss = 0.00509017\n",
      "Iteration 1780, loss = 0.00508341\n",
      "Iteration 1781, loss = 0.00507667\n",
      "Iteration 1782, loss = 0.00506994\n",
      "Iteration 1783, loss = 0.00506322\n",
      "Iteration 1784, loss = 0.00505651\n",
      "Iteration 1785, loss = 0.00504982\n",
      "Iteration 1786, loss = 0.00504314\n",
      "Iteration 1787, loss = 0.00503647\n",
      "Iteration 1788, loss = 0.00502982\n",
      "Iteration 1789, loss = 0.00502317\n",
      "Iteration 1790, loss = 0.00501654\n",
      "Iteration 1791, loss = 0.00500992\n",
      "Iteration 1792, loss = 0.00500332\n",
      "Iteration 1793, loss = 0.00499672\n",
      "Iteration 1794, loss = 0.00499014\n",
      "Iteration 1795, loss = 0.00498357\n",
      "Iteration 1796, loss = 0.00497702\n",
      "Iteration 1797, loss = 0.00497047\n",
      "Iteration 1798, loss = 0.00496394\n",
      "Iteration 1799, loss = 0.00495742\n",
      "Iteration 1800, loss = 0.00495091\n",
      "Iteration 1801, loss = 0.00494441\n",
      "Iteration 1802, loss = 0.00493793\n",
      "Iteration 1803, loss = 0.00493146\n",
      "Iteration 1804, loss = 0.00492500\n",
      "Iteration 1805, loss = 0.00491855\n",
      "Iteration 1806, loss = 0.00491211\n",
      "Iteration 1807, loss = 0.00490569\n",
      "Iteration 1808, loss = 0.00489927\n",
      "Iteration 1809, loss = 0.00489287\n",
      "Iteration 1810, loss = 0.00488648\n",
      "Iteration 1811, loss = 0.00488011\n",
      "Iteration 1812, loss = 0.00487374\n",
      "Iteration 1813, loss = 0.00486739\n",
      "Iteration 1814, loss = 0.00486104\n",
      "Iteration 1815, loss = 0.00485471\n",
      "Iteration 1816, loss = 0.00484839\n",
      "Iteration 1817, loss = 0.00484209\n",
      "Iteration 1818, loss = 0.00483579\n",
      "Iteration 1819, loss = 0.00482951\n",
      "Iteration 1820, loss = 0.00482323\n",
      "Iteration 1821, loss = 0.00481697\n",
      "Iteration 1822, loss = 0.00481072\n",
      "Iteration 1823, loss = 0.00480448\n",
      "Iteration 1824, loss = 0.00479826\n",
      "Iteration 1825, loss = 0.00479204\n",
      "Iteration 1826, loss = 0.00478584\n",
      "Iteration 1827, loss = 0.00477964\n",
      "Iteration 1828, loss = 0.00477346\n",
      "Iteration 1829, loss = 0.00476729\n",
      "Iteration 1830, loss = 0.00476113\n",
      "Iteration 1831, loss = 0.00475498\n",
      "Iteration 1832, loss = 0.00474885\n",
      "Iteration 1833, loss = 0.00474272\n",
      "Iteration 1834, loss = 0.00473661\n",
      "Iteration 1835, loss = 0.00473050\n",
      "Iteration 1836, loss = 0.00472441\n",
      "Iteration 1837, loss = 0.00471833\n",
      "Iteration 1838, loss = 0.00471226\n",
      "Iteration 1839, loss = 0.00470620\n",
      "Iteration 1840, loss = 0.00470015\n",
      "Iteration 1841, loss = 0.00469412\n",
      "Iteration 1842, loss = 0.00468809\n",
      "Iteration 1843, loss = 0.00468207\n",
      "Iteration 1844, loss = 0.00467607\n",
      "Iteration 1845, loss = 0.00467008\n",
      "Iteration 1846, loss = 0.00466409\n",
      "Iteration 1847, loss = 0.00465812\n",
      "Iteration 1848, loss = 0.00465216\n",
      "Iteration 1849, loss = 0.00464621\n",
      "Iteration 1850, loss = 0.00464027\n",
      "Iteration 1851, loss = 0.00463434\n",
      "Iteration 1852, loss = 0.00462842\n",
      "Iteration 1853, loss = 0.00462252\n",
      "Iteration 1854, loss = 0.00461662\n",
      "Iteration 1855, loss = 0.00461073\n",
      "Iteration 1856, loss = 0.00460486\n",
      "Iteration 1857, loss = 0.00459899\n",
      "Iteration 1858, loss = 0.00459314\n",
      "Iteration 1859, loss = 0.00458729\n",
      "Iteration 1860, loss = 0.00458146\n",
      "Iteration 1861, loss = 0.00457564\n",
      "Iteration 1862, loss = 0.00456982\n",
      "Iteration 1863, loss = 0.00456402\n",
      "Iteration 1864, loss = 0.00455823\n",
      "Iteration 1865, loss = 0.00455245\n",
      "Iteration 1866, loss = 0.00454668\n",
      "Iteration 1867, loss = 0.00454092\n",
      "Iteration 1868, loss = 0.00453517\n",
      "Iteration 1869, loss = 0.00452943\n",
      "Iteration 1870, loss = 0.00452370\n",
      "Iteration 1871, loss = 0.00451798\n",
      "Iteration 1872, loss = 0.00451227\n",
      "Iteration 1873, loss = 0.00450657\n",
      "Iteration 1874, loss = 0.00450088\n",
      "Iteration 1875, loss = 0.00449520\n",
      "Iteration 1876, loss = 0.00448953\n",
      "Iteration 1877, loss = 0.00448387\n",
      "Iteration 1878, loss = 0.00447822\n",
      "Iteration 1879, loss = 0.00447258\n",
      "Iteration 1880, loss = 0.00446695\n",
      "Iteration 1881, loss = 0.00446134\n",
      "Iteration 1882, loss = 0.00445573\n",
      "Iteration 1883, loss = 0.00445013\n",
      "Iteration 1884, loss = 0.00444454\n",
      "Iteration 1885, loss = 0.00443896\n",
      "Iteration 1886, loss = 0.00443339\n",
      "Iteration 1887, loss = 0.00442783\n",
      "Iteration 1888, loss = 0.00442228\n",
      "Iteration 1889, loss = 0.00441674\n",
      "Iteration 1890, loss = 0.00441121\n",
      "Iteration 1891, loss = 0.00440569\n",
      "Iteration 1892, loss = 0.00440018\n",
      "Iteration 1893, loss = 0.00439468\n",
      "Iteration 1894, loss = 0.00438919\n",
      "Iteration 1895, loss = 0.00438371\n",
      "Iteration 1896, loss = 0.00437824\n",
      "Iteration 1897, loss = 0.00437278\n",
      "Iteration 1898, loss = 0.00436732\n",
      "Iteration 1899, loss = 0.00436188\n",
      "Iteration 1900, loss = 0.00435645\n",
      "Iteration 1901, loss = 0.00435103\n",
      "Iteration 1902, loss = 0.00434561\n",
      "Iteration 1903, loss = 0.00434021\n",
      "Iteration 1904, loss = 0.00433481\n",
      "Iteration 1905, loss = 0.00432943\n",
      "Iteration 1906, loss = 0.00432405\n",
      "Iteration 1907, loss = 0.00431868\n",
      "Iteration 1908, loss = 0.00431333\n",
      "Iteration 1909, loss = 0.00430798\n",
      "Iteration 1910, loss = 0.00430264\n",
      "Iteration 1911, loss = 0.00429731\n",
      "Iteration 1912, loss = 0.00429199\n",
      "Iteration 1913, loss = 0.00428668\n",
      "Iteration 1914, loss = 0.00428138\n",
      "Iteration 1915, loss = 0.00427609\n",
      "Iteration 1916, loss = 0.00427080\n",
      "Iteration 1917, loss = 0.00426553\n",
      "Iteration 1918, loss = 0.00426027\n",
      "Iteration 1919, loss = 0.00425501\n",
      "Iteration 1920, loss = 0.00424976\n",
      "Iteration 1921, loss = 0.00424453\n",
      "Iteration 1922, loss = 0.00423930\n",
      "Iteration 1923, loss = 0.00423408\n",
      "Iteration 1924, loss = 0.00422887\n",
      "Iteration 1925, loss = 0.00422367\n",
      "Iteration 1926, loss = 0.00421848\n",
      "Iteration 1927, loss = 0.00421330\n",
      "Iteration 1928, loss = 0.00420812\n",
      "Iteration 1929, loss = 0.00420296\n",
      "Iteration 1930, loss = 0.00419780\n",
      "Iteration 1931, loss = 0.00419265\n",
      "Iteration 1932, loss = 0.00418752\n",
      "Iteration 1933, loss = 0.00418239\n",
      "Iteration 1934, loss = 0.00417727\n",
      "Iteration 1935, loss = 0.00417216\n",
      "Iteration 1936, loss = 0.00416705\n",
      "Iteration 1937, loss = 0.00416196\n",
      "Iteration 1938, loss = 0.00415687\n",
      "Iteration 1939, loss = 0.00415180\n",
      "Iteration 1940, loss = 0.00414673\n",
      "Iteration 1941, loss = 0.00414167\n",
      "Iteration 1942, loss = 0.00413662\n",
      "Iteration 1943, loss = 0.00413158\n",
      "Iteration 1944, loss = 0.00412655\n",
      "Iteration 1945, loss = 0.00412152\n",
      "Iteration 1946, loss = 0.00411651\n",
      "Iteration 1947, loss = 0.00411150\n",
      "Iteration 1948, loss = 0.00410650\n",
      "Iteration 1949, loss = 0.00410151\n",
      "Iteration 1950, loss = 0.00409653\n",
      "Iteration 1951, loss = 0.00409156\n",
      "Iteration 1952, loss = 0.00408660\n",
      "Iteration 1953, loss = 0.00408164\n",
      "Iteration 1954, loss = 0.00407670\n",
      "Iteration 1955, loss = 0.00407176\n",
      "Iteration 1956, loss = 0.00406683\n",
      "Iteration 1957, loss = 0.00406191\n",
      "Iteration 1958, loss = 0.00405699\n",
      "Iteration 1959, loss = 0.00405209\n",
      "Iteration 1960, loss = 0.00404719\n",
      "Iteration 1961, loss = 0.00404230\n",
      "Iteration 1962, loss = 0.00403742\n",
      "Iteration 1963, loss = 0.00403255\n",
      "Iteration 1964, loss = 0.00402769\n",
      "Iteration 1965, loss = 0.00402283\n",
      "Iteration 1966, loss = 0.00401799\n",
      "Iteration 1967, loss = 0.00401315\n",
      "Iteration 1968, loss = 0.00400832\n",
      "Iteration 1969, loss = 0.00400350\n",
      "Iteration 1970, loss = 0.00399868\n",
      "Iteration 1971, loss = 0.00399388\n",
      "Iteration 1972, loss = 0.00398908\n",
      "Iteration 1973, loss = 0.00398429\n",
      "Iteration 1974, loss = 0.00397951\n",
      "Iteration 1975, loss = 0.00397474\n",
      "Iteration 1976, loss = 0.00396997\n",
      "Iteration 1977, loss = 0.00396521\n",
      "Iteration 1978, loss = 0.00396047\n",
      "Iteration 1979, loss = 0.00395573\n",
      "Iteration 1980, loss = 0.00395099\n",
      "Iteration 1981, loss = 0.00394627\n",
      "Iteration 1982, loss = 0.00394155\n",
      "Iteration 1983, loss = 0.00393684\n",
      "Iteration 1984, loss = 0.00393214\n",
      "Iteration 1985, loss = 0.00392745\n",
      "Iteration 1986, loss = 0.00392276\n",
      "Iteration 1987, loss = 0.00391809\n",
      "Iteration 1988, loss = 0.00391342\n",
      "Iteration 1989, loss = 0.00390875\n",
      "Iteration 1990, loss = 0.00390410\n",
      "Iteration 1991, loss = 0.00389946\n",
      "Iteration 1992, loss = 0.00389482\n",
      "Iteration 1993, loss = 0.00389019\n",
      "Iteration 1994, loss = 0.00388556\n",
      "Iteration 1995, loss = 0.00388095\n",
      "Iteration 1996, loss = 0.00387634\n",
      "Iteration 1997, loss = 0.00387174\n",
      "Iteration 1998, loss = 0.00386715\n",
      "Iteration 1999, loss = 0.00386257\n",
      "Iteration 2000, loss = 0.00385799\n",
      "Iteration 2001, loss = 0.00385342\n",
      "Iteration 2002, loss = 0.00384886\n",
      "Iteration 2003, loss = 0.00384431\n",
      "Iteration 2004, loss = 0.00383976\n",
      "Iteration 2005, loss = 0.00383523\n",
      "Iteration 2006, loss = 0.00383070\n",
      "Iteration 2007, loss = 0.00382617\n",
      "Iteration 2008, loss = 0.00382166\n",
      "Iteration 2009, loss = 0.00381715\n",
      "Iteration 2010, loss = 0.00381265\n",
      "Iteration 2011, loss = 0.00380816\n",
      "Iteration 2012, loss = 0.00380367\n",
      "Iteration 2013, loss = 0.00379919\n",
      "Iteration 2014, loss = 0.00379472\n",
      "Iteration 2015, loss = 0.00379026\n",
      "Iteration 2016, loss = 0.00378580\n",
      "Iteration 2017, loss = 0.00378135\n",
      "Iteration 2018, loss = 0.00377691\n",
      "Iteration 2019, loss = 0.00377248\n",
      "Iteration 2020, loss = 0.00376805\n",
      "Iteration 2021, loss = 0.00376364\n",
      "Iteration 2022, loss = 0.00375922\n",
      "Iteration 2023, loss = 0.00375482\n",
      "Iteration 2024, loss = 0.00375042\n",
      "Iteration 2025, loss = 0.00374603\n",
      "Iteration 2026, loss = 0.00374165\n",
      "Iteration 2027, loss = 0.00373728\n",
      "Iteration 2028, loss = 0.00373291\n",
      "Iteration 2029, loss = 0.00372855\n",
      "Iteration 2030, loss = 0.00372419\n",
      "Iteration 2031, loss = 0.00371985\n",
      "Iteration 2032, loss = 0.00371551\n",
      "Iteration 2033, loss = 0.00371118\n",
      "Iteration 2034, loss = 0.00370685\n",
      "Iteration 2035, loss = 0.00370253\n",
      "Iteration 2036, loss = 0.00369822\n",
      "Iteration 2037, loss = 0.00369392\n",
      "Iteration 2038, loss = 0.00368962\n",
      "Iteration 2039, loss = 0.00368533\n",
      "Iteration 2040, loss = 0.00368105\n",
      "Iteration 2041, loss = 0.00367677\n",
      "Iteration 2042, loss = 0.00367251\n",
      "Iteration 2043, loss = 0.00366824\n",
      "Iteration 2044, loss = 0.00366399\n",
      "Iteration 2045, loss = 0.00365974\n",
      "Iteration 2046, loss = 0.00365550\n",
      "Iteration 2047, loss = 0.00365127\n",
      "Iteration 2048, loss = 0.00364704\n",
      "Iteration 2049, loss = 0.00364282\n",
      "Iteration 2050, loss = 0.00363861\n",
      "Iteration 2051, loss = 0.00363440\n",
      "Iteration 2052, loss = 0.00363020\n",
      "Iteration 2053, loss = 0.00362601\n",
      "Iteration 2054, loss = 0.00362183\n",
      "Iteration 2055, loss = 0.00361765\n",
      "Iteration 2056, loss = 0.00361348\n",
      "Iteration 2057, loss = 0.00360931\n",
      "Iteration 2058, loss = 0.00360515\n",
      "Iteration 2059, loss = 0.00360100\n",
      "Iteration 2060, loss = 0.00359686\n",
      "Iteration 2061, loss = 0.00359272\n",
      "Iteration 2062, loss = 0.00358859\n",
      "Iteration 2063, loss = 0.00358446\n",
      "Iteration 2064, loss = 0.00358035\n",
      "Iteration 2065, loss = 0.00357624\n",
      "Iteration 2066, loss = 0.00357213\n",
      "Iteration 2067, loss = 0.00356803\n",
      "Iteration 2068, loss = 0.00356394\n",
      "Iteration 2069, loss = 0.00355986\n",
      "Iteration 2070, loss = 0.00355578\n",
      "Iteration 2071, loss = 0.00355171\n",
      "Iteration 2072, loss = 0.00354764\n",
      "Iteration 2073, loss = 0.00354358\n",
      "Iteration 2074, loss = 0.00353953\n",
      "Iteration 2075, loss = 0.00353549\n",
      "Iteration 2076, loss = 0.00353145\n",
      "Iteration 2077, loss = 0.00352742\n",
      "Iteration 2078, loss = 0.00352339\n",
      "Iteration 2079, loss = 0.00351937\n",
      "Iteration 2080, loss = 0.00351536\n",
      "Iteration 2081, loss = 0.00351135\n",
      "Iteration 2082, loss = 0.00350735\n",
      "Iteration 2083, loss = 0.00350336\n",
      "Iteration 2084, loss = 0.00349937\n",
      "Iteration 2085, loss = 0.00349539\n",
      "Iteration 2086, loss = 0.00349142\n",
      "Iteration 2087, loss = 0.00348745\n",
      "Iteration 2088, loss = 0.00348349\n",
      "Iteration 2089, loss = 0.00347954\n",
      "Iteration 2090, loss = 0.00347559\n",
      "Iteration 2091, loss = 0.00347165\n",
      "Iteration 2092, loss = 0.00346771\n",
      "Iteration 2093, loss = 0.00346378\n",
      "Iteration 2094, loss = 0.00345986\n",
      "Iteration 2095, loss = 0.00345594\n",
      "Iteration 2096, loss = 0.00345203\n",
      "Iteration 2097, loss = 0.00344812\n",
      "Iteration 2098, loss = 0.00344423\n",
      "Iteration 2099, loss = 0.00344033\n",
      "Iteration 2100, loss = 0.00343645\n",
      "Iteration 2101, loss = 0.00343257\n",
      "Iteration 2102, loss = 0.00342870\n",
      "Iteration 2103, loss = 0.00342483\n",
      "Iteration 2104, loss = 0.00342097\n",
      "Iteration 2105, loss = 0.00341711\n",
      "Iteration 2106, loss = 0.00341326\n",
      "Iteration 2107, loss = 0.00340942\n",
      "Iteration 2108, loss = 0.00340558\n",
      "Iteration 2109, loss = 0.00340175\n",
      "Iteration 2110, loss = 0.00339793\n",
      "Iteration 2111, loss = 0.00339411\n",
      "Iteration 2112, loss = 0.00339030\n",
      "Iteration 2113, loss = 0.00338649\n",
      "Iteration 2114, loss = 0.00338269\n",
      "Iteration 2115, loss = 0.00337890\n",
      "Iteration 2116, loss = 0.00337511\n",
      "Iteration 2117, loss = 0.00337133\n",
      "Iteration 2118, loss = 0.00336755\n",
      "Iteration 2119, loss = 0.00336378\n",
      "Iteration 2120, loss = 0.00336002\n",
      "Iteration 2121, loss = 0.00335626\n",
      "Iteration 2122, loss = 0.00335251\n",
      "Iteration 2123, loss = 0.00334876\n",
      "Iteration 2124, loss = 0.00334502\n",
      "Iteration 2125, loss = 0.00334129\n",
      "Iteration 2126, loss = 0.00333756\n",
      "Iteration 2127, loss = 0.00333383\n",
      "Iteration 2128, loss = 0.00333012\n",
      "Iteration 2129, loss = 0.00332641\n",
      "Iteration 2130, loss = 0.00332270\n",
      "Iteration 2131, loss = 0.00331900\n",
      "Iteration 2132, loss = 0.00331531\n",
      "Iteration 2133, loss = 0.00331162\n",
      "Iteration 2134, loss = 0.00330794\n",
      "Iteration 2135, loss = 0.00330426\n",
      "Iteration 2136, loss = 0.00330059\n",
      "Iteration 2137, loss = 0.00329693\n",
      "Iteration 2138, loss = 0.00329327\n",
      "Iteration 2139, loss = 0.00328961\n",
      "Iteration 2140, loss = 0.00328597\n",
      "Iteration 2141, loss = 0.00328233\n",
      "Iteration 2142, loss = 0.00327869\n",
      "Iteration 2143, loss = 0.00327506\n",
      "Iteration 2144, loss = 0.00327143\n",
      "Iteration 2145, loss = 0.00326781\n",
      "Iteration 2146, loss = 0.00326420\n",
      "Iteration 2147, loss = 0.00326059\n",
      "Iteration 2148, loss = 0.00325699\n",
      "Iteration 2149, loss = 0.00325340\n",
      "Iteration 2150, loss = 0.00324980\n",
      "Iteration 2151, loss = 0.00324622\n",
      "Iteration 2152, loss = 0.00324264\n",
      "Iteration 2153, loss = 0.00323907\n",
      "Iteration 2154, loss = 0.00323550\n",
      "Iteration 2155, loss = 0.00323193\n",
      "Iteration 2156, loss = 0.00322838\n",
      "Iteration 2157, loss = 0.00322482\n",
      "Iteration 2158, loss = 0.00322128\n",
      "Iteration 2159, loss = 0.00321774\n",
      "Iteration 2160, loss = 0.00321420\n",
      "Iteration 2161, loss = 0.00321067\n",
      "Iteration 2162, loss = 0.00320715\n",
      "Iteration 2163, loss = 0.00320363\n",
      "Iteration 2164, loss = 0.00320012\n",
      "Iteration 2165, loss = 0.00319661\n",
      "Iteration 2166, loss = 0.00319311\n",
      "Iteration 2167, loss = 0.00318961\n",
      "Iteration 2168, loss = 0.00318612\n",
      "Iteration 2169, loss = 0.00318263\n",
      "Iteration 2170, loss = 0.00317915\n",
      "Iteration 2171, loss = 0.00317567\n",
      "Iteration 2172, loss = 0.00317220\n",
      "Iteration 2173, loss = 0.00316874\n",
      "Iteration 2174, loss = 0.00316528\n",
      "Iteration 2175, loss = 0.00316182\n",
      "Iteration 2176, loss = 0.00315838\n",
      "Iteration 2177, loss = 0.00315493\n",
      "Iteration 2178, loss = 0.00315149\n",
      "Iteration 2179, loss = 0.00314806\n",
      "Iteration 2180, loss = 0.00314463\n",
      "Iteration 2181, loss = 0.00314121\n",
      "Iteration 2182, loss = 0.00313779\n",
      "Iteration 2183, loss = 0.00313438\n",
      "Iteration 2184, loss = 0.00313098\n",
      "Iteration 2185, loss = 0.00312757\n",
      "Iteration 2186, loss = 0.00312418\n",
      "Iteration 2187, loss = 0.00312079\n",
      "Iteration 2188, loss = 0.00311740\n",
      "Iteration 2189, loss = 0.00311402\n",
      "Iteration 2190, loss = 0.00311065\n",
      "Iteration 2191, loss = 0.00310728\n",
      "Iteration 2192, loss = 0.00310391\n",
      "Iteration 2193, loss = 0.00310055\n",
      "Iteration 2194, loss = 0.00309720\n",
      "Iteration 2195, loss = 0.00309385\n",
      "Iteration 2196, loss = 0.00309050\n",
      "Iteration 2197, loss = 0.00308716\n",
      "Iteration 2198, loss = 0.00308383\n",
      "Iteration 2199, loss = 0.00308050\n",
      "Iteration 2200, loss = 0.00307718\n",
      "Iteration 2201, loss = 0.00307386\n",
      "Iteration 2202, loss = 0.00307054\n",
      "Iteration 2203, loss = 0.00306723\n",
      "Iteration 2204, loss = 0.00306393\n",
      "Iteration 2205, loss = 0.00306063\n",
      "Iteration 2206, loss = 0.00305734\n",
      "Iteration 2207, loss = 0.00305405\n",
      "Iteration 2208, loss = 0.00305076\n",
      "Iteration 2209, loss = 0.00304749\n",
      "Iteration 2210, loss = 0.00304421\n",
      "Iteration 2211, loss = 0.00304094\n",
      "Iteration 2212, loss = 0.00303768\n",
      "Iteration 2213, loss = 0.00303442\n",
      "Iteration 2214, loss = 0.00303117\n",
      "Iteration 2215, loss = 0.00302792\n",
      "Iteration 2216, loss = 0.00302467\n",
      "Iteration 2217, loss = 0.00302143\n",
      "Iteration 2218, loss = 0.00301820\n",
      "Iteration 2219, loss = 0.00301497\n",
      "Iteration 2220, loss = 0.00301175\n",
      "Iteration 2221, loss = 0.00300853\n",
      "Iteration 2222, loss = 0.00300531\n",
      "Iteration 2223, loss = 0.00300210\n",
      "Iteration 2224, loss = 0.00299890\n",
      "Iteration 2225, loss = 0.00299570\n",
      "Iteration 2226, loss = 0.00299250\n",
      "Iteration 2227, loss = 0.00298931\n",
      "Iteration 2228, loss = 0.00298612\n",
      "Iteration 2229, loss = 0.00298294\n",
      "Iteration 2230, loss = 0.00297977\n",
      "Iteration 2231, loss = 0.00297659\n",
      "Iteration 2232, loss = 0.00297343\n",
      "Iteration 2233, loss = 0.00297027\n",
      "Iteration 2234, loss = 0.00296711\n",
      "Iteration 2235, loss = 0.00296396\n",
      "Iteration 2236, loss = 0.00296081\n",
      "Iteration 2237, loss = 0.00295767\n",
      "Iteration 2238, loss = 0.00295453\n",
      "Iteration 2239, loss = 0.00295139\n",
      "Iteration 2240, loss = 0.00294827\n",
      "Iteration 2241, loss = 0.00294514\n",
      "Iteration 2242, loss = 0.00294202\n",
      "Iteration 2243, loss = 0.00293891\n",
      "Iteration 2244, loss = 0.00293580\n",
      "Iteration 2245, loss = 0.00293269\n",
      "Iteration 2246, loss = 0.00292959\n",
      "Iteration 2247, loss = 0.00292649\n",
      "Iteration 2248, loss = 0.00292340\n",
      "Iteration 2249, loss = 0.00292032\n",
      "Iteration 2250, loss = 0.00291723\n",
      "Iteration 2251, loss = 0.00291416\n",
      "Iteration 2252, loss = 0.00291108\n",
      "Iteration 2253, loss = 0.00290801\n",
      "Iteration 2254, loss = 0.00290495\n",
      "Iteration 2255, loss = 0.00290189\n",
      "Iteration 2256, loss = 0.00289884\n",
      "Iteration 2257, loss = 0.00289579\n",
      "Iteration 2258, loss = 0.00289274\n",
      "Iteration 2259, loss = 0.00288970\n",
      "Iteration 2260, loss = 0.00288666\n",
      "Iteration 2261, loss = 0.00288363\n",
      "Iteration 2262, loss = 0.00288060\n",
      "Iteration 2263, loss = 0.00287758\n",
      "Iteration 2264, loss = 0.00287456\n",
      "Iteration 2265, loss = 0.00287155\n",
      "Iteration 2266, loss = 0.00286854\n",
      "Iteration 2267, loss = 0.00286553\n",
      "Iteration 2268, loss = 0.00286253\n",
      "Iteration 2269, loss = 0.00285953\n",
      "Iteration 2270, loss = 0.00285654\n",
      "Iteration 2271, loss = 0.00285355\n",
      "Iteration 2272, loss = 0.00285057\n",
      "Iteration 2273, loss = 0.00284759\n",
      "Iteration 2274, loss = 0.00284462\n",
      "Iteration 2275, loss = 0.00284165\n",
      "Iteration 2276, loss = 0.00283868\n",
      "Iteration 2277, loss = 0.00283572\n",
      "Iteration 2278, loss = 0.00283276\n",
      "Iteration 2279, loss = 0.00282981\n",
      "Iteration 2280, loss = 0.00282686\n",
      "Iteration 2281, loss = 0.00282392\n",
      "Iteration 2282, loss = 0.00282098\n",
      "Iteration 2283, loss = 0.00281805\n",
      "Iteration 2284, loss = 0.00281511\n",
      "Iteration 2285, loss = 0.00281219\n",
      "Iteration 2286, loss = 0.00280927\n",
      "Iteration 2287, loss = 0.00280635\n",
      "Iteration 2288, loss = 0.00280343\n",
      "Iteration 2289, loss = 0.00280052\n",
      "Iteration 2290, loss = 0.00279762\n",
      "Iteration 2291, loss = 0.00279472\n",
      "Iteration 2292, loss = 0.00279182\n",
      "Iteration 2293, loss = 0.00278893\n",
      "Iteration 2294, loss = 0.00278604\n",
      "Iteration 2295, loss = 0.00278316\n",
      "Iteration 2296, loss = 0.00278028\n",
      "Iteration 2297, loss = 0.00277740\n",
      "Iteration 2298, loss = 0.00277453\n",
      "Iteration 2299, loss = 0.00277167\n",
      "Iteration 2300, loss = 0.00276880\n",
      "Iteration 2301, loss = 0.00276594\n",
      "Iteration 2302, loss = 0.00276309\n",
      "Iteration 2303, loss = 0.00276024\n",
      "Iteration 2304, loss = 0.00275739\n",
      "Iteration 2305, loss = 0.00275455\n",
      "Iteration 2306, loss = 0.00275171\n",
      "Iteration 2307, loss = 0.00274888\n",
      "Iteration 2308, loss = 0.00274605\n",
      "Iteration 2309, loss = 0.00274322\n",
      "Iteration 2310, loss = 0.00274040\n",
      "Iteration 2311, loss = 0.00273759\n",
      "Iteration 2312, loss = 0.00273477\n",
      "Iteration 2313, loss = 0.00273196\n",
      "Iteration 2314, loss = 0.00272916\n",
      "Iteration 2315, loss = 0.00272636\n",
      "Iteration 2316, loss = 0.00272356\n",
      "Iteration 2317, loss = 0.00272077\n",
      "Iteration 2318, loss = 0.00271798\n",
      "Iteration 2319, loss = 0.00271520\n",
      "Iteration 2320, loss = 0.00271242\n",
      "Iteration 2321, loss = 0.00270964\n",
      "Iteration 2322, loss = 0.00270687\n",
      "Iteration 2323, loss = 0.00270410\n",
      "Iteration 2324, loss = 0.00270133\n",
      "Iteration 2325, loss = 0.00269857\n",
      "Iteration 2326, loss = 0.00269582\n",
      "Iteration 2327, loss = 0.00269306\n",
      "Iteration 2328, loss = 0.00269031\n",
      "Iteration 2329, loss = 0.00268757\n",
      "Iteration 2330, loss = 0.00268483\n",
      "Iteration 2331, loss = 0.00268209\n",
      "Iteration 2332, loss = 0.00267936\n",
      "Iteration 2333, loss = 0.00267663\n",
      "Iteration 2334, loss = 0.00267391\n",
      "Iteration 2335, loss = 0.00267119\n",
      "Iteration 2336, loss = 0.00266847\n",
      "Iteration 2337, loss = 0.00266576\n",
      "Iteration 2338, loss = 0.00266305\n",
      "Iteration 2339, loss = 0.00266034\n",
      "Iteration 2340, loss = 0.00265764\n",
      "Iteration 2341, loss = 0.00265494\n",
      "Iteration 2342, loss = 0.00265225\n",
      "Iteration 2343, loss = 0.00264956\n",
      "Iteration 2344, loss = 0.00264687\n",
      "Iteration 2345, loss = 0.00264419\n",
      "Iteration 2346, loss = 0.00264151\n",
      "Iteration 2347, loss = 0.00263884\n",
      "Iteration 2348, loss = 0.00263617\n",
      "Iteration 2349, loss = 0.00263350\n",
      "Iteration 2350, loss = 0.00263084\n",
      "Iteration 2351, loss = 0.00262818\n",
      "Iteration 2352, loss = 0.00262552\n",
      "Iteration 2353, loss = 0.00262287\n",
      "Iteration 2354, loss = 0.00262022\n",
      "Iteration 2355, loss = 0.00261758\n",
      "Iteration 2356, loss = 0.00261494\n",
      "Iteration 2357, loss = 0.00261230\n",
      "Iteration 2358, loss = 0.00260967\n",
      "Iteration 2359, loss = 0.00260704\n",
      "Iteration 2360, loss = 0.00260442\n",
      "Iteration 2361, loss = 0.00260180\n",
      "Iteration 2362, loss = 0.00259918\n",
      "Iteration 2363, loss = 0.00259656\n",
      "Iteration 2364, loss = 0.00259395\n",
      "Iteration 2365, loss = 0.00259135\n",
      "Iteration 2366, loss = 0.00258874\n",
      "Iteration 2367, loss = 0.00258614\n",
      "Iteration 2368, loss = 0.00258355\n",
      "Iteration 2369, loss = 0.00258096\n",
      "Iteration 2370, loss = 0.00257837\n",
      "Iteration 2371, loss = 0.00257579\n",
      "Iteration 2372, loss = 0.00257320\n",
      "Iteration 2373, loss = 0.00257063\n",
      "Iteration 2374, loss = 0.00256805\n",
      "Iteration 2375, loss = 0.00256548\n",
      "Iteration 2376, loss = 0.00256292\n",
      "Iteration 2377, loss = 0.00256036\n",
      "Iteration 2378, loss = 0.00255780\n",
      "Iteration 2379, loss = 0.00255524\n",
      "Iteration 2380, loss = 0.00255269\n",
      "Iteration 2381, loss = 0.00255014\n",
      "Iteration 2382, loss = 0.00254760\n",
      "Iteration 2383, loss = 0.00254506\n",
      "Iteration 2384, loss = 0.00254252\n",
      "Iteration 2385, loss = 0.00253999\n",
      "Iteration 2386, loss = 0.00253746\n",
      "Iteration 2387, loss = 0.00253493\n",
      "Iteration 2388, loss = 0.00253241\n",
      "Iteration 2389, loss = 0.00252989\n",
      "Iteration 2390, loss = 0.00252737\n",
      "Iteration 2391, loss = 0.00252486\n",
      "Iteration 2392, loss = 0.00252235\n",
      "Iteration 2393, loss = 0.00251985\n",
      "Iteration 2394, loss = 0.00251734\n",
      "Iteration 2395, loss = 0.00251485\n",
      "Iteration 2396, loss = 0.00251235\n",
      "Iteration 2397, loss = 0.00250986\n",
      "Iteration 2398, loss = 0.00250737\n",
      "Iteration 2399, loss = 0.00250489\n",
      "Iteration 2400, loss = 0.00250241\n",
      "Iteration 2401, loss = 0.00249993\n",
      "Iteration 2402, loss = 0.00249746\n",
      "Iteration 2403, loss = 0.00249499\n",
      "Iteration 2404, loss = 0.00249252\n",
      "Iteration 2405, loss = 0.00249006\n",
      "Iteration 2406, loss = 0.00248760\n",
      "Iteration 2407, loss = 0.00248514\n",
      "Iteration 2408, loss = 0.00248269\n",
      "Iteration 2409, loss = 0.00248024\n",
      "Iteration 2410, loss = 0.00247779\n",
      "Iteration 2411, loss = 0.00247535\n",
      "Iteration 2412, loss = 0.00247291\n",
      "Iteration 2413, loss = 0.00247047\n",
      "Iteration 2414, loss = 0.00246804\n",
      "Iteration 2415, loss = 0.00246561\n",
      "Iteration 2416, loss = 0.00246318\n",
      "Iteration 2417, loss = 0.00246076\n",
      "Iteration 2418, loss = 0.00245834\n",
      "Iteration 2419, loss = 0.00245593\n",
      "Iteration 2420, loss = 0.00245352\n",
      "Iteration 2421, loss = 0.00245111\n",
      "Iteration 2422, loss = 0.00244870\n",
      "Iteration 2423, loss = 0.00244630\n",
      "Iteration 2424, loss = 0.00244390\n",
      "Iteration 2425, loss = 0.00244150\n",
      "Iteration 2426, loss = 0.00243911\n",
      "Iteration 2427, loss = 0.00243672\n",
      "Iteration 2428, loss = 0.00243434\n",
      "Iteration 2429, loss = 0.00243195\n",
      "Iteration 2430, loss = 0.00242958\n",
      "Iteration 2431, loss = 0.00242720\n",
      "Iteration 2432, loss = 0.00242483\n",
      "Iteration 2433, loss = 0.00242246\n",
      "Iteration 2434, loss = 0.00242009\n",
      "Iteration 2435, loss = 0.00241773\n",
      "Iteration 2436, loss = 0.00241537\n",
      "Iteration 2437, loss = 0.00241301\n",
      "Iteration 2438, loss = 0.00241066\n",
      "Iteration 2439, loss = 0.00240831\n",
      "Iteration 2440, loss = 0.00240597\n",
      "Iteration 2441, loss = 0.00240362\n",
      "Iteration 2442, loss = 0.00240128\n",
      "Iteration 2443, loss = 0.00239895\n",
      "Iteration 2444, loss = 0.00239661\n",
      "Iteration 2445, loss = 0.00239428\n",
      "Iteration 2446, loss = 0.00239196\n",
      "Iteration 2447, loss = 0.00238963\n",
      "Iteration 2448, loss = 0.00238731\n",
      "Iteration 2449, loss = 0.00238499\n",
      "Iteration 2450, loss = 0.00238268\n",
      "Iteration 2451, loss = 0.00238037\n",
      "Iteration 2452, loss = 0.00237806\n",
      "Iteration 2453, loss = 0.00237576\n",
      "Iteration 2454, loss = 0.00237345\n",
      "Iteration 2455, loss = 0.00237116\n",
      "Iteration 2456, loss = 0.00236886\n",
      "Iteration 2457, loss = 0.00236657\n",
      "Iteration 2458, loss = 0.00236428\n",
      "Iteration 2459, loss = 0.00236199\n",
      "Iteration 2460, loss = 0.00235971\n",
      "Iteration 2461, loss = 0.00235743\n",
      "Iteration 2462, loss = 0.00235516\n",
      "Iteration 2463, loss = 0.00235288\n",
      "Iteration 2464, loss = 0.00235061\n",
      "Iteration 2465, loss = 0.00234835\n",
      "Iteration 2466, loss = 0.00234608\n",
      "Iteration 2467, loss = 0.00234382\n",
      "Iteration 2468, loss = 0.00234156\n",
      "Iteration 2469, loss = 0.00233931\n",
      "Iteration 2470, loss = 0.00233706\n",
      "Iteration 2471, loss = 0.00233481\n",
      "Iteration 2472, loss = 0.00233256\n",
      "Iteration 2473, loss = 0.00233032\n",
      "Iteration 2474, loss = 0.00232808\n",
      "Iteration 2475, loss = 0.00232584\n",
      "Iteration 2476, loss = 0.00232361\n",
      "Iteration 2477, loss = 0.00232138\n",
      "Iteration 2478, loss = 0.00231915\n",
      "Iteration 2479, loss = 0.00231693\n",
      "Iteration 2480, loss = 0.00231471\n",
      "Iteration 2481, loss = 0.00231249\n",
      "Iteration 2482, loss = 0.00231028\n",
      "Iteration 2483, loss = 0.00230806\n",
      "Iteration 2484, loss = 0.00230585\n",
      "Iteration 2485, loss = 0.00230365\n",
      "Iteration 2486, loss = 0.00230144\n",
      "Iteration 2487, loss = 0.00229924\n",
      "Iteration 2488, loss = 0.00229705\n",
      "Iteration 2489, loss = 0.00229485\n",
      "Iteration 2490, loss = 0.00229266\n",
      "Iteration 2491, loss = 0.00229047\n",
      "Iteration 2492, loss = 0.00228829\n",
      "Iteration 2493, loss = 0.00228611\n",
      "Iteration 2494, loss = 0.00228393\n",
      "Iteration 2495, loss = 0.00228175\n",
      "Iteration 2496, loss = 0.00227958\n",
      "Iteration 2497, loss = 0.00227741\n",
      "Iteration 2498, loss = 0.00227524\n",
      "Iteration 2499, loss = 0.00227307\n",
      "Iteration 2500, loss = 0.00227091\n",
      "Iteration 2501, loss = 0.00226875\n",
      "Iteration 2502, loss = 0.00226660\n",
      "Iteration 2503, loss = 0.00226444\n",
      "Iteration 2504, loss = 0.00226229\n",
      "Iteration 2505, loss = 0.00226015\n",
      "Iteration 2506, loss = 0.00225800\n",
      "Iteration 2507, loss = 0.00225586\n",
      "Iteration 2508, loss = 0.00225372\n",
      "Iteration 2509, loss = 0.00225159\n",
      "Iteration 2510, loss = 0.00224945\n",
      "Iteration 2511, loss = 0.00224732\n",
      "Iteration 2512, loss = 0.00224520\n",
      "Iteration 2513, loss = 0.00224307\n",
      "Iteration 2514, loss = 0.00224095\n",
      "Iteration 2515, loss = 0.00223883\n",
      "Iteration 2516, loss = 0.00223672\n",
      "Iteration 2517, loss = 0.00223460\n",
      "Iteration 2518, loss = 0.00223249\n",
      "Iteration 2519, loss = 0.00223039\n",
      "Iteration 2520, loss = 0.00222828\n",
      "Iteration 2521, loss = 0.00222618\n",
      "Iteration 2522, loss = 0.00222408\n",
      "Iteration 2523, loss = 0.00222199\n",
      "Iteration 2524, loss = 0.00221989\n",
      "Iteration 2525, loss = 0.00221780\n",
      "Iteration 2526, loss = 0.00221571\n",
      "Iteration 2527, loss = 0.00221363\n",
      "Iteration 2528, loss = 0.00221155\n",
      "Iteration 2529, loss = 0.00220947\n",
      "Iteration 2530, loss = 0.00220739\n",
      "Iteration 2531, loss = 0.00220532\n",
      "Iteration 2532, loss = 0.00220325\n",
      "Iteration 2533, loss = 0.00220118\n",
      "Iteration 2534, loss = 0.00219911\n",
      "Iteration 2535, loss = 0.00219705\n",
      "Iteration 2536, loss = 0.00219499\n",
      "Iteration 2537, loss = 0.00219293\n",
      "Iteration 2538, loss = 0.00219088\n",
      "Iteration 2539, loss = 0.00218883\n",
      "Iteration 2540, loss = 0.00218678\n",
      "Iteration 2541, loss = 0.00218473\n",
      "Iteration 2542, loss = 0.00218269\n",
      "Iteration 2543, loss = 0.00218065\n",
      "Iteration 2544, loss = 0.00217861\n",
      "Iteration 2545, loss = 0.00217657\n",
      "Iteration 2546, loss = 0.00217454\n",
      "Iteration 2547, loss = 0.00217251\n",
      "Iteration 2548, loss = 0.00217048\n",
      "Iteration 2549, loss = 0.00216846\n",
      "Iteration 2550, loss = 0.00216644\n",
      "Iteration 2551, loss = 0.00216442\n",
      "Iteration 2552, loss = 0.00216240\n",
      "Iteration 2553, loss = 0.00216039\n",
      "Iteration 2554, loss = 0.00215838\n",
      "Iteration 2555, loss = 0.00215637\n",
      "Iteration 2556, loss = 0.00215436\n",
      "Iteration 2557, loss = 0.00215236\n",
      "Iteration 2558, loss = 0.00215036\n",
      "Iteration 2559, loss = 0.00214836\n",
      "Iteration 2560, loss = 0.00214636\n",
      "Iteration 2561, loss = 0.00214437\n",
      "Iteration 2562, loss = 0.00214238\n",
      "Iteration 2563, loss = 0.00214039\n",
      "Iteration 2564, loss = 0.00213841\n",
      "Iteration 2565, loss = 0.00213642\n",
      "Iteration 2566, loss = 0.00213445\n",
      "Iteration 2567, loss = 0.00213247\n",
      "Iteration 2568, loss = 0.00213049\n",
      "Iteration 2569, loss = 0.00212852\n",
      "Iteration 2570, loss = 0.00212655\n",
      "Iteration 2571, loss = 0.00212459\n",
      "Iteration 2572, loss = 0.00212262\n",
      "Iteration 2573, loss = 0.00212066\n",
      "Iteration 2574, loss = 0.00211870\n",
      "Iteration 2575, loss = 0.00211674\n",
      "Iteration 2576, loss = 0.00211479\n",
      "Iteration 2577, loss = 0.00211284\n",
      "Iteration 2578, loss = 0.00211089\n",
      "Iteration 2579, loss = 0.00210894\n",
      "Iteration 2580, loss = 0.00210700\n",
      "Iteration 2581, loss = 0.00210506\n",
      "Iteration 2582, loss = 0.00210312\n",
      "Iteration 2583, loss = 0.00210118\n",
      "Iteration 2584, loss = 0.00209925\n",
      "Iteration 2585, loss = 0.00209732\n",
      "Iteration 2586, loss = 0.00209539\n",
      "Iteration 2587, loss = 0.00209346\n",
      "Iteration 2588, loss = 0.00209154\n",
      "Iteration 2589, loss = 0.00208962\n",
      "Iteration 2590, loss = 0.00208770\n",
      "Iteration 2591, loss = 0.00208579\n",
      "Iteration 2592, loss = 0.00208387\n",
      "Iteration 2593, loss = 0.00208196\n",
      "Iteration 2594, loss = 0.00208005\n",
      "Iteration 2595, loss = 0.00207815\n",
      "Iteration 2596, loss = 0.00207624\n",
      "Iteration 2597, loss = 0.00207434\n",
      "Iteration 2598, loss = 0.00207244\n",
      "Iteration 2599, loss = 0.00207055\n",
      "Iteration 2600, loss = 0.00206865\n",
      "Iteration 2601, loss = 0.00206676\n",
      "Iteration 2602, loss = 0.00206487\n",
      "Iteration 2603, loss = 0.00206299\n",
      "Iteration 2604, loss = 0.00206110\n",
      "Iteration 2605, loss = 0.00205922\n",
      "Iteration 2606, loss = 0.00205734\n",
      "Iteration 2607, loss = 0.00205547\n",
      "Iteration 2608, loss = 0.00205359\n",
      "Iteration 2609, loss = 0.00205172\n",
      "Iteration 2610, loss = 0.00204985\n",
      "Iteration 2611, loss = 0.00204799\n",
      "Iteration 2612, loss = 0.00204612\n",
      "Iteration 2613, loss = 0.00204426\n",
      "Iteration 2614, loss = 0.00204240\n",
      "Iteration 2615, loss = 0.00204054\n",
      "Iteration 2616, loss = 0.00203869\n",
      "Iteration 2617, loss = 0.00203684\n",
      "Iteration 2618, loss = 0.00203499\n",
      "Iteration 2619, loss = 0.00203314\n",
      "Iteration 2620, loss = 0.00203129\n",
      "Iteration 2621, loss = 0.00202945\n",
      "Iteration 2622, loss = 0.00202761\n",
      "Iteration 2623, loss = 0.00202577\n",
      "Iteration 2624, loss = 0.00202394\n",
      "Iteration 2625, loss = 0.00202210\n",
      "Iteration 2626, loss = 0.00202027\n",
      "Iteration 2627, loss = 0.00201844\n",
      "Iteration 2628, loss = 0.00201662\n",
      "Iteration 2629, loss = 0.00201479\n",
      "Iteration 2630, loss = 0.00201297\n",
      "Iteration 2631, loss = 0.00201115\n",
      "Iteration 2632, loss = 0.00200933\n",
      "Iteration 2633, loss = 0.00200752\n",
      "Iteration 2634, loss = 0.00200571\n",
      "Iteration 2635, loss = 0.00200390\n",
      "Iteration 2636, loss = 0.00200209\n",
      "Iteration 2637, loss = 0.00200028\n",
      "Iteration 2638, loss = 0.00199848\n",
      "Iteration 2639, loss = 0.00199668\n",
      "Iteration 2640, loss = 0.00199488\n",
      "Iteration 2641, loss = 0.00199309\n",
      "Iteration 2642, loss = 0.00199129\n",
      "Iteration 2643, loss = 0.00198950\n",
      "Iteration 2644, loss = 0.00198771\n",
      "Iteration 2645, loss = 0.00198592\n",
      "Iteration 2646, loss = 0.00198414\n",
      "Iteration 2647, loss = 0.00198236\n",
      "Iteration 2648, loss = 0.00198058\n",
      "Iteration 2649, loss = 0.00197880\n",
      "Iteration 2650, loss = 0.00197702\n",
      "Iteration 2651, loss = 0.00197525\n",
      "Iteration 2652, loss = 0.00197348\n",
      "Iteration 2653, loss = 0.00197171\n",
      "Iteration 2654, loss = 0.00196994\n",
      "Iteration 2655, loss = 0.00196818\n",
      "Iteration 2656, loss = 0.00196642\n",
      "Iteration 2657, loss = 0.00196466\n",
      "Iteration 2658, loss = 0.00196290\n",
      "Iteration 2659, loss = 0.00196115\n",
      "Iteration 2660, loss = 0.00195939\n",
      "Iteration 2661, loss = 0.00195764\n",
      "Iteration 2662, loss = 0.00195589\n",
      "Iteration 2663, loss = 0.00195415\n",
      "Iteration 2664, loss = 0.00195240\n",
      "Iteration 2665, loss = 0.00195066\n",
      "Iteration 2666, loss = 0.00194892\n",
      "Iteration 2667, loss = 0.00194718\n",
      "Iteration 2668, loss = 0.00194545\n",
      "Iteration 2669, loss = 0.00194372\n",
      "Iteration 2670, loss = 0.00194198\n",
      "Iteration 2671, loss = 0.00194026\n",
      "Iteration 2672, loss = 0.00193853\n",
      "Iteration 2673, loss = 0.00193680\n",
      "Iteration 2674, loss = 0.00193508\n",
      "Iteration 2675, loss = 0.00193336\n",
      "Iteration 2676, loss = 0.00193164\n",
      "Iteration 2677, loss = 0.00192993\n",
      "Iteration 2678, loss = 0.00192822\n",
      "Iteration 2679, loss = 0.00192650\n",
      "Iteration 2680, loss = 0.00192479\n",
      "Iteration 2681, loss = 0.00192309\n",
      "Iteration 2682, loss = 0.00192138\n",
      "Iteration 2683, loss = 0.00191968\n",
      "Iteration 2684, loss = 0.00191798\n",
      "Iteration 2685, loss = 0.00191628\n",
      "Iteration 2686, loss = 0.00191459\n",
      "Iteration 2687, loss = 0.00191289\n",
      "Iteration 2688, loss = 0.00191120\n",
      "Iteration 2689, loss = 0.00190951\n",
      "Iteration 2690, loss = 0.00190782\n",
      "Iteration 2691, loss = 0.00190614\n",
      "Iteration 2692, loss = 0.00190445\n",
      "Iteration 2693, loss = 0.00190277\n",
      "Iteration 2694, loss = 0.00190109\n",
      "Iteration 2695, loss = 0.00189941\n",
      "Iteration 2696, loss = 0.00189774\n",
      "Iteration 2697, loss = 0.00189607\n",
      "Iteration 2698, loss = 0.00189440\n",
      "Iteration 2699, loss = 0.00189273\n",
      "Iteration 2700, loss = 0.00189106\n",
      "Iteration 2701, loss = 0.00188940\n",
      "Iteration 2702, loss = 0.00188773\n",
      "Iteration 2703, loss = 0.00188607\n",
      "Iteration 2704, loss = 0.00188441\n",
      "Iteration 2705, loss = 0.00188276\n",
      "Iteration 2706, loss = 0.00188110\n",
      "Iteration 2707, loss = 0.00187945\n",
      "Iteration 2708, loss = 0.00187780\n",
      "Iteration 2709, loss = 0.00187615\n",
      "Iteration 2710, loss = 0.00187451\n",
      "Iteration 2711, loss = 0.00187286\n",
      "Iteration 2712, loss = 0.00187122\n",
      "Iteration 2713, loss = 0.00186958\n",
      "Iteration 2714, loss = 0.00186794\n",
      "Iteration 2715, loss = 0.00186631\n",
      "Iteration 2716, loss = 0.00186467\n",
      "Iteration 2717, loss = 0.00186304\n",
      "Iteration 2718, loss = 0.00186141\n",
      "Iteration 2719, loss = 0.00185979\n",
      "Iteration 2720, loss = 0.00185816\n",
      "Iteration 2721, loss = 0.00185654\n",
      "Iteration 2722, loss = 0.00185492\n",
      "Iteration 2723, loss = 0.00185330\n",
      "Iteration 2724, loss = 0.00185168\n",
      "Iteration 2725, loss = 0.00185006\n",
      "Iteration 2726, loss = 0.00184845\n",
      "Iteration 2727, loss = 0.00184684\n",
      "Iteration 2728, loss = 0.00184523\n",
      "Iteration 2729, loss = 0.00184362\n",
      "Iteration 2730, loss = 0.00184202\n",
      "Iteration 2731, loss = 0.00184041\n",
      "Iteration 2732, loss = 0.00183881\n",
      "Iteration 2733, loss = 0.00183721\n",
      "Iteration 2734, loss = 0.00183561\n",
      "Iteration 2735, loss = 0.00183402\n",
      "Iteration 2736, loss = 0.00183242\n",
      "Iteration 2737, loss = 0.00183083\n",
      "Iteration 2738, loss = 0.00182924\n",
      "Iteration 2739, loss = 0.00182766\n",
      "Iteration 2740, loss = 0.00182607\n",
      "Iteration 2741, loss = 0.00182449\n",
      "Iteration 2742, loss = 0.00182290\n",
      "Iteration 2743, loss = 0.00182132\n",
      "Iteration 2744, loss = 0.00181975\n",
      "Iteration 2745, loss = 0.00181817\n",
      "Iteration 2746, loss = 0.00181660\n",
      "Iteration 2747, loss = 0.00181502\n",
      "Iteration 2748, loss = 0.00181345\n",
      "Iteration 2749, loss = 0.00181189\n",
      "Iteration 2750, loss = 0.00181032\n",
      "Iteration 2751, loss = 0.00180876\n",
      "Iteration 2752, loss = 0.00180719\n",
      "Iteration 2753, loss = 0.00180563\n",
      "Iteration 2754, loss = 0.00180407\n",
      "Iteration 2755, loss = 0.00180252\n",
      "Iteration 2756, loss = 0.00180096\n",
      "Iteration 2757, loss = 0.00179941\n",
      "Iteration 2758, loss = 0.00179786\n",
      "Iteration 2759, loss = 0.00179631\n",
      "Iteration 2760, loss = 0.00179476\n",
      "Iteration 2761, loss = 0.00179322\n",
      "Iteration 2762, loss = 0.00179167\n",
      "Iteration 2763, loss = 0.00179013\n",
      "Iteration 2764, loss = 0.00178859\n",
      "Iteration 2765, loss = 0.00178706\n",
      "Iteration 2766, loss = 0.00178552\n",
      "Iteration 2767, loss = 0.00178399\n",
      "Iteration 2768, loss = 0.00178245\n",
      "Iteration 2769, loss = 0.00178092\n",
      "Iteration 2770, loss = 0.00177940\n",
      "Iteration 2771, loss = 0.00177787\n",
      "Iteration 2772, loss = 0.00177635\n",
      "Iteration 2773, loss = 0.00177482\n",
      "Iteration 2774, loss = 0.00177330\n",
      "Iteration 2775, loss = 0.00177178\n",
      "Iteration 2776, loss = 0.00177027\n",
      "Iteration 2777, loss = 0.00176875\n",
      "Iteration 2778, loss = 0.00176724\n",
      "Iteration 2779, loss = 0.00176573\n",
      "Iteration 2780, loss = 0.00176422\n",
      "Iteration 2781, loss = 0.00176271\n",
      "Iteration 2782, loss = 0.00176120\n",
      "Iteration 2783, loss = 0.00175970\n",
      "Iteration 2784, loss = 0.00175820\n",
      "Iteration 2785, loss = 0.00175670\n",
      "Iteration 2786, loss = 0.00175520\n",
      "Iteration 2787, loss = 0.00175370\n",
      "Iteration 2788, loss = 0.00175221\n",
      "Iteration 2789, loss = 0.00175071\n",
      "Iteration 2790, loss = 0.00174922\n",
      "Iteration 2791, loss = 0.00174773\n",
      "Iteration 2792, loss = 0.00174624\n",
      "Iteration 2793, loss = 0.00174476\n",
      "Iteration 2794, loss = 0.00174327\n",
      "Iteration 2795, loss = 0.00174179\n",
      "Iteration 2796, loss = 0.00174031\n",
      "Iteration 2797, loss = 0.00173883\n",
      "Iteration 2798, loss = 0.00173735\n",
      "Iteration 2799, loss = 0.00173588\n",
      "Iteration 2800, loss = 0.00173441\n",
      "Iteration 2801, loss = 0.00173293\n",
      "Iteration 2802, loss = 0.00173146\n",
      "Iteration 2803, loss = 0.00173000\n",
      "Iteration 2804, loss = 0.00172853\n",
      "Iteration 2805, loss = 0.00172707\n",
      "Iteration 2806, loss = 0.00172560\n",
      "Iteration 2807, loss = 0.00172414\n",
      "Iteration 2808, loss = 0.00172268\n",
      "Iteration 2809, loss = 0.00172123\n",
      "Iteration 2810, loss = 0.00171977\n",
      "Iteration 2811, loss = 0.00171832\n",
      "Iteration 2812, loss = 0.00171686\n",
      "Iteration 2813, loss = 0.00171541\n",
      "Iteration 2814, loss = 0.00171396\n",
      "Iteration 2815, loss = 0.00171252\n",
      "Iteration 2816, loss = 0.00171107\n",
      "Iteration 2817, loss = 0.00170963\n",
      "Iteration 2818, loss = 0.00170819\n",
      "Iteration 2819, loss = 0.00170675\n",
      "Iteration 2820, loss = 0.00170531\n",
      "Iteration 2821, loss = 0.00170387\n",
      "Iteration 2822, loss = 0.00170244\n",
      "Iteration 2823, loss = 0.00170101\n",
      "Iteration 2824, loss = 0.00169957\n",
      "Iteration 2825, loss = 0.00169814\n",
      "Iteration 2826, loss = 0.00169672\n",
      "Iteration 2827, loss = 0.00169529\n",
      "Iteration 2828, loss = 0.00169387\n",
      "Iteration 2829, loss = 0.00169244\n",
      "Iteration 2830, loss = 0.00169102\n",
      "Iteration 2831, loss = 0.00168960\n",
      "Iteration 2832, loss = 0.00168819\n",
      "Iteration 2833, loss = 0.00168677\n",
      "Iteration 2834, loss = 0.00168535\n",
      "Iteration 2835, loss = 0.00168394\n",
      "Iteration 2836, loss = 0.00168253\n",
      "Iteration 2837, loss = 0.00168112\n",
      "Iteration 2838, loss = 0.00167971\n",
      "Iteration 2839, loss = 0.00167831\n",
      "Iteration 2840, loss = 0.00167691\n",
      "Iteration 2841, loss = 0.00167550\n",
      "Iteration 2842, loss = 0.00167410\n",
      "Iteration 2843, loss = 0.00167270\n",
      "Iteration 2844, loss = 0.00167131\n",
      "Iteration 2845, loss = 0.00166991\n",
      "Iteration 2846, loss = 0.00166852\n",
      "Iteration 2847, loss = 0.00166712\n",
      "Iteration 2848, loss = 0.00166573\n",
      "Iteration 2849, loss = 0.00166434\n",
      "Iteration 2850, loss = 0.00166296\n",
      "Iteration 2851, loss = 0.00166157\n",
      "Iteration 2852, loss = 0.00166019\n",
      "Iteration 2853, loss = 0.00165880\n",
      "Iteration 2854, loss = 0.00165742\n",
      "Iteration 2855, loss = 0.00165604\n",
      "Iteration 2856, loss = 0.00165467\n",
      "Iteration 2857, loss = 0.00165329\n",
      "Iteration 2858, loss = 0.00165192\n",
      "Iteration 2859, loss = 0.00165054\n",
      "Iteration 2860, loss = 0.00164917\n",
      "Iteration 2861, loss = 0.00164780\n",
      "Iteration 2862, loss = 0.00164643\n",
      "Iteration 2863, loss = 0.00164507\n",
      "Iteration 2864, loss = 0.00164370\n",
      "Iteration 2865, loss = 0.00164234\n",
      "Iteration 2866, loss = 0.00164098\n",
      "Iteration 2867, loss = 0.00163962\n",
      "Iteration 2868, loss = 0.00163826\n",
      "Iteration 2869, loss = 0.00163691\n",
      "Iteration 2870, loss = 0.00163555\n",
      "Iteration 2871, loss = 0.00163420\n",
      "Iteration 2872, loss = 0.00163285\n",
      "Iteration 2873, loss = 0.00163150\n",
      "Iteration 2874, loss = 0.00163015\n",
      "Iteration 2875, loss = 0.00162880\n",
      "Iteration 2876, loss = 0.00162746\n",
      "Iteration 2877, loss = 0.00162611\n",
      "Iteration 2878, loss = 0.00162477\n",
      "Iteration 2879, loss = 0.00162343\n",
      "Iteration 2880, loss = 0.00162209\n",
      "Iteration 2881, loss = 0.00162075\n",
      "Iteration 2882, loss = 0.00161942\n",
      "Iteration 2883, loss = 0.00161808\n",
      "Iteration 2884, loss = 0.00161675\n",
      "Iteration 2885, loss = 0.00161542\n",
      "Iteration 2886, loss = 0.00161409\n",
      "Iteration 2887, loss = 0.00161276\n",
      "Iteration 2888, loss = 0.00161143\n",
      "Iteration 2889, loss = 0.00161011\n",
      "Iteration 2890, loss = 0.00160879\n",
      "Iteration 2891, loss = 0.00160747\n",
      "Iteration 2892, loss = 0.00160615\n",
      "Iteration 2893, loss = 0.00160483\n",
      "Iteration 2894, loss = 0.00160351\n",
      "Iteration 2895, loss = 0.00160219\n",
      "Iteration 2896, loss = 0.00160088\n",
      "Iteration 2897, loss = 0.00159957\n",
      "Iteration 2898, loss = 0.00159826\n",
      "Iteration 2899, loss = 0.00159695\n",
      "Iteration 2900, loss = 0.00159564\n",
      "Iteration 2901, loss = 0.00159433\n",
      "Iteration 2902, loss = 0.00159303\n",
      "Iteration 2903, loss = 0.00159173\n",
      "Iteration 2904, loss = 0.00159042\n",
      "Iteration 2905, loss = 0.00158912\n",
      "Iteration 2906, loss = 0.00158782\n",
      "Iteration 2907, loss = 0.00158653\n",
      "Iteration 2908, loss = 0.00158523\n",
      "Iteration 2909, loss = 0.00158394\n",
      "Iteration 2910, loss = 0.00158265\n",
      "Iteration 2911, loss = 0.00158135\n",
      "Iteration 2912, loss = 0.00158007\n",
      "Iteration 2913, loss = 0.00157878\n",
      "Iteration 2914, loss = 0.00157749\n",
      "Iteration 2915, loss = 0.00157621\n",
      "Iteration 2916, loss = 0.00157492\n",
      "Iteration 2917, loss = 0.00157364\n",
      "Iteration 2918, loss = 0.00157236\n",
      "Iteration 2919, loss = 0.00157108\n",
      "Iteration 2920, loss = 0.00156980\n",
      "Iteration 2921, loss = 0.00156853\n",
      "Iteration 2922, loss = 0.00156725\n",
      "Iteration 2923, loss = 0.00156598\n",
      "Iteration 2924, loss = 0.00156471\n",
      "Iteration 2925, loss = 0.00156344\n",
      "Iteration 2926, loss = 0.00156217\n",
      "Iteration 2927, loss = 0.00156090\n",
      "Iteration 2928, loss = 0.00155964\n",
      "Iteration 2929, loss = 0.00155837\n",
      "Iteration 2930, loss = 0.00155711\n",
      "Iteration 2931, loss = 0.00155585\n",
      "Iteration 2932, loss = 0.00155459\n",
      "Iteration 2933, loss = 0.00155333\n",
      "Iteration 2934, loss = 0.00155207\n",
      "Iteration 2935, loss = 0.00155082\n",
      "Iteration 2936, loss = 0.00154956\n",
      "Iteration 2937, loss = 0.00154831\n",
      "Iteration 2938, loss = 0.00154706\n",
      "Iteration 2939, loss = 0.00154581\n",
      "Iteration 2940, loss = 0.00154456\n",
      "Iteration 2941, loss = 0.00154331\n",
      "Iteration 2942, loss = 0.00154207\n",
      "Iteration 2943, loss = 0.00154083\n",
      "Iteration 2944, loss = 0.00153958\n",
      "Iteration 2945, loss = 0.00153834\n",
      "Iteration 2946, loss = 0.00153710\n",
      "Iteration 2947, loss = 0.00153586\n",
      "Iteration 2948, loss = 0.00153463\n",
      "Iteration 2949, loss = 0.00153339\n",
      "Iteration 2950, loss = 0.00153216\n",
      "Iteration 2951, loss = 0.00153093\n",
      "Iteration 2952, loss = 0.00152969\n",
      "Iteration 2953, loss = 0.00152846\n",
      "Iteration 2954, loss = 0.00152724\n",
      "Iteration 2955, loss = 0.00152601\n",
      "Iteration 2956, loss = 0.00152478\n",
      "Iteration 2957, loss = 0.00152356\n",
      "Iteration 2958, loss = 0.00152234\n",
      "Iteration 2959, loss = 0.00152112\n",
      "Iteration 2960, loss = 0.00151990\n",
      "Iteration 2961, loss = 0.00151868\n",
      "Iteration 2962, loss = 0.00151746\n",
      "Iteration 2963, loss = 0.00151625\n",
      "Iteration 2964, loss = 0.00151503\n",
      "Iteration 2965, loss = 0.00151382\n",
      "Iteration 2966, loss = 0.00151261\n",
      "Iteration 2967, loss = 0.00151140\n",
      "Iteration 2968, loss = 0.00151019\n",
      "Iteration 2969, loss = 0.00150898\n",
      "Iteration 2970, loss = 0.00150778\n",
      "Iteration 2971, loss = 0.00150657\n",
      "Iteration 2972, loss = 0.00150537\n",
      "Iteration 2973, loss = 0.00150417\n",
      "Iteration 2974, loss = 0.00150297\n",
      "Iteration 2975, loss = 0.00150177\n",
      "Iteration 2976, loss = 0.00150057\n",
      "Iteration 2977, loss = 0.00149938\n",
      "Iteration 2978, loss = 0.00149818\n",
      "Iteration 2979, loss = 0.00149699\n",
      "Iteration 2980, loss = 0.00149579\n",
      "Iteration 2981, loss = 0.00149460\n",
      "Iteration 2982, loss = 0.00149341\n",
      "Iteration 2983, loss = 0.00149223\n",
      "Iteration 2984, loss = 0.00149104\n",
      "Iteration 2985, loss = 0.00148985\n",
      "Iteration 2986, loss = 0.00148867\n",
      "Iteration 2987, loss = 0.00148749\n",
      "Iteration 2988, loss = 0.00148631\n",
      "Iteration 2989, loss = 0.00148513\n",
      "Iteration 2990, loss = 0.00148395\n",
      "Iteration 2991, loss = 0.00148277\n",
      "Iteration 2992, loss = 0.00148160\n",
      "Iteration 2993, loss = 0.00148042\n",
      "Iteration 2994, loss = 0.00147925\n",
      "Iteration 2995, loss = 0.00147808\n",
      "Iteration 2996, loss = 0.00147691\n",
      "Iteration 2997, loss = 0.00147574\n",
      "Iteration 2998, loss = 0.00147457\n",
      "Iteration 2999, loss = 0.00147340\n",
      "Iteration 3000, loss = 0.00147224\n",
      "Iteration 3001, loss = 0.00147107\n",
      "Iteration 3002, loss = 0.00146991\n",
      "Iteration 3003, loss = 0.00146875\n",
      "Iteration 3004, loss = 0.00146759\n",
      "Iteration 3005, loss = 0.00146643\n",
      "Iteration 3006, loss = 0.00146527\n",
      "Iteration 3007, loss = 0.00146412\n",
      "Iteration 3008, loss = 0.00146296\n",
      "Iteration 3009, loss = 0.00146181\n",
      "Iteration 3010, loss = 0.00146066\n",
      "Iteration 3011, loss = 0.00145951\n",
      "Iteration 3012, loss = 0.00145836\n",
      "Iteration 3013, loss = 0.00145721\n",
      "Iteration 3014, loss = 0.00145606\n",
      "Iteration 3015, loss = 0.00145492\n",
      "Iteration 3016, loss = 0.00145377\n",
      "Iteration 3017, loss = 0.00145263\n",
      "Iteration 3018, loss = 0.00145149\n",
      "Iteration 3019, loss = 0.00145035\n",
      "Iteration 3020, loss = 0.00144921\n",
      "Iteration 3021, loss = 0.00144807\n",
      "Iteration 3022, loss = 0.00144694\n",
      "Iteration 3023, loss = 0.00144580\n",
      "Iteration 3024, loss = 0.00144467\n",
      "Iteration 3025, loss = 0.00144353\n",
      "Iteration 3026, loss = 0.00144240\n",
      "Iteration 3027, loss = 0.00144127\n",
      "Iteration 3028, loss = 0.00144014\n",
      "Iteration 3029, loss = 0.00143902\n",
      "Iteration 3030, loss = 0.00143789\n",
      "Iteration 3031, loss = 0.00143676\n",
      "Iteration 3032, loss = 0.00143564\n",
      "Iteration 3033, loss = 0.00143452\n",
      "Iteration 3034, loss = 0.00143340\n",
      "Iteration 3035, loss = 0.00143228\n",
      "Iteration 3036, loss = 0.00143116\n",
      "Iteration 3037, loss = 0.00143004\n",
      "Iteration 3038, loss = 0.00142892\n",
      "Iteration 3039, loss = 0.00142781\n",
      "Iteration 3040, loss = 0.00142670\n",
      "Iteration 3041, loss = 0.00142558\n",
      "Iteration 3042, loss = 0.00142447\n",
      "Iteration 3043, loss = 0.00142336\n",
      "Iteration 3044, loss = 0.00142225\n",
      "Iteration 3045, loss = 0.00142115\n",
      "Iteration 3046, loss = 0.00142004\n",
      "Iteration 3047, loss = 0.00141894\n",
      "Iteration 3048, loss = 0.00141783\n",
      "Iteration 3049, loss = 0.00141673\n",
      "Iteration 3050, loss = 0.00141563\n",
      "Iteration 3051, loss = 0.00141453\n",
      "Iteration 3052, loss = 0.00141343\n",
      "Iteration 3053, loss = 0.00141233\n",
      "Iteration 3054, loss = 0.00141124\n",
      "Iteration 3055, loss = 0.00141014\n",
      "Iteration 3056, loss = 0.00140905\n",
      "Iteration 3057, loss = 0.00140795\n",
      "Iteration 3058, loss = 0.00140686\n",
      "Iteration 3059, loss = 0.00140577\n",
      "Iteration 3060, loss = 0.00140468\n",
      "Iteration 3061, loss = 0.00140360\n",
      "Iteration 3062, loss = 0.00140251\n",
      "Iteration 3063, loss = 0.00140142\n",
      "Iteration 3064, loss = 0.00140034\n",
      "Iteration 3065, loss = 0.00139926\n",
      "Iteration 3066, loss = 0.00139817\n",
      "Iteration 3067, loss = 0.00139709\n",
      "Iteration 3068, loss = 0.00139601\n",
      "Iteration 3069, loss = 0.00139494\n",
      "Iteration 3070, loss = 0.00139386\n",
      "Iteration 3071, loss = 0.00139278\n",
      "Iteration 3072, loss = 0.00139171\n",
      "Iteration 3073, loss = 0.00139064\n",
      "Iteration 3074, loss = 0.00138956\n",
      "Iteration 3075, loss = 0.00138849\n",
      "Iteration 3076, loss = 0.00138742\n",
      "Iteration 3077, loss = 0.00138635\n",
      "Iteration 3078, loss = 0.00138529\n",
      "Iteration 3079, loss = 0.00138422\n",
      "Iteration 3080, loss = 0.00138316\n",
      "Iteration 3081, loss = 0.00138209\n",
      "Iteration 3082, loss = 0.00138103\n",
      "Iteration 3083, loss = 0.00137997\n",
      "Iteration 3084, loss = 0.00137891\n",
      "Iteration 3085, loss = 0.00137785\n",
      "Iteration 3086, loss = 0.00137679\n",
      "Iteration 3087, loss = 0.00137573\n",
      "Iteration 3088, loss = 0.00137468\n",
      "Iteration 3089, loss = 0.00137362\n",
      "Iteration 3090, loss = 0.00137257\n",
      "Iteration 3091, loss = 0.00137152\n",
      "Iteration 3092, loss = 0.00137047\n",
      "Iteration 3093, loss = 0.00136942\n",
      "Iteration 3094, loss = 0.00136837\n",
      "Iteration 3095, loss = 0.00136732\n",
      "Iteration 3096, loss = 0.00136628\n",
      "Iteration 3097, loss = 0.00136523\n",
      "Iteration 3098, loss = 0.00136419\n",
      "Iteration 3099, loss = 0.00136314\n",
      "Iteration 3100, loss = 0.00136210\n",
      "Iteration 3101, loss = 0.00136106\n",
      "Iteration 3102, loss = 0.00136002\n",
      "Iteration 3103, loss = 0.00135898\n",
      "Iteration 3104, loss = 0.00135795\n",
      "Iteration 3105, loss = 0.00135691\n",
      "Iteration 3106, loss = 0.00135588\n",
      "Iteration 3107, loss = 0.00135484\n",
      "Iteration 3108, loss = 0.00135381\n",
      "Iteration 3109, loss = 0.00135278\n",
      "Iteration 3110, loss = 0.00135175\n",
      "Iteration 3111, loss = 0.00135072\n",
      "Iteration 3112, loss = 0.00134969\n",
      "Iteration 3113, loss = 0.00134866\n",
      "Iteration 3114, loss = 0.00134764\n",
      "Iteration 3115, loss = 0.00134661\n",
      "Iteration 3116, loss = 0.00134559\n",
      "Iteration 3117, loss = 0.00134457\n",
      "Iteration 3118, loss = 0.00134355\n",
      "Iteration 3119, loss = 0.00134253\n",
      "Iteration 3120, loss = 0.00134151\n",
      "Iteration 3121, loss = 0.00134049\n",
      "Iteration 3122, loss = 0.00133947\n",
      "Iteration 3123, loss = 0.00133846\n",
      "Iteration 3124, loss = 0.00133744\n",
      "Iteration 3125, loss = 0.00133643\n",
      "Iteration 3126, loss = 0.00133542\n",
      "Iteration 3127, loss = 0.00133440\n",
      "Iteration 3128, loss = 0.00133339\n",
      "Iteration 3129, loss = 0.00133239\n",
      "Iteration 3130, loss = 0.00133138\n",
      "Iteration 3131, loss = 0.00133037\n",
      "Iteration 3132, loss = 0.00132937\n",
      "Iteration 3133, loss = 0.00132836\n",
      "Iteration 3134, loss = 0.00132736\n",
      "Iteration 3135, loss = 0.00132635\n",
      "Iteration 3136, loss = 0.00132535\n",
      "Iteration 3137, loss = 0.00132435\n",
      "Iteration 3138, loss = 0.00132335\n",
      "Iteration 3139, loss = 0.00132236\n",
      "Iteration 3140, loss = 0.00132136\n",
      "Iteration 3141, loss = 0.00132036\n",
      "Iteration 3142, loss = 0.00131937\n",
      "Iteration 3143, loss = 0.00131837\n",
      "Iteration 3144, loss = 0.00131738\n",
      "Iteration 3145, loss = 0.00131639\n",
      "Iteration 3146, loss = 0.00131540\n",
      "Iteration 3147, loss = 0.00131441\n",
      "Iteration 3148, loss = 0.00131342\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.20660423\n",
      "Iteration 2, loss = 1.19821703\n",
      "Iteration 3, loss = 1.18995731\n",
      "Iteration 4, loss = 1.18182906\n",
      "Iteration 5, loss = 1.17383621\n",
      "Iteration 6, loss = 1.16598253\n",
      "Iteration 7, loss = 1.15827159\n",
      "Iteration 8, loss = 1.15070673\n",
      "Iteration 9, loss = 1.14329104\n",
      "Iteration 10, loss = 1.13602741\n",
      "Iteration 11, loss = 1.12891845\n",
      "Iteration 12, loss = 1.12196650\n",
      "Iteration 13, loss = 1.11517354\n",
      "Iteration 14, loss = 1.10854122\n",
      "Iteration 15, loss = 1.10207077\n",
      "Iteration 16, loss = 1.09576302\n",
      "Iteration 17, loss = 1.08961831\n",
      "Iteration 18, loss = 1.08363652\n",
      "Iteration 19, loss = 1.07781698\n",
      "Iteration 20, loss = 1.07215851\n",
      "Iteration 21, loss = 1.06665931\n",
      "Iteration 22, loss = 1.06131703\n",
      "Iteration 23, loss = 1.05612872\n",
      "Iteration 24, loss = 1.05109083\n",
      "Iteration 25, loss = 1.04619923\n",
      "Iteration 26, loss = 1.04144925\n",
      "Iteration 27, loss = 1.03683568\n",
      "Iteration 28, loss = 1.03235286\n",
      "Iteration 29, loss = 1.02799471\n",
      "Iteration 30, loss = 1.02375482\n",
      "Iteration 31, loss = 1.01962652\n",
      "Iteration 32, loss = 1.01560296\n",
      "Iteration 33, loss = 1.01167717\n",
      "Iteration 34, loss = 1.00784219\n",
      "Iteration 35, loss = 1.00409107\n",
      "Iteration 36, loss = 1.00041699\n",
      "Iteration 37, loss = 0.99681330\n",
      "Iteration 38, loss = 0.99327354\n",
      "Iteration 39, loss = 0.98979152\n",
      "Iteration 40, loss = 0.98636132\n",
      "Iteration 41, loss = 0.98297731\n",
      "Iteration 42, loss = 0.97963422\n",
      "Iteration 43, loss = 0.97632708\n",
      "Iteration 44, loss = 0.97305128\n",
      "Iteration 45, loss = 0.96980256\n",
      "Iteration 46, loss = 0.96657700\n",
      "Iteration 47, loss = 0.96337102\n",
      "Iteration 48, loss = 0.96018138\n",
      "Iteration 49, loss = 0.95700517\n",
      "Iteration 50, loss = 0.95383979\n",
      "Iteration 51, loss = 0.95068291\n",
      "Iteration 52, loss = 0.94753251\n",
      "Iteration 53, loss = 0.94438681\n",
      "Iteration 54, loss = 0.94124428\n",
      "Iteration 55, loss = 0.93810362\n",
      "Iteration 56, loss = 0.93496369\n",
      "Iteration 57, loss = 0.93182359\n",
      "Iteration 58, loss = 0.92868253\n",
      "Iteration 59, loss = 0.92553988\n",
      "Iteration 60, loss = 0.92239515\n",
      "Iteration 61, loss = 0.91924795\n",
      "Iteration 62, loss = 0.91609796\n",
      "Iteration 63, loss = 0.91294498\n",
      "Iteration 64, loss = 0.90978884\n",
      "Iteration 65, loss = 0.90662945\n",
      "Iteration 66, loss = 0.90346674\n",
      "Iteration 67, loss = 0.90030070\n",
      "Iteration 68, loss = 0.89713134\n",
      "Iteration 69, loss = 0.89395867\n",
      "Iteration 70, loss = 0.89078274\n",
      "Iteration 71, loss = 0.88760360\n",
      "Iteration 72, loss = 0.88442131\n",
      "Iteration 73, loss = 0.88123594\n",
      "Iteration 74, loss = 0.87804754\n",
      "Iteration 75, loss = 0.87485619\n",
      "Iteration 76, loss = 0.87166195\n",
      "Iteration 77, loss = 0.86846488\n",
      "Iteration 78, loss = 0.86526505\n",
      "Iteration 79, loss = 0.86206251\n",
      "Iteration 80, loss = 0.85885734\n",
      "Iteration 81, loss = 0.85564958\n",
      "Iteration 82, loss = 0.85243929\n",
      "Iteration 83, loss = 0.84922655\n",
      "Iteration 84, loss = 0.84601141\n",
      "Iteration 85, loss = 0.84279394\n",
      "Iteration 86, loss = 0.83957421\n",
      "Iteration 87, loss = 0.83635228\n",
      "Iteration 88, loss = 0.83312824\n",
      "Iteration 89, loss = 0.82990217\n",
      "Iteration 90, loss = 0.82667415\n",
      "Iteration 91, loss = 0.82344428\n",
      "Iteration 92, loss = 0.82021265\n",
      "Iteration 93, loss = 0.81697937\n",
      "Iteration 94, loss = 0.81374454\n",
      "Iteration 95, loss = 0.81050829\n",
      "Iteration 96, loss = 0.80727072\n",
      "Iteration 97, loss = 0.80403198\n",
      "Iteration 98, loss = 0.80079219\n",
      "Iteration 99, loss = 0.79755150\n",
      "Iteration 100, loss = 0.79431004\n",
      "Iteration 101, loss = 0.79106796\n",
      "Iteration 102, loss = 0.78782542\n",
      "Iteration 103, loss = 0.78458258\n",
      "Iteration 104, loss = 0.78133959\n",
      "Iteration 105, loss = 0.77809663\n",
      "Iteration 106, loss = 0.77485385\n",
      "Iteration 107, loss = 0.77161144\n",
      "Iteration 108, loss = 0.76836957\n",
      "Iteration 109, loss = 0.76512840\n",
      "Iteration 110, loss = 0.76188814\n",
      "Iteration 111, loss = 0.75864894\n",
      "Iteration 112, loss = 0.75541101\n",
      "Iteration 113, loss = 0.75217451\n",
      "Iteration 114, loss = 0.74893964\n",
      "Iteration 115, loss = 0.74570659\n",
      "Iteration 116, loss = 0.74247554\n",
      "Iteration 117, loss = 0.73924668\n",
      "Iteration 118, loss = 0.73602020\n",
      "Iteration 119, loss = 0.73279629\n",
      "Iteration 120, loss = 0.72957514\n",
      "Iteration 121, loss = 0.72635694\n",
      "Iteration 122, loss = 0.72314188\n",
      "Iteration 123, loss = 0.71993015\n",
      "Iteration 124, loss = 0.71672193\n",
      "Iteration 125, loss = 0.71351743\n",
      "Iteration 126, loss = 0.71031683\n",
      "Iteration 127, loss = 0.70712032\n",
      "Iteration 128, loss = 0.70392808\n",
      "Iteration 129, loss = 0.70074031\n",
      "Iteration 130, loss = 0.69755720\n",
      "Iteration 131, loss = 0.69437893\n",
      "Iteration 132, loss = 0.69120568\n",
      "Iteration 133, loss = 0.68803765\n",
      "Iteration 134, loss = 0.68487501\n",
      "Iteration 135, loss = 0.68171796\n",
      "Iteration 136, loss = 0.67856667\n",
      "Iteration 137, loss = 0.67542132\n",
      "Iteration 138, loss = 0.67228211\n",
      "Iteration 139, loss = 0.66914919\n",
      "Iteration 140, loss = 0.66602276\n",
      "Iteration 141, loss = 0.66290299\n",
      "Iteration 142, loss = 0.65979005\n",
      "Iteration 143, loss = 0.65668412\n",
      "Iteration 144, loss = 0.65358536\n",
      "Iteration 145, loss = 0.65049396\n",
      "Iteration 146, loss = 0.64741007\n",
      "Iteration 147, loss = 0.64433386\n",
      "Iteration 148, loss = 0.64126550\n",
      "Iteration 149, loss = 0.63820515\n",
      "Iteration 150, loss = 0.63515296\n",
      "Iteration 151, loss = 0.63210911\n",
      "Iteration 152, loss = 0.62907374\n",
      "Iteration 153, loss = 0.62604701\n",
      "Iteration 154, loss = 0.62302906\n",
      "Iteration 155, loss = 0.62002006\n",
      "Iteration 156, loss = 0.61702015\n",
      "Iteration 157, loss = 0.61402947\n",
      "Iteration 158, loss = 0.61104816\n",
      "Iteration 159, loss = 0.60807638\n",
      "Iteration 160, loss = 0.60511424\n",
      "Iteration 161, loss = 0.60216190\n",
      "Iteration 162, loss = 0.59921948\n",
      "Iteration 163, loss = 0.59628712\n",
      "Iteration 164, loss = 0.59336494\n",
      "Iteration 165, loss = 0.59045306\n",
      "Iteration 166, loss = 0.58755163\n",
      "Iteration 167, loss = 0.58466074\n",
      "Iteration 168, loss = 0.58178052\n",
      "Iteration 169, loss = 0.57891109\n",
      "Iteration 170, loss = 0.57605256\n",
      "Iteration 171, loss = 0.57320505\n",
      "Iteration 172, loss = 0.57036864\n",
      "Iteration 173, loss = 0.56754347\n",
      "Iteration 174, loss = 0.56472962\n",
      "Iteration 175, loss = 0.56192719\n",
      "Iteration 176, loss = 0.55913629\n",
      "Iteration 177, loss = 0.55635700\n",
      "Iteration 178, loss = 0.55358943\n",
      "Iteration 179, loss = 0.55083365\n",
      "Iteration 180, loss = 0.54808976\n",
      "Iteration 181, loss = 0.54535785\n",
      "Iteration 182, loss = 0.54263798\n",
      "Iteration 183, loss = 0.53993024\n",
      "Iteration 184, loss = 0.53723472\n",
      "Iteration 185, loss = 0.53455147\n",
      "Iteration 186, loss = 0.53188057\n",
      "Iteration 187, loss = 0.52922209\n",
      "Iteration 188, loss = 0.52657610\n",
      "Iteration 189, loss = 0.52394266\n",
      "Iteration 190, loss = 0.52132183\n",
      "Iteration 191, loss = 0.51871367\n",
      "Iteration 192, loss = 0.51611823\n",
      "Iteration 193, loss = 0.51353557\n",
      "Iteration 194, loss = 0.51096574\n",
      "Iteration 195, loss = 0.50840878\n",
      "Iteration 196, loss = 0.50586475\n",
      "Iteration 197, loss = 0.50333368\n",
      "Iteration 198, loss = 0.50081561\n",
      "Iteration 199, loss = 0.49831059\n",
      "Iteration 200, loss = 0.49581865\n",
      "Iteration 201, loss = 0.49333982\n",
      "Iteration 202, loss = 0.49087413\n",
      "Iteration 203, loss = 0.48842162\n",
      "Iteration 204, loss = 0.48598231\n",
      "Iteration 205, loss = 0.48355622\n",
      "Iteration 206, loss = 0.48114338\n",
      "Iteration 207, loss = 0.47874381\n",
      "Iteration 208, loss = 0.47635751\n",
      "Iteration 209, loss = 0.47398452\n",
      "Iteration 210, loss = 0.47162484\n",
      "Iteration 211, loss = 0.46927848\n",
      "Iteration 212, loss = 0.46694546\n",
      "Iteration 213, loss = 0.46462577\n",
      "Iteration 214, loss = 0.46231943\n",
      "Iteration 215, loss = 0.46002643\n",
      "Iteration 216, loss = 0.45774678\n",
      "Iteration 217, loss = 0.45548048\n",
      "Iteration 218, loss = 0.45322752\n",
      "Iteration 219, loss = 0.45098790\n",
      "Iteration 220, loss = 0.44876162\n",
      "Iteration 221, loss = 0.44654865\n",
      "Iteration 222, loss = 0.44434900\n",
      "Iteration 223, loss = 0.44216265\n",
      "Iteration 224, loss = 0.43998959\n",
      "Iteration 225, loss = 0.43782979\n",
      "Iteration 226, loss = 0.43568325\n",
      "Iteration 227, loss = 0.43354995\n",
      "Iteration 228, loss = 0.43142986\n",
      "Iteration 229, loss = 0.42932296\n",
      "Iteration 230, loss = 0.42722923\n",
      "Iteration 231, loss = 0.42514864\n",
      "Iteration 232, loss = 0.42308117\n",
      "Iteration 233, loss = 0.42102679\n",
      "Iteration 234, loss = 0.41898547\n",
      "Iteration 235, loss = 0.41695717\n",
      "Iteration 236, loss = 0.41494187\n",
      "Iteration 237, loss = 0.41293953\n",
      "Iteration 238, loss = 0.41095011\n",
      "Iteration 239, loss = 0.40897359\n",
      "Iteration 240, loss = 0.40700992\n",
      "Iteration 241, loss = 0.40505906\n",
      "Iteration 242, loss = 0.40312098\n",
      "Iteration 243, loss = 0.40119563\n",
      "Iteration 244, loss = 0.39928297\n",
      "Iteration 245, loss = 0.39738296\n",
      "Iteration 246, loss = 0.39549555\n",
      "Iteration 247, loss = 0.39362070\n",
      "Iteration 248, loss = 0.39175836\n",
      "Iteration 249, loss = 0.38990849\n",
      "Iteration 250, loss = 0.38807103\n",
      "Iteration 251, loss = 0.38624594\n",
      "Iteration 252, loss = 0.38443317\n",
      "Iteration 253, loss = 0.38263267\n",
      "Iteration 254, loss = 0.38084438\n",
      "Iteration 255, loss = 0.37906825\n",
      "Iteration 256, loss = 0.37730423\n",
      "Iteration 257, loss = 0.37555227\n",
      "Iteration 258, loss = 0.37381231\n",
      "Iteration 259, loss = 0.37208430\n",
      "Iteration 260, loss = 0.37036818\n",
      "Iteration 261, loss = 0.36866389\n",
      "Iteration 262, loss = 0.36697138\n",
      "Iteration 263, loss = 0.36529058\n",
      "Iteration 264, loss = 0.36362145\n",
      "Iteration 265, loss = 0.36196391\n",
      "Iteration 266, loss = 0.36031792\n",
      "Iteration 267, loss = 0.35868342\n",
      "Iteration 268, loss = 0.35706033\n",
      "Iteration 269, loss = 0.35544861\n",
      "Iteration 270, loss = 0.35384818\n",
      "Iteration 271, loss = 0.35225900\n",
      "Iteration 272, loss = 0.35068099\n",
      "Iteration 273, loss = 0.34911410\n",
      "Iteration 274, loss = 0.34755826\n",
      "Iteration 275, loss = 0.34601340\n",
      "Iteration 276, loss = 0.34447948\n",
      "Iteration 277, loss = 0.34295642\n",
      "Iteration 278, loss = 0.34144415\n",
      "Iteration 279, loss = 0.33994262\n",
      "Iteration 280, loss = 0.33845177\n",
      "Iteration 281, loss = 0.33697152\n",
      "Iteration 282, loss = 0.33550181\n",
      "Iteration 283, loss = 0.33404258\n",
      "Iteration 284, loss = 0.33259376\n",
      "Iteration 285, loss = 0.33115529\n",
      "Iteration 286, loss = 0.32972711\n",
      "Iteration 287, loss = 0.32830914\n",
      "Iteration 288, loss = 0.32690132\n",
      "Iteration 289, loss = 0.32550360\n",
      "Iteration 290, loss = 0.32411589\n",
      "Iteration 291, loss = 0.32273815\n",
      "Iteration 292, loss = 0.32137029\n",
      "Iteration 293, loss = 0.32001226\n",
      "Iteration 294, loss = 0.31866399\n",
      "Iteration 295, loss = 0.31732542\n",
      "Iteration 296, loss = 0.31599648\n",
      "Iteration 297, loss = 0.31467711\n",
      "Iteration 298, loss = 0.31336724\n",
      "Iteration 299, loss = 0.31206680\n",
      "Iteration 300, loss = 0.31077573\n",
      "Iteration 301, loss = 0.30949397\n",
      "Iteration 302, loss = 0.30822145\n",
      "Iteration 303, loss = 0.30695810\n",
      "Iteration 304, loss = 0.30570387\n",
      "Iteration 305, loss = 0.30445869\n",
      "Iteration 306, loss = 0.30322249\n",
      "Iteration 307, loss = 0.30199521\n",
      "Iteration 308, loss = 0.30077678\n",
      "Iteration 309, loss = 0.29956715\n",
      "Iteration 310, loss = 0.29836624\n",
      "Iteration 311, loss = 0.29717400\n",
      "Iteration 312, loss = 0.29599037\n",
      "Iteration 313, loss = 0.29481527\n",
      "Iteration 314, loss = 0.29364865\n",
      "Iteration 315, loss = 0.29249044\n",
      "Iteration 316, loss = 0.29134059\n",
      "Iteration 317, loss = 0.29019902\n",
      "Iteration 318, loss = 0.28906569\n",
      "Iteration 319, loss = 0.28794052\n",
      "Iteration 320, loss = 0.28682345\n",
      "Iteration 321, loss = 0.28571444\n",
      "Iteration 322, loss = 0.28461340\n",
      "Iteration 323, loss = 0.28352029\n",
      "Iteration 324, loss = 0.28243504\n",
      "Iteration 325, loss = 0.28135760\n",
      "Iteration 326, loss = 0.28028790\n",
      "Iteration 327, loss = 0.27922589\n",
      "Iteration 328, loss = 0.27817150\n",
      "Iteration 329, loss = 0.27712468\n",
      "Iteration 330, loss = 0.27608537\n",
      "Iteration 331, loss = 0.27505351\n",
      "Iteration 332, loss = 0.27402904\n",
      "Iteration 333, loss = 0.27301191\n",
      "Iteration 334, loss = 0.27200206\n",
      "Iteration 335, loss = 0.27099943\n",
      "Iteration 336, loss = 0.27000396\n",
      "Iteration 337, loss = 0.26901561\n",
      "Iteration 338, loss = 0.26803431\n",
      "Iteration 339, loss = 0.26706000\n",
      "Iteration 340, loss = 0.26609265\n",
      "Iteration 341, loss = 0.26513217\n",
      "Iteration 342, loss = 0.26417854\n",
      "Iteration 343, loss = 0.26323168\n",
      "Iteration 344, loss = 0.26229155\n",
      "Iteration 345, loss = 0.26135810\n",
      "Iteration 346, loss = 0.26043126\n",
      "Iteration 347, loss = 0.25951099\n",
      "Iteration 348, loss = 0.25859723\n",
      "Iteration 349, loss = 0.25768994\n",
      "Iteration 350, loss = 0.25678906\n",
      "Iteration 351, loss = 0.25589454\n",
      "Iteration 352, loss = 0.25500633\n",
      "Iteration 353, loss = 0.25412437\n",
      "Iteration 354, loss = 0.25324862\n",
      "Iteration 355, loss = 0.25237904\n",
      "Iteration 356, loss = 0.25151556\n",
      "Iteration 357, loss = 0.25065814\n",
      "Iteration 358, loss = 0.24980673\n",
      "Iteration 359, loss = 0.24896128\n",
      "Iteration 360, loss = 0.24812175\n",
      "Iteration 361, loss = 0.24728808\n",
      "Iteration 362, loss = 0.24646023\n",
      "Iteration 363, loss = 0.24563815\n",
      "Iteration 364, loss = 0.24482180\n",
      "Iteration 365, loss = 0.24401112\n",
      "Iteration 366, loss = 0.24320608\n",
      "Iteration 367, loss = 0.24240663\n",
      "Iteration 368, loss = 0.24161271\n",
      "Iteration 369, loss = 0.24082429\n",
      "Iteration 370, loss = 0.24004133\n",
      "Iteration 371, loss = 0.23926377\n",
      "Iteration 372, loss = 0.23849157\n",
      "Iteration 373, loss = 0.23772470\n",
      "Iteration 374, loss = 0.23696310\n",
      "Iteration 375, loss = 0.23620673\n",
      "Iteration 376, loss = 0.23545556\n",
      "Iteration 377, loss = 0.23470953\n",
      "Iteration 378, loss = 0.23396861\n",
      "Iteration 379, loss = 0.23323276\n",
      "Iteration 380, loss = 0.23250193\n",
      "Iteration 381, loss = 0.23177608\n",
      "Iteration 382, loss = 0.23105518\n",
      "Iteration 383, loss = 0.23033918\n",
      "Iteration 384, loss = 0.22962804\n",
      "Iteration 385, loss = 0.22892172\n",
      "Iteration 386, loss = 0.22822019\n",
      "Iteration 387, loss = 0.22752340\n",
      "Iteration 388, loss = 0.22683132\n",
      "Iteration 389, loss = 0.22614390\n",
      "Iteration 390, loss = 0.22546111\n",
      "Iteration 391, loss = 0.22478292\n",
      "Iteration 392, loss = 0.22410928\n",
      "Iteration 393, loss = 0.22344015\n",
      "Iteration 394, loss = 0.22277551\n",
      "Iteration 395, loss = 0.22211531\n",
      "Iteration 396, loss = 0.22145952\n",
      "Iteration 397, loss = 0.22080811\n",
      "Iteration 398, loss = 0.22016103\n",
      "Iteration 399, loss = 0.21951825\n",
      "Iteration 400, loss = 0.21887973\n",
      "Iteration 401, loss = 0.21824545\n",
      "Iteration 402, loss = 0.21761537\n",
      "Iteration 403, loss = 0.21698945\n",
      "Iteration 404, loss = 0.21636766\n",
      "Iteration 405, loss = 0.21574997\n",
      "Iteration 406, loss = 0.21513634\n",
      "Iteration 407, loss = 0.21452674\n",
      "Iteration 408, loss = 0.21392114\n",
      "Iteration 409, loss = 0.21331950\n",
      "Iteration 410, loss = 0.21272180\n",
      "Iteration 411, loss = 0.21212800\n",
      "Iteration 412, loss = 0.21153807\n",
      "Iteration 413, loss = 0.21095198\n",
      "Iteration 414, loss = 0.21036969\n",
      "Iteration 415, loss = 0.20979119\n",
      "Iteration 416, loss = 0.20921643\n",
      "Iteration 417, loss = 0.20864539\n",
      "Iteration 418, loss = 0.20807804\n",
      "Iteration 419, loss = 0.20751434\n",
      "Iteration 420, loss = 0.20695427\n",
      "Iteration 421, loss = 0.20639780\n",
      "Iteration 422, loss = 0.20584490\n",
      "Iteration 423, loss = 0.20529555\n",
      "Iteration 424, loss = 0.20474970\n",
      "Iteration 425, loss = 0.20420735\n",
      "Iteration 426, loss = 0.20366844\n",
      "Iteration 427, loss = 0.20313297\n",
      "Iteration 428, loss = 0.20260091\n",
      "Iteration 429, loss = 0.20207221\n",
      "Iteration 430, loss = 0.20154687\n",
      "Iteration 431, loss = 0.20102485\n",
      "Iteration 432, loss = 0.20050612\n",
      "Iteration 433, loss = 0.19999067\n",
      "Iteration 434, loss = 0.19947846\n",
      "Iteration 435, loss = 0.19896946\n",
      "Iteration 436, loss = 0.19846366\n",
      "Iteration 437, loss = 0.19796103\n",
      "Iteration 438, loss = 0.19746153\n",
      "Iteration 439, loss = 0.19696516\n",
      "Iteration 440, loss = 0.19647188\n",
      "Iteration 441, loss = 0.19598167\n",
      "Iteration 442, loss = 0.19549450\n",
      "Iteration 443, loss = 0.19501035\n",
      "Iteration 444, loss = 0.19452920\n",
      "Iteration 445, loss = 0.19405102\n",
      "Iteration 446, loss = 0.19357580\n",
      "Iteration 447, loss = 0.19310350\n",
      "Iteration 448, loss = 0.19263410\n",
      "Iteration 449, loss = 0.19216759\n",
      "Iteration 450, loss = 0.19170393\n",
      "Iteration 451, loss = 0.19124311\n",
      "Iteration 452, loss = 0.19078511\n",
      "Iteration 453, loss = 0.19032990\n",
      "Iteration 454, loss = 0.18987746\n",
      "Iteration 455, loss = 0.18942777\n",
      "Iteration 456, loss = 0.18898081\n",
      "Iteration 457, loss = 0.18853656\n",
      "Iteration 458, loss = 0.18809499\n",
      "Iteration 459, loss = 0.18765609\n",
      "Iteration 460, loss = 0.18721983\n",
      "Iteration 461, loss = 0.18678620\n",
      "Iteration 462, loss = 0.18635518\n",
      "Iteration 463, loss = 0.18592674\n",
      "Iteration 464, loss = 0.18550086\n",
      "Iteration 465, loss = 0.18507753\n",
      "Iteration 466, loss = 0.18465672\n",
      "Iteration 467, loss = 0.18423842\n",
      "Iteration 468, loss = 0.18382261\n",
      "Iteration 469, loss = 0.18340927\n",
      "Iteration 470, loss = 0.18299838\n",
      "Iteration 471, loss = 0.18258991\n",
      "Iteration 472, loss = 0.18218386\n",
      "Iteration 473, loss = 0.18178021\n",
      "Iteration 474, loss = 0.18137893\n",
      "Iteration 475, loss = 0.18098001\n",
      "Iteration 476, loss = 0.18058343\n",
      "Iteration 477, loss = 0.18018917\n",
      "Iteration 478, loss = 0.17979721\n",
      "Iteration 479, loss = 0.17940755\n",
      "Iteration 480, loss = 0.17902016\n",
      "Iteration 481, loss = 0.17863501\n",
      "Iteration 482, loss = 0.17825211\n",
      "Iteration 483, loss = 0.17787143\n",
      "Iteration 484, loss = 0.17749295\n",
      "Iteration 485, loss = 0.17711665\n",
      "Iteration 486, loss = 0.17674253\n",
      "Iteration 487, loss = 0.17637056\n",
      "Iteration 488, loss = 0.17600073\n",
      "Iteration 489, loss = 0.17563303\n",
      "Iteration 490, loss = 0.17526742\n",
      "Iteration 491, loss = 0.17490392\n",
      "Iteration 492, loss = 0.17454248\n",
      "Iteration 493, loss = 0.17418311\n",
      "Iteration 494, loss = 0.17382578\n",
      "Iteration 495, loss = 0.17347049\n",
      "Iteration 496, loss = 0.17311721\n",
      "Iteration 497, loss = 0.17276593\n",
      "Iteration 498, loss = 0.17241663\n",
      "Iteration 499, loss = 0.17206931\n",
      "Iteration 500, loss = 0.17172394\n",
      "Iteration 501, loss = 0.17138052\n",
      "Iteration 502, loss = 0.17103902\n",
      "Iteration 503, loss = 0.17069944\n",
      "Iteration 504, loss = 0.17036176\n",
      "Iteration 505, loss = 0.17002597\n",
      "Iteration 506, loss = 0.16969205\n",
      "Iteration 507, loss = 0.16935999\n",
      "Iteration 508, loss = 0.16902977\n",
      "Iteration 509, loss = 0.16870139\n",
      "Iteration 510, loss = 0.16837483\n",
      "Iteration 511, loss = 0.16805007\n",
      "Iteration 512, loss = 0.16772711\n",
      "Iteration 513, loss = 0.16740592\n",
      "Iteration 514, loss = 0.16708651\n",
      "Iteration 515, loss = 0.16676885\n",
      "Iteration 516, loss = 0.16645293\n",
      "Iteration 517, loss = 0.16613875\n",
      "Iteration 518, loss = 0.16582628\n",
      "Iteration 519, loss = 0.16551551\n",
      "Iteration 520, loss = 0.16520644\n",
      "Iteration 521, loss = 0.16489906\n",
      "Iteration 522, loss = 0.16459334\n",
      "Iteration 523, loss = 0.16428928\n",
      "Iteration 524, loss = 0.16398686\n",
      "Iteration 525, loss = 0.16368608\n",
      "Iteration 526, loss = 0.16338692\n",
      "Iteration 527, loss = 0.16308938\n",
      "Iteration 528, loss = 0.16279343\n",
      "Iteration 529, loss = 0.16249907\n",
      "Iteration 530, loss = 0.16220630\n",
      "Iteration 531, loss = 0.16191508\n",
      "Iteration 532, loss = 0.16162543\n",
      "Iteration 533, loss = 0.16133731\n",
      "Iteration 534, loss = 0.16105074\n",
      "Iteration 535, loss = 0.16076568\n",
      "Iteration 536, loss = 0.16048214\n",
      "Iteration 537, loss = 0.16020010\n",
      "Iteration 538, loss = 0.15991956\n",
      "Iteration 539, loss = 0.15964049\n",
      "Iteration 540, loss = 0.15936290\n",
      "Iteration 541, loss = 0.15908677\n",
      "Iteration 542, loss = 0.15881208\n",
      "Iteration 543, loss = 0.15853885\n",
      "Iteration 544, loss = 0.15826704\n",
      "Iteration 545, loss = 0.15799665\n",
      "Iteration 546, loss = 0.15772768\n",
      "Iteration 547, loss = 0.15746010\n",
      "Iteration 548, loss = 0.15719392\n",
      "Iteration 549, loss = 0.15692913\n",
      "Iteration 550, loss = 0.15666571\n",
      "Iteration 551, loss = 0.15640365\n",
      "Iteration 552, loss = 0.15614295\n",
      "Iteration 553, loss = 0.15588359\n",
      "Iteration 554, loss = 0.15562557\n",
      "Iteration 555, loss = 0.15536888\n",
      "Iteration 556, loss = 0.15511350\n",
      "Iteration 557, loss = 0.15485944\n",
      "Iteration 558, loss = 0.15460668\n",
      "Iteration 559, loss = 0.15435521\n",
      "Iteration 560, loss = 0.15410502\n",
      "Iteration 561, loss = 0.15385611\n",
      "Iteration 562, loss = 0.15360847\n",
      "Iteration 563, loss = 0.15336208\n",
      "Iteration 564, loss = 0.15311695\n",
      "Iteration 565, loss = 0.15287305\n",
      "Iteration 566, loss = 0.15263039\n",
      "Iteration 567, loss = 0.15238896\n",
      "Iteration 568, loss = 0.15214874\n",
      "Iteration 569, loss = 0.15190973\n",
      "Iteration 570, loss = 0.15167192\n",
      "Iteration 571, loss = 0.15143530\n",
      "Iteration 572, loss = 0.15119987\n",
      "Iteration 573, loss = 0.15096562\n",
      "Iteration 574, loss = 0.15073254\n",
      "Iteration 575, loss = 0.15050062\n",
      "Iteration 576, loss = 0.15026985\n",
      "Iteration 577, loss = 0.15004023\n",
      "Iteration 578, loss = 0.14981175\n",
      "Iteration 579, loss = 0.14958440\n",
      "Iteration 580, loss = 0.14935817\n",
      "Iteration 581, loss = 0.14913306\n",
      "Iteration 582, loss = 0.14890907\n",
      "Iteration 583, loss = 0.14868617\n",
      "Iteration 584, loss = 0.14846437\n",
      "Iteration 585, loss = 0.14824366\n",
      "Iteration 586, loss = 0.14802403\n",
      "Iteration 587, loss = 0.14780548\n",
      "Iteration 588, loss = 0.14758799\n",
      "Iteration 589, loss = 0.14737156\n",
      "Iteration 590, loss = 0.14715619\n",
      "Iteration 591, loss = 0.14694187\n",
      "Iteration 592, loss = 0.14672858\n",
      "Iteration 593, loss = 0.14651633\n",
      "Iteration 594, loss = 0.14630511\n",
      "Iteration 595, loss = 0.14609490\n",
      "Iteration 596, loss = 0.14588571\n",
      "Iteration 597, loss = 0.14567753\n",
      "Iteration 598, loss = 0.14547035\n",
      "Iteration 599, loss = 0.14526417\n",
      "Iteration 600, loss = 0.14505897\n",
      "Iteration 601, loss = 0.14485476\n",
      "Iteration 602, loss = 0.14465152\n",
      "Iteration 603, loss = 0.14444925\n",
      "Iteration 604, loss = 0.14424795\n",
      "Iteration 605, loss = 0.14404760\n",
      "Iteration 606, loss = 0.14384821\n",
      "Iteration 607, loss = 0.14364976\n",
      "Iteration 608, loss = 0.14345225\n",
      "Iteration 609, loss = 0.14325568\n",
      "Iteration 610, loss = 0.14306004\n",
      "Iteration 611, loss = 0.14286531\n",
      "Iteration 612, loss = 0.14267151\n",
      "Iteration 613, loss = 0.14247861\n",
      "Iteration 614, loss = 0.14228662\n",
      "Iteration 615, loss = 0.14209554\n",
      "Iteration 616, loss = 0.14190534\n",
      "Iteration 617, loss = 0.14171603\n",
      "Iteration 618, loss = 0.14152761\n",
      "Iteration 619, loss = 0.14134007\n",
      "Iteration 620, loss = 0.14115340\n",
      "Iteration 621, loss = 0.14096759\n",
      "Iteration 622, loss = 0.14078265\n",
      "Iteration 623, loss = 0.14059856\n",
      "Iteration 624, loss = 0.14041532\n",
      "Iteration 625, loss = 0.14023293\n",
      "Iteration 626, loss = 0.14005139\n",
      "Iteration 627, loss = 0.13987067\n",
      "Iteration 628, loss = 0.13969079\n",
      "Iteration 629, loss = 0.13951173\n",
      "Iteration 630, loss = 0.13933350\n",
      "Iteration 631, loss = 0.13915608\n",
      "Iteration 632, loss = 0.13897947\n",
      "Iteration 633, loss = 0.13880367\n",
      "Iteration 634, loss = 0.13862866\n",
      "Iteration 635, loss = 0.13845446\n",
      "Iteration 636, loss = 0.13828104\n",
      "Iteration 637, loss = 0.13810841\n",
      "Iteration 638, loss = 0.13793657\n",
      "Iteration 639, loss = 0.13776550\n",
      "Iteration 640, loss = 0.13759520\n",
      "Iteration 641, loss = 0.13742567\n",
      "Iteration 642, loss = 0.13725691\n",
      "Iteration 643, loss = 0.13708890\n",
      "Iteration 644, loss = 0.13692165\n",
      "Iteration 645, loss = 0.13675515\n",
      "Iteration 646, loss = 0.13658939\n",
      "Iteration 647, loss = 0.13642438\n",
      "Iteration 648, loss = 0.13626010\n",
      "Iteration 649, loss = 0.13609655\n",
      "Iteration 650, loss = 0.13593373\n",
      "Iteration 651, loss = 0.13577164\n",
      "Iteration 652, loss = 0.13561026\n",
      "Iteration 653, loss = 0.13544960\n",
      "Iteration 654, loss = 0.13528965\n",
      "Iteration 655, loss = 0.13513041\n",
      "Iteration 656, loss = 0.13497186\n",
      "Iteration 657, loss = 0.13481402\n",
      "Iteration 658, loss = 0.13465688\n",
      "Iteration 659, loss = 0.13450042\n",
      "Iteration 660, loss = 0.13434465\n",
      "Iteration 661, loss = 0.13418956\n",
      "Iteration 662, loss = 0.13403515\n",
      "Iteration 663, loss = 0.13388141\n",
      "Iteration 664, loss = 0.13372835\n",
      "Iteration 665, loss = 0.13357595\n",
      "Iteration 666, loss = 0.13342422\n",
      "Iteration 667, loss = 0.13327314\n",
      "Iteration 668, loss = 0.13312272\n",
      "Iteration 669, loss = 0.13297295\n",
      "Iteration 670, loss = 0.13282383\n",
      "Iteration 671, loss = 0.13267535\n",
      "Iteration 672, loss = 0.13252751\n",
      "Iteration 673, loss = 0.13238031\n",
      "Iteration 674, loss = 0.13223375\n",
      "Iteration 675, loss = 0.13208781\n",
      "Iteration 676, loss = 0.13194250\n",
      "Iteration 677, loss = 0.13179781\n",
      "Iteration 678, loss = 0.13165374\n",
      "Iteration 679, loss = 0.13151028\n",
      "Iteration 680, loss = 0.13136744\n",
      "Iteration 681, loss = 0.13122520\n",
      "Iteration 682, loss = 0.13108357\n",
      "Iteration 683, loss = 0.13094254\n",
      "Iteration 684, loss = 0.13080211\n",
      "Iteration 685, loss = 0.13066227\n",
      "Iteration 686, loss = 0.13052302\n",
      "Iteration 687, loss = 0.13038437\n",
      "Iteration 688, loss = 0.13024629\n",
      "Iteration 689, loss = 0.13010880\n",
      "Iteration 690, loss = 0.12997189\n",
      "Iteration 691, loss = 0.12983555\n",
      "Iteration 692, loss = 0.12969978\n",
      "Iteration 693, loss = 0.12956458\n",
      "Iteration 694, loss = 0.12942995\n",
      "Iteration 695, loss = 0.12929588\n",
      "Iteration 696, loss = 0.12916237\n",
      "Iteration 697, loss = 0.12902941\n",
      "Iteration 698, loss = 0.12889701\n",
      "Iteration 699, loss = 0.12876516\n",
      "Iteration 700, loss = 0.12863385\n",
      "Iteration 701, loss = 0.12850309\n",
      "Iteration 702, loss = 0.12837287\n",
      "Iteration 703, loss = 0.12824318\n",
      "Iteration 704, loss = 0.12811403\n",
      "Iteration 705, loss = 0.12798542\n",
      "Iteration 706, loss = 0.12785733\n",
      "Iteration 707, loss = 0.12772977\n",
      "Iteration 708, loss = 0.12760273\n",
      "Iteration 709, loss = 0.12747621\n",
      "Iteration 710, loss = 0.12735021\n",
      "Iteration 711, loss = 0.12722472\n",
      "Iteration 712, loss = 0.12709975\n",
      "Iteration 713, loss = 0.12697528\n",
      "Iteration 714, loss = 0.12685133\n",
      "Iteration 715, loss = 0.12672787\n",
      "Iteration 716, loss = 0.12660492\n",
      "Iteration 717, loss = 0.12648246\n",
      "Iteration 718, loss = 0.12636050\n",
      "Iteration 719, loss = 0.12623903\n",
      "Iteration 720, loss = 0.12611805\n",
      "Iteration 721, loss = 0.12599756\n",
      "Iteration 722, loss = 0.12587756\n",
      "Iteration 723, loss = 0.12575804\n",
      "Iteration 724, loss = 0.12563899\n",
      "Iteration 725, loss = 0.12552043\n",
      "Iteration 726, loss = 0.12540234\n",
      "Iteration 727, loss = 0.12528472\n",
      "Iteration 728, loss = 0.12516757\n",
      "Iteration 729, loss = 0.12505088\n",
      "Iteration 730, loss = 0.12493467\n",
      "Iteration 731, loss = 0.12481891\n",
      "Iteration 732, loss = 0.12470361\n",
      "Iteration 733, loss = 0.12458877\n",
      "Iteration 734, loss = 0.12447439\n",
      "Iteration 735, loss = 0.12436046\n",
      "Iteration 736, loss = 0.12424697\n",
      "Iteration 737, loss = 0.12413394\n",
      "Iteration 738, loss = 0.12402135\n",
      "Iteration 739, loss = 0.12390920\n",
      "Iteration 740, loss = 0.12379749\n",
      "Iteration 741, loss = 0.12368623\n",
      "Iteration 742, loss = 0.12357539\n",
      "Iteration 743, loss = 0.12346500\n",
      "Iteration 744, loss = 0.12335503\n",
      "Iteration 745, loss = 0.12324549\n",
      "Iteration 746, loss = 0.12313638\n",
      "Iteration 747, loss = 0.12302769\n",
      "Iteration 748, loss = 0.12291943\n",
      "Iteration 749, loss = 0.12281158\n",
      "Iteration 750, loss = 0.12270416\n",
      "Iteration 751, loss = 0.12259715\n",
      "Iteration 752, loss = 0.12249055\n",
      "Iteration 753, loss = 0.12238436\n",
      "Iteration 754, loss = 0.12227859\n",
      "Iteration 755, loss = 0.12217322\n",
      "Iteration 756, loss = 0.12206826\n",
      "Iteration 757, loss = 0.12196370\n",
      "Iteration 758, loss = 0.12185954\n",
      "Iteration 759, loss = 0.12175578\n",
      "Iteration 760, loss = 0.12165241\n",
      "Iteration 761, loss = 0.12154945\n",
      "Iteration 762, loss = 0.12144687\n",
      "Iteration 763, loss = 0.12134469\n",
      "Iteration 764, loss = 0.12124289\n",
      "Iteration 765, loss = 0.12114148\n",
      "Iteration 766, loss = 0.12104046\n",
      "Iteration 767, loss = 0.12093982\n",
      "Iteration 768, loss = 0.12083956\n",
      "Iteration 769, loss = 0.12073968\n",
      "Iteration 770, loss = 0.12064017\n",
      "Iteration 771, loss = 0.12054104\n",
      "Iteration 772, loss = 0.12044229\n",
      "Iteration 773, loss = 0.12034390\n",
      "Iteration 774, loss = 0.12024589\n",
      "Iteration 775, loss = 0.12014824\n",
      "Iteration 776, loss = 0.12005096\n",
      "Iteration 777, loss = 0.11995404\n",
      "Iteration 778, loss = 0.11985749\n",
      "Iteration 779, loss = 0.11976129\n",
      "Iteration 780, loss = 0.11966546\n",
      "Iteration 781, loss = 0.11956998\n",
      "Iteration 782, loss = 0.11947485\n",
      "Iteration 783, loss = 0.11938008\n",
      "Iteration 784, loss = 0.11928566\n",
      "Iteration 785, loss = 0.11919159\n",
      "Iteration 786, loss = 0.11909786\n",
      "Iteration 787, loss = 0.11900449\n",
      "Iteration 788, loss = 0.11891145\n",
      "Iteration 789, loss = 0.11881876\n",
      "Iteration 790, loss = 0.11872641\n",
      "Iteration 791, loss = 0.11863440\n",
      "Iteration 792, loss = 0.11854273\n",
      "Iteration 793, loss = 0.11845140\n",
      "Iteration 794, loss = 0.11836039\n",
      "Iteration 795, loss = 0.11826972\n",
      "Iteration 796, loss = 0.11817939\n",
      "Iteration 797, loss = 0.11808938\n",
      "Iteration 798, loss = 0.11799970\n",
      "Iteration 799, loss = 0.11791034\n",
      "Iteration 800, loss = 0.11782131\n",
      "Iteration 801, loss = 0.11773260\n",
      "Iteration 802, loss = 0.11764421\n",
      "Iteration 803, loss = 0.11755615\n",
      "Iteration 804, loss = 0.11746840\n",
      "Iteration 805, loss = 0.11738097\n",
      "Iteration 806, loss = 0.11729385\n",
      "Iteration 807, loss = 0.11720704\n",
      "Iteration 808, loss = 0.11712055\n",
      "Iteration 809, loss = 0.11703437\n",
      "Iteration 810, loss = 0.11694850\n",
      "Iteration 811, loss = 0.11686293\n",
      "Iteration 812, loss = 0.11677767\n",
      "Iteration 813, loss = 0.11669272\n",
      "Iteration 814, loss = 0.11660807\n",
      "Iteration 815, loss = 0.11652372\n",
      "Iteration 816, loss = 0.11643967\n",
      "Iteration 817, loss = 0.11635592\n",
      "Iteration 818, loss = 0.11627246\n",
      "Iteration 819, loss = 0.11618930\n",
      "Iteration 820, loss = 0.11610644\n",
      "Iteration 821, loss = 0.11602387\n",
      "Iteration 822, loss = 0.11594159\n",
      "Iteration 823, loss = 0.11585960\n",
      "Iteration 824, loss = 0.11577790\n",
      "Iteration 825, loss = 0.11569648\n",
      "Iteration 826, loss = 0.11561536\n",
      "Iteration 827, loss = 0.11553451\n",
      "Iteration 828, loss = 0.11545395\n",
      "Iteration 829, loss = 0.11537368\n",
      "Iteration 830, loss = 0.11529368\n",
      "Iteration 831, loss = 0.11521396\n",
      "Iteration 832, loss = 0.11513452\n",
      "Iteration 833, loss = 0.11505536\n",
      "Iteration 834, loss = 0.11497647\n",
      "Iteration 835, loss = 0.11489785\n",
      "Iteration 836, loss = 0.11481951\n",
      "Iteration 837, loss = 0.11474144\n",
      "Iteration 838, loss = 0.11466364\n",
      "Iteration 839, loss = 0.11458611\n",
      "Iteration 840, loss = 0.11450884\n",
      "Iteration 841, loss = 0.11443184\n",
      "Iteration 842, loss = 0.11435511\n",
      "Iteration 843, loss = 0.11427864\n",
      "Iteration 844, loss = 0.11420243\n",
      "Iteration 845, loss = 0.11412649\n",
      "Iteration 846, loss = 0.11405080\n",
      "Iteration 847, loss = 0.11397538\n",
      "Iteration 848, loss = 0.11390021\n",
      "Iteration 849, loss = 0.11382529\n",
      "Iteration 850, loss = 0.11375064\n",
      "Iteration 851, loss = 0.11367623\n",
      "Iteration 852, loss = 0.11360208\n",
      "Iteration 853, loss = 0.11352818\n",
      "Iteration 854, loss = 0.11345453\n",
      "Iteration 855, loss = 0.11338113\n",
      "Iteration 856, loss = 0.11330798\n",
      "Iteration 857, loss = 0.11323508\n",
      "Iteration 858, loss = 0.11316242\n",
      "Iteration 859, loss = 0.11309001\n",
      "Iteration 860, loss = 0.11301784\n",
      "Iteration 861, loss = 0.11294591\n",
      "Iteration 862, loss = 0.11287422\n",
      "Iteration 863, loss = 0.11280278\n",
      "Iteration 864, loss = 0.11273157\n",
      "Iteration 865, loss = 0.11266060\n",
      "Iteration 866, loss = 0.11258987\n",
      "Iteration 867, loss = 0.11251937\n",
      "Iteration 868, loss = 0.11244911\n",
      "Iteration 869, loss = 0.11237909\n",
      "Iteration 870, loss = 0.11230929\n",
      "Iteration 871, loss = 0.11223973\n",
      "Iteration 872, loss = 0.11217039\n",
      "Iteration 873, loss = 0.11210129\n",
      "Iteration 874, loss = 0.11203242\n",
      "Iteration 875, loss = 0.11196377\n",
      "Iteration 876, loss = 0.11189535\n",
      "Iteration 877, loss = 0.11182715\n",
      "Iteration 878, loss = 0.11175918\n",
      "Iteration 879, loss = 0.11169143\n",
      "Iteration 880, loss = 0.11162390\n",
      "Iteration 881, loss = 0.11155660\n",
      "Iteration 882, loss = 0.11148951\n",
      "Iteration 883, loss = 0.11142265\n",
      "Iteration 884, loss = 0.11135600\n",
      "Iteration 885, loss = 0.11128957\n",
      "Iteration 886, loss = 0.11122336\n",
      "Iteration 887, loss = 0.11115736\n",
      "Iteration 888, loss = 0.11109157\n",
      "Iteration 889, loss = 0.11102600\n",
      "Iteration 890, loss = 0.11096064\n",
      "Iteration 891, loss = 0.11089549\n",
      "Iteration 892, loss = 0.11083056\n",
      "Iteration 893, loss = 0.11076583\n",
      "Iteration 894, loss = 0.11070131\n",
      "Iteration 895, loss = 0.11063700\n",
      "Iteration 896, loss = 0.11057290\n",
      "Iteration 897, loss = 0.11050900\n",
      "Iteration 898, loss = 0.11044530\n",
      "Iteration 899, loss = 0.11038181\n",
      "Iteration 900, loss = 0.11031853\n",
      "Iteration 901, loss = 0.11025544\n",
      "Iteration 902, loss = 0.11019256\n",
      "Iteration 903, loss = 0.11012988\n",
      "Iteration 904, loss = 0.11006739\n",
      "Iteration 905, loss = 0.11000511\n",
      "Iteration 906, loss = 0.10994302\n",
      "Iteration 907, loss = 0.10988113\n",
      "Iteration 908, loss = 0.10981944\n",
      "Iteration 909, loss = 0.10975794\n",
      "Iteration 910, loss = 0.10969663\n",
      "Iteration 911, loss = 0.10963552\n",
      "Iteration 912, loss = 0.10957460\n",
      "Iteration 913, loss = 0.10951388\n",
      "Iteration 914, loss = 0.10945334\n",
      "Iteration 915, loss = 0.10939299\n",
      "Iteration 916, loss = 0.10933284\n",
      "Iteration 917, loss = 0.10927287\n",
      "Iteration 918, loss = 0.10921309\n",
      "Iteration 919, loss = 0.10915349\n",
      "Iteration 920, loss = 0.10909408\n",
      "Iteration 921, loss = 0.10903486\n",
      "Iteration 922, loss = 0.10897582\n",
      "Iteration 923, loss = 0.10891696\n",
      "Iteration 924, loss = 0.10885829\n",
      "Iteration 925, loss = 0.10879980\n",
      "Iteration 926, loss = 0.10874149\n",
      "Iteration 927, loss = 0.10868336\n",
      "Iteration 928, loss = 0.10862541\n",
      "Iteration 929, loss = 0.10856763\n",
      "Iteration 930, loss = 0.10851004\n",
      "Iteration 931, loss = 0.10845262\n",
      "Iteration 932, loss = 0.10839538\n",
      "Iteration 933, loss = 0.10833832\n",
      "Iteration 934, loss = 0.10828143\n",
      "Iteration 935, loss = 0.10822471\n",
      "Iteration 936, loss = 0.10816817\n",
      "Iteration 937, loss = 0.10811180\n",
      "Iteration 938, loss = 0.10805560\n",
      "Iteration 939, loss = 0.10799957\n",
      "Iteration 940, loss = 0.10794371\n",
      "Iteration 941, loss = 0.10788803\n",
      "Iteration 942, loss = 0.10783251\n",
      "Iteration 943, loss = 0.10777716\n",
      "Iteration 944, loss = 0.10772198\n",
      "Iteration 945, loss = 0.10766696\n",
      "Iteration 946, loss = 0.10761211\n",
      "Iteration 947, loss = 0.10755743\n",
      "Iteration 948, loss = 0.10750291\n",
      "Iteration 949, loss = 0.10744855\n",
      "Iteration 950, loss = 0.10739436\n",
      "Iteration 951, loss = 0.10734033\n",
      "Iteration 952, loss = 0.10728646\n",
      "Iteration 953, loss = 0.10723275\n",
      "Iteration 954, loss = 0.10717920\n",
      "Iteration 955, loss = 0.10712582\n",
      "Iteration 956, loss = 0.10707259\n",
      "Iteration 957, loss = 0.10701952\n",
      "Iteration 958, loss = 0.10696661\n",
      "Iteration 959, loss = 0.10691385\n",
      "Iteration 960, loss = 0.10686126\n",
      "Iteration 961, loss = 0.10680881\n",
      "Iteration 962, loss = 0.10675653\n",
      "Iteration 963, loss = 0.10670439\n",
      "Iteration 964, loss = 0.10665241\n",
      "Iteration 965, loss = 0.10660059\n",
      "Iteration 966, loss = 0.10654892\n",
      "Iteration 967, loss = 0.10649739\n",
      "Iteration 968, loss = 0.10644602\n",
      "Iteration 969, loss = 0.10639480\n",
      "Iteration 970, loss = 0.10634373\n",
      "Iteration 971, loss = 0.10629281\n",
      "Iteration 972, loss = 0.10624204\n",
      "Iteration 973, loss = 0.10619142\n",
      "Iteration 974, loss = 0.10614094\n",
      "Iteration 975, loss = 0.10609061\n",
      "Iteration 976, loss = 0.10604043\n",
      "Iteration 977, loss = 0.10599039\n",
      "Iteration 978, loss = 0.10594050\n",
      "Iteration 979, loss = 0.10589075\n",
      "Iteration 980, loss = 0.10584115\n",
      "Iteration 981, loss = 0.10579168\n",
      "Iteration 982, loss = 0.10574237\n",
      "Iteration 983, loss = 0.10569319\n",
      "Iteration 984, loss = 0.10564415\n",
      "Iteration 985, loss = 0.10559526\n",
      "Iteration 986, loss = 0.10554651\n",
      "Iteration 987, loss = 0.10549789\n",
      "Iteration 988, loss = 0.10544942\n",
      "Iteration 989, loss = 0.10540108\n",
      "Iteration 990, loss = 0.10535288\n",
      "Iteration 991, loss = 0.10530482\n",
      "Iteration 992, loss = 0.10525689\n",
      "Iteration 993, loss = 0.10520911\n",
      "Iteration 994, loss = 0.10516145\n",
      "Iteration 995, loss = 0.10511394\n",
      "Iteration 996, loss = 0.10506655\n",
      "Iteration 997, loss = 0.10501930\n",
      "Iteration 998, loss = 0.10497219\n",
      "Iteration 999, loss = 0.10492521\n",
      "Iteration 1000, loss = 0.10487836\n",
      "Iteration 1001, loss = 0.10483164\n",
      "Iteration 1002, loss = 0.10478505\n",
      "Iteration 1003, loss = 0.10473859\n",
      "Iteration 1004, loss = 0.10469227\n",
      "Iteration 1005, loss = 0.10464607\n",
      "Iteration 1006, loss = 0.10460000\n",
      "Iteration 1007, loss = 0.10455407\n",
      "Iteration 1008, loss = 0.10450825\n",
      "Iteration 1009, loss = 0.10446257\n",
      "Iteration 1010, loss = 0.10441702\n",
      "Iteration 1011, loss = 0.10437159\n",
      "Iteration 1012, loss = 0.10432628\n",
      "Iteration 1013, loss = 0.10428111\n",
      "Iteration 1014, loss = 0.10423605\n",
      "Iteration 1015, loss = 0.10419112\n",
      "Iteration 1016, loss = 0.10414632\n",
      "Iteration 1017, loss = 0.10410164\n",
      "Iteration 1018, loss = 0.10405708\n",
      "Iteration 1019, loss = 0.10401265\n",
      "Iteration 1020, loss = 0.10396833\n",
      "Iteration 1021, loss = 0.10392414\n",
      "Iteration 1022, loss = 0.10388007\n",
      "Iteration 1023, loss = 0.10383612\n",
      "Iteration 1024, loss = 0.10379229\n",
      "Iteration 1025, loss = 0.10374858\n",
      "Iteration 1026, loss = 0.10370499\n",
      "Iteration 1027, loss = 0.10366152\n",
      "Iteration 1028, loss = 0.10361816\n",
      "Iteration 1029, loss = 0.10357492\n",
      "Iteration 1030, loss = 0.10353180\n",
      "Iteration 1031, loss = 0.10348880\n",
      "Iteration 1032, loss = 0.10344591\n",
      "Iteration 1033, loss = 0.10340314\n",
      "Iteration 1034, loss = 0.10336049\n",
      "Iteration 1035, loss = 0.10331795\n",
      "Iteration 1036, loss = 0.10327552\n",
      "Iteration 1037, loss = 0.10323321\n",
      "Iteration 1038, loss = 0.10319101\n",
      "Iteration 1039, loss = 0.10314892\n",
      "Iteration 1040, loss = 0.10310695\n",
      "Iteration 1041, loss = 0.10306509\n",
      "Iteration 1042, loss = 0.10302334\n",
      "Iteration 1043, loss = 0.10298170\n",
      "Iteration 1044, loss = 0.10294017\n",
      "Iteration 1045, loss = 0.10289876\n",
      "Iteration 1046, loss = 0.10285745\n",
      "Iteration 1047, loss = 0.10281625\n",
      "Iteration 1048, loss = 0.10277516\n",
      "Iteration 1049, loss = 0.10273418\n",
      "Iteration 1050, loss = 0.10269331\n",
      "Iteration 1051, loss = 0.10265255\n",
      "Iteration 1052, loss = 0.10261189\n",
      "Iteration 1053, loss = 0.10257134\n",
      "Iteration 1054, loss = 0.10253090\n",
      "Iteration 1055, loss = 0.10249056\n",
      "Iteration 1056, loss = 0.10245033\n",
      "Iteration 1057, loss = 0.10241020\n",
      "Iteration 1058, loss = 0.10237018\n",
      "Iteration 1059, loss = 0.10233027\n",
      "Iteration 1060, loss = 0.10229045\n",
      "Iteration 1061, loss = 0.10225074\n",
      "Iteration 1062, loss = 0.10221114\n",
      "Iteration 1063, loss = 0.10217163\n",
      "Iteration 1064, loss = 0.10213223\n",
      "Iteration 1065, loss = 0.10209294\n",
      "Iteration 1066, loss = 0.10205374\n",
      "Iteration 1067, loss = 0.10201464\n",
      "Iteration 1068, loss = 0.10197565\n",
      "Iteration 1069, loss = 0.10193675\n",
      "Iteration 1070, loss = 0.10189796\n",
      "Iteration 1071, loss = 0.10185926\n",
      "Iteration 1072, loss = 0.10182067\n",
      "Iteration 1073, loss = 0.10178217\n",
      "Iteration 1074, loss = 0.10174377\n",
      "Iteration 1075, loss = 0.10170547\n",
      "Iteration 1076, loss = 0.10166727\n",
      "Iteration 1077, loss = 0.10162916\n",
      "Iteration 1078, loss = 0.10159116\n",
      "Iteration 1079, loss = 0.10155324\n",
      "Iteration 1080, loss = 0.10151543\n",
      "Iteration 1081, loss = 0.10147771\n",
      "Iteration 1082, loss = 0.10144008\n",
      "Iteration 1083, loss = 0.10140256\n",
      "Iteration 1084, loss = 0.10136512\n",
      "Iteration 1085, loss = 0.10132778\n",
      "Iteration 1086, loss = 0.10129054\n",
      "Iteration 1087, loss = 0.10125338\n",
      "Iteration 1088, loss = 0.10121632\n",
      "Iteration 1089, loss = 0.10117936\n",
      "Iteration 1090, loss = 0.10114248\n",
      "Iteration 1091, loss = 0.10110570\n",
      "Iteration 1092, loss = 0.10106901\n",
      "Iteration 1093, loss = 0.10103242\n",
      "Iteration 1094, loss = 0.10099591\n",
      "Iteration 1095, loss = 0.10095949\n",
      "Iteration 1096, loss = 0.10092317\n",
      "Iteration 1097, loss = 0.10088693\n",
      "Iteration 1098, loss = 0.10085079\n",
      "Iteration 1099, loss = 0.10081473\n",
      "Iteration 1100, loss = 0.10077876\n",
      "Iteration 1101, loss = 0.10074288\n",
      "Iteration 1102, loss = 0.10070709\n",
      "Iteration 1103, loss = 0.10067139\n",
      "Iteration 1104, loss = 0.10063578\n",
      "Iteration 1105, loss = 0.10060025\n",
      "Iteration 1106, loss = 0.10056481\n",
      "Iteration 1107, loss = 0.10052946\n",
      "Iteration 1108, loss = 0.10049419\n",
      "Iteration 1109, loss = 0.10045901\n",
      "Iteration 1110, loss = 0.10042392\n",
      "Iteration 1111, loss = 0.10038891\n",
      "Iteration 1112, loss = 0.10035398\n",
      "Iteration 1113, loss = 0.10031914\n",
      "Iteration 1114, loss = 0.10028439\n",
      "Iteration 1115, loss = 0.10024972\n",
      "Iteration 1116, loss = 0.10021513\n",
      "Iteration 1117, loss = 0.10018063\n",
      "Iteration 1118, loss = 0.10014621\n",
      "Iteration 1119, loss = 0.10011187\n",
      "Iteration 1120, loss = 0.10007761\n",
      "Iteration 1121, loss = 0.10004344\n",
      "Iteration 1122, loss = 0.10000935\n",
      "Iteration 1123, loss = 0.09997534\n",
      "Iteration 1124, loss = 0.09994141\n",
      "Iteration 1125, loss = 0.09990757\n",
      "Iteration 1126, loss = 0.09987380\n",
      "Iteration 1127, loss = 0.09984012\n",
      "Iteration 1128, loss = 0.09980651\n",
      "Iteration 1129, loss = 0.09977298\n",
      "Iteration 1130, loss = 0.09973954\n",
      "Iteration 1131, loss = 0.09970617\n",
      "Iteration 1132, loss = 0.09967288\n",
      "Iteration 1133, loss = 0.09963967\n",
      "Iteration 1134, loss = 0.09960654\n",
      "Iteration 1135, loss = 0.09957349\n",
      "Iteration 1136, loss = 0.09954051\n",
      "Iteration 1137, loss = 0.09950762\n",
      "Iteration 1138, loss = 0.09947480\n",
      "Iteration 1139, loss = 0.09944205\n",
      "Iteration 1140, loss = 0.09940938\n",
      "Iteration 1141, loss = 0.09937679\n",
      "Iteration 1142, loss = 0.09934428\n",
      "Iteration 1143, loss = 0.09931184\n",
      "Iteration 1144, loss = 0.09927948\n",
      "Iteration 1145, loss = 0.09924719\n",
      "Iteration 1146, loss = 0.09921497\n",
      "Iteration 1147, loss = 0.09918284\n",
      "Iteration 1148, loss = 0.09915077\n",
      "Iteration 1149, loss = 0.09911878\n",
      "Iteration 1150, loss = 0.09908686\n",
      "Iteration 1151, loss = 0.09905502\n",
      "Iteration 1152, loss = 0.09902325\n",
      "Iteration 1153, loss = 0.09899155\n",
      "Iteration 1154, loss = 0.09895993\n",
      "Iteration 1155, loss = 0.09892838\n",
      "Iteration 1156, loss = 0.09889690\n",
      "Iteration 1157, loss = 0.09886549\n",
      "Iteration 1158, loss = 0.09883415\n",
      "Iteration 1159, loss = 0.09880289\n",
      "Iteration 1160, loss = 0.09877170\n",
      "Iteration 1161, loss = 0.09874057\n",
      "Iteration 1162, loss = 0.09870952\n",
      "Iteration 1163, loss = 0.09867854\n",
      "Iteration 1164, loss = 0.09864763\n",
      "Iteration 1165, loss = 0.09861679\n",
      "Iteration 1166, loss = 0.09858601\n",
      "Iteration 1167, loss = 0.09855531\n",
      "Iteration 1168, loss = 0.09852468\n",
      "Iteration 1169, loss = 0.09849411\n",
      "Iteration 1170, loss = 0.09846361\n",
      "Iteration 1171, loss = 0.09843319\n",
      "Iteration 1172, loss = 0.09840283\n",
      "Iteration 1173, loss = 0.09837253\n",
      "Iteration 1174, loss = 0.09834231\n",
      "Iteration 1175, loss = 0.09831215\n",
      "Iteration 1176, loss = 0.09828206\n",
      "Iteration 1177, loss = 0.09825204\n",
      "Iteration 1178, loss = 0.09822208\n",
      "Iteration 1179, loss = 0.09819219\n",
      "Iteration 1180, loss = 0.09816237\n",
      "Iteration 1181, loss = 0.09813261\n",
      "Iteration 1182, loss = 0.09810291\n",
      "Iteration 1183, loss = 0.09807329\n",
      "Iteration 1184, loss = 0.09804372\n",
      "Iteration 1185, loss = 0.09801423\n",
      "Iteration 1186, loss = 0.09798479\n",
      "Iteration 1187, loss = 0.09795542\n",
      "Iteration 1188, loss = 0.09792612\n",
      "Iteration 1189, loss = 0.09789688\n",
      "Iteration 1190, loss = 0.09786770\n",
      "Iteration 1191, loss = 0.09783859\n",
      "Iteration 1192, loss = 0.09780954\n",
      "Iteration 1193, loss = 0.09778055\n",
      "Iteration 1194, loss = 0.09775163\n",
      "Iteration 1195, loss = 0.09772277\n",
      "Iteration 1196, loss = 0.09769397\n",
      "Iteration 1197, loss = 0.09766523\n",
      "Iteration 1198, loss = 0.09763656\n",
      "Iteration 1199, loss = 0.09760794\n",
      "Iteration 1200, loss = 0.09757939\n",
      "Iteration 1201, loss = 0.09755090\n",
      "Iteration 1202, loss = 0.09752247\n",
      "Iteration 1203, loss = 0.09749410\n",
      "Iteration 1204, loss = 0.09746580\n",
      "Iteration 1205, loss = 0.09743755\n",
      "Iteration 1206, loss = 0.09740936\n",
      "Iteration 1207, loss = 0.09738123\n",
      "Iteration 1208, loss = 0.09735317\n",
      "Iteration 1209, loss = 0.09732516\n",
      "Iteration 1210, loss = 0.09729721\n",
      "Iteration 1211, loss = 0.09726932\n",
      "Iteration 1212, loss = 0.09724149\n",
      "Iteration 1213, loss = 0.09721372\n",
      "Iteration 1214, loss = 0.09718600\n",
      "Iteration 1215, loss = 0.09715835\n",
      "Iteration 1216, loss = 0.09713075\n",
      "Iteration 1217, loss = 0.09710321\n",
      "Iteration 1218, loss = 0.09707573\n",
      "Iteration 1219, loss = 0.09704830\n",
      "Iteration 1220, loss = 0.09702094\n",
      "Iteration 1221, loss = 0.09699363\n",
      "Iteration 1222, loss = 0.09696637\n",
      "Iteration 1223, loss = 0.09693918\n",
      "Iteration 1224, loss = 0.09691204\n",
      "Iteration 1225, loss = 0.09688495\n",
      "Iteration 1226, loss = 0.09685792\n",
      "Iteration 1227, loss = 0.09683095\n",
      "Iteration 1228, loss = 0.09680404\n",
      "Iteration 1229, loss = 0.09677717\n",
      "Iteration 1230, loss = 0.09675037\n",
      "Iteration 1231, loss = 0.09672362\n",
      "Iteration 1232, loss = 0.09669692\n",
      "Iteration 1233, loss = 0.09667028\n",
      "Iteration 1234, loss = 0.09664369\n",
      "Iteration 1235, loss = 0.09661716\n",
      "Iteration 1236, loss = 0.09659068\n",
      "Iteration 1237, loss = 0.09656426\n",
      "Iteration 1238, loss = 0.09653789\n",
      "Iteration 1239, loss = 0.09651157\n",
      "Iteration 1240, loss = 0.09648530\n",
      "Iteration 1241, loss = 0.09645909\n",
      "Iteration 1242, loss = 0.09643294\n",
      "Iteration 1243, loss = 0.09640683\n",
      "Iteration 1244, loss = 0.09638078\n",
      "Iteration 1245, loss = 0.09635478\n",
      "Iteration 1246, loss = 0.09632883\n",
      "Iteration 1247, loss = 0.09630293\n",
      "Iteration 1248, loss = 0.09627709\n",
      "Iteration 1249, loss = 0.09625130\n",
      "Iteration 1250, loss = 0.09622555\n",
      "Iteration 1251, loss = 0.09619987\n",
      "Iteration 1252, loss = 0.09617423\n",
      "Iteration 1253, loss = 0.09614864\n",
      "Iteration 1254, loss = 0.09612310\n",
      "Iteration 1255, loss = 0.09609762\n",
      "Iteration 1256, loss = 0.09607218\n",
      "Iteration 1257, loss = 0.09604679\n",
      "Iteration 1258, loss = 0.09602146\n",
      "Iteration 1259, loss = 0.09599617\n",
      "Iteration 1260, loss = 0.09597094\n",
      "Iteration 1261, loss = 0.09594575\n",
      "Iteration 1262, loss = 0.09592061\n",
      "Iteration 1263, loss = 0.09589552\n",
      "Iteration 1264, loss = 0.09587049\n",
      "Iteration 1265, loss = 0.09584549\n",
      "Iteration 1266, loss = 0.09582055\n",
      "Iteration 1267, loss = 0.09579566\n",
      "Iteration 1268, loss = 0.09577082\n",
      "Iteration 1269, loss = 0.09574602\n",
      "Iteration 1270, loss = 0.09572127\n",
      "Iteration 1271, loss = 0.09569657\n",
      "Iteration 1272, loss = 0.09567192\n",
      "Iteration 1273, loss = 0.09564731\n",
      "Iteration 1274, loss = 0.09562275\n",
      "Iteration 1275, loss = 0.09559824\n",
      "Iteration 1276, loss = 0.09557378\n",
      "Iteration 1277, loss = 0.09554936\n",
      "Iteration 1278, loss = 0.09552499\n",
      "Iteration 1279, loss = 0.09550067\n",
      "Iteration 1280, loss = 0.09547639\n",
      "Iteration 1281, loss = 0.09545216\n",
      "Iteration 1282, loss = 0.09542798\n",
      "Iteration 1283, loss = 0.09540384\n",
      "Iteration 1284, loss = 0.09537974\n",
      "Iteration 1285, loss = 0.09535570\n",
      "Iteration 1286, loss = 0.09533169\n",
      "Iteration 1287, loss = 0.09530774\n",
      "Iteration 1288, loss = 0.09528382\n",
      "Iteration 1289, loss = 0.09525996\n",
      "Iteration 1290, loss = 0.09523613\n",
      "Iteration 1291, loss = 0.09521236\n",
      "Iteration 1292, loss = 0.09518862\n",
      "Iteration 1293, loss = 0.09516493\n",
      "Iteration 1294, loss = 0.09514129\n",
      "Iteration 1295, loss = 0.09511769\n",
      "Iteration 1296, loss = 0.09509413\n",
      "Iteration 1297, loss = 0.09507062\n",
      "Iteration 1298, loss = 0.09504715\n",
      "Iteration 1299, loss = 0.09502373\n",
      "Iteration 1300, loss = 0.09500034\n",
      "Iteration 1301, loss = 0.09497700\n",
      "Iteration 1302, loss = 0.09495371\n",
      "Iteration 1303, loss = 0.09493045\n",
      "Iteration 1304, loss = 0.09490724\n",
      "Iteration 1305, loss = 0.09488407\n",
      "Iteration 1306, loss = 0.09486095\n",
      "Iteration 1307, loss = 0.09483787\n",
      "Iteration 1308, loss = 0.09481482\n",
      "Iteration 1309, loss = 0.09479182\n",
      "Iteration 1310, loss = 0.09476887\n",
      "Iteration 1311, loss = 0.09474595\n",
      "Iteration 1312, loss = 0.09472308\n",
      "Iteration 1313, loss = 0.09470024\n",
      "Iteration 1314, loss = 0.09467745\n",
      "Iteration 1315, loss = 0.09465470\n",
      "Iteration 1316, loss = 0.09463199\n",
      "Iteration 1317, loss = 0.09460932\n",
      "Iteration 1318, loss = 0.09458670\n",
      "Iteration 1319, loss = 0.09456411\n",
      "Iteration 1320, loss = 0.09454156\n",
      "Iteration 1321, loss = 0.09451906\n",
      "Iteration 1322, loss = 0.09449659\n",
      "Iteration 1323, loss = 0.09447416\n",
      "Iteration 1324, loss = 0.09445178\n",
      "Iteration 1325, loss = 0.09442943\n",
      "Iteration 1326, loss = 0.09440712\n",
      "Iteration 1327, loss = 0.09438486\n",
      "Iteration 1328, loss = 0.09436263\n",
      "Iteration 1329, loss = 0.09434044\n",
      "Iteration 1330, loss = 0.09431829\n",
      "Iteration 1331, loss = 0.09429618\n",
      "Iteration 1332, loss = 0.09427411\n",
      "Iteration 1333, loss = 0.09425207\n",
      "Iteration 1334, loss = 0.09423008\n",
      "Iteration 1335, loss = 0.09420812\n",
      "Iteration 1336, loss = 0.09418620\n",
      "Iteration 1337, loss = 0.09416432\n",
      "Iteration 1338, loss = 0.09414248\n",
      "Iteration 1339, loss = 0.09412068\n",
      "Iteration 1340, loss = 0.09409891\n",
      "Iteration 1341, loss = 0.09407718\n",
      "Iteration 1342, loss = 0.09405549\n",
      "Iteration 1343, loss = 0.09403384\n",
      "Iteration 1344, loss = 0.09401222\n",
      "Iteration 1345, loss = 0.09399065\n",
      "Iteration 1346, loss = 0.09396910\n",
      "Iteration 1347, loss = 0.09394760\n",
      "Iteration 1348, loss = 0.09392613\n",
      "Iteration 1349, loss = 0.09390470\n",
      "Iteration 1350, loss = 0.09388331\n",
      "Iteration 1351, loss = 0.09386195\n",
      "Iteration 1352, loss = 0.09384062\n",
      "Iteration 1353, loss = 0.09381934\n",
      "Iteration 1354, loss = 0.09379809\n",
      "Iteration 1355, loss = 0.09377688\n",
      "Iteration 1356, loss = 0.09375570\n",
      "Iteration 1357, loss = 0.09373456\n",
      "Iteration 1358, loss = 0.09371345\n",
      "Iteration 1359, loss = 0.09369238\n",
      "Iteration 1360, loss = 0.09367134\n",
      "Iteration 1361, loss = 0.09365034\n",
      "Iteration 1362, loss = 0.09362938\n",
      "Iteration 1363, loss = 0.09360845\n",
      "Iteration 1364, loss = 0.09358755\n",
      "Iteration 1365, loss = 0.09356669\n",
      "Iteration 1366, loss = 0.09354586\n",
      "Iteration 1367, loss = 0.09352507\n",
      "Iteration 1368, loss = 0.09350431\n",
      "Iteration 1369, loss = 0.09348359\n",
      "Iteration 1370, loss = 0.09346290\n",
      "Iteration 1371, loss = 0.09344225\n",
      "Iteration 1372, loss = 0.09342163\n",
      "Iteration 1373, loss = 0.09340104\n",
      "Iteration 1374, loss = 0.09338049\n",
      "Iteration 1375, loss = 0.09335997\n",
      "Iteration 1376, loss = 0.09333948\n",
      "Iteration 1377, loss = 0.09331903\n",
      "Iteration 1378, loss = 0.09329861\n",
      "Iteration 1379, loss = 0.09327823\n",
      "Iteration 1380, loss = 0.09325787\n",
      "Iteration 1381, loss = 0.09323755\n",
      "Iteration 1382, loss = 0.09321727\n",
      "Iteration 1383, loss = 0.09319701\n",
      "Iteration 1384, loss = 0.09317679\n",
      "Iteration 1385, loss = 0.09315660\n",
      "Iteration 1386, loss = 0.09313645\n",
      "Iteration 1387, loss = 0.09311632\n",
      "Iteration 1388, loss = 0.09309623\n",
      "Iteration 1389, loss = 0.09307617\n",
      "Iteration 1390, loss = 0.09305614\n",
      "Iteration 1391, loss = 0.09303615\n",
      "Iteration 1392, loss = 0.09301618\n",
      "Iteration 1393, loss = 0.09299625\n",
      "Iteration 1394, loss = 0.09297635\n",
      "Iteration 1395, loss = 0.09295648\n",
      "Iteration 1396, loss = 0.09293664\n",
      "Iteration 1397, loss = 0.09291684\n",
      "Iteration 1398, loss = 0.09289706\n",
      "Iteration 1399, loss = 0.09287732\n",
      "Iteration 1400, loss = 0.09285761\n",
      "Iteration 1401, loss = 0.09283792\n",
      "Iteration 1402, loss = 0.09281827\n",
      "Iteration 1403, loss = 0.09279865\n",
      "Iteration 1404, loss = 0.09277906\n",
      "Iteration 1405, loss = 0.09275950\n",
      "Iteration 1406, loss = 0.09273997\n",
      "Iteration 1407, loss = 0.09272047\n",
      "Iteration 1408, loss = 0.09270100\n",
      "Iteration 1409, loss = 0.09268156\n",
      "Iteration 1410, loss = 0.09266215\n",
      "Iteration 1411, loss = 0.09264278\n",
      "Iteration 1412, loss = 0.09262343\n",
      "Iteration 1413, loss = 0.09260411\n",
      "Iteration 1414, loss = 0.09258482\n",
      "Iteration 1415, loss = 0.09256556\n",
      "Iteration 1416, loss = 0.09254633\n",
      "Iteration 1417, loss = 0.09252712\n",
      "Iteration 1418, loss = 0.09250795\n",
      "Iteration 1419, loss = 0.09248881\n",
      "Iteration 1420, loss = 0.09246969\n",
      "Iteration 1421, loss = 0.09245061\n",
      "Iteration 1422, loss = 0.09243155\n",
      "Iteration 1423, loss = 0.09241252\n",
      "Iteration 1424, loss = 0.09239352\n",
      "Iteration 1425, loss = 0.09237455\n",
      "Iteration 1426, loss = 0.09235561\n",
      "Iteration 1427, loss = 0.09233670\n",
      "Iteration 1428, loss = 0.09231781\n",
      "Iteration 1429, loss = 0.09229895\n",
      "Iteration 1430, loss = 0.09228013\n",
      "Iteration 1431, loss = 0.09226132\n",
      "Iteration 1432, loss = 0.09224255\n",
      "Iteration 1433, loss = 0.09222381\n",
      "Iteration 1434, loss = 0.09220509\n",
      "Iteration 1435, loss = 0.09218640\n",
      "Iteration 1436, loss = 0.09216774\n",
      "Iteration 1437, loss = 0.09214910\n",
      "Iteration 1438, loss = 0.09213049\n",
      "Iteration 1439, loss = 0.09211191\n",
      "Iteration 1440, loss = 0.09209336\n",
      "Iteration 1441, loss = 0.09207483\n",
      "Iteration 1442, loss = 0.09205633\n",
      "Iteration 1443, loss = 0.09203786\n",
      "Iteration 1444, loss = 0.09201942\n",
      "Iteration 1445, loss = 0.09200100\n",
      "Iteration 1446, loss = 0.09198261\n",
      "Iteration 1447, loss = 0.09196424\n",
      "Iteration 1448, loss = 0.09194590\n",
      "Iteration 1449, loss = 0.09192759\n",
      "Iteration 1450, loss = 0.09190930\n",
      "Iteration 1451, loss = 0.09189104\n",
      "Iteration 1452, loss = 0.09187281\n",
      "Iteration 1453, loss = 0.09185460\n",
      "Iteration 1454, loss = 0.09183642\n",
      "Iteration 1455, loss = 0.09181827\n",
      "Iteration 1456, loss = 0.09180014\n",
      "Iteration 1457, loss = 0.09178203\n",
      "Iteration 1458, loss = 0.09176395\n",
      "Iteration 1459, loss = 0.09174590\n",
      "Iteration 1460, loss = 0.09172787\n",
      "Iteration 1461, loss = 0.09170987\n",
      "Iteration 1462, loss = 0.09169190\n",
      "Iteration 1463, loss = 0.09167394\n",
      "Iteration 1464, loss = 0.09165602\n",
      "Iteration 1465, loss = 0.09163812\n",
      "Iteration 1466, loss = 0.09162024\n",
      "Iteration 1467, loss = 0.09160239\n",
      "Iteration 1468, loss = 0.09158457\n",
      "Iteration 1469, loss = 0.09156676\n",
      "Iteration 1470, loss = 0.09154899\n",
      "Iteration 1471, loss = 0.09153124\n",
      "Iteration 1472, loss = 0.09151351\n",
      "Iteration 1473, loss = 0.09149581\n",
      "Iteration 1474, loss = 0.09147813\n",
      "Iteration 1475, loss = 0.09146047\n",
      "Iteration 1476, loss = 0.09144284\n",
      "Iteration 1477, loss = 0.09142524\n",
      "Iteration 1478, loss = 0.09140766\n",
      "Iteration 1479, loss = 0.09139010\n",
      "Iteration 1480, loss = 0.09137256\n",
      "Iteration 1481, loss = 0.09135505\n",
      "Iteration 1482, loss = 0.09133757\n",
      "Iteration 1483, loss = 0.09132011\n",
      "Iteration 1484, loss = 0.09130267\n",
      "Iteration 1485, loss = 0.09128525\n",
      "Iteration 1486, loss = 0.09126786\n",
      "Iteration 1487, loss = 0.09125049\n",
      "Iteration 1488, loss = 0.09123315\n",
      "Iteration 1489, loss = 0.09121582\n",
      "Iteration 1490, loss = 0.09119853\n",
      "Iteration 1491, loss = 0.09118125\n",
      "Iteration 1492, loss = 0.09116400\n",
      "Iteration 1493, loss = 0.09114677\n",
      "Iteration 1494, loss = 0.09112956\n",
      "Iteration 1495, loss = 0.09111238\n",
      "Iteration 1496, loss = 0.09109522\n",
      "Iteration 1497, loss = 0.09107808\n",
      "Iteration 1498, loss = 0.09106096\n",
      "Iteration 1499, loss = 0.09104387\n",
      "Iteration 1500, loss = 0.09102680\n",
      "Iteration 1501, loss = 0.09100975\n",
      "Iteration 1502, loss = 0.09099272\n",
      "Iteration 1503, loss = 0.09097572\n",
      "Iteration 1504, loss = 0.09095874\n",
      "Iteration 1505, loss = 0.09094178\n",
      "Iteration 1506, loss = 0.09092484\n",
      "Iteration 1507, loss = 0.09090793\n",
      "Iteration 1508, loss = 0.09089103\n",
      "Iteration 1509, loss = 0.09087416\n",
      "Iteration 1510, loss = 0.09085731\n",
      "Iteration 1511, loss = 0.09084048\n",
      "Iteration 1512, loss = 0.09082368\n",
      "Iteration 1513, loss = 0.09080689\n",
      "Iteration 1514, loss = 0.09079013\n",
      "Iteration 1515, loss = 0.09077339\n",
      "Iteration 1516, loss = 0.09075666\n",
      "Iteration 1517, loss = 0.09073996\n",
      "Iteration 1518, loss = 0.09072329\n",
      "Iteration 1519, loss = 0.09070663\n",
      "Iteration 1520, loss = 0.09068999\n",
      "Iteration 1521, loss = 0.09067338\n",
      "Iteration 1522, loss = 0.09065678\n",
      "Iteration 1523, loss = 0.09064021\n",
      "Iteration 1524, loss = 0.09062366\n",
      "Iteration 1525, loss = 0.09060713\n",
      "Iteration 1526, loss = 0.09059062\n",
      "Iteration 1527, loss = 0.09057413\n",
      "Iteration 1528, loss = 0.09055766\n",
      "Iteration 1529, loss = 0.09054121\n",
      "Iteration 1530, loss = 0.09052478\n",
      "Iteration 1531, loss = 0.09050837\n",
      "Iteration 1532, loss = 0.09049198\n",
      "Iteration 1533, loss = 0.09047561\n",
      "Iteration 1534, loss = 0.09045927\n",
      "Iteration 1535, loss = 0.09044294\n",
      "Iteration 1536, loss = 0.09042663\n",
      "Iteration 1537, loss = 0.09041034\n",
      "Iteration 1538, loss = 0.09039408\n",
      "Iteration 1539, loss = 0.09037783\n",
      "Iteration 1540, loss = 0.09036160\n",
      "Iteration 1541, loss = 0.09034539\n",
      "Iteration 1542, loss = 0.09032920\n",
      "Iteration 1543, loss = 0.09031303\n",
      "Iteration 1544, loss = 0.09029688\n",
      "Iteration 1545, loss = 0.09028076\n",
      "Iteration 1546, loss = 0.09026464\n",
      "Iteration 1547, loss = 0.09024855\n",
      "Iteration 1548, loss = 0.09023248\n",
      "Iteration 1549, loss = 0.09021643\n",
      "Iteration 1550, loss = 0.09020040\n",
      "Iteration 1551, loss = 0.09018438\n",
      "Iteration 1552, loss = 0.09016839\n",
      "Iteration 1553, loss = 0.09015241\n",
      "Iteration 1554, loss = 0.09013646\n",
      "Iteration 1555, loss = 0.09012052\n",
      "Iteration 1556, loss = 0.09010460\n",
      "Iteration 1557, loss = 0.09008870\n",
      "Iteration 1558, loss = 0.09007282\n",
      "Iteration 1559, loss = 0.09005695\n",
      "Iteration 1560, loss = 0.09004111\n",
      "Iteration 1561, loss = 0.09002528\n",
      "Iteration 1562, loss = 0.09000948\n",
      "Iteration 1563, loss = 0.08999369\n",
      "Iteration 1564, loss = 0.08997792\n",
      "Iteration 1565, loss = 0.08996217\n",
      "Iteration 1566, loss = 0.08994643\n",
      "Iteration 1567, loss = 0.08993072\n",
      "Iteration 1568, loss = 0.08991502\n",
      "Iteration 1569, loss = 0.08989934\n",
      "Iteration 1570, loss = 0.08988368\n",
      "Iteration 1571, loss = 0.08986803\n",
      "Iteration 1572, loss = 0.08985241\n",
      "Iteration 1573, loss = 0.08983680\n",
      "Iteration 1574, loss = 0.08982121\n",
      "Iteration 1575, loss = 0.08980564\n",
      "Iteration 1576, loss = 0.08979009\n",
      "Iteration 1577, loss = 0.08977455\n",
      "Iteration 1578, loss = 0.08975903\n",
      "Iteration 1579, loss = 0.08974353\n",
      "Iteration 1580, loss = 0.08972805\n",
      "Iteration 1581, loss = 0.08971258\n",
      "Iteration 1582, loss = 0.08969713\n",
      "Iteration 1583, loss = 0.08968170\n",
      "Iteration 1584, loss = 0.08966629\n",
      "Iteration 1585, loss = 0.08965089\n",
      "Iteration 1586, loss = 0.08963551\n",
      "Iteration 1587, loss = 0.08962015\n",
      "Iteration 1588, loss = 0.08960480\n",
      "Iteration 1589, loss = 0.08958947\n",
      "Iteration 1590, loss = 0.08957416\n",
      "Iteration 1591, loss = 0.08955887\n",
      "Iteration 1592, loss = 0.08954359\n",
      "Iteration 1593, loss = 0.08952833\n",
      "Iteration 1594, loss = 0.08951308\n",
      "Iteration 1595, loss = 0.08949786\n",
      "Iteration 1596, loss = 0.08948265\n",
      "Iteration 1597, loss = 0.08946745\n",
      "Iteration 1598, loss = 0.08945227\n",
      "Iteration 1599, loss = 0.08943711\n",
      "Iteration 1600, loss = 0.08942197\n",
      "Iteration 1601, loss = 0.08940684\n",
      "Iteration 1602, loss = 0.08939173\n",
      "Iteration 1603, loss = 0.08937663\n",
      "Iteration 1604, loss = 0.08936156\n",
      "Iteration 1605, loss = 0.08934649\n",
      "Iteration 1606, loss = 0.08933145\n",
      "Iteration 1607, loss = 0.08931642\n",
      "Iteration 1608, loss = 0.08930140\n",
      "Iteration 1609, loss = 0.08928640\n",
      "Iteration 1610, loss = 0.08927142\n",
      "Iteration 1611, loss = 0.08925645\n",
      "Iteration 1612, loss = 0.08924150\n",
      "Iteration 1613, loss = 0.08922657\n",
      "Iteration 1614, loss = 0.08921165\n",
      "Iteration 1615, loss = 0.08919675\n",
      "Iteration 1616, loss = 0.08918186\n",
      "Iteration 1617, loss = 0.08916699\n",
      "Iteration 1618, loss = 0.08915213\n",
      "Iteration 1619, loss = 0.08913729\n",
      "Iteration 1620, loss = 0.08912246\n",
      "Iteration 1621, loss = 0.08910765\n",
      "Iteration 1622, loss = 0.08909286\n",
      "Iteration 1623, loss = 0.08907808\n",
      "Iteration 1624, loss = 0.08906332\n",
      "Iteration 1625, loss = 0.08904857\n",
      "Iteration 1626, loss = 0.08903383\n",
      "Iteration 1627, loss = 0.08901912\n",
      "Iteration 1628, loss = 0.08900441\n",
      "Iteration 1629, loss = 0.08898973\n",
      "Iteration 1630, loss = 0.08897505\n",
      "Iteration 1631, loss = 0.08896040\n",
      "Iteration 1632, loss = 0.08894575\n",
      "Iteration 1633, loss = 0.08893112\n",
      "Iteration 1634, loss = 0.08891651\n",
      "Iteration 1635, loss = 0.08890191\n",
      "Iteration 1636, loss = 0.08888733\n",
      "Iteration 1637, loss = 0.08887276\n",
      "Iteration 1638, loss = 0.08885821\n",
      "Iteration 1639, loss = 0.08884367\n",
      "Iteration 1640, loss = 0.08882914\n",
      "Iteration 1641, loss = 0.08881463\n",
      "Iteration 1642, loss = 0.08880014\n",
      "Iteration 1643, loss = 0.08878566\n",
      "Iteration 1644, loss = 0.08877119\n",
      "Iteration 1645, loss = 0.08875674\n",
      "Iteration 1646, loss = 0.08874230\n",
      "Iteration 1647, loss = 0.08872788\n",
      "Iteration 1648, loss = 0.08871347\n",
      "Iteration 1649, loss = 0.08869907\n",
      "Iteration 1650, loss = 0.08868469\n",
      "Iteration 1651, loss = 0.08867032\n",
      "Iteration 1652, loss = 0.08865597\n",
      "Iteration 1653, loss = 0.08864163\n",
      "Iteration 1654, loss = 0.08862731\n",
      "Iteration 1655, loss = 0.08861299\n",
      "Iteration 1656, loss = 0.08859870\n",
      "Iteration 1657, loss = 0.08858441\n",
      "Iteration 1658, loss = 0.08857015\n",
      "Iteration 1659, loss = 0.08855589\n",
      "Iteration 1660, loss = 0.08854165\n",
      "Iteration 1661, loss = 0.08852742\n",
      "Iteration 1662, loss = 0.08851321\n",
      "Iteration 1663, loss = 0.08849901\n",
      "Iteration 1664, loss = 0.08848482\n",
      "Iteration 1665, loss = 0.08847065\n",
      "Iteration 1666, loss = 0.08845649\n",
      "Iteration 1667, loss = 0.08844234\n",
      "Iteration 1668, loss = 0.08842821\n",
      "Iteration 1669, loss = 0.08841409\n",
      "Iteration 1670, loss = 0.08839998\n",
      "Iteration 1671, loss = 0.08838589\n",
      "Iteration 1672, loss = 0.08837181\n",
      "Iteration 1673, loss = 0.08835774\n",
      "Iteration 1674, loss = 0.08834369\n",
      "Iteration 1675, loss = 0.08832965\n",
      "Iteration 1676, loss = 0.08831562\n",
      "Iteration 1677, loss = 0.08830161\n",
      "Iteration 1678, loss = 0.08828761\n",
      "Iteration 1679, loss = 0.08827362\n",
      "Iteration 1680, loss = 0.08825964\n",
      "Iteration 1681, loss = 0.08824568\n",
      "Iteration 1682, loss = 0.08823173\n",
      "Iteration 1683, loss = 0.08821780\n",
      "Iteration 1684, loss = 0.08820387\n",
      "Iteration 1685, loss = 0.08818996\n",
      "Iteration 1686, loss = 0.08817607\n",
      "Iteration 1687, loss = 0.08816218\n",
      "Iteration 1688, loss = 0.08814831\n",
      "Iteration 1689, loss = 0.08813445\n",
      "Iteration 1690, loss = 0.08812060\n",
      "Iteration 1691, loss = 0.08810677\n",
      "Iteration 1692, loss = 0.08809295\n",
      "Iteration 1693, loss = 0.08807914\n",
      "Iteration 1694, loss = 0.08806534\n",
      "Iteration 1695, loss = 0.08805155\n",
      "Iteration 1696, loss = 0.08803778\n",
      "Iteration 1697, loss = 0.08802402\n",
      "Iteration 1698, loss = 0.08801028\n",
      "Iteration 1699, loss = 0.08799654\n",
      "Iteration 1700, loss = 0.08798282\n",
      "Iteration 1701, loss = 0.08796911\n",
      "Iteration 1702, loss = 0.08795541\n",
      "Iteration 1703, loss = 0.08794172\n",
      "Iteration 1704, loss = 0.08792805\n",
      "Iteration 1705, loss = 0.08791438\n",
      "Iteration 1706, loss = 0.08790073\n",
      "Iteration 1707, loss = 0.08788710\n",
      "Iteration 1708, loss = 0.08787347\n",
      "Iteration 1709, loss = 0.08785986\n",
      "Iteration 1710, loss = 0.08784625\n",
      "Iteration 1711, loss = 0.08783266\n",
      "Iteration 1712, loss = 0.08781908\n",
      "Iteration 1713, loss = 0.08780552\n",
      "Iteration 1714, loss = 0.08779196\n",
      "Iteration 1715, loss = 0.08777842\n",
      "Iteration 1716, loss = 0.08776489\n",
      "Iteration 1717, loss = 0.08775137\n",
      "Iteration 1718, loss = 0.08773786\n",
      "Iteration 1719, loss = 0.08772436\n",
      "Iteration 1720, loss = 0.08771087\n",
      "Iteration 1721, loss = 0.08769740\n",
      "Iteration 1722, loss = 0.08768394\n",
      "Iteration 1723, loss = 0.08767049\n",
      "Iteration 1724, loss = 0.08765705\n",
      "Iteration 1725, loss = 0.08764362\n",
      "Iteration 1726, loss = 0.08763020\n",
      "Iteration 1727, loss = 0.08761680\n",
      "Iteration 1728, loss = 0.08760340\n",
      "Iteration 1729, loss = 0.08759002\n",
      "Iteration 1730, loss = 0.08757665\n",
      "Iteration 1731, loss = 0.08756329\n",
      "Iteration 1732, loss = 0.08754994\n",
      "Iteration 1733, loss = 0.08753660\n",
      "Iteration 1734, loss = 0.08752328\n",
      "Iteration 1735, loss = 0.08750996\n",
      "Iteration 1736, loss = 0.08749665\n",
      "Iteration 1737, loss = 0.08748336\n",
      "Iteration 1738, loss = 0.08747008\n",
      "Iteration 1739, loss = 0.08745681\n",
      "Iteration 1740, loss = 0.08744355\n",
      "Iteration 1741, loss = 0.08743030\n",
      "Iteration 1742, loss = 0.08741706\n",
      "Iteration 1743, loss = 0.08740383\n",
      "Iteration 1744, loss = 0.08739061\n",
      "Iteration 1745, loss = 0.08737740\n",
      "Iteration 1746, loss = 0.08736421\n",
      "Iteration 1747, loss = 0.08735102\n",
      "Iteration 1748, loss = 0.08733785\n",
      "Iteration 1749, loss = 0.08732468\n",
      "Iteration 1750, loss = 0.08731153\n",
      "Iteration 1751, loss = 0.08729839\n",
      "Iteration 1752, loss = 0.08728526\n",
      "Iteration 1753, loss = 0.08727213\n",
      "Iteration 1754, loss = 0.08725902\n",
      "Iteration 1755, loss = 0.08724592\n",
      "Iteration 1756, loss = 0.08723283\n",
      "Iteration 1757, loss = 0.08721975\n",
      "Iteration 1758, loss = 0.08720668\n",
      "Iteration 1759, loss = 0.08719362\n",
      "Iteration 1760, loss = 0.08718058\n",
      "Iteration 1761, loss = 0.08716754\n",
      "Iteration 1762, loss = 0.08715451\n",
      "Iteration 1763, loss = 0.08714149\n",
      "Iteration 1764, loss = 0.08712848\n",
      "Iteration 1765, loss = 0.08711549\n",
      "Iteration 1766, loss = 0.08710250\n",
      "Iteration 1767, loss = 0.08708952\n",
      "Iteration 1768, loss = 0.08707655\n",
      "Iteration 1769, loss = 0.08706360\n",
      "Iteration 1770, loss = 0.08705065\n",
      "Iteration 1771, loss = 0.08703771\n",
      "Iteration 1772, loss = 0.08702479\n",
      "Iteration 1773, loss = 0.08701187\n",
      "Iteration 1774, loss = 0.08699896\n",
      "Iteration 1775, loss = 0.08698607\n",
      "Iteration 1776, loss = 0.08697318\n",
      "Iteration 1777, loss = 0.08696030\n",
      "Iteration 1778, loss = 0.08694743\n",
      "Iteration 1779, loss = 0.08693458\n",
      "Iteration 1780, loss = 0.08692173\n",
      "Iteration 1781, loss = 0.08690889\n",
      "Iteration 1782, loss = 0.08689606\n",
      "Iteration 1783, loss = 0.08688324\n",
      "Iteration 1784, loss = 0.08687043\n",
      "Iteration 1785, loss = 0.08685763\n",
      "Iteration 1786, loss = 0.08684484\n",
      "Iteration 1787, loss = 0.08683206\n",
      "Iteration 1788, loss = 0.08681929\n",
      "Iteration 1789, loss = 0.08680653\n",
      "Iteration 1790, loss = 0.08679378\n",
      "Iteration 1791, loss = 0.08678104\n",
      "Iteration 1792, loss = 0.08676831\n",
      "Iteration 1793, loss = 0.08675558\n",
      "Iteration 1794, loss = 0.08674287\n",
      "Iteration 1795, loss = 0.08673017\n",
      "Iteration 1796, loss = 0.08671747\n",
      "Iteration 1797, loss = 0.08670478\n",
      "Iteration 1798, loss = 0.08669211\n",
      "Iteration 1799, loss = 0.08667944\n",
      "Iteration 1800, loss = 0.08666678\n",
      "Iteration 1801, loss = 0.08665414\n",
      "Iteration 1802, loss = 0.08664150\n",
      "Iteration 1803, loss = 0.08662887\n",
      "Iteration 1804, loss = 0.08661624\n",
      "Iteration 1805, loss = 0.08660363\n",
      "Iteration 1806, loss = 0.08659103\n",
      "Iteration 1807, loss = 0.08657844\n",
      "Iteration 1808, loss = 0.08656585\n",
      "Iteration 1809, loss = 0.08655328\n",
      "Iteration 1810, loss = 0.08654071\n",
      "Iteration 1811, loss = 0.08652815\n",
      "Iteration 1812, loss = 0.08651560\n",
      "Iteration 1813, loss = 0.08650306\n",
      "Iteration 1814, loss = 0.08649053\n",
      "Iteration 1815, loss = 0.08647801\n",
      "Iteration 1816, loss = 0.08646550\n",
      "Iteration 1817, loss = 0.08645300\n",
      "Iteration 1818, loss = 0.08644050\n",
      "Iteration 1819, loss = 0.08642801\n",
      "Iteration 1820, loss = 0.08641554\n",
      "Iteration 1821, loss = 0.08640307\n",
      "Iteration 1822, loss = 0.08639061\n",
      "Iteration 1823, loss = 0.08637816\n",
      "Iteration 1824, loss = 0.08636571\n",
      "Iteration 1825, loss = 0.08635328\n",
      "Iteration 1826, loss = 0.08634085\n",
      "Iteration 1827, loss = 0.08632844\n",
      "Iteration 1828, loss = 0.08631603\n",
      "Iteration 1829, loss = 0.08630363\n",
      "Iteration 1830, loss = 0.08629124\n",
      "Iteration 1831, loss = 0.08627886\n",
      "Iteration 1832, loss = 0.08626648\n",
      "Iteration 1833, loss = 0.08625412\n",
      "Iteration 1834, loss = 0.08624176\n",
      "Iteration 1835, loss = 0.08622941\n",
      "Iteration 1836, loss = 0.08621707\n",
      "Iteration 1837, loss = 0.08620474\n",
      "Iteration 1838, loss = 0.08619242\n",
      "Iteration 1839, loss = 0.08618010\n",
      "Iteration 1840, loss = 0.08616780\n",
      "Iteration 1841, loss = 0.08615550\n",
      "Iteration 1842, loss = 0.08614321\n",
      "Iteration 1843, loss = 0.08613093\n",
      "Iteration 1844, loss = 0.08611865\n",
      "Iteration 1845, loss = 0.08610639\n",
      "Iteration 1846, loss = 0.08609413\n",
      "Iteration 1847, loss = 0.08608188\n",
      "Iteration 1848, loss = 0.08606964\n",
      "Iteration 1849, loss = 0.08605741\n",
      "Iteration 1850, loss = 0.08604518\n",
      "Iteration 1851, loss = 0.08603297\n",
      "Iteration 1852, loss = 0.08602076\n",
      "Iteration 1853, loss = 0.08600856\n",
      "Iteration 1854, loss = 0.08599637\n",
      "Iteration 1855, loss = 0.08598418\n",
      "Iteration 1856, loss = 0.08597201\n",
      "Iteration 1857, loss = 0.08595984\n",
      "Iteration 1858, loss = 0.08594768\n",
      "Iteration 1859, loss = 0.08593553\n",
      "Iteration 1860, loss = 0.08592338\n",
      "Iteration 1861, loss = 0.08591125\n",
      "Iteration 1862, loss = 0.08589912\n",
      "Iteration 1863, loss = 0.08588700\n",
      "Iteration 1864, loss = 0.08587488\n",
      "Iteration 1865, loss = 0.08586278\n",
      "Iteration 1866, loss = 0.08585068\n",
      "Iteration 1867, loss = 0.08583859\n",
      "Iteration 1868, loss = 0.08582651\n",
      "Iteration 1869, loss = 0.08581444\n",
      "Iteration 1870, loss = 0.08580237\n",
      "Iteration 1871, loss = 0.08579031\n",
      "Iteration 1872, loss = 0.08577826\n",
      "Iteration 1873, loss = 0.08576622\n",
      "Iteration 1874, loss = 0.08575418\n",
      "Iteration 1875, loss = 0.08574216\n",
      "Iteration 1876, loss = 0.08573014\n",
      "Iteration 1877, loss = 0.08571812\n",
      "Iteration 1878, loss = 0.08570612\n",
      "Iteration 1879, loss = 0.08569412\n",
      "Iteration 1880, loss = 0.08568213\n",
      "Iteration 1881, loss = 0.08567015\n",
      "Iteration 1882, loss = 0.08565818\n",
      "Iteration 1883, loss = 0.08564621\n",
      "Iteration 1884, loss = 0.08563425\n",
      "Iteration 1885, loss = 0.08562230\n",
      "Iteration 1886, loss = 0.08561035\n",
      "Iteration 1887, loss = 0.08559841\n",
      "Iteration 1888, loss = 0.08558648\n",
      "Iteration 1889, loss = 0.08557456\n",
      "Iteration 1890, loss = 0.08556264\n",
      "Iteration 1891, loss = 0.08555074\n",
      "Iteration 1892, loss = 0.08553883\n",
      "Iteration 1893, loss = 0.08552694\n",
      "Iteration 1894, loss = 0.08551505\n",
      "Iteration 1895, loss = 0.08550318\n",
      "Iteration 1896, loss = 0.08549130\n",
      "Iteration 1897, loss = 0.08547944\n",
      "Iteration 1898, loss = 0.08546758\n",
      "Iteration 1899, loss = 0.08545573\n",
      "Iteration 1900, loss = 0.08544389\n",
      "Iteration 1901, loss = 0.08543205\n",
      "Iteration 1902, loss = 0.08542022\n",
      "Iteration 1903, loss = 0.08540840\n",
      "Iteration 1904, loss = 0.08539659\n",
      "Iteration 1905, loss = 0.08538478\n",
      "Iteration 1906, loss = 0.08537298\n",
      "Iteration 1907, loss = 0.08536119\n",
      "Iteration 1908, loss = 0.08534940\n",
      "Iteration 1909, loss = 0.08533762\n",
      "Iteration 1910, loss = 0.08532585\n",
      "Iteration 1911, loss = 0.08531408\n",
      "Iteration 1912, loss = 0.08530233\n",
      "Iteration 1913, loss = 0.08529057\n",
      "Iteration 1914, loss = 0.08527883\n",
      "Iteration 1915, loss = 0.08526709\n",
      "Iteration 1916, loss = 0.08525536\n",
      "Iteration 1917, loss = 0.08524364\n",
      "Iteration 1918, loss = 0.08523192\n",
      "Iteration 1919, loss = 0.08522021\n",
      "Iteration 1920, loss = 0.08520851\n",
      "Iteration 1921, loss = 0.08519681\n",
      "Iteration 1922, loss = 0.08518512\n",
      "Iteration 1923, loss = 0.08517344\n",
      "Iteration 1924, loss = 0.08516176\n",
      "Iteration 1925, loss = 0.08515009\n",
      "Iteration 1926, loss = 0.08513843\n",
      "Iteration 1927, loss = 0.08512678\n",
      "Iteration 1928, loss = 0.08511513\n",
      "Iteration 1929, loss = 0.08510349\n",
      "Iteration 1930, loss = 0.08509185\n",
      "Iteration 1931, loss = 0.08508022\n",
      "Iteration 1932, loss = 0.08506860\n",
      "Iteration 1933, loss = 0.08505698\n",
      "Iteration 1934, loss = 0.08504537\n",
      "Iteration 1935, loss = 0.08503377\n",
      "Iteration 1936, loss = 0.08502217\n",
      "Iteration 1937, loss = 0.08501059\n",
      "Iteration 1938, loss = 0.08499900\n",
      "Iteration 1939, loss = 0.08498743\n",
      "Iteration 1940, loss = 0.08497586\n",
      "Iteration 1941, loss = 0.08496429\n",
      "Iteration 1942, loss = 0.08495274\n",
      "Iteration 1943, loss = 0.08494119\n",
      "Iteration 1944, loss = 0.08492964\n",
      "Iteration 1945, loss = 0.08491810\n",
      "Iteration 1946, loss = 0.08490657\n",
      "Iteration 1947, loss = 0.08489505\n",
      "Iteration 1948, loss = 0.08488353\n",
      "Iteration 1949, loss = 0.08487202\n",
      "Iteration 1950, loss = 0.08486051\n",
      "Iteration 1951, loss = 0.08484901\n",
      "Iteration 1952, loss = 0.08483752\n",
      "Iteration 1953, loss = 0.08482603\n",
      "Iteration 1954, loss = 0.08481455\n",
      "Iteration 1955, loss = 0.08480308\n",
      "Iteration 1956, loss = 0.08479161\n",
      "Iteration 1957, loss = 0.08478015\n",
      "Iteration 1958, loss = 0.08476870\n",
      "Iteration 1959, loss = 0.08475725\n",
      "Iteration 1960, loss = 0.08474580\n",
      "Iteration 1961, loss = 0.08473437\n",
      "Iteration 1962, loss = 0.08472294\n",
      "Iteration 1963, loss = 0.08471151\n",
      "Iteration 1964, loss = 0.08470010\n",
      "Iteration 1965, loss = 0.08468868\n",
      "Iteration 1966, loss = 0.08467728\n",
      "Iteration 1967, loss = 0.08466588\n",
      "Iteration 1968, loss = 0.08465449\n",
      "Iteration 1969, loss = 0.08464310\n",
      "Iteration 1970, loss = 0.08463172\n",
      "Iteration 1971, loss = 0.08462034\n",
      "Iteration 1972, loss = 0.08460897\n",
      "Iteration 1973, loss = 0.08459761\n",
      "Iteration 1974, loss = 0.08458625\n",
      "Iteration 1975, loss = 0.08457490\n",
      "Iteration 1976, loss = 0.08456356\n",
      "Iteration 1977, loss = 0.08455222\n",
      "Iteration 1978, loss = 0.08454089\n",
      "Iteration 1979, loss = 0.08452956\n",
      "Iteration 1980, loss = 0.08451824\n",
      "Iteration 1981, loss = 0.08450692\n",
      "Iteration 1982, loss = 0.08449561\n",
      "Iteration 1983, loss = 0.08448431\n",
      "Iteration 1984, loss = 0.08447301\n",
      "Iteration 1985, loss = 0.08446172\n",
      "Iteration 1986, loss = 0.08445043\n",
      "Iteration 1987, loss = 0.08443915\n",
      "Iteration 1988, loss = 0.08442788\n",
      "Iteration 1989, loss = 0.08441661\n",
      "Iteration 1990, loss = 0.08440535\n",
      "Iteration 1991, loss = 0.08439409\n",
      "Iteration 1992, loss = 0.08438284\n",
      "Iteration 1993, loss = 0.08437160\n",
      "Iteration 1994, loss = 0.08436036\n",
      "Iteration 1995, loss = 0.08434912\n",
      "Iteration 1996, loss = 0.08433790\n",
      "Iteration 1997, loss = 0.08432667\n",
      "Iteration 1998, loss = 0.08431546\n",
      "Iteration 1999, loss = 0.08430425\n",
      "Iteration 2000, loss = 0.08429304\n",
      "Iteration 2001, loss = 0.08428184\n",
      "Iteration 2002, loss = 0.08427065\n",
      "Iteration 2003, loss = 0.08425946\n",
      "Iteration 2004, loss = 0.08424828\n",
      "Iteration 2005, loss = 0.08423710\n",
      "Iteration 2006, loss = 0.08422593\n",
      "Iteration 2007, loss = 0.08421476\n",
      "Iteration 2008, loss = 0.08420360\n",
      "Iteration 2009, loss = 0.08419245\n",
      "Iteration 2010, loss = 0.08418130\n",
      "Iteration 2011, loss = 0.08417016\n",
      "Iteration 2012, loss = 0.08415902\n",
      "Iteration 2013, loss = 0.08414788\n",
      "Iteration 2014, loss = 0.08413676\n",
      "Iteration 2015, loss = 0.08412564\n",
      "Iteration 2016, loss = 0.08411452\n",
      "Iteration 2017, loss = 0.08410341\n",
      "Iteration 2018, loss = 0.08409230\n",
      "Iteration 2019, loss = 0.08408121\n",
      "Iteration 2020, loss = 0.08407011\n",
      "Iteration 2021, loss = 0.08405902\n",
      "Iteration 2022, loss = 0.08404794\n",
      "Iteration 2023, loss = 0.08403686\n",
      "Iteration 2024, loss = 0.08402579\n",
      "Iteration 2025, loss = 0.08401472\n",
      "Iteration 2026, loss = 0.08400366\n",
      "Iteration 2027, loss = 0.08399260\n",
      "Iteration 2028, loss = 0.08398155\n",
      "Iteration 2029, loss = 0.08397051\n",
      "Iteration 2030, loss = 0.08395947\n",
      "Iteration 2031, loss = 0.08394843\n",
      "Iteration 2032, loss = 0.08393740\n",
      "Iteration 2033, loss = 0.08392638\n",
      "Iteration 2034, loss = 0.08391536\n",
      "Iteration 2035, loss = 0.08390434\n",
      "Iteration 2036, loss = 0.08389333\n",
      "Iteration 2037, loss = 0.08388233\n",
      "Iteration 2038, loss = 0.08387133\n",
      "Iteration 2039, loss = 0.08386034\n",
      "Iteration 2040, loss = 0.08384935\n",
      "Iteration 2041, loss = 0.08383837\n",
      "Iteration 2042, loss = 0.08382739\n",
      "Iteration 2043, loss = 0.08381642\n",
      "Iteration 2044, loss = 0.08380545\n",
      "Iteration 2045, loss = 0.08379449\n",
      "Iteration 2046, loss = 0.08378353\n",
      "Iteration 2047, loss = 0.08377258\n",
      "Iteration 2048, loss = 0.08376163\n",
      "Iteration 2049, loss = 0.08375069\n",
      "Iteration 2050, loss = 0.08373975\n",
      "Iteration 2051, loss = 0.08372882\n",
      "Iteration 2052, loss = 0.08371789\n",
      "Iteration 2053, loss = 0.08370697\n",
      "Iteration 2054, loss = 0.08369606\n",
      "Iteration 2055, loss = 0.08368515\n",
      "Iteration 2056, loss = 0.08367424\n",
      "Iteration 2057, loss = 0.08366334\n",
      "Iteration 2058, loss = 0.08365244\n",
      "Iteration 2059, loss = 0.08364155\n",
      "Iteration 2060, loss = 0.08363066\n",
      "Iteration 2061, loss = 0.08361978\n",
      "Iteration 2062, loss = 0.08360890\n",
      "Iteration 2063, loss = 0.08359803\n",
      "Iteration 2064, loss = 0.08358717\n",
      "Iteration 2065, loss = 0.08357630\n",
      "Iteration 2066, loss = 0.08356545\n",
      "Iteration 2067, loss = 0.08355460\n",
      "Iteration 2068, loss = 0.08354375\n",
      "Iteration 2069, loss = 0.08353291\n",
      "Iteration 2070, loss = 0.08352207\n",
      "Iteration 2071, loss = 0.08351124\n",
      "Iteration 2072, loss = 0.08350041\n",
      "Iteration 2073, loss = 0.08348959\n",
      "Iteration 2074, loss = 0.08347877\n",
      "Iteration 2075, loss = 0.08346796\n",
      "Iteration 2076, loss = 0.08345715\n",
      "Iteration 2077, loss = 0.08344634\n",
      "Iteration 2078, loss = 0.08343554\n",
      "Iteration 2079, loss = 0.08342475\n",
      "Iteration 2080, loss = 0.08341396\n",
      "Iteration 2081, loss = 0.08340318\n",
      "Iteration 2082, loss = 0.08339240\n",
      "Iteration 2083, loss = 0.08338162\n",
      "Iteration 2084, loss = 0.08337085\n",
      "Iteration 2085, loss = 0.08336009\n",
      "Iteration 2086, loss = 0.08334933\n",
      "Iteration 2087, loss = 0.08333857\n",
      "Iteration 2088, loss = 0.08332782\n",
      "Iteration 2089, loss = 0.08331707\n",
      "Iteration 2090, loss = 0.08330633\n",
      "Iteration 2091, loss = 0.08329559\n",
      "Iteration 2092, loss = 0.08328486\n",
      "Iteration 2093, loss = 0.08327413\n",
      "Iteration 2094, loss = 0.08326341\n",
      "Iteration 2095, loss = 0.08325269\n",
      "Iteration 2096, loss = 0.08324197\n",
      "Iteration 2097, loss = 0.08323126\n",
      "Iteration 2098, loss = 0.08322056\n",
      "Iteration 2099, loss = 0.08320986\n",
      "Iteration 2100, loss = 0.08319916\n",
      "Iteration 2101, loss = 0.08318847\n",
      "Iteration 2102, loss = 0.08317778\n",
      "Iteration 2103, loss = 0.08316710\n",
      "Iteration 2104, loss = 0.08315642\n",
      "Iteration 2105, loss = 0.08314575\n",
      "Iteration 2106, loss = 0.08313508\n",
      "Iteration 2107, loss = 0.08312441\n",
      "Iteration 2108, loss = 0.08311375\n",
      "Iteration 2109, loss = 0.08310310\n",
      "Iteration 2110, loss = 0.08309245\n",
      "Iteration 2111, loss = 0.08308180\n",
      "Iteration 2112, loss = 0.08307116\n",
      "Iteration 2113, loss = 0.08306052\n",
      "Iteration 2114, loss = 0.08304989\n",
      "Iteration 2115, loss = 0.08303926\n",
      "Iteration 2116, loss = 0.08302863\n",
      "Iteration 2117, loss = 0.08301801\n",
      "Iteration 2118, loss = 0.08300740\n",
      "Iteration 2119, loss = 0.08299679\n",
      "Iteration 2120, loss = 0.08298618\n",
      "Iteration 2121, loss = 0.08297558\n",
      "Iteration 2122, loss = 0.08296498\n",
      "Iteration 2123, loss = 0.08295438\n",
      "Iteration 2124, loss = 0.08294380\n",
      "Iteration 2125, loss = 0.08293321\n",
      "Iteration 2126, loss = 0.08292263\n",
      "Iteration 2127, loss = 0.08291205\n",
      "Iteration 2128, loss = 0.08290148\n",
      "Iteration 2129, loss = 0.08289091\n",
      "Iteration 2130, loss = 0.08288035\n",
      "Iteration 2131, loss = 0.08286979\n",
      "Iteration 2132, loss = 0.08285923\n",
      "Iteration 2133, loss = 0.08284868\n",
      "Iteration 2134, loss = 0.08283813\n",
      "Iteration 2135, loss = 0.08282759\n",
      "Iteration 2136, loss = 0.08281705\n",
      "Iteration 2137, loss = 0.08280652\n",
      "Iteration 2138, loss = 0.08279599\n",
      "Iteration 2139, loss = 0.08278546\n",
      "Iteration 2140, loss = 0.08277494\n",
      "Iteration 2141, loss = 0.08276442\n",
      "Iteration 2142, loss = 0.08275391\n",
      "Iteration 2143, loss = 0.08274340\n",
      "Iteration 2144, loss = 0.08273290\n",
      "Iteration 2145, loss = 0.08272239\n",
      "Iteration 2146, loss = 0.08271190\n",
      "Iteration 2147, loss = 0.08270141\n",
      "Iteration 2148, loss = 0.08269092\n",
      "Iteration 2149, loss = 0.08268043\n",
      "Iteration 2150, loss = 0.08266995\n",
      "Iteration 2151, loss = 0.08265947\n",
      "Iteration 2152, loss = 0.08264900\n",
      "Iteration 2153, loss = 0.08263853\n",
      "Iteration 2154, loss = 0.08262807\n",
      "Iteration 2155, loss = 0.08261761\n",
      "Iteration 2156, loss = 0.08260715\n",
      "Iteration 2157, loss = 0.08259670\n",
      "Iteration 2158, loss = 0.08258625\n",
      "Iteration 2159, loss = 0.08257581\n",
      "Iteration 2160, loss = 0.08256537\n",
      "Iteration 2161, loss = 0.08255493\n",
      "Iteration 2162, loss = 0.08254450\n",
      "Iteration 2163, loss = 0.08253407\n",
      "Iteration 2164, loss = 0.08252365\n",
      "Iteration 2165, loss = 0.08251323\n",
      "Iteration 2166, loss = 0.08250281\n",
      "Iteration 2167, loss = 0.08249240\n",
      "Iteration 2168, loss = 0.08248199\n",
      "Iteration 2169, loss = 0.08247159\n",
      "Iteration 2170, loss = 0.08246119\n",
      "Iteration 2171, loss = 0.08245079\n",
      "Iteration 2172, loss = 0.08244040\n",
      "Iteration 2173, loss = 0.08243001\n",
      "Iteration 2174, loss = 0.08241962\n",
      "Iteration 2175, loss = 0.08240924\n",
      "Iteration 2176, loss = 0.08239886\n",
      "Iteration 2177, loss = 0.08238849\n",
      "Iteration 2178, loss = 0.08237812\n",
      "Iteration 2179, loss = 0.08236776\n",
      "Iteration 2180, loss = 0.08235739\n",
      "Iteration 2181, loss = 0.08234704\n",
      "Iteration 2182, loss = 0.08233668\n",
      "Iteration 2183, loss = 0.08232633\n",
      "Iteration 2184, loss = 0.08231598\n",
      "Iteration 2185, loss = 0.08230564\n",
      "Iteration 2186, loss = 0.08229530\n",
      "Iteration 2187, loss = 0.08228497\n",
      "Iteration 2188, loss = 0.08227463\n",
      "Iteration 2189, loss = 0.08226431\n",
      "Iteration 2190, loss = 0.08225398\n",
      "Iteration 2191, loss = 0.08224366\n",
      "Iteration 2192, loss = 0.08223334\n",
      "Iteration 2193, loss = 0.08222303\n",
      "Iteration 2194, loss = 0.08221272\n",
      "Iteration 2195, loss = 0.08220242\n",
      "Iteration 2196, loss = 0.08219211\n",
      "Iteration 2197, loss = 0.08218182\n",
      "Iteration 2198, loss = 0.08217152\n",
      "Iteration 2199, loss = 0.08216123\n",
      "Iteration 2200, loss = 0.08215094\n",
      "Iteration 2201, loss = 0.08214066\n",
      "Iteration 2202, loss = 0.08213038\n",
      "Iteration 2203, loss = 0.08212010\n",
      "Iteration 2204, loss = 0.08210983\n",
      "Iteration 2205, loss = 0.08209956\n",
      "Iteration 2206, loss = 0.08208930\n",
      "Iteration 2207, loss = 0.08207903\n",
      "Iteration 2208, loss = 0.08206878\n",
      "Iteration 2209, loss = 0.08205852\n",
      "Iteration 2210, loss = 0.08204827\n",
      "Iteration 2211, loss = 0.08203802\n",
      "Iteration 2212, loss = 0.08202778\n",
      "Iteration 2213, loss = 0.08201754\n",
      "Iteration 2214, loss = 0.08200730\n",
      "Iteration 2215, loss = 0.08199707\n",
      "Iteration 2216, loss = 0.08198684\n",
      "Iteration 2217, loss = 0.08197661\n",
      "Iteration 2218, loss = 0.08196639\n",
      "Iteration 2219, loss = 0.08195617\n",
      "Iteration 2220, loss = 0.08194595\n",
      "Iteration 2221, loss = 0.08193574\n",
      "Iteration 2222, loss = 0.08192553\n",
      "Iteration 2223, loss = 0.08191533\n",
      "Iteration 2224, loss = 0.08190512\n",
      "Iteration 2225, loss = 0.08189492\n",
      "Iteration 2226, loss = 0.08188473\n",
      "Iteration 2227, loss = 0.08187454\n",
      "Iteration 2228, loss = 0.08186435\n",
      "Iteration 2229, loss = 0.08185416\n",
      "Iteration 2230, loss = 0.08184398\n",
      "Iteration 2231, loss = 0.08183381\n",
      "Iteration 2232, loss = 0.08182363\n",
      "Iteration 2233, loss = 0.08181346\n",
      "Iteration 2234, loss = 0.08180329\n",
      "Iteration 2235, loss = 0.08179313\n",
      "Iteration 2236, loss = 0.08178297\n",
      "Iteration 2237, loss = 0.08177281\n",
      "Iteration 2238, loss = 0.08176265\n",
      "Iteration 2239, loss = 0.08175250\n",
      "Iteration 2240, loss = 0.08174235\n",
      "Iteration 2241, loss = 0.08173221\n",
      "Iteration 2242, loss = 0.08172207\n",
      "Iteration 2243, loss = 0.08171193\n",
      "Iteration 2244, loss = 0.08170180\n",
      "Iteration 2245, loss = 0.08169167\n",
      "Iteration 2246, loss = 0.08168154\n",
      "Iteration 2247, loss = 0.08167141\n",
      "Iteration 2248, loss = 0.08166129\n",
      "Iteration 2249, loss = 0.08165117\n",
      "Iteration 2250, loss = 0.08164106\n",
      "Iteration 2251, loss = 0.08163095\n",
      "Iteration 2252, loss = 0.08162084\n",
      "Iteration 2253, loss = 0.08161073\n",
      "Iteration 2254, loss = 0.08160063\n",
      "Iteration 2255, loss = 0.08159053\n",
      "Iteration 2256, loss = 0.08158044\n",
      "Iteration 2257, loss = 0.08157034\n",
      "Iteration 2258, loss = 0.08156025\n",
      "Iteration 2259, loss = 0.08155017\n",
      "Iteration 2260, loss = 0.08154009\n",
      "Iteration 2261, loss = 0.08153001\n",
      "Iteration 2262, loss = 0.08151993\n",
      "Iteration 2263, loss = 0.08150986\n",
      "Iteration 2264, loss = 0.08149979\n",
      "Iteration 2265, loss = 0.08148972\n",
      "Iteration 2266, loss = 0.08147966\n",
      "Iteration 2267, loss = 0.08146959\n",
      "Iteration 2268, loss = 0.08145954\n",
      "Iteration 2269, loss = 0.08144948\n",
      "Iteration 2270, loss = 0.08143943\n",
      "Iteration 2271, loss = 0.08142938\n",
      "Iteration 2272, loss = 0.08141934\n",
      "Iteration 2273, loss = 0.08140930\n",
      "Iteration 2274, loss = 0.08139926\n",
      "Iteration 2275, loss = 0.08138922\n",
      "Iteration 2276, loss = 0.08137919\n",
      "Iteration 2277, loss = 0.08136916\n",
      "Iteration 2278, loss = 0.08135913\n",
      "Iteration 2279, loss = 0.08134911\n",
      "Iteration 2280, loss = 0.08133909\n",
      "Iteration 2281, loss = 0.08132907\n",
      "Iteration 2282, loss = 0.08131905\n",
      "Iteration 2283, loss = 0.08130904\n",
      "Iteration 2284, loss = 0.08129903\n",
      "Iteration 2285, loss = 0.08128903\n",
      "Iteration 2286, loss = 0.08127902\n",
      "Iteration 2287, loss = 0.08126902\n",
      "Iteration 2288, loss = 0.08125903\n",
      "Iteration 2289, loss = 0.08124903\n",
      "Iteration 2290, loss = 0.08123904\n",
      "Iteration 2291, loss = 0.08122905\n",
      "Iteration 2292, loss = 0.08121907\n",
      "Iteration 2293, loss = 0.08120909\n",
      "Iteration 2294, loss = 0.08119911\n",
      "Iteration 2295, loss = 0.08118913\n",
      "Iteration 2296, loss = 0.08117916\n",
      "Iteration 2297, loss = 0.08116919\n",
      "Iteration 2298, loss = 0.08115922\n",
      "Iteration 2299, loss = 0.08114925\n",
      "Iteration 2300, loss = 0.08113929\n",
      "Iteration 2301, loss = 0.08112933\n",
      "Iteration 2302, loss = 0.08111938\n",
      "Iteration 2303, loss = 0.08110942\n",
      "Iteration 2304, loss = 0.08109947\n",
      "Iteration 2305, loss = 0.08108953\n",
      "Iteration 2306, loss = 0.08107958\n",
      "Iteration 2307, loss = 0.08106964\n",
      "Iteration 2308, loss = 0.08105970\n",
      "Iteration 2309, loss = 0.08104976\n",
      "Iteration 2310, loss = 0.08103983\n",
      "Iteration 2311, loss = 0.08102990\n",
      "Iteration 2312, loss = 0.08101997\n",
      "Iteration 2313, loss = 0.08101005\n",
      "Iteration 2314, loss = 0.08100013\n",
      "Iteration 2315, loss = 0.08099021\n",
      "Iteration 2316, loss = 0.08098029\n",
      "Iteration 2317, loss = 0.08097038\n",
      "Iteration 2318, loss = 0.08096046\n",
      "Iteration 2319, loss = 0.08095056\n",
      "Iteration 2320, loss = 0.08094065\n",
      "Iteration 2321, loss = 0.08093075\n",
      "Iteration 2322, loss = 0.08092085\n",
      "Iteration 2323, loss = 0.08091095\n",
      "Iteration 2324, loss = 0.08090105\n",
      "Iteration 2325, loss = 0.08089116\n",
      "Iteration 2326, loss = 0.08088127\n",
      "Iteration 2327, loss = 0.08087139\n",
      "Iteration 2328, loss = 0.08086150\n",
      "Iteration 2329, loss = 0.08085162\n",
      "Iteration 2330, loss = 0.08084174\n",
      "Iteration 2331, loss = 0.08083187\n",
      "Iteration 2332, loss = 0.08082199\n",
      "Iteration 2333, loss = 0.08081212\n",
      "Iteration 2334, loss = 0.08080225\n",
      "Iteration 2335, loss = 0.08079239\n",
      "Iteration 2336, loss = 0.08078253\n",
      "Iteration 2337, loss = 0.08077266\n",
      "Iteration 2338, loss = 0.08076281\n",
      "Iteration 2339, loss = 0.08075295\n",
      "Iteration 2340, loss = 0.08074310\n",
      "Iteration 2341, loss = 0.08073325\n",
      "Iteration 2342, loss = 0.08072340\n",
      "Iteration 2343, loss = 0.08071356\n",
      "Iteration 2344, loss = 0.08070372\n",
      "Iteration 2345, loss = 0.08069388\n",
      "Iteration 2346, loss = 0.08068404\n",
      "Iteration 2347, loss = 0.08067421\n",
      "Iteration 2348, loss = 0.08066437\n",
      "Iteration 2349, loss = 0.08065454\n",
      "Iteration 2350, loss = 0.08064472\n",
      "Iteration 2351, loss = 0.08063489\n",
      "Iteration 2352, loss = 0.08062507\n",
      "Iteration 2353, loss = 0.08061525\n",
      "Iteration 2354, loss = 0.08060543\n",
      "Iteration 2355, loss = 0.08059562\n",
      "Iteration 2356, loss = 0.08058581\n",
      "Iteration 2357, loss = 0.08057600\n",
      "Iteration 2358, loss = 0.08056619\n",
      "Iteration 2359, loss = 0.08055639\n",
      "Iteration 2360, loss = 0.08054659\n",
      "Iteration 2361, loss = 0.08053679\n",
      "Iteration 2362, loss = 0.08052699\n",
      "Iteration 2363, loss = 0.08051720\n",
      "Iteration 2364, loss = 0.08050740\n",
      "Iteration 2365, loss = 0.08049761\n",
      "Iteration 2366, loss = 0.08048783\n",
      "Iteration 2367, loss = 0.08047804\n",
      "Iteration 2368, loss = 0.08046826\n",
      "Iteration 2369, loss = 0.08045848\n",
      "Iteration 2370, loss = 0.08044870\n",
      "Iteration 2371, loss = 0.08043893\n",
      "Iteration 2372, loss = 0.08042915\n",
      "Iteration 2373, loss = 0.08041938\n",
      "Iteration 2374, loss = 0.08040962\n",
      "Iteration 2375, loss = 0.08039985\n",
      "Iteration 2376, loss = 0.08039009\n",
      "Iteration 2377, loss = 0.08038033\n",
      "Iteration 2378, loss = 0.08037057\n",
      "Iteration 2379, loss = 0.08036081\n",
      "Iteration 2380, loss = 0.08035106\n",
      "Iteration 2381, loss = 0.08034131\n",
      "Iteration 2382, loss = 0.08033156\n",
      "Iteration 2383, loss = 0.08032181\n",
      "Iteration 2384, loss = 0.08031206\n",
      "Iteration 2385, loss = 0.08030232\n",
      "Iteration 2386, loss = 0.08029258\n",
      "Iteration 2387, loss = 0.08028284\n",
      "Iteration 2388, loss = 0.08027311\n",
      "Iteration 2389, loss = 0.08026338\n",
      "Iteration 2390, loss = 0.08025365\n",
      "Iteration 2391, loss = 0.08024392\n",
      "Iteration 2392, loss = 0.08023419\n",
      "Iteration 2393, loss = 0.08022447\n",
      "Iteration 2394, loss = 0.08021474\n",
      "Iteration 2395, loss = 0.08020502\n",
      "Iteration 2396, loss = 0.08019531\n",
      "Iteration 2397, loss = 0.08018559\n",
      "Iteration 2398, loss = 0.08017588\n",
      "Iteration 2399, loss = 0.08016617\n",
      "Iteration 2400, loss = 0.08015646\n",
      "Iteration 2401, loss = 0.08014675\n",
      "Iteration 2402, loss = 0.08013705\n",
      "Iteration 2403, loss = 0.08012735\n",
      "Iteration 2404, loss = 0.08011765\n",
      "Iteration 2405, loss = 0.08010795\n",
      "Iteration 2406, loss = 0.08009825\n",
      "Iteration 2407, loss = 0.08008856\n",
      "Iteration 2408, loss = 0.08007887\n",
      "Iteration 2409, loss = 0.08006918\n",
      "Iteration 2410, loss = 0.08005949\n",
      "Iteration 2411, loss = 0.08004981\n",
      "Iteration 2412, loss = 0.08004013\n",
      "Iteration 2413, loss = 0.08003045\n",
      "Iteration 2414, loss = 0.08002077\n",
      "Iteration 2415, loss = 0.08001109\n",
      "Iteration 2416, loss = 0.08000142\n",
      "Iteration 2417, loss = 0.07999175\n",
      "Iteration 2418, loss = 0.07998208\n",
      "Iteration 2419, loss = 0.07997241\n",
      "Iteration 2420, loss = 0.07996274\n",
      "Iteration 2421, loss = 0.07995308\n",
      "Iteration 2422, loss = 0.07994342\n",
      "Iteration 2423, loss = 0.07993376\n",
      "Iteration 2424, loss = 0.07992410\n",
      "Iteration 2425, loss = 0.07991444\n",
      "Iteration 2426, loss = 0.07990479\n",
      "Iteration 2427, loss = 0.07989514\n",
      "Iteration 2428, loss = 0.07988549\n",
      "Iteration 2429, loss = 0.07987584\n",
      "Iteration 2430, loss = 0.07986620\n",
      "Iteration 2431, loss = 0.07985656\n",
      "Iteration 2432, loss = 0.07984691\n",
      "Iteration 2433, loss = 0.07983728\n",
      "Iteration 2434, loss = 0.07982764\n",
      "Iteration 2435, loss = 0.07981800\n",
      "Iteration 2436, loss = 0.07980837\n",
      "Iteration 2437, loss = 0.07979874\n",
      "Iteration 2438, loss = 0.07978911\n",
      "Iteration 2439, loss = 0.07977948\n",
      "Iteration 2440, loss = 0.07976986\n",
      "Iteration 2441, loss = 0.07976023\n",
      "Iteration 2442, loss = 0.07975061\n",
      "Iteration 2443, loss = 0.07974099\n",
      "Iteration 2444, loss = 0.07973138\n",
      "Iteration 2445, loss = 0.07972176\n",
      "Iteration 2446, loss = 0.07971215\n",
      "Iteration 2447, loss = 0.07970254\n",
      "Iteration 2448, loss = 0.07969293\n",
      "Iteration 2449, loss = 0.07968332\n",
      "Iteration 2450, loss = 0.07967371\n",
      "Iteration 2451, loss = 0.07966411\n",
      "Iteration 2452, loss = 0.07965451\n",
      "Iteration 2453, loss = 0.07964491\n",
      "Iteration 2454, loss = 0.07963531\n",
      "Iteration 2455, loss = 0.07962571\n",
      "Iteration 2456, loss = 0.07961612\n",
      "Iteration 2457, loss = 0.07960652\n",
      "Iteration 2458, loss = 0.07959693\n",
      "Iteration 2459, loss = 0.07958734\n",
      "Iteration 2460, loss = 0.07957776\n",
      "Iteration 2461, loss = 0.07956817\n",
      "Iteration 2462, loss = 0.07955859\n",
      "Iteration 2463, loss = 0.07954901\n",
      "Iteration 2464, loss = 0.07953943\n",
      "Iteration 2465, loss = 0.07952985\n",
      "Iteration 2466, loss = 0.07952027\n",
      "Iteration 2467, loss = 0.07951070\n",
      "Iteration 2468, loss = 0.07950113\n",
      "Iteration 2469, loss = 0.07949156\n",
      "Iteration 2470, loss = 0.07948199\n",
      "Iteration 2471, loss = 0.07947242\n",
      "Iteration 2472, loss = 0.07946285\n",
      "Iteration 2473, loss = 0.07945329\n",
      "Iteration 2474, loss = 0.07944373\n",
      "Iteration 2475, loss = 0.07943417\n",
      "Iteration 2476, loss = 0.07942461\n",
      "Iteration 2477, loss = 0.07941505\n",
      "Iteration 2478, loss = 0.07940550\n",
      "Iteration 2479, loss = 0.07939595\n",
      "Iteration 2480, loss = 0.07938640\n",
      "Iteration 2481, loss = 0.07937685\n",
      "Iteration 2482, loss = 0.07936730\n",
      "Iteration 2483, loss = 0.07935775\n",
      "Iteration 2484, loss = 0.07934821\n",
      "Iteration 2485, loss = 0.07933867\n",
      "Iteration 2486, loss = 0.07932912\n",
      "Iteration 2487, loss = 0.07931958\n",
      "Iteration 2488, loss = 0.07931005\n",
      "Iteration 2489, loss = 0.07930051\n",
      "Iteration 2490, loss = 0.07929098\n",
      "Iteration 2491, loss = 0.07928145\n",
      "Iteration 2492, loss = 0.07927191\n",
      "Iteration 2493, loss = 0.07926239\n",
      "Iteration 2494, loss = 0.07925286\n",
      "Iteration 2495, loss = 0.07924333\n",
      "Iteration 2496, loss = 0.07923381\n",
      "Iteration 2497, loss = 0.07922429\n",
      "Iteration 2498, loss = 0.07921477\n",
      "Iteration 2499, loss = 0.07920525\n",
      "Iteration 2500, loss = 0.07919573\n",
      "Iteration 2501, loss = 0.07918621\n",
      "Iteration 2502, loss = 0.07917670\n",
      "Iteration 2503, loss = 0.07916719\n",
      "Iteration 2504, loss = 0.07915767\n",
      "Iteration 2505, loss = 0.07914817\n",
      "Iteration 2506, loss = 0.07913866\n",
      "Iteration 2507, loss = 0.07912915\n",
      "Iteration 2508, loss = 0.07911965\n",
      "Iteration 2509, loss = 0.07911014\n",
      "Iteration 2510, loss = 0.07910064\n",
      "Iteration 2511, loss = 0.07909114\n",
      "Iteration 2512, loss = 0.07908164\n",
      "Iteration 2513, loss = 0.07907215\n",
      "Iteration 2514, loss = 0.07906265\n",
      "Iteration 2515, loss = 0.07905316\n",
      "Iteration 2516, loss = 0.07904367\n",
      "Iteration 2517, loss = 0.07903418\n",
      "Iteration 2518, loss = 0.07902469\n",
      "Iteration 2519, loss = 0.07901520\n",
      "Iteration 2520, loss = 0.07900571\n",
      "Iteration 2521, loss = 0.07899623\n",
      "Iteration 2522, loss = 0.07898675\n",
      "Iteration 2523, loss = 0.07897727\n",
      "Iteration 2524, loss = 0.07896779\n",
      "Iteration 2525, loss = 0.07895831\n",
      "Iteration 2526, loss = 0.07894883\n",
      "Iteration 2527, loss = 0.07893936\n",
      "Iteration 2528, loss = 0.07892988\n",
      "Iteration 2529, loss = 0.07892041\n",
      "Iteration 2530, loss = 0.07891094\n",
      "Iteration 2531, loss = 0.07890147\n",
      "Iteration 2532, loss = 0.07889200\n",
      "Iteration 2533, loss = 0.07888254\n",
      "Iteration 2534, loss = 0.07887307\n",
      "Iteration 2535, loss = 0.07886361\n",
      "Iteration 2536, loss = 0.07885415\n",
      "Iteration 2537, loss = 0.07884469\n",
      "Iteration 2538, loss = 0.07883523\n",
      "Iteration 2539, loss = 0.07882577\n",
      "Iteration 2540, loss = 0.07881631\n",
      "Iteration 2541, loss = 0.07880686\n",
      "Iteration 2542, loss = 0.07879740\n",
      "Iteration 2543, loss = 0.07878795\n",
      "Iteration 2544, loss = 0.07877850\n",
      "Iteration 2545, loss = 0.07876905\n",
      "Iteration 2546, loss = 0.07875961\n",
      "Iteration 2547, loss = 0.07875016\n",
      "Iteration 2548, loss = 0.07874071\n",
      "Iteration 2549, loss = 0.07873127\n",
      "Iteration 2550, loss = 0.07872183\n",
      "Iteration 2551, loss = 0.07871239\n",
      "Iteration 2552, loss = 0.07870295\n",
      "Iteration 2553, loss = 0.07869351\n",
      "Iteration 2554, loss = 0.07868407\n",
      "Iteration 2555, loss = 0.07867464\n",
      "Iteration 2556, loss = 0.07866521\n",
      "Iteration 2557, loss = 0.07865577\n",
      "Iteration 2558, loss = 0.07864634\n",
      "Iteration 2559, loss = 0.07863691\n",
      "Iteration 2560, loss = 0.07862748\n",
      "Iteration 2561, loss = 0.07861806\n",
      "Iteration 2562, loss = 0.07860863\n",
      "Iteration 2563, loss = 0.07859921\n",
      "Iteration 2564, loss = 0.07858978\n",
      "Iteration 2565, loss = 0.07858036\n",
      "Iteration 2566, loss = 0.07857094\n",
      "Iteration 2567, loss = 0.07856152\n",
      "Iteration 2568, loss = 0.07855210\n",
      "Iteration 2569, loss = 0.07854269\n",
      "Iteration 2570, loss = 0.07853327\n",
      "Iteration 2571, loss = 0.07852386\n",
      "Iteration 2572, loss = 0.07851444\n",
      "Iteration 2573, loss = 0.07850503\n",
      "Iteration 2574, loss = 0.07849562\n",
      "Iteration 2575, loss = 0.07848621\n",
      "Iteration 2576, loss = 0.07847681\n",
      "Iteration 2577, loss = 0.07846740\n",
      "Iteration 2578, loss = 0.07845800\n",
      "Iteration 2579, loss = 0.07844859\n",
      "Iteration 2580, loss = 0.07843919\n",
      "Iteration 2581, loss = 0.07842979\n",
      "Iteration 2582, loss = 0.07842039\n",
      "Iteration 2583, loss = 0.07841099\n",
      "Iteration 2584, loss = 0.07840159\n",
      "Iteration 2585, loss = 0.07839219\n",
      "Iteration 2586, loss = 0.07838280\n",
      "Iteration 2587, loss = 0.07837341\n",
      "Iteration 2588, loss = 0.07836401\n",
      "Iteration 2589, loss = 0.07835462\n",
      "Iteration 2590, loss = 0.07834523\n",
      "Iteration 2591, loss = 0.07833584\n",
      "Iteration 2592, loss = 0.07832645\n",
      "Iteration 2593, loss = 0.07831707\n",
      "Iteration 2594, loss = 0.07830768\n",
      "Iteration 2595, loss = 0.07829830\n",
      "Iteration 2596, loss = 0.07828891\n",
      "Iteration 2597, loss = 0.07827953\n",
      "Iteration 2598, loss = 0.07827015\n",
      "Iteration 2599, loss = 0.07826077\n",
      "Iteration 2600, loss = 0.07825139\n",
      "Iteration 2601, loss = 0.07824202\n",
      "Iteration 2602, loss = 0.07823264\n",
      "Iteration 2603, loss = 0.07822327\n",
      "Iteration 2604, loss = 0.07821389\n",
      "Iteration 2605, loss = 0.07820452\n",
      "Iteration 2606, loss = 0.07819515\n",
      "Iteration 2607, loss = 0.07818578\n",
      "Iteration 2608, loss = 0.07817641\n",
      "Iteration 2609, loss = 0.07816704\n",
      "Iteration 2610, loss = 0.07815767\n",
      "Iteration 2611, loss = 0.07814831\n",
      "Iteration 2612, loss = 0.07813894\n",
      "Iteration 2613, loss = 0.07812958\n",
      "Iteration 2614, loss = 0.07812022\n",
      "Iteration 2615, loss = 0.07811086\n",
      "Iteration 2616, loss = 0.07810149\n",
      "Iteration 2617, loss = 0.07809214\n",
      "Iteration 2618, loss = 0.07808278\n",
      "Iteration 2619, loss = 0.07807342\n",
      "Iteration 2620, loss = 0.07806406\n",
      "Iteration 2621, loss = 0.07805471\n",
      "Iteration 2622, loss = 0.07804536\n",
      "Iteration 2623, loss = 0.07803600\n",
      "Iteration 2624, loss = 0.07802665\n",
      "Iteration 2625, loss = 0.07801730\n",
      "Iteration 2626, loss = 0.07800795\n",
      "Iteration 2627, loss = 0.07799860\n",
      "Iteration 2628, loss = 0.07798926\n",
      "Iteration 2629, loss = 0.07797991\n",
      "Iteration 2630, loss = 0.07797056\n",
      "Iteration 2631, loss = 0.07796122\n",
      "Iteration 2632, loss = 0.07795188\n",
      "Iteration 2633, loss = 0.07794253\n",
      "Iteration 2634, loss = 0.07793319\n",
      "Iteration 2635, loss = 0.07792385\n",
      "Iteration 2636, loss = 0.07791451\n",
      "Iteration 2637, loss = 0.07790517\n",
      "Iteration 2638, loss = 0.07789584\n",
      "Iteration 2639, loss = 0.07788650\n",
      "Iteration 2640, loss = 0.07787717\n",
      "Iteration 2641, loss = 0.07786783\n",
      "Iteration 2642, loss = 0.07785850\n",
      "Iteration 2643, loss = 0.07784917\n",
      "Iteration 2644, loss = 0.07783984\n",
      "Iteration 2645, loss = 0.07783051\n",
      "Iteration 2646, loss = 0.07782118\n",
      "Iteration 2647, loss = 0.07781185\n",
      "Iteration 2648, loss = 0.07780252\n",
      "Iteration 2649, loss = 0.07779319\n",
      "Iteration 2650, loss = 0.07778387\n",
      "Iteration 2651, loss = 0.07777454\n",
      "Iteration 2652, loss = 0.07776522\n",
      "Iteration 2653, loss = 0.07775590\n",
      "Iteration 2654, loss = 0.07774658\n",
      "Iteration 2655, loss = 0.07773726\n",
      "Iteration 2656, loss = 0.07772794\n",
      "Iteration 2657, loss = 0.07771862\n",
      "Iteration 2658, loss = 0.07770930\n",
      "Iteration 2659, loss = 0.07769998\n",
      "Iteration 2660, loss = 0.07769067\n",
      "Iteration 2661, loss = 0.07768135\n",
      "Iteration 2662, loss = 0.07767204\n",
      "Iteration 2663, loss = 0.07766273\n",
      "Iteration 2664, loss = 0.07765341\n",
      "Iteration 2665, loss = 0.07764410\n",
      "Iteration 2666, loss = 0.07763479\n",
      "Iteration 2667, loss = 0.07762548\n",
      "Iteration 2668, loss = 0.07761617\n",
      "Iteration 2669, loss = 0.07760687\n",
      "Iteration 2670, loss = 0.07759756\n",
      "Iteration 2671, loss = 0.07758825\n",
      "Iteration 2672, loss = 0.07757895\n",
      "Iteration 2673, loss = 0.07756965\n",
      "Iteration 2674, loss = 0.07756034\n",
      "Iteration 2675, loss = 0.07755104\n",
      "Iteration 2676, loss = 0.07754174\n",
      "Iteration 2677, loss = 0.07753244\n",
      "Iteration 2678, loss = 0.07752314\n",
      "Iteration 2679, loss = 0.07751384\n",
      "Iteration 2680, loss = 0.07750454\n",
      "Iteration 2681, loss = 0.07749524\n",
      "Iteration 2682, loss = 0.07748595\n",
      "Iteration 2683, loss = 0.07747665\n",
      "Iteration 2684, loss = 0.07746736\n",
      "Iteration 2685, loss = 0.07745806\n",
      "Iteration 2686, loss = 0.07744877\n",
      "Iteration 2687, loss = 0.07743948\n",
      "Iteration 2688, loss = 0.07743019\n",
      "Iteration 2689, loss = 0.07742090\n",
      "Iteration 2690, loss = 0.07741161\n",
      "Iteration 2691, loss = 0.07740232\n",
      "Iteration 2692, loss = 0.07739303\n",
      "Iteration 2693, loss = 0.07738374\n",
      "Iteration 2694, loss = 0.07737446\n",
      "Iteration 2695, loss = 0.07736517\n",
      "Iteration 2696, loss = 0.07735589\n",
      "Iteration 2697, loss = 0.07734660\n",
      "Iteration 2698, loss = 0.07733732\n",
      "Iteration 2699, loss = 0.07732804\n",
      "Iteration 2700, loss = 0.07731875\n",
      "Iteration 2701, loss = 0.07730947\n",
      "Iteration 2702, loss = 0.07730019\n",
      "Iteration 2703, loss = 0.07729091\n",
      "Iteration 2704, loss = 0.07728164\n",
      "Iteration 2705, loss = 0.07727236\n",
      "Iteration 2706, loss = 0.07726308\n",
      "Iteration 2707, loss = 0.07725380\n",
      "Iteration 2708, loss = 0.07724453\n",
      "Iteration 2709, loss = 0.07723525\n",
      "Iteration 2710, loss = 0.07722598\n",
      "Iteration 2711, loss = 0.07721671\n",
      "Iteration 2712, loss = 0.07720744\n",
      "Iteration 2713, loss = 0.07719816\n",
      "Iteration 2714, loss = 0.07718889\n",
      "Iteration 2715, loss = 0.07717962\n",
      "Iteration 2716, loss = 0.07717035\n",
      "Iteration 2717, loss = 0.07716108\n",
      "Iteration 2718, loss = 0.07715182\n",
      "Iteration 2719, loss = 0.07714255\n",
      "Iteration 2720, loss = 0.07713328\n",
      "Iteration 2721, loss = 0.07712402\n",
      "Iteration 2722, loss = 0.07711475\n",
      "Iteration 2723, loss = 0.07710549\n",
      "Iteration 2724, loss = 0.07709622\n",
      "Iteration 2725, loss = 0.07708696\n",
      "Iteration 2726, loss = 0.07707770\n",
      "Iteration 2727, loss = 0.07706844\n",
      "Iteration 2728, loss = 0.07705917\n",
      "Iteration 2729, loss = 0.07704991\n",
      "Iteration 2730, loss = 0.07704065\n",
      "Iteration 2731, loss = 0.07703140\n",
      "Iteration 2732, loss = 0.07702214\n",
      "Iteration 2733, loss = 0.07701288\n",
      "Iteration 2734, loss = 0.07700362\n",
      "Iteration 2735, loss = 0.07699437\n",
      "Iteration 2736, loss = 0.07698511\n",
      "Iteration 2737, loss = 0.07697586\n",
      "Iteration 2738, loss = 0.07696660\n",
      "Iteration 2739, loss = 0.07695735\n",
      "Iteration 2740, loss = 0.07694810\n",
      "Iteration 2741, loss = 0.07693884\n",
      "Iteration 2742, loss = 0.07692959\n",
      "Iteration 2743, loss = 0.07692034\n",
      "Iteration 2744, loss = 0.07691109\n",
      "Iteration 2745, loss = 0.07690184\n",
      "Iteration 2746, loss = 0.07689259\n",
      "Iteration 2747, loss = 0.07688334\n",
      "Iteration 2748, loss = 0.07687410\n",
      "Iteration 2749, loss = 0.07686485\n",
      "Iteration 2750, loss = 0.07685560\n",
      "Iteration 2751, loss = 0.07684636\n",
      "Iteration 2752, loss = 0.07683711\n",
      "Iteration 2753, loss = 0.07682787\n",
      "Iteration 2754, loss = 0.07681862\n",
      "Iteration 2755, loss = 0.07680938\n",
      "Iteration 2756, loss = 0.07680014\n",
      "Iteration 2757, loss = 0.07679089\n",
      "Iteration 2758, loss = 0.07678165\n",
      "Iteration 2759, loss = 0.07677241\n",
      "Iteration 2760, loss = 0.07676317\n",
      "Iteration 2761, loss = 0.07675393\n",
      "Iteration 2762, loss = 0.07674469\n",
      "Iteration 2763, loss = 0.07673545\n",
      "Iteration 2764, loss = 0.07672622\n",
      "Iteration 2765, loss = 0.07671698\n",
      "Iteration 2766, loss = 0.07670774\n",
      "Iteration 2767, loss = 0.07669851\n",
      "Iteration 2768, loss = 0.07668927\n",
      "Iteration 2769, loss = 0.07668004\n",
      "Iteration 2770, loss = 0.07667080\n",
      "Iteration 2771, loss = 0.07666157\n",
      "Iteration 2772, loss = 0.07665233\n",
      "Iteration 2773, loss = 0.07664310\n",
      "Iteration 2774, loss = 0.07663387\n",
      "Iteration 2775, loss = 0.07662464\n",
      "Iteration 2776, loss = 0.07661540\n",
      "Iteration 2777, loss = 0.07660617\n",
      "Iteration 2778, loss = 0.07659694\n",
      "Iteration 2779, loss = 0.07658771\n",
      "Iteration 2780, loss = 0.07657849\n",
      "Iteration 2781, loss = 0.07656926\n",
      "Iteration 2782, loss = 0.07656003\n",
      "Iteration 2783, loss = 0.07655080\n",
      "Iteration 2784, loss = 0.07654157\n",
      "Iteration 2785, loss = 0.07653235\n",
      "Iteration 2786, loss = 0.07652312\n",
      "Iteration 2787, loss = 0.07651390\n",
      "Iteration 2788, loss = 0.07650467\n",
      "Iteration 2789, loss = 0.07649545\n",
      "Iteration 2790, loss = 0.07648622\n",
      "Iteration 2791, loss = 0.07647700\n",
      "Iteration 2792, loss = 0.07646778\n",
      "Iteration 2793, loss = 0.07645855\n",
      "Iteration 2794, loss = 0.07644933\n",
      "Iteration 2795, loss = 0.07644011\n",
      "Iteration 2796, loss = 0.07643089\n",
      "Iteration 2797, loss = 0.07642167\n",
      "Iteration 2798, loss = 0.07641245\n",
      "Iteration 2799, loss = 0.07640323\n",
      "Iteration 2800, loss = 0.07639401\n",
      "Iteration 2801, loss = 0.07638479\n",
      "Iteration 2802, loss = 0.07637558\n",
      "Iteration 2803, loss = 0.07636636\n",
      "Iteration 2804, loss = 0.07635714\n",
      "Iteration 2805, loss = 0.07634792\n",
      "Iteration 2806, loss = 0.07633871\n",
      "Iteration 2807, loss = 0.07632949\n",
      "Iteration 2808, loss = 0.07632028\n",
      "Iteration 2809, loss = 0.07631106\n",
      "Iteration 2810, loss = 0.07630185\n",
      "Iteration 2811, loss = 0.07629263\n",
      "Iteration 2812, loss = 0.07628342\n",
      "Iteration 2813, loss = 0.07627421\n",
      "Iteration 2814, loss = 0.07626500\n",
      "Iteration 2815, loss = 0.07625578\n",
      "Iteration 2816, loss = 0.07624657\n",
      "Iteration 2817, loss = 0.07623736\n",
      "Iteration 2818, loss = 0.07622815\n",
      "Iteration 2819, loss = 0.07621894\n",
      "Iteration 2820, loss = 0.07620973\n",
      "Iteration 2821, loss = 0.07620052\n",
      "Iteration 2822, loss = 0.07619131\n",
      "Iteration 2823, loss = 0.07618210\n",
      "Iteration 2824, loss = 0.07617289\n",
      "Iteration 2825, loss = 0.07616369\n",
      "Iteration 2826, loss = 0.07615448\n",
      "Iteration 2827, loss = 0.07614527\n",
      "Iteration 2828, loss = 0.07613607\n",
      "Iteration 2829, loss = 0.07612686\n",
      "Iteration 2830, loss = 0.07611765\n",
      "Iteration 2831, loss = 0.07610845\n",
      "Iteration 2832, loss = 0.07609924\n",
      "Iteration 2833, loss = 0.07609004\n",
      "Iteration 2834, loss = 0.07608083\n",
      "Iteration 2835, loss = 0.07607163\n",
      "Iteration 2836, loss = 0.07606243\n",
      "Iteration 2837, loss = 0.07605322\n",
      "Iteration 2838, loss = 0.07604402\n",
      "Iteration 2839, loss = 0.07603482\n",
      "Iteration 2840, loss = 0.07602562\n",
      "Iteration 2841, loss = 0.07601642\n",
      "Iteration 2842, loss = 0.07600721\n",
      "Iteration 2843, loss = 0.07599801\n",
      "Iteration 2844, loss = 0.07598881\n",
      "Iteration 2845, loss = 0.07597961\n",
      "Iteration 2846, loss = 0.07597041\n",
      "Iteration 2847, loss = 0.07596121\n",
      "Iteration 2848, loss = 0.07595202\n",
      "Iteration 2849, loss = 0.07594282\n",
      "Iteration 2850, loss = 0.07593362\n",
      "Iteration 2851, loss = 0.07592442\n",
      "Iteration 2852, loss = 0.07591522\n",
      "Iteration 2853, loss = 0.07590603\n",
      "Iteration 2854, loss = 0.07589683\n",
      "Iteration 2855, loss = 0.07588763\n",
      "Iteration 2856, loss = 0.07587844\n",
      "Iteration 2857, loss = 0.07586924\n",
      "Iteration 2858, loss = 0.07586004\n",
      "Iteration 2859, loss = 0.07585085\n",
      "Iteration 2860, loss = 0.07584165\n",
      "Iteration 2861, loss = 0.07583246\n",
      "Iteration 2862, loss = 0.07582326\n",
      "Iteration 2863, loss = 0.07581407\n",
      "Iteration 2864, loss = 0.07580488\n",
      "Iteration 2865, loss = 0.07579568\n",
      "Iteration 2866, loss = 0.07578649\n",
      "Iteration 2867, loss = 0.07577730\n",
      "Iteration 2868, loss = 0.07576811\n",
      "Iteration 2869, loss = 0.07575891\n",
      "Iteration 2870, loss = 0.07574972\n",
      "Iteration 2871, loss = 0.07574053\n",
      "Iteration 2872, loss = 0.07573134\n",
      "Iteration 2873, loss = 0.07572215\n",
      "Iteration 2874, loss = 0.07571296\n",
      "Iteration 2875, loss = 0.07570377\n",
      "Iteration 2876, loss = 0.07569458\n",
      "Iteration 2877, loss = 0.07568539\n",
      "Iteration 2878, loss = 0.07567620\n",
      "Iteration 2879, loss = 0.07566701\n",
      "Iteration 2880, loss = 0.07565782\n",
      "Iteration 2881, loss = 0.07564863\n",
      "Iteration 2882, loss = 0.07563944\n",
      "Iteration 2883, loss = 0.07563025\n",
      "Iteration 2884, loss = 0.07562107\n",
      "Iteration 2885, loss = 0.07561188\n",
      "Iteration 2886, loss = 0.07560269\n",
      "Iteration 2887, loss = 0.07559350\n",
      "Iteration 2888, loss = 0.07558432\n",
      "Iteration 2889, loss = 0.07557513\n",
      "Iteration 2890, loss = 0.07556594\n",
      "Iteration 2891, loss = 0.07555676\n",
      "Iteration 2892, loss = 0.07554757\n",
      "Iteration 2893, loss = 0.07553839\n",
      "Iteration 2894, loss = 0.07552920\n",
      "Iteration 2895, loss = 0.07552002\n",
      "Iteration 2896, loss = 0.07551083\n",
      "Iteration 2897, loss = 0.07550165\n",
      "Iteration 2898, loss = 0.07549246\n",
      "Iteration 2899, loss = 0.07548328\n",
      "Iteration 2900, loss = 0.07547409\n",
      "Iteration 2901, loss = 0.07546491\n",
      "Iteration 2902, loss = 0.07545573\n",
      "Iteration 2903, loss = 0.07544654\n",
      "Iteration 2904, loss = 0.07543736\n",
      "Iteration 2905, loss = 0.07542818\n",
      "Iteration 2906, loss = 0.07541899\n",
      "Iteration 2907, loss = 0.07540981\n",
      "Iteration 2908, loss = 0.07540063\n",
      "Iteration 2909, loss = 0.07539145\n",
      "Iteration 2910, loss = 0.07538227\n",
      "Iteration 2911, loss = 0.07537308\n",
      "Iteration 2912, loss = 0.07536390\n",
      "Iteration 2913, loss = 0.07535472\n",
      "Iteration 2914, loss = 0.07534554\n",
      "Iteration 2915, loss = 0.07533636\n",
      "Iteration 2916, loss = 0.07532718\n",
      "Iteration 2917, loss = 0.07531800\n",
      "Iteration 2918, loss = 0.07530882\n",
      "Iteration 2919, loss = 0.07529964\n",
      "Iteration 2920, loss = 0.07529046\n",
      "Iteration 2921, loss = 0.07528128\n",
      "Iteration 2922, loss = 0.07527210\n",
      "Iteration 2923, loss = 0.07526292\n",
      "Iteration 2924, loss = 0.07525374\n",
      "Iteration 2925, loss = 0.07524456\n",
      "Iteration 2926, loss = 0.07523538\n",
      "Iteration 2927, loss = 0.07522620\n",
      "Iteration 2928, loss = 0.07521703\n",
      "Iteration 2929, loss = 0.07520785\n",
      "Iteration 2930, loss = 0.07519867\n",
      "Iteration 2931, loss = 0.07518949\n",
      "Iteration 2932, loss = 0.07518031\n",
      "Iteration 2933, loss = 0.07517114\n",
      "Iteration 2934, loss = 0.07516196\n",
      "Iteration 2935, loss = 0.07515278\n",
      "Iteration 2936, loss = 0.07514360\n",
      "Iteration 2937, loss = 0.07513443\n",
      "Iteration 2938, loss = 0.07512525\n",
      "Iteration 2939, loss = 0.07511607\n",
      "Iteration 2940, loss = 0.07510690\n",
      "Iteration 2941, loss = 0.07509772\n",
      "Iteration 2942, loss = 0.07508854\n",
      "Iteration 2943, loss = 0.07507937\n",
      "Iteration 2944, loss = 0.07507019\n",
      "Iteration 2945, loss = 0.07506101\n",
      "Iteration 2946, loss = 0.07505184\n",
      "Iteration 2947, loss = 0.07504266\n",
      "Iteration 2948, loss = 0.07503349\n",
      "Iteration 2949, loss = 0.07502431\n",
      "Iteration 2950, loss = 0.07501514\n",
      "Iteration 2951, loss = 0.07500596\n",
      "Iteration 2952, loss = 0.07499679\n",
      "Iteration 2953, loss = 0.07498761\n",
      "Iteration 2954, loss = 0.07497844\n",
      "Iteration 2955, loss = 0.07496926\n",
      "Iteration 2956, loss = 0.07496009\n",
      "Iteration 2957, loss = 0.07495091\n",
      "Iteration 2958, loss = 0.07494174\n",
      "Iteration 2959, loss = 0.07493256\n",
      "Iteration 2960, loss = 0.07492339\n",
      "Iteration 2961, loss = 0.07491421\n",
      "Iteration 2962, loss = 0.07490504\n",
      "Iteration 2963, loss = 0.07489587\n",
      "Iteration 2964, loss = 0.07488669\n",
      "Iteration 2965, loss = 0.07487752\n",
      "Iteration 2966, loss = 0.07486834\n",
      "Iteration 2967, loss = 0.07485917\n",
      "Iteration 2968, loss = 0.07485000\n",
      "Iteration 2969, loss = 0.07484082\n",
      "Iteration 2970, loss = 0.07483165\n",
      "Iteration 2971, loss = 0.07482248\n",
      "Iteration 2972, loss = 0.07481330\n",
      "Iteration 2973, loss = 0.07480413\n",
      "Iteration 2974, loss = 0.07479496\n",
      "Iteration 2975, loss = 0.07478578\n",
      "Iteration 2976, loss = 0.07477661\n",
      "Iteration 2977, loss = 0.07476744\n",
      "Iteration 2978, loss = 0.07475827\n",
      "Iteration 2979, loss = 0.07474909\n",
      "Iteration 2980, loss = 0.07473992\n",
      "Iteration 2981, loss = 0.07473075\n",
      "Iteration 2982, loss = 0.07472157\n",
      "Iteration 2983, loss = 0.07471240\n",
      "Iteration 2984, loss = 0.07470323\n",
      "Iteration 2985, loss = 0.07469406\n",
      "Iteration 2986, loss = 0.07468488\n",
      "Iteration 2987, loss = 0.07467571\n",
      "Iteration 2988, loss = 0.07466654\n",
      "Iteration 2989, loss = 0.07465737\n",
      "Iteration 2990, loss = 0.07464819\n",
      "Iteration 2991, loss = 0.07463902\n",
      "Iteration 2992, loss = 0.07462985\n",
      "Iteration 2993, loss = 0.07462068\n",
      "Iteration 2994, loss = 0.07461151\n",
      "Iteration 2995, loss = 0.07460233\n",
      "Iteration 2996, loss = 0.07459316\n",
      "Iteration 2997, loss = 0.07458399\n",
      "Iteration 2998, loss = 0.07457482\n",
      "Iteration 2999, loss = 0.07456565\n",
      "Iteration 3000, loss = 0.07455647\n",
      "Iteration 3001, loss = 0.07454730\n",
      "Iteration 3002, loss = 0.07453813\n",
      "Iteration 3003, loss = 0.07452896\n",
      "Iteration 3004, loss = 0.07451979\n",
      "Iteration 3005, loss = 0.07451061\n",
      "Iteration 3006, loss = 0.07450144\n",
      "Iteration 3007, loss = 0.07449227\n",
      "Iteration 3008, loss = 0.07448310\n",
      "Iteration 3009, loss = 0.07447393\n",
      "Iteration 3010, loss = 0.07446475\n",
      "Iteration 3011, loss = 0.07445558\n",
      "Iteration 3012, loss = 0.07444641\n",
      "Iteration 3013, loss = 0.07443724\n",
      "Iteration 3014, loss = 0.07442807\n",
      "Iteration 3015, loss = 0.07441889\n",
      "Iteration 3016, loss = 0.07440972\n",
      "Iteration 3017, loss = 0.07440055\n",
      "Iteration 3018, loss = 0.07439138\n",
      "Iteration 3019, loss = 0.07438221\n",
      "Iteration 3020, loss = 0.07437303\n",
      "Iteration 3021, loss = 0.07436386\n",
      "Iteration 3022, loss = 0.07435469\n",
      "Iteration 3023, loss = 0.07434552\n",
      "Iteration 3024, loss = 0.07433634\n",
      "Iteration 3025, loss = 0.07432717\n",
      "Iteration 3026, loss = 0.07431800\n",
      "Iteration 3027, loss = 0.07430883\n",
      "Iteration 3028, loss = 0.07429966\n",
      "Iteration 3029, loss = 0.07429048\n",
      "Iteration 3030, loss = 0.07428131\n",
      "Iteration 3031, loss = 0.07427214\n",
      "Iteration 3032, loss = 0.07426297\n",
      "Iteration 3033, loss = 0.07425379\n",
      "Iteration 3034, loss = 0.07424462\n",
      "Iteration 3035, loss = 0.07423545\n",
      "Iteration 3036, loss = 0.07422628\n",
      "Iteration 3037, loss = 0.07421710\n",
      "Iteration 3038, loss = 0.07420793\n",
      "Iteration 3039, loss = 0.07419876\n",
      "Iteration 3040, loss = 0.07418959\n",
      "Iteration 3041, loss = 0.07418041\n",
      "Iteration 3042, loss = 0.07417124\n",
      "Iteration 3043, loss = 0.07416207\n",
      "Iteration 3044, loss = 0.07415289\n",
      "Iteration 3045, loss = 0.07414372\n",
      "Iteration 3046, loss = 0.07413455\n",
      "Iteration 3047, loss = 0.07412537\n",
      "Iteration 3048, loss = 0.07411620\n",
      "Iteration 3049, loss = 0.07410703\n",
      "Iteration 3050, loss = 0.07409785\n",
      "Iteration 3051, loss = 0.07408868\n",
      "Iteration 3052, loss = 0.07407951\n",
      "Iteration 3053, loss = 0.07407033\n",
      "Iteration 3054, loss = 0.07406116\n",
      "Iteration 3055, loss = 0.07405198\n",
      "Iteration 3056, loss = 0.07404281\n",
      "Iteration 3057, loss = 0.07403364\n",
      "Iteration 3058, loss = 0.07402446\n",
      "Iteration 3059, loss = 0.07401529\n",
      "Iteration 3060, loss = 0.07400611\n",
      "Iteration 3061, loss = 0.07399694\n",
      "Iteration 3062, loss = 0.07398777\n",
      "Iteration 3063, loss = 0.07397859\n",
      "Iteration 3064, loss = 0.07396942\n",
      "Iteration 3065, loss = 0.07396024\n",
      "Iteration 3066, loss = 0.07395107\n",
      "Iteration 3067, loss = 0.07394189\n",
      "Iteration 3068, loss = 0.07393272\n",
      "Iteration 3069, loss = 0.07392354\n",
      "Iteration 3070, loss = 0.07391437\n",
      "Iteration 3071, loss = 0.07390519\n",
      "Iteration 3072, loss = 0.07389601\n",
      "Iteration 3073, loss = 0.07388684\n",
      "Iteration 3074, loss = 0.07387766\n",
      "Iteration 3075, loss = 0.07386849\n",
      "Iteration 3076, loss = 0.07385931\n",
      "Iteration 3077, loss = 0.07385014\n",
      "Iteration 3078, loss = 0.07384096\n",
      "Iteration 3079, loss = 0.07383178\n",
      "Iteration 3080, loss = 0.07382261\n",
      "Iteration 3081, loss = 0.07381343\n",
      "Iteration 3082, loss = 0.07380425\n",
      "Iteration 3083, loss = 0.07379508\n",
      "Iteration 3084, loss = 0.07378590\n",
      "Iteration 3085, loss = 0.07377672\n",
      "Iteration 3086, loss = 0.07376755\n",
      "Iteration 3087, loss = 0.07375837\n",
      "Iteration 3088, loss = 0.07374919\n",
      "Iteration 3089, loss = 0.07374001\n",
      "Iteration 3090, loss = 0.07373084\n",
      "Iteration 3091, loss = 0.07372166\n",
      "Iteration 3092, loss = 0.07371248\n",
      "Iteration 3093, loss = 0.07370330\n",
      "Iteration 3094, loss = 0.07369412\n",
      "Iteration 3095, loss = 0.07368494\n",
      "Iteration 3096, loss = 0.07367577\n",
      "Iteration 3097, loss = 0.07366659\n",
      "Iteration 3098, loss = 0.07365741\n",
      "Iteration 3099, loss = 0.07364823\n",
      "Iteration 3100, loss = 0.07363905\n",
      "Iteration 3101, loss = 0.07362987\n",
      "Iteration 3102, loss = 0.07362069\n",
      "Iteration 3103, loss = 0.07361151\n",
      "Iteration 3104, loss = 0.07360233\n",
      "Iteration 3105, loss = 0.07359315\n",
      "Iteration 3106, loss = 0.07358397\n",
      "Iteration 3107, loss = 0.07357479\n",
      "Iteration 3108, loss = 0.07356561\n",
      "Iteration 3109, loss = 0.07355643\n",
      "Iteration 3110, loss = 0.07354725\n",
      "Iteration 3111, loss = 0.07353807\n",
      "Iteration 3112, loss = 0.07352889\n",
      "Iteration 3113, loss = 0.07351971\n",
      "Iteration 3114, loss = 0.07351052\n",
      "Iteration 3115, loss = 0.07350134\n",
      "Iteration 3116, loss = 0.07349216\n",
      "Iteration 3117, loss = 0.07348298\n",
      "Iteration 3118, loss = 0.07347380\n",
      "Iteration 3119, loss = 0.07346461\n",
      "Iteration 3120, loss = 0.07345543\n",
      "Iteration 3121, loss = 0.07344625\n",
      "Iteration 3122, loss = 0.07343706\n",
      "Iteration 3123, loss = 0.07342788\n",
      "Iteration 3124, loss = 0.07341870\n",
      "Iteration 3125, loss = 0.07340951\n",
      "Iteration 3126, loss = 0.07340033\n",
      "Iteration 3127, loss = 0.07339115\n",
      "Iteration 3128, loss = 0.07338196\n",
      "Iteration 3129, loss = 0.07337278\n",
      "Iteration 3130, loss = 0.07336359\n",
      "Iteration 3131, loss = 0.07335441\n",
      "Iteration 3132, loss = 0.07334522\n",
      "Iteration 3133, loss = 0.07333604\n",
      "Iteration 3134, loss = 0.07332685\n",
      "Iteration 3135, loss = 0.07331767\n",
      "Iteration 3136, loss = 0.07330848\n",
      "Iteration 3137, loss = 0.07329930\n",
      "Iteration 3138, loss = 0.07329011\n",
      "Iteration 3139, loss = 0.07328092\n",
      "Iteration 3140, loss = 0.07327174\n",
      "Iteration 3141, loss = 0.07326255\n",
      "Iteration 3142, loss = 0.07325336\n",
      "Iteration 3143, loss = 0.07324418\n",
      "Iteration 3144, loss = 0.07323499\n",
      "Iteration 3145, loss = 0.07322580\n",
      "Iteration 3146, loss = 0.07321661\n",
      "Iteration 3147, loss = 0.07320743\n",
      "Iteration 3148, loss = 0.07319824\n",
      "Iteration 3149, loss = 0.07318905\n",
      "Iteration 3150, loss = 0.07317986\n",
      "Iteration 3151, loss = 0.07317067\n",
      "Iteration 3152, loss = 0.07316148\n",
      "Iteration 3153, loss = 0.07315229\n",
      "Iteration 3154, loss = 0.07314310\n",
      "Iteration 3155, loss = 0.07313391\n",
      "Iteration 3156, loss = 0.07312472\n",
      "Iteration 3157, loss = 0.07311553\n",
      "Iteration 3158, loss = 0.07310634\n",
      "Iteration 3159, loss = 0.07309715\n",
      "Iteration 3160, loss = 0.07308796\n",
      "Iteration 3161, loss = 0.07307877\n",
      "Iteration 3162, loss = 0.07306958\n",
      "Iteration 3163, loss = 0.07306039\n",
      "Iteration 3164, loss = 0.07305120\n",
      "Iteration 3165, loss = 0.07304200\n",
      "Iteration 3166, loss = 0.07303281\n",
      "Iteration 3167, loss = 0.07302362\n",
      "Iteration 3168, loss = 0.07301443\n",
      "Iteration 3169, loss = 0.07300523\n",
      "Iteration 3170, loss = 0.07299604\n",
      "Iteration 3171, loss = 0.07298685\n",
      "Iteration 3172, loss = 0.07297765\n",
      "Iteration 3173, loss = 0.07296846\n",
      "Iteration 3174, loss = 0.07295926\n",
      "Iteration 3175, loss = 0.07295007\n",
      "Iteration 3176, loss = 0.07294087\n",
      "Iteration 3177, loss = 0.07293168\n",
      "Iteration 3178, loss = 0.07292248\n",
      "Iteration 3179, loss = 0.07291329\n",
      "Iteration 3180, loss = 0.07290409\n",
      "Iteration 3181, loss = 0.07289490\n",
      "Iteration 3182, loss = 0.07288570\n",
      "Iteration 3183, loss = 0.07287650\n",
      "Iteration 3184, loss = 0.07286731\n",
      "Iteration 3185, loss = 0.07285811\n",
      "Iteration 3186, loss = 0.07284891\n",
      "Iteration 3187, loss = 0.07283972\n",
      "Iteration 3188, loss = 0.07283052\n",
      "Iteration 3189, loss = 0.07282132\n",
      "Iteration 3190, loss = 0.07281212\n",
      "Iteration 3191, loss = 0.07280292\n",
      "Iteration 3192, loss = 0.07279372\n",
      "Iteration 3193, loss = 0.07278452\n",
      "Iteration 3194, loss = 0.07277532\n",
      "Iteration 3195, loss = 0.07276613\n",
      "Iteration 3196, loss = 0.07275693\n",
      "Iteration 3197, loss = 0.07274772\n",
      "Iteration 3198, loss = 0.07273852\n",
      "Iteration 3199, loss = 0.07272932\n",
      "Iteration 3200, loss = 0.07272012\n",
      "Iteration 3201, loss = 0.07271092\n",
      "Iteration 3202, loss = 0.07270172\n",
      "Iteration 3203, loss = 0.07269252\n",
      "Iteration 3204, loss = 0.07268332\n",
      "Iteration 3205, loss = 0.07267411\n",
      "Iteration 3206, loss = 0.07266491\n",
      "Iteration 3207, loss = 0.07265571\n",
      "Iteration 3208, loss = 0.07264650\n",
      "Iteration 3209, loss = 0.07263730\n",
      "Iteration 3210, loss = 0.07262810\n",
      "Iteration 3211, loss = 0.07261889\n",
      "Iteration 3212, loss = 0.07260969\n",
      "Iteration 3213, loss = 0.07260048\n",
      "Iteration 3214, loss = 0.07259128\n",
      "Iteration 3215, loss = 0.07258207\n",
      "Iteration 3216, loss = 0.07257287\n",
      "Iteration 3217, loss = 0.07256366\n",
      "Iteration 3218, loss = 0.07255446\n",
      "Iteration 3219, loss = 0.07254525\n",
      "Iteration 3220, loss = 0.07253604\n",
      "Iteration 3221, loss = 0.07252684\n",
      "Iteration 3222, loss = 0.07251763\n",
      "Iteration 3223, loss = 0.07250842\n",
      "Iteration 3224, loss = 0.07249921\n",
      "Iteration 3225, loss = 0.07249001\n",
      "Iteration 3226, loss = 0.07248080\n",
      "Iteration 3227, loss = 0.07247159\n",
      "Iteration 3228, loss = 0.07246238\n",
      "Iteration 3229, loss = 0.07245317\n",
      "Iteration 3230, loss = 0.07244396\n",
      "Iteration 3231, loss = 0.07243475\n",
      "Iteration 3232, loss = 0.07242554\n",
      "Iteration 3233, loss = 0.07241633\n",
      "Iteration 3234, loss = 0.07240712\n",
      "Iteration 3235, loss = 0.07239791\n",
      "Iteration 3236, loss = 0.07238870\n",
      "Iteration 3237, loss = 0.07237949\n",
      "Iteration 3238, loss = 0.07237028\n",
      "Iteration 3239, loss = 0.07236106\n",
      "Iteration 3240, loss = 0.07235185\n",
      "Iteration 3241, loss = 0.07234264\n",
      "Iteration 3242, loss = 0.07233343\n",
      "Iteration 3243, loss = 0.07232421\n",
      "Iteration 3244, loss = 0.07231500\n",
      "Iteration 3245, loss = 0.07230579\n",
      "Iteration 3246, loss = 0.07229657\n",
      "Iteration 3247, loss = 0.07228736\n",
      "Iteration 3248, loss = 0.07227814\n",
      "Iteration 3249, loss = 0.07226893\n",
      "Iteration 3250, loss = 0.07225971\n",
      "Iteration 3251, loss = 0.07225050\n",
      "Iteration 3252, loss = 0.07224128\n",
      "Iteration 3253, loss = 0.07223206\n",
      "Iteration 3254, loss = 0.07222285\n",
      "Iteration 3255, loss = 0.07221363\n",
      "Iteration 3256, loss = 0.07220441\n",
      "Iteration 3257, loss = 0.07219520\n",
      "Iteration 3258, loss = 0.07218598\n",
      "Iteration 3259, loss = 0.07217676\n",
      "Iteration 3260, loss = 0.07216754\n",
      "Iteration 3261, loss = 0.07215832\n",
      "Iteration 3262, loss = 0.07214911\n",
      "Iteration 3263, loss = 0.07213989\n",
      "Iteration 3264, loss = 0.07213067\n",
      "Iteration 3265, loss = 0.07212145\n",
      "Iteration 3266, loss = 0.07211223\n",
      "Iteration 3267, loss = 0.07210301\n",
      "Iteration 3268, loss = 0.07209379\n",
      "Iteration 3269, loss = 0.07208456\n",
      "Iteration 3270, loss = 0.07207534\n",
      "Iteration 3271, loss = 0.07206612\n",
      "Iteration 3272, loss = 0.07205690\n",
      "Iteration 3273, loss = 0.07204768\n",
      "Iteration 3274, loss = 0.07203846\n",
      "Iteration 3275, loss = 0.07202923\n",
      "Iteration 3276, loss = 0.07202001\n",
      "Iteration 3277, loss = 0.07201079\n",
      "Iteration 3278, loss = 0.07200156\n",
      "Iteration 3279, loss = 0.07199234\n",
      "Iteration 3280, loss = 0.07198311\n",
      "Iteration 3281, loss = 0.07197389\n",
      "Iteration 3282, loss = 0.07196467\n",
      "Iteration 3283, loss = 0.07195544\n",
      "Iteration 3284, loss = 0.07194621\n",
      "Iteration 3285, loss = 0.07193699\n",
      "Iteration 3286, loss = 0.07192776\n",
      "Iteration 3287, loss = 0.07191854\n",
      "Iteration 3288, loss = 0.07190931\n",
      "Iteration 3289, loss = 0.07190008\n",
      "Iteration 3290, loss = 0.07189086\n",
      "Iteration 3291, loss = 0.07188163\n",
      "Iteration 3292, loss = 0.07187240\n",
      "Iteration 3293, loss = 0.07186317\n",
      "Iteration 3294, loss = 0.07185394\n",
      "Iteration 3295, loss = 0.07184472\n",
      "Iteration 3296, loss = 0.07183549\n",
      "Iteration 3297, loss = 0.07182626\n",
      "Iteration 3298, loss = 0.07181703\n",
      "Iteration 3299, loss = 0.07180780\n",
      "Iteration 3300, loss = 0.07179857\n",
      "Iteration 3301, loss = 0.07178934\n",
      "Iteration 3302, loss = 0.07178011\n",
      "Iteration 3303, loss = 0.07177088\n",
      "Iteration 3304, loss = 0.07176164\n",
      "Iteration 3305, loss = 0.07175241\n",
      "Iteration 3306, loss = 0.07174318\n",
      "Iteration 3307, loss = 0.07173395\n",
      "Iteration 3308, loss = 0.07172472\n",
      "Iteration 3309, loss = 0.07171548\n",
      "Iteration 3310, loss = 0.07170625\n",
      "Iteration 3311, loss = 0.07169702\n",
      "Iteration 3312, loss = 0.07168778\n",
      "Iteration 3313, loss = 0.07167855\n",
      "Iteration 3314, loss = 0.07166932\n",
      "Iteration 3315, loss = 0.07166008\n",
      "Iteration 3316, loss = 0.07165085\n",
      "Iteration 3317, loss = 0.07164161\n",
      "Iteration 3318, loss = 0.07163238\n",
      "Iteration 3319, loss = 0.07162314\n",
      "Iteration 3320, loss = 0.07161391\n",
      "Iteration 3321, loss = 0.07160467\n",
      "Iteration 3322, loss = 0.07159543\n",
      "Iteration 3323, loss = 0.07158620\n",
      "Iteration 3324, loss = 0.07157696\n",
      "Iteration 3325, loss = 0.07156772\n",
      "Iteration 3326, loss = 0.07155848\n",
      "Iteration 3327, loss = 0.07154925\n",
      "Iteration 3328, loss = 0.07154001\n",
      "Iteration 3329, loss = 0.07153077\n",
      "Iteration 3330, loss = 0.07152153\n",
      "Iteration 3331, loss = 0.07151229\n",
      "Iteration 3332, loss = 0.07150305\n",
      "Iteration 3333, loss = 0.07149381\n",
      "Iteration 3334, loss = 0.07148457\n",
      "Iteration 3335, loss = 0.07147533\n",
      "Iteration 3336, loss = 0.07146609\n",
      "Iteration 3337, loss = 0.07145685\n",
      "Iteration 3338, loss = 0.07144761\n",
      "Iteration 3339, loss = 0.07143837\n",
      "Iteration 3340, loss = 0.07142913\n",
      "Iteration 3341, loss = 0.07141989\n",
      "Iteration 3342, loss = 0.07141065\n",
      "Iteration 3343, loss = 0.07140141\n",
      "Iteration 3344, loss = 0.07139216\n",
      "Iteration 3345, loss = 0.07138292\n",
      "Iteration 3346, loss = 0.07137368\n",
      "Iteration 3347, loss = 0.07136443\n",
      "Iteration 3348, loss = 0.07135519\n",
      "Iteration 3349, loss = 0.07134595\n",
      "Iteration 3350, loss = 0.07133670\n",
      "Iteration 3351, loss = 0.07132746\n",
      "Iteration 3352, loss = 0.07131822\n",
      "Iteration 3353, loss = 0.07130897\n",
      "Iteration 3354, loss = 0.07129973\n",
      "Iteration 3355, loss = 0.07129048\n",
      "Iteration 3356, loss = 0.07128124\n",
      "Iteration 3357, loss = 0.07127199\n",
      "Iteration 3358, loss = 0.07126274\n",
      "Iteration 3359, loss = 0.07125350\n",
      "Iteration 3360, loss = 0.07124425\n",
      "Iteration 3361, loss = 0.07123501\n",
      "Iteration 3362, loss = 0.07122576\n",
      "Iteration 3363, loss = 0.07121651\n",
      "Iteration 3364, loss = 0.07120726\n",
      "Iteration 3365, loss = 0.07119802\n",
      "Iteration 3366, loss = 0.07118877\n",
      "Iteration 3367, loss = 0.07117952\n",
      "Iteration 3368, loss = 0.07117027\n",
      "Iteration 3369, loss = 0.07116102\n",
      "Iteration 3370, loss = 0.07115178\n",
      "Iteration 3371, loss = 0.07114253\n",
      "Iteration 3372, loss = 0.07113328\n",
      "Iteration 3373, loss = 0.07112403\n",
      "Iteration 3374, loss = 0.07111478\n",
      "Iteration 3375, loss = 0.07110553\n",
      "Iteration 3376, loss = 0.07109628\n",
      "Iteration 3377, loss = 0.07108703\n",
      "Iteration 3378, loss = 0.07107778\n",
      "Iteration 3379, loss = 0.07106853\n",
      "Iteration 3380, loss = 0.07105928\n",
      "Iteration 3381, loss = 0.07105003\n",
      "Iteration 3382, loss = 0.07104077\n",
      "Iteration 3383, loss = 0.07103152\n",
      "Iteration 3384, loss = 0.07102227\n",
      "Iteration 3385, loss = 0.07101302\n",
      "Iteration 3386, loss = 0.07100377\n",
      "Iteration 3387, loss = 0.07099451\n",
      "Iteration 3388, loss = 0.07098526\n",
      "Iteration 3389, loss = 0.07097601\n",
      "Iteration 3390, loss = 0.07096676\n",
      "Iteration 3391, loss = 0.07095750\n",
      "Iteration 3392, loss = 0.07094825\n",
      "Iteration 3393, loss = 0.07093900\n",
      "Iteration 3394, loss = 0.07092974\n",
      "Iteration 3395, loss = 0.07092049\n",
      "Iteration 3396, loss = 0.07091124\n",
      "Iteration 3397, loss = 0.07090198\n",
      "Iteration 3398, loss = 0.07089273\n",
      "Iteration 3399, loss = 0.07088347\n",
      "Iteration 3400, loss = 0.07087422\n",
      "Iteration 3401, loss = 0.07086496\n",
      "Iteration 3402, loss = 0.07085571\n",
      "Iteration 3403, loss = 0.07084645\n",
      "Iteration 3404, loss = 0.07083720\n",
      "Iteration 3405, loss = 0.07082794\n",
      "Iteration 3406, loss = 0.07081868\n",
      "Iteration 3407, loss = 0.07080943\n",
      "Iteration 3408, loss = 0.07080017\n",
      "Iteration 3409, loss = 0.07079092\n",
      "Iteration 3410, loss = 0.07078166\n",
      "Iteration 3411, loss = 0.07077240\n",
      "Iteration 3412, loss = 0.07076315\n",
      "Iteration 3413, loss = 0.07075389\n",
      "Iteration 3414, loss = 0.07074463\n",
      "Iteration 3415, loss = 0.07073538\n",
      "Iteration 3416, loss = 0.07072612\n",
      "Iteration 3417, loss = 0.07071686\n",
      "Iteration 3418, loss = 0.07070760\n",
      "Iteration 3419, loss = 0.07069834\n",
      "Iteration 3420, loss = 0.07068909\n",
      "Iteration 3421, loss = 0.07067983\n",
      "Iteration 3422, loss = 0.07067057\n",
      "Iteration 3423, loss = 0.07066131\n",
      "Iteration 3424, loss = 0.07065205\n",
      "Iteration 3425, loss = 0.07064279\n",
      "Iteration 3426, loss = 0.07063354\n",
      "Iteration 3427, loss = 0.07062428\n",
      "Iteration 3428, loss = 0.07061502\n",
      "Iteration 3429, loss = 0.07060576\n",
      "Iteration 3430, loss = 0.07059650\n",
      "Iteration 3431, loss = 0.07058724\n",
      "Iteration 3432, loss = 0.07057798\n",
      "Iteration 3433, loss = 0.07056872\n",
      "Iteration 3434, loss = 0.07055946\n",
      "Iteration 3435, loss = 0.07055020\n",
      "Iteration 3436, loss = 0.07054094\n",
      "Iteration 3437, loss = 0.07053168\n",
      "Iteration 3438, loss = 0.07052242\n",
      "Iteration 3439, loss = 0.07051316\n",
      "Iteration 3440, loss = 0.07050390\n",
      "Iteration 3441, loss = 0.07049464\n",
      "Iteration 3442, loss = 0.07048538\n",
      "Iteration 3443, loss = 0.07047612\n",
      "Iteration 3444, loss = 0.07046686\n",
      "Iteration 3445, loss = 0.07045760\n",
      "Iteration 3446, loss = 0.07044834\n",
      "Iteration 3447, loss = 0.07043907\n",
      "Iteration 3448, loss = 0.07042981\n",
      "Iteration 3449, loss = 0.07042055\n",
      "Iteration 3450, loss = 0.07041129\n",
      "Iteration 3451, loss = 0.07040203\n",
      "Iteration 3452, loss = 0.07039277\n",
      "Iteration 3453, loss = 0.07038351\n",
      "Iteration 3454, loss = 0.07037425\n",
      "Iteration 3455, loss = 0.07036498\n",
      "Iteration 3456, loss = 0.07035572\n",
      "Iteration 3457, loss = 0.07034646\n",
      "Iteration 3458, loss = 0.07033720\n",
      "Iteration 3459, loss = 0.07032794\n",
      "Iteration 3460, loss = 0.07031868\n",
      "Iteration 3461, loss = 0.07030941\n",
      "Iteration 3462, loss = 0.07030015\n",
      "Iteration 3463, loss = 0.07029089\n",
      "Iteration 3464, loss = 0.07028163\n",
      "Iteration 3465, loss = 0.07027237\n",
      "Iteration 3466, loss = 0.07026310\n",
      "Iteration 3467, loss = 0.07025384\n",
      "Iteration 3468, loss = 0.07024458\n",
      "Iteration 3469, loss = 0.07023532\n",
      "Iteration 3470, loss = 0.07022605\n",
      "Iteration 3471, loss = 0.07021679\n",
      "Iteration 3472, loss = 0.07020753\n",
      "Iteration 3473, loss = 0.07019827\n",
      "Iteration 3474, loss = 0.07018901\n",
      "Iteration 3475, loss = 0.07017974\n",
      "Iteration 3476, loss = 0.07017048\n",
      "Iteration 3477, loss = 0.07016122\n",
      "Iteration 3478, loss = 0.07015196\n",
      "Iteration 3479, loss = 0.07014269\n",
      "Iteration 3480, loss = 0.07013343\n",
      "Iteration 3481, loss = 0.07012417\n",
      "Iteration 3482, loss = 0.07011491\n",
      "Iteration 3483, loss = 0.07010564\n",
      "Iteration 3484, loss = 0.07009638\n",
      "Iteration 3485, loss = 0.07008712\n",
      "Iteration 3486, loss = 0.07007786\n",
      "Iteration 3487, loss = 0.07006859\n",
      "Iteration 3488, loss = 0.07005933\n",
      "Iteration 3489, loss = 0.07005007\n",
      "Iteration 3490, loss = 0.07004081\n",
      "Iteration 3491, loss = 0.07003155\n",
      "Iteration 3492, loss = 0.07002228\n",
      "Iteration 3493, loss = 0.07001302\n",
      "Iteration 3494, loss = 0.07000376\n",
      "Iteration 3495, loss = 0.06999450\n",
      "Iteration 3496, loss = 0.06998523\n",
      "Iteration 3497, loss = 0.06997597\n",
      "Iteration 3498, loss = 0.06996671\n",
      "Iteration 3499, loss = 0.06995745\n",
      "Iteration 3500, loss = 0.06994819\n",
      "Iteration 3501, loss = 0.06993893\n",
      "Iteration 3502, loss = 0.06992966\n",
      "Iteration 3503, loss = 0.06992040\n",
      "Iteration 3504, loss = 0.06991114\n",
      "Iteration 3505, loss = 0.06990188\n",
      "Iteration 3506, loss = 0.06989262\n",
      "Iteration 3507, loss = 0.06988336\n",
      "Iteration 3508, loss = 0.06987409\n",
      "Iteration 3509, loss = 0.06986483\n",
      "Iteration 3510, loss = 0.06985557\n",
      "Iteration 3511, loss = 0.06984631\n",
      "Iteration 3512, loss = 0.06983705\n",
      "Iteration 3513, loss = 0.06982779\n",
      "Iteration 3514, loss = 0.06981853\n",
      "Iteration 3515, loss = 0.06980927\n",
      "Iteration 3516, loss = 0.06980001\n",
      "Iteration 3517, loss = 0.06979075\n",
      "Iteration 3518, loss = 0.06978149\n",
      "Iteration 3519, loss = 0.06977222\n",
      "Iteration 3520, loss = 0.06976296\n",
      "Iteration 3521, loss = 0.06975370\n",
      "Iteration 3522, loss = 0.06974444\n",
      "Iteration 3523, loss = 0.06973518\n",
      "Iteration 3524, loss = 0.06972592\n",
      "Iteration 3525, loss = 0.06971667\n",
      "Iteration 3526, loss = 0.06970741\n",
      "Iteration 3527, loss = 0.06969815\n",
      "Iteration 3528, loss = 0.06968889\n",
      "Iteration 3529, loss = 0.06967963\n",
      "Iteration 3530, loss = 0.06967037\n",
      "Iteration 3531, loss = 0.06966111\n",
      "Iteration 3532, loss = 0.06965185\n",
      "Iteration 3533, loss = 0.06964259\n",
      "Iteration 3534, loss = 0.06963333\n",
      "Iteration 3535, loss = 0.06962408\n",
      "Iteration 3536, loss = 0.06961482\n",
      "Iteration 3537, loss = 0.06960556\n",
      "Iteration 3538, loss = 0.06959630\n",
      "Iteration 3539, loss = 0.06958704\n",
      "Iteration 3540, loss = 0.06957779\n",
      "Iteration 3541, loss = 0.06956853\n",
      "Iteration 3542, loss = 0.06955927\n",
      "Iteration 3543, loss = 0.06955001\n",
      "Iteration 3544, loss = 0.06954076\n",
      "Iteration 3545, loss = 0.06953150\n",
      "Iteration 3546, loss = 0.06952224\n",
      "Iteration 3547, loss = 0.06951299\n",
      "Iteration 3548, loss = 0.06950373\n",
      "Iteration 3549, loss = 0.06949448\n",
      "Iteration 3550, loss = 0.06948522\n",
      "Iteration 3551, loss = 0.06947596\n",
      "Iteration 3552, loss = 0.06946671\n",
      "Iteration 3553, loss = 0.06945745\n",
      "Iteration 3554, loss = 0.06944820\n",
      "Iteration 3555, loss = 0.06943894\n",
      "Iteration 3556, loss = 0.06942969\n",
      "Iteration 3557, loss = 0.06942043\n",
      "Iteration 3558, loss = 0.06941118\n",
      "Iteration 3559, loss = 0.06940193\n",
      "Iteration 3560, loss = 0.06939267\n",
      "Iteration 3561, loss = 0.06938342\n",
      "Iteration 3562, loss = 0.06937417\n",
      "Iteration 3563, loss = 0.06936491\n",
      "Iteration 3564, loss = 0.06935566\n",
      "Iteration 3565, loss = 0.06934641\n",
      "Iteration 3566, loss = 0.06933715\n",
      "Iteration 3567, loss = 0.06932790\n",
      "Iteration 3568, loss = 0.06931865\n",
      "Iteration 3569, loss = 0.06930940\n",
      "Iteration 3570, loss = 0.06930015\n",
      "Iteration 3571, loss = 0.06929090\n",
      "Iteration 3572, loss = 0.06928164\n",
      "Iteration 3573, loss = 0.06927239\n",
      "Iteration 3574, loss = 0.06926314\n",
      "Iteration 3575, loss = 0.06925389\n",
      "Iteration 3576, loss = 0.06924464\n",
      "Iteration 3577, loss = 0.06923539\n",
      "Iteration 3578, loss = 0.06922614\n",
      "Iteration 3579, loss = 0.06921689\n",
      "Iteration 3580, loss = 0.06920765\n",
      "Iteration 3581, loss = 0.06919840\n",
      "Iteration 3582, loss = 0.06918915\n",
      "Iteration 3583, loss = 0.06917990\n",
      "Iteration 3584, loss = 0.06917065\n",
      "Iteration 3585, loss = 0.06916141\n",
      "Iteration 3586, loss = 0.06915216\n",
      "Iteration 3587, loss = 0.06914291\n",
      "Iteration 3588, loss = 0.06913366\n",
      "Iteration 3589, loss = 0.06912442\n",
      "Iteration 3590, loss = 0.06911517\n",
      "Iteration 3591, loss = 0.06910593\n",
      "Iteration 3592, loss = 0.06909668\n",
      "Iteration 3593, loss = 0.06908744\n",
      "Iteration 3594, loss = 0.06907819\n",
      "Iteration 3595, loss = 0.06906895\n",
      "Iteration 3596, loss = 0.06905970\n",
      "Iteration 3597, loss = 0.06905046\n",
      "Iteration 3598, loss = 0.06904121\n",
      "Iteration 3599, loss = 0.06903197\n",
      "Iteration 3600, loss = 0.06902273\n",
      "Iteration 3601, loss = 0.06901349\n",
      "Iteration 3602, loss = 0.06900424\n",
      "Iteration 3603, loss = 0.06899500\n",
      "Iteration 3604, loss = 0.06898576\n",
      "Iteration 3605, loss = 0.06897652\n",
      "Iteration 3606, loss = 0.06896728\n",
      "Iteration 3607, loss = 0.06895804\n",
      "Iteration 3608, loss = 0.06894880\n",
      "Iteration 3609, loss = 0.06893956\n",
      "Iteration 3610, loss = 0.06893032\n",
      "Iteration 3611, loss = 0.06892108\n",
      "Iteration 3612, loss = 0.06891184\n",
      "Iteration 3613, loss = 0.06890260\n",
      "Iteration 3614, loss = 0.06889336\n",
      "Iteration 3615, loss = 0.06888412\n",
      "Iteration 3616, loss = 0.06887489\n",
      "Iteration 3617, loss = 0.06886565\n",
      "Iteration 3618, loss = 0.06885641\n",
      "Iteration 3619, loss = 0.06884718\n",
      "Iteration 3620, loss = 0.06883794\n",
      "Iteration 3621, loss = 0.06882870\n",
      "Iteration 3622, loss = 0.06881947\n",
      "Iteration 3623, loss = 0.06881023\n",
      "Iteration 3624, loss = 0.06880100\n",
      "Iteration 3625, loss = 0.06879176\n",
      "Iteration 3626, loss = 0.06878253\n",
      "Iteration 3627, loss = 0.06877330\n",
      "Iteration 3628, loss = 0.06876406\n",
      "Iteration 3629, loss = 0.06875483\n",
      "Iteration 3630, loss = 0.06874560\n",
      "Iteration 3631, loss = 0.06873637\n",
      "Iteration 3632, loss = 0.06872714\n",
      "Iteration 3633, loss = 0.06871790\n",
      "Iteration 3634, loss = 0.06870867\n",
      "Iteration 3635, loss = 0.06869944\n",
      "Iteration 3636, loss = 0.06869021\n",
      "Iteration 3637, loss = 0.06868098\n",
      "Iteration 3638, loss = 0.06867176\n",
      "Iteration 3639, loss = 0.06866253\n",
      "Iteration 3640, loss = 0.06865330\n",
      "Iteration 3641, loss = 0.06864407\n",
      "Iteration 3642, loss = 0.06863484\n",
      "Iteration 3643, loss = 0.06862562\n",
      "Iteration 3644, loss = 0.06861639\n",
      "Iteration 3645, loss = 0.06860716\n",
      "Iteration 3646, loss = 0.06859794\n",
      "Iteration 3647, loss = 0.06858871\n",
      "Iteration 3648, loss = 0.06857949\n",
      "Iteration 3649, loss = 0.06857026\n",
      "Iteration 3650, loss = 0.06856104\n",
      "Iteration 3651, loss = 0.06855182\n",
      "Iteration 3652, loss = 0.06854259\n",
      "Iteration 3653, loss = 0.06853337\n",
      "Iteration 3654, loss = 0.06852415\n",
      "Iteration 3655, loss = 0.06851493\n",
      "Iteration 3656, loss = 0.06850571\n",
      "Iteration 3657, loss = 0.06849648\n",
      "Iteration 3658, loss = 0.06848726\n",
      "Iteration 3659, loss = 0.06847804\n",
      "Iteration 3660, loss = 0.06846882\n",
      "Iteration 3661, loss = 0.06845961\n",
      "Iteration 3662, loss = 0.06845039\n",
      "Iteration 3663, loss = 0.06844117\n",
      "Iteration 3664, loss = 0.06843195\n",
      "Iteration 3665, loss = 0.06842273\n",
      "Iteration 3666, loss = 0.06841352\n",
      "Iteration 3667, loss = 0.06840430\n",
      "Iteration 3668, loss = 0.06839508\n",
      "Iteration 3669, loss = 0.06838587\n",
      "Iteration 3670, loss = 0.06837665\n",
      "Iteration 3671, loss = 0.06836744\n",
      "Iteration 3672, loss = 0.06835823\n",
      "Iteration 3673, loss = 0.06834901\n",
      "Iteration 3674, loss = 0.06833980\n",
      "Iteration 3675, loss = 0.06833059\n",
      "Iteration 3676, loss = 0.06832137\n",
      "Iteration 3677, loss = 0.06831216\n",
      "Iteration 3678, loss = 0.06830295\n",
      "Iteration 3679, loss = 0.06829374\n",
      "Iteration 3680, loss = 0.06828453\n",
      "Iteration 3681, loss = 0.06827532\n",
      "Iteration 3682, loss = 0.06826611\n",
      "Iteration 3683, loss = 0.06825690\n",
      "Iteration 3684, loss = 0.06824770\n",
      "Iteration 3685, loss = 0.06823849\n",
      "Iteration 3686, loss = 0.06822928\n",
      "Iteration 3687, loss = 0.06822007\n",
      "Iteration 3688, loss = 0.06821087\n",
      "Iteration 3689, loss = 0.06820166\n",
      "Iteration 3690, loss = 0.06819246\n",
      "Iteration 3691, loss = 0.06818325\n",
      "Iteration 3692, loss = 0.06817405\n",
      "Iteration 3693, loss = 0.06816484\n",
      "Iteration 3694, loss = 0.06815564\n",
      "Iteration 3695, loss = 0.06814644\n",
      "Iteration 3696, loss = 0.06813724\n",
      "Iteration 3697, loss = 0.06812804\n",
      "Iteration 3698, loss = 0.06811883\n",
      "Iteration 3699, loss = 0.06810963\n",
      "Iteration 3700, loss = 0.06810043\n",
      "Iteration 3701, loss = 0.06809123\n",
      "Iteration 3702, loss = 0.06808203\n",
      "Iteration 3703, loss = 0.06807284\n",
      "Iteration 3704, loss = 0.06806364\n",
      "Iteration 3705, loss = 0.06805444\n",
      "Iteration 3706, loss = 0.06804524\n",
      "Iteration 3707, loss = 0.06803605\n",
      "Iteration 3708, loss = 0.06802685\n",
      "Iteration 3709, loss = 0.06801766\n",
      "Iteration 3710, loss = 0.06800846\n",
      "Iteration 3711, loss = 0.06799927\n",
      "Iteration 3712, loss = 0.06799007\n",
      "Iteration 3713, loss = 0.06798088\n",
      "Iteration 3714, loss = 0.06797169\n",
      "Iteration 3715, loss = 0.06796249\n",
      "Iteration 3716, loss = 0.06795330\n",
      "Iteration 3717, loss = 0.06794411\n",
      "Iteration 3718, loss = 0.06793492\n",
      "Iteration 3719, loss = 0.06792573\n",
      "Iteration 3720, loss = 0.06791654\n",
      "Iteration 3721, loss = 0.06790735\n",
      "Iteration 3722, loss = 0.06789816\n",
      "Iteration 3723, loss = 0.06788898\n",
      "Iteration 3724, loss = 0.06787979\n",
      "Iteration 3725, loss = 0.06787060\n",
      "Iteration 3726, loss = 0.06786142\n",
      "Iteration 3727, loss = 0.06785223\n",
      "Iteration 3728, loss = 0.06784304\n",
      "Iteration 3729, loss = 0.06783386\n",
      "Iteration 3730, loss = 0.06782468\n",
      "Iteration 3731, loss = 0.06781549\n",
      "Iteration 3732, loss = 0.06780631\n",
      "Iteration 3733, loss = 0.06779713\n",
      "Iteration 3734, loss = 0.06778794\n",
      "Iteration 3735, loss = 0.06777876\n",
      "Iteration 3736, loss = 0.06776958\n",
      "Iteration 3737, loss = 0.06776040\n",
      "Iteration 3738, loss = 0.06775122\n",
      "Iteration 3739, loss = 0.06774204\n",
      "Iteration 3740, loss = 0.06773286\n",
      "Iteration 3741, loss = 0.06772369\n",
      "Iteration 3742, loss = 0.06771451\n",
      "Iteration 3743, loss = 0.06770533\n",
      "Iteration 3744, loss = 0.06769615\n",
      "Iteration 3745, loss = 0.06768698\n",
      "Iteration 3746, loss = 0.06767780\n",
      "Iteration 3747, loss = 0.06766863\n",
      "Iteration 3748, loss = 0.06765945\n",
      "Iteration 3749, loss = 0.06765028\n",
      "Iteration 3750, loss = 0.06764111\n",
      "Iteration 3751, loss = 0.06763193\n",
      "Iteration 3752, loss = 0.06762276\n",
      "Iteration 3753, loss = 0.06761359\n",
      "Iteration 3754, loss = 0.06760442\n",
      "Iteration 3755, loss = 0.06759525\n",
      "Iteration 3756, loss = 0.06758608\n",
      "Iteration 3757, loss = 0.06757691\n",
      "Iteration 3758, loss = 0.06756774\n",
      "Iteration 3759, loss = 0.06755857\n",
      "Iteration 3760, loss = 0.06754941\n",
      "Iteration 3761, loss = 0.06754024\n",
      "Iteration 3762, loss = 0.06753107\n",
      "Iteration 3763, loss = 0.06752191\n",
      "Iteration 3764, loss = 0.06751274\n",
      "Iteration 3765, loss = 0.06750358\n",
      "Iteration 3766, loss = 0.06749441\n",
      "Iteration 3767, loss = 0.06748525\n",
      "Iteration 3768, loss = 0.06747608\n",
      "Iteration 3769, loss = 0.06746692\n",
      "Iteration 3770, loss = 0.06745776\n",
      "Iteration 3771, loss = 0.06744860\n",
      "Iteration 3772, loss = 0.06743944\n",
      "Iteration 3773, loss = 0.06743028\n",
      "Iteration 3774, loss = 0.06742112\n",
      "Iteration 3775, loss = 0.06741196\n",
      "Iteration 3776, loss = 0.06740280\n",
      "Iteration 3777, loss = 0.06739364\n",
      "Iteration 3778, loss = 0.06738448\n",
      "Iteration 3779, loss = 0.06737533\n",
      "Iteration 3780, loss = 0.06736617\n",
      "Iteration 3781, loss = 0.06735701\n",
      "Iteration 3782, loss = 0.06734786\n",
      "Iteration 3783, loss = 0.06733870\n",
      "Iteration 3784, loss = 0.06732955\n",
      "Iteration 3785, loss = 0.06732039\n",
      "Iteration 3786, loss = 0.06731124\n",
      "Iteration 3787, loss = 0.06730209\n",
      "Iteration 3788, loss = 0.06729294\n",
      "Iteration 3789, loss = 0.06728378\n",
      "Iteration 3790, loss = 0.06727463\n",
      "Iteration 3791, loss = 0.06726548\n",
      "Iteration 3792, loss = 0.06725633\n",
      "Iteration 3793, loss = 0.06724718\n",
      "Iteration 3794, loss = 0.06723804\n",
      "Iteration 3795, loss = 0.06722889\n",
      "Iteration 3796, loss = 0.06721974\n",
      "Iteration 3797, loss = 0.06721059\n",
      "Iteration 3798, loss = 0.06720144\n",
      "Iteration 3799, loss = 0.06719230\n",
      "Iteration 3800, loss = 0.06718315\n",
      "Iteration 3801, loss = 0.06717401\n",
      "Iteration 3802, loss = 0.06716486\n",
      "Iteration 3803, loss = 0.06715572\n",
      "Iteration 3804, loss = 0.06714658\n",
      "Iteration 3805, loss = 0.06713743\n",
      "Iteration 3806, loss = 0.06712829\n",
      "Iteration 3807, loss = 0.06711915\n",
      "Iteration 3808, loss = 0.06711001\n",
      "Iteration 3809, loss = 0.06710087\n",
      "Iteration 3810, loss = 0.06709173\n",
      "Iteration 3811, loss = 0.06708259\n",
      "Iteration 3812, loss = 0.06707345\n",
      "Iteration 3813, loss = 0.06706431\n",
      "Iteration 3814, loss = 0.06705517\n",
      "Iteration 3815, loss = 0.06704603\n",
      "Iteration 3816, loss = 0.06703690\n",
      "Iteration 3817, loss = 0.06702776\n",
      "Iteration 3818, loss = 0.06701862\n",
      "Iteration 3819, loss = 0.06700949\n",
      "Iteration 3820, loss = 0.06700035\n",
      "Iteration 3821, loss = 0.06699122\n",
      "Iteration 3822, loss = 0.06698208\n",
      "Iteration 3823, loss = 0.06697295\n",
      "Iteration 3824, loss = 0.06696382\n",
      "Iteration 3825, loss = 0.06695469\n",
      "Iteration 3826, loss = 0.06694555\n",
      "Iteration 3827, loss = 0.06693642\n",
      "Iteration 3828, loss = 0.06692729\n",
      "Iteration 3829, loss = 0.06691816\n",
      "Iteration 3830, loss = 0.06690903\n",
      "Iteration 3831, loss = 0.06689990\n",
      "Iteration 3832, loss = 0.06689077\n",
      "Iteration 3833, loss = 0.06688164\n",
      "Iteration 3834, loss = 0.06687252\n",
      "Iteration 3835, loss = 0.06686339\n",
      "Iteration 3836, loss = 0.06685426\n",
      "Iteration 3837, loss = 0.06684514\n",
      "Iteration 3838, loss = 0.06683601\n",
      "Iteration 3839, loss = 0.06682689\n",
      "Iteration 3840, loss = 0.06681776\n",
      "Iteration 3841, loss = 0.06680864\n",
      "Iteration 3842, loss = 0.06679951\n",
      "Iteration 3843, loss = 0.06679039\n",
      "Iteration 3844, loss = 0.06678127\n",
      "Iteration 3845, loss = 0.06677214\n",
      "Iteration 3846, loss = 0.06676302\n",
      "Iteration 3847, loss = 0.06675390\n",
      "Iteration 3848, loss = 0.06674478\n",
      "Iteration 3849, loss = 0.06673566\n",
      "Iteration 3850, loss = 0.06672654\n",
      "Iteration 3851, loss = 0.06671742\n",
      "Iteration 3852, loss = 0.06670830\n",
      "Iteration 3853, loss = 0.06669918\n",
      "Iteration 3854, loss = 0.06669006\n",
      "Iteration 3855, loss = 0.06668095\n",
      "Iteration 3856, loss = 0.06667183\n",
      "Iteration 3857, loss = 0.06666271\n",
      "Iteration 3858, loss = 0.06665360\n",
      "Iteration 3859, loss = 0.06664448\n",
      "Iteration 3860, loss = 0.06663536\n",
      "Iteration 3861, loss = 0.06662625\n",
      "Iteration 3862, loss = 0.06661714\n",
      "Iteration 3863, loss = 0.06660802\n",
      "Iteration 3864, loss = 0.06659891\n",
      "Iteration 3865, loss = 0.06658980\n",
      "Iteration 3866, loss = 0.06658068\n",
      "Iteration 3867, loss = 0.06657157\n",
      "Iteration 3868, loss = 0.06656246\n",
      "Iteration 3869, loss = 0.06655335\n",
      "Iteration 3870, loss = 0.06654424\n",
      "Iteration 3871, loss = 0.06653513\n",
      "Iteration 3872, loss = 0.06652602\n",
      "Iteration 3873, loss = 0.06651691\n",
      "Iteration 3874, loss = 0.06650780\n",
      "Iteration 3875, loss = 0.06649869\n",
      "Iteration 3876, loss = 0.06648958\n",
      "Iteration 3877, loss = 0.06648047\n",
      "Iteration 3878, loss = 0.06647137\n",
      "Iteration 3879, loss = 0.06646226\n",
      "Iteration 3880, loss = 0.06645315\n",
      "Iteration 3881, loss = 0.06644405\n",
      "Iteration 3882, loss = 0.06643494\n",
      "Iteration 3883, loss = 0.06642583\n",
      "Iteration 3884, loss = 0.06641673\n",
      "Iteration 3885, loss = 0.06640762\n",
      "Iteration 3886, loss = 0.06639852\n",
      "Iteration 3887, loss = 0.06638942\n",
      "Iteration 3888, loss = 0.06638031\n",
      "Iteration 3889, loss = 0.06637121\n",
      "Iteration 3890, loss = 0.06636211\n",
      "Iteration 3891, loss = 0.06635301\n",
      "Iteration 3892, loss = 0.06634390\n",
      "Iteration 3893, loss = 0.06633480\n",
      "Iteration 3894, loss = 0.06632570\n",
      "Iteration 3895, loss = 0.06631660\n",
      "Iteration 3896, loss = 0.06630750\n",
      "Iteration 3897, loss = 0.06629840\n",
      "Iteration 3898, loss = 0.06628930\n",
      "Iteration 3899, loss = 0.06628020\n",
      "Iteration 3900, loss = 0.06627110\n",
      "Iteration 3901, loss = 0.06626200\n",
      "Iteration 3902, loss = 0.06625291\n",
      "Iteration 3903, loss = 0.06624381\n",
      "Iteration 3904, loss = 0.06623471\n",
      "Iteration 3905, loss = 0.06622561\n",
      "Iteration 3906, loss = 0.06621652\n",
      "Iteration 3907, loss = 0.06620742\n",
      "Iteration 3908, loss = 0.06619832\n",
      "Iteration 3909, loss = 0.06618923\n",
      "Iteration 3910, loss = 0.06618013\n",
      "Iteration 3911, loss = 0.06617104\n",
      "Iteration 3912, loss = 0.06616194\n",
      "Iteration 3913, loss = 0.06615285\n",
      "Iteration 3914, loss = 0.06614376\n",
      "Iteration 3915, loss = 0.06613466\n",
      "Iteration 3916, loss = 0.06612557\n",
      "Iteration 3917, loss = 0.06611647\n",
      "Iteration 3918, loss = 0.06610738\n",
      "Iteration 3919, loss = 0.06609829\n",
      "Iteration 3920, loss = 0.06608920\n",
      "Iteration 3921, loss = 0.06608011\n",
      "Iteration 3922, loss = 0.06607101\n",
      "Iteration 3923, loss = 0.06606192\n",
      "Iteration 3924, loss = 0.06605283\n",
      "Iteration 3925, loss = 0.06604374\n",
      "Iteration 3926, loss = 0.06603465\n",
      "Iteration 3927, loss = 0.06602556\n",
      "Iteration 3928, loss = 0.06601647\n",
      "Iteration 3929, loss = 0.06600738\n",
      "Iteration 3930, loss = 0.06599829\n",
      "Iteration 3931, loss = 0.06598920\n",
      "Iteration 3932, loss = 0.06598011\n",
      "Iteration 3933, loss = 0.06597102\n",
      "Iteration 3934, loss = 0.06596194\n",
      "Iteration 3935, loss = 0.06595285\n",
      "Iteration 3936, loss = 0.06594376\n",
      "Iteration 3937, loss = 0.06593467\n",
      "Iteration 3938, loss = 0.06592558\n",
      "Iteration 3939, loss = 0.06591650\n",
      "Iteration 3940, loss = 0.06590741\n",
      "Iteration 3941, loss = 0.06589832\n",
      "Iteration 3942, loss = 0.06588924\n",
      "Iteration 3943, loss = 0.06588015\n",
      "Iteration 3944, loss = 0.06587107\n",
      "Iteration 3945, loss = 0.06586198\n",
      "Iteration 3946, loss = 0.06585289\n",
      "Iteration 3947, loss = 0.06584381\n",
      "Iteration 3948, loss = 0.06583472\n",
      "Iteration 3949, loss = 0.06582564\n",
      "Iteration 3950, loss = 0.06581655\n",
      "Iteration 3951, loss = 0.06580747\n",
      "Iteration 3952, loss = 0.06579839\n",
      "Iteration 3953, loss = 0.06578930\n",
      "Iteration 3954, loss = 0.06578022\n",
      "Iteration 3955, loss = 0.06577113\n",
      "Iteration 3956, loss = 0.06576205\n",
      "Iteration 3957, loss = 0.06575297\n",
      "Iteration 3958, loss = 0.06574388\n",
      "Iteration 3959, loss = 0.06573480\n",
      "Iteration 3960, loss = 0.06572572\n",
      "Iteration 3961, loss = 0.06571663\n",
      "Iteration 3962, loss = 0.06570755\n",
      "Iteration 3963, loss = 0.06569847\n",
      "Iteration 3964, loss = 0.06568939\n",
      "Iteration 3965, loss = 0.06568031\n",
      "Iteration 3966, loss = 0.06567122\n",
      "Iteration 3967, loss = 0.06566214\n",
      "Iteration 3968, loss = 0.06565306\n",
      "Iteration 3969, loss = 0.06564398\n",
      "Iteration 3970, loss = 0.06563490\n",
      "Iteration 3971, loss = 0.06562581\n",
      "Iteration 3972, loss = 0.06561673\n",
      "Iteration 3973, loss = 0.06560765\n",
      "Iteration 3974, loss = 0.06559857\n",
      "Iteration 3975, loss = 0.06558949\n",
      "Iteration 3976, loss = 0.06558041\n",
      "Iteration 3977, loss = 0.06557133\n",
      "Iteration 3978, loss = 0.06556225\n",
      "Iteration 3979, loss = 0.06555317\n",
      "Iteration 3980, loss = 0.06554409\n",
      "Iteration 3981, loss = 0.06553501\n",
      "Iteration 3982, loss = 0.06552593\n",
      "Iteration 3983, loss = 0.06551685\n",
      "Iteration 3984, loss = 0.06550777\n",
      "Iteration 3985, loss = 0.06549869\n",
      "Iteration 3986, loss = 0.06548961\n",
      "Iteration 3987, loss = 0.06548053\n",
      "Iteration 3988, loss = 0.06547145\n",
      "Iteration 3989, loss = 0.06546237\n",
      "Iteration 3990, loss = 0.06545329\n",
      "Iteration 3991, loss = 0.06544421\n",
      "Iteration 3992, loss = 0.06543513\n",
      "Iteration 3993, loss = 0.06542605\n",
      "Iteration 3994, loss = 0.06541697\n",
      "Iteration 3995, loss = 0.06540789\n",
      "Iteration 3996, loss = 0.06539881\n",
      "Iteration 3997, loss = 0.06538973\n",
      "Iteration 3998, loss = 0.06538065\n",
      "Iteration 3999, loss = 0.06537157\n",
      "Iteration 4000, loss = 0.06536249\n",
      "Iteration 4001, loss = 0.06535341\n",
      "Iteration 4002, loss = 0.06534433\n",
      "Iteration 4003, loss = 0.06533525\n",
      "Iteration 4004, loss = 0.06532617\n",
      "Iteration 4005, loss = 0.06531709\n",
      "Iteration 4006, loss = 0.06530801\n",
      "Iteration 4007, loss = 0.06529893\n",
      "Iteration 4008, loss = 0.06528985\n",
      "Iteration 4009, loss = 0.06528077\n",
      "Iteration 4010, loss = 0.06527169\n",
      "Iteration 4011, loss = 0.06526261\n",
      "Iteration 4012, loss = 0.06525353\n",
      "Iteration 4013, loss = 0.06524445\n",
      "Iteration 4014, loss = 0.06523537\n",
      "Iteration 4015, loss = 0.06522629\n",
      "Iteration 4016, loss = 0.06521721\n",
      "Iteration 4017, loss = 0.06520813\n",
      "Iteration 4018, loss = 0.06519905\n",
      "Iteration 4019, loss = 0.06518997\n",
      "Iteration 4020, loss = 0.06518089\n",
      "Iteration 4021, loss = 0.06517181\n",
      "Iteration 4022, loss = 0.06516273\n",
      "Iteration 4023, loss = 0.06515365\n",
      "Iteration 4024, loss = 0.06514457\n",
      "Iteration 4025, loss = 0.06513549\n",
      "Iteration 4026, loss = 0.06512641\n",
      "Iteration 4027, loss = 0.06511732\n",
      "Iteration 4028, loss = 0.06510824\n",
      "Iteration 4029, loss = 0.06509916\n",
      "Iteration 4030, loss = 0.06509008\n",
      "Iteration 4031, loss = 0.06508100\n",
      "Iteration 4032, loss = 0.06507192\n",
      "Iteration 4033, loss = 0.06506283\n",
      "Iteration 4034, loss = 0.06505375\n",
      "Iteration 4035, loss = 0.06504467\n",
      "Iteration 4036, loss = 0.06503559\n",
      "Iteration 4037, loss = 0.06502650\n",
      "Iteration 4038, loss = 0.06501742\n",
      "Iteration 4039, loss = 0.06500834\n",
      "Iteration 4040, loss = 0.06499926\n",
      "Iteration 4041, loss = 0.06499017\n",
      "Iteration 4042, loss = 0.06498109\n",
      "Iteration 4043, loss = 0.06497201\n",
      "Iteration 4044, loss = 0.06496292\n",
      "Iteration 4045, loss = 0.06495384\n",
      "Iteration 4046, loss = 0.06494475\n",
      "Iteration 4047, loss = 0.06493567\n",
      "Iteration 4048, loss = 0.06492658\n",
      "Iteration 4049, loss = 0.06491750\n",
      "Iteration 4050, loss = 0.06490841\n",
      "Iteration 4051, loss = 0.06489933\n",
      "Iteration 4052, loss = 0.06489024\n",
      "Iteration 4053, loss = 0.06488116\n",
      "Iteration 4054, loss = 0.06487207\n",
      "Iteration 4055, loss = 0.06486298\n",
      "Iteration 4056, loss = 0.06485390\n",
      "Iteration 4057, loss = 0.06484481\n",
      "Iteration 4058, loss = 0.06483572\n",
      "Iteration 4059, loss = 0.06482664\n",
      "Iteration 4060, loss = 0.06481755\n",
      "Iteration 4061, loss = 0.06480846\n",
      "Iteration 4062, loss = 0.06479937\n",
      "Iteration 4063, loss = 0.06479029\n",
      "Iteration 4064, loss = 0.06478120\n",
      "Iteration 4065, loss = 0.06477211\n",
      "Iteration 4066, loss = 0.06476302\n",
      "Iteration 4067, loss = 0.06475393\n",
      "Iteration 4068, loss = 0.06474484\n",
      "Iteration 4069, loss = 0.06473575\n",
      "Iteration 4070, loss = 0.06472666\n",
      "Iteration 4071, loss = 0.06471757\n",
      "Iteration 4072, loss = 0.06470848\n",
      "Iteration 4073, loss = 0.06469939\n",
      "Iteration 4074, loss = 0.06469029\n",
      "Iteration 4075, loss = 0.06468120\n",
      "Iteration 4076, loss = 0.06467211\n",
      "Iteration 4077, loss = 0.06466302\n",
      "Iteration 4078, loss = 0.06465392\n",
      "Iteration 4079, loss = 0.06464483\n",
      "Iteration 4080, loss = 0.06463574\n",
      "Iteration 4081, loss = 0.06462664\n",
      "Iteration 4082, loss = 0.06461755\n",
      "Iteration 4083, loss = 0.06460845\n",
      "Iteration 4084, loss = 0.06459936\n",
      "Iteration 4085, loss = 0.06459026\n",
      "Iteration 4086, loss = 0.06458117\n",
      "Iteration 4087, loss = 0.06457207\n",
      "Iteration 4088, loss = 0.06456297\n",
      "Iteration 4089, loss = 0.06455388\n",
      "Iteration 4090, loss = 0.06454478\n",
      "Iteration 4091, loss = 0.06453568\n",
      "Iteration 4092, loss = 0.06452658\n",
      "Iteration 4093, loss = 0.06451748\n",
      "Iteration 4094, loss = 0.06450838\n",
      "Iteration 4095, loss = 0.06449928\n",
      "Iteration 4096, loss = 0.06449018\n",
      "Iteration 4097, loss = 0.06448108\n",
      "Iteration 4098, loss = 0.06447198\n",
      "Iteration 4099, loss = 0.06446288\n",
      "Iteration 4100, loss = 0.06445378\n",
      "Iteration 4101, loss = 0.06444468\n",
      "Iteration 4102, loss = 0.06443557\n",
      "Iteration 4103, loss = 0.06442647\n",
      "Iteration 4104, loss = 0.06441737\n",
      "Iteration 4105, loss = 0.06440826\n",
      "Iteration 4106, loss = 0.06439916\n",
      "Iteration 4107, loss = 0.06439005\n",
      "Iteration 4108, loss = 0.06438095\n",
      "Iteration 4109, loss = 0.06437184\n",
      "Iteration 4110, loss = 0.06436274\n",
      "Iteration 4111, loss = 0.06435363\n",
      "Iteration 4112, loss = 0.06434452\n",
      "Iteration 4113, loss = 0.06433541\n",
      "Iteration 4114, loss = 0.06432631\n",
      "Iteration 4115, loss = 0.06431720\n",
      "Iteration 4116, loss = 0.06430809\n",
      "Iteration 4117, loss = 0.06429898\n",
      "Iteration 4118, loss = 0.06428987\n",
      "Iteration 4119, loss = 0.06428076\n",
      "Iteration 4120, loss = 0.06427164\n",
      "Iteration 4121, loss = 0.06426253\n",
      "Iteration 4122, loss = 0.06425342\n",
      "Iteration 4123, loss = 0.06424431\n",
      "Iteration 4124, loss = 0.06423519\n",
      "Iteration 4125, loss = 0.06422608\n",
      "Iteration 4126, loss = 0.06421696\n",
      "Iteration 4127, loss = 0.06420785\n",
      "Iteration 4128, loss = 0.06419873\n",
      "Iteration 4129, loss = 0.06418961\n",
      "Iteration 4130, loss = 0.06418050\n",
      "Iteration 4131, loss = 0.06417138\n",
      "Iteration 4132, loss = 0.06416226\n",
      "Iteration 4133, loss = 0.06415314\n",
      "Iteration 4134, loss = 0.06414402\n",
      "Iteration 4135, loss = 0.06413490\n",
      "Iteration 4136, loss = 0.06412578\n",
      "Iteration 4137, loss = 0.06411666\n",
      "Iteration 4138, loss = 0.06410754\n",
      "Iteration 4139, loss = 0.06409842\n",
      "Iteration 4140, loss = 0.06408929\n",
      "Iteration 4141, loss = 0.06408017\n",
      "Iteration 4142, loss = 0.06407104\n",
      "Iteration 4143, loss = 0.06406192\n",
      "Iteration 4144, loss = 0.06405279\n",
      "Iteration 4145, loss = 0.06404367\n",
      "Iteration 4146, loss = 0.06403454\n",
      "Iteration 4147, loss = 0.06402541\n",
      "Iteration 4148, loss = 0.06401629\n",
      "Iteration 4149, loss = 0.06400716\n",
      "Iteration 4150, loss = 0.06399803\n",
      "Iteration 4151, loss = 0.06398890\n",
      "Iteration 4152, loss = 0.06397977\n",
      "Iteration 4153, loss = 0.06397063\n",
      "Iteration 4154, loss = 0.06396150\n",
      "Iteration 4155, loss = 0.06395237\n",
      "Iteration 4156, loss = 0.06394324\n",
      "Iteration 4157, loss = 0.06393410\n",
      "Iteration 4158, loss = 0.06392497\n",
      "Iteration 4159, loss = 0.06391583\n",
      "Iteration 4160, loss = 0.06390669\n",
      "Iteration 4161, loss = 0.06389756\n",
      "Iteration 4162, loss = 0.06388842\n",
      "Iteration 4163, loss = 0.06387928\n",
      "Iteration 4164, loss = 0.06387014\n",
      "Iteration 4165, loss = 0.06386100\n",
      "Iteration 4166, loss = 0.06385186\n",
      "Iteration 4167, loss = 0.06384272\n",
      "Iteration 4168, loss = 0.06383358\n",
      "Iteration 4169, loss = 0.06382444\n",
      "Iteration 4170, loss = 0.06381529\n",
      "Iteration 4171, loss = 0.06380615\n",
      "Iteration 4172, loss = 0.06379700\n",
      "Iteration 4173, loss = 0.06378786\n",
      "Iteration 4174, loss = 0.06377871\n",
      "Iteration 4175, loss = 0.06376956\n",
      "Iteration 4176, loss = 0.06376042\n",
      "Iteration 4177, loss = 0.06375127\n",
      "Iteration 4178, loss = 0.06374212\n",
      "Iteration 4179, loss = 0.06373297\n",
      "Iteration 4180, loss = 0.06372382\n",
      "Iteration 4181, loss = 0.06371466\n",
      "Iteration 4182, loss = 0.06370551\n",
      "Iteration 4183, loss = 0.06369636\n",
      "Iteration 4184, loss = 0.06368720\n",
      "Iteration 4185, loss = 0.06367805\n",
      "Iteration 4186, loss = 0.06366889\n",
      "Iteration 4187, loss = 0.06365974\n",
      "Iteration 4188, loss = 0.06365058\n",
      "Iteration 4189, loss = 0.06364142\n",
      "Iteration 4190, loss = 0.06363226\n",
      "Iteration 4191, loss = 0.06362310\n",
      "Iteration 4192, loss = 0.06361394\n",
      "Iteration 4193, loss = 0.06360478\n",
      "Iteration 4194, loss = 0.06359562\n",
      "Iteration 4195, loss = 0.06358646\n",
      "Iteration 4196, loss = 0.06357729\n",
      "Iteration 4197, loss = 0.06356813\n",
      "Iteration 4198, loss = 0.06355896\n",
      "Iteration 4199, loss = 0.06354980\n",
      "Iteration 4200, loss = 0.06354063\n",
      "Iteration 4201, loss = 0.06353146\n",
      "Iteration 4202, loss = 0.06352229\n",
      "Iteration 4203, loss = 0.06351312\n",
      "Iteration 4204, loss = 0.06350395\n",
      "Iteration 4205, loss = 0.06349478\n",
      "Iteration 4206, loss = 0.06348561\n",
      "Iteration 4207, loss = 0.06347644\n",
      "Iteration 4208, loss = 0.06346726\n",
      "Iteration 4209, loss = 0.06345809\n",
      "Iteration 4210, loss = 0.06344891\n",
      "Iteration 4211, loss = 0.06343974\n",
      "Iteration 4212, loss = 0.06343056\n",
      "Iteration 4213, loss = 0.06342138\n",
      "Iteration 4214, loss = 0.06341220\n",
      "Iteration 4215, loss = 0.06340302\n",
      "Iteration 4216, loss = 0.06339384\n",
      "Iteration 4217, loss = 0.06338466\n",
      "Iteration 4218, loss = 0.06337548\n",
      "Iteration 4219, loss = 0.06336629\n",
      "Iteration 4220, loss = 0.06335711\n",
      "Iteration 4221, loss = 0.06334792\n",
      "Iteration 4222, loss = 0.06333874\n",
      "Iteration 4223, loss = 0.06332955\n",
      "Iteration 4224, loss = 0.06332036\n",
      "Iteration 4225, loss = 0.06331117\n",
      "Iteration 4226, loss = 0.06330198\n",
      "Iteration 4227, loss = 0.06329279\n",
      "Iteration 4228, loss = 0.06328360\n",
      "Iteration 4229, loss = 0.06327441\n",
      "Iteration 4230, loss = 0.06326521\n",
      "Iteration 4231, loss = 0.06325602\n",
      "Iteration 4232, loss = 0.06324682\n",
      "Iteration 4233, loss = 0.06323763\n",
      "Iteration 4234, loss = 0.06322843\n",
      "Iteration 4235, loss = 0.06321923\n",
      "Iteration 4236, loss = 0.06321003\n",
      "Iteration 4237, loss = 0.06320083\n",
      "Iteration 4238, loss = 0.06319163\n",
      "Iteration 4239, loss = 0.06318243\n",
      "Iteration 4240, loss = 0.06317322\n",
      "Iteration 4241, loss = 0.06316402\n",
      "Iteration 4242, loss = 0.06315482\n",
      "Iteration 4243, loss = 0.06314561\n",
      "Iteration 4244, loss = 0.06313640\n",
      "Iteration 4245, loss = 0.06312720\n",
      "Iteration 4246, loss = 0.06311799\n",
      "Iteration 4247, loss = 0.06310878\n",
      "Iteration 4248, loss = 0.06309957\n",
      "Iteration 4249, loss = 0.06309035\n",
      "Iteration 4250, loss = 0.06308114\n",
      "Iteration 4251, loss = 0.06307193\n",
      "Iteration 4252, loss = 0.06306271\n",
      "Iteration 4253, loss = 0.06305350\n",
      "Iteration 4254, loss = 0.06304428\n",
      "Iteration 4255, loss = 0.06303506\n",
      "Iteration 4256, loss = 0.06302585\n",
      "Iteration 4257, loss = 0.06301663\n",
      "Iteration 4258, loss = 0.06300741\n",
      "Iteration 4259, loss = 0.06299818\n",
      "Iteration 4260, loss = 0.06298896\n",
      "Iteration 4261, loss = 0.06297974\n",
      "Iteration 4262, loss = 0.06297051\n",
      "Iteration 4263, loss = 0.06296129\n",
      "Iteration 4264, loss = 0.06295206\n",
      "Iteration 4265, loss = 0.06294283\n",
      "Iteration 4266, loss = 0.06293360\n",
      "Iteration 4267, loss = 0.06292438\n",
      "Iteration 4268, loss = 0.06291514\n",
      "Iteration 4269, loss = 0.06290591\n",
      "Iteration 4270, loss = 0.06289668\n",
      "Iteration 4271, loss = 0.06288745\n",
      "Iteration 4272, loss = 0.06287821\n",
      "Iteration 4273, loss = 0.06286898\n",
      "Iteration 4274, loss = 0.06285974\n",
      "Iteration 4275, loss = 0.06285050\n",
      "Iteration 4276, loss = 0.06284126\n",
      "Iteration 4277, loss = 0.06283202\n",
      "Iteration 4278, loss = 0.06282278\n",
      "Iteration 4279, loss = 0.06281354\n",
      "Iteration 4280, loss = 0.06280430\n",
      "Iteration 4281, loss = 0.06279505\n",
      "Iteration 4282, loss = 0.06278581\n",
      "Iteration 4283, loss = 0.06277656\n",
      "Iteration 4284, loss = 0.06276732\n",
      "Iteration 4285, loss = 0.06275807\n",
      "Iteration 4286, loss = 0.06274882\n",
      "Iteration 4287, loss = 0.06273957\n",
      "Iteration 4288, loss = 0.06273032\n",
      "Iteration 4289, loss = 0.06272107\n",
      "Iteration 4290, loss = 0.06271181\n",
      "Iteration 4291, loss = 0.06270256\n",
      "Iteration 4292, loss = 0.06269330\n",
      "Iteration 4293, loss = 0.06268405\n",
      "Iteration 4294, loss = 0.06267479\n",
      "Iteration 4295, loss = 0.06266553\n",
      "Iteration 4296, loss = 0.06265627\n",
      "Iteration 4297, loss = 0.06264701\n",
      "Iteration 4298, loss = 0.06263775\n",
      "Iteration 4299, loss = 0.06262849\n",
      "Iteration 4300, loss = 0.06261922\n",
      "Iteration 4301, loss = 0.06260996\n",
      "Iteration 4302, loss = 0.06260069\n",
      "Iteration 4303, loss = 0.06259142\n",
      "Iteration 4304, loss = 0.06258216\n",
      "Iteration 4305, loss = 0.06257289\n",
      "Iteration 4306, loss = 0.06256362\n",
      "Iteration 4307, loss = 0.06255435\n",
      "Iteration 4308, loss = 0.06254507\n",
      "Iteration 4309, loss = 0.06253580\n",
      "Iteration 4310, loss = 0.06252652\n",
      "Iteration 4311, loss = 0.06251725\n",
      "Iteration 4312, loss = 0.06250797\n",
      "Iteration 4313, loss = 0.06249869\n",
      "Iteration 4314, loss = 0.06248942\n",
      "Iteration 4315, loss = 0.06248014\n",
      "Iteration 4316, loss = 0.06247085\n",
      "Iteration 4317, loss = 0.06246157\n",
      "Iteration 4318, loss = 0.06245229\n",
      "Iteration 4319, loss = 0.06244301\n",
      "Iteration 4320, loss = 0.06243372\n",
      "Iteration 4321, loss = 0.06242443\n",
      "Iteration 4322, loss = 0.06241515\n",
      "Iteration 4323, loss = 0.06240586\n",
      "Iteration 4324, loss = 0.06239657\n",
      "Iteration 4325, loss = 0.06238728\n",
      "Iteration 4326, loss = 0.06237798\n",
      "Iteration 4327, loss = 0.06236869\n",
      "Iteration 4328, loss = 0.06235940\n",
      "Iteration 4329, loss = 0.06235010\n",
      "Iteration 4330, loss = 0.06234081\n",
      "Iteration 4331, loss = 0.06233151\n",
      "Iteration 4332, loss = 0.06232221\n",
      "Iteration 4333, loss = 0.06231291\n",
      "Iteration 4334, loss = 0.06230361\n",
      "Iteration 4335, loss = 0.06229431\n",
      "Iteration 4336, loss = 0.06228500\n",
      "Iteration 4337, loss = 0.06227570\n",
      "Iteration 4338, loss = 0.06226640\n",
      "Iteration 4339, loss = 0.06225709\n",
      "Iteration 4340, loss = 0.06224778\n",
      "Iteration 4341, loss = 0.06223847\n",
      "Iteration 4342, loss = 0.06222916\n",
      "Iteration 4343, loss = 0.06221985\n",
      "Iteration 4344, loss = 0.06221054\n",
      "Iteration 4345, loss = 0.06220123\n",
      "Iteration 4346, loss = 0.06219191\n",
      "Iteration 4347, loss = 0.06218260\n",
      "Iteration 4348, loss = 0.06217328\n",
      "Iteration 4349, loss = 0.06216397\n",
      "Iteration 4350, loss = 0.06215465\n",
      "Iteration 4351, loss = 0.06214533\n",
      "Iteration 4352, loss = 0.06213601\n",
      "Iteration 4353, loss = 0.06212669\n",
      "Iteration 4354, loss = 0.06211736\n",
      "Iteration 4355, loss = 0.06210804\n",
      "Iteration 4356, loss = 0.06209871\n",
      "Iteration 4357, loss = 0.06208939\n",
      "Iteration 4358, loss = 0.06208006\n",
      "Iteration 4359, loss = 0.06207073\n",
      "Iteration 4360, loss = 0.06206140\n",
      "Iteration 4361, loss = 0.06205207\n",
      "Iteration 4362, loss = 0.06204274\n",
      "Iteration 4363, loss = 0.06203341\n",
      "Iteration 4364, loss = 0.06202407\n",
      "Iteration 4365, loss = 0.06201474\n",
      "Iteration 4366, loss = 0.06200540\n",
      "Iteration 4367, loss = 0.06199606\n",
      "Iteration 4368, loss = 0.06198672\n",
      "Iteration 4369, loss = 0.06197739\n",
      "Iteration 4370, loss = 0.06196804\n",
      "Iteration 4371, loss = 0.06195870\n",
      "Iteration 4372, loss = 0.06194936\n",
      "Iteration 4373, loss = 0.06194002\n",
      "Iteration 4374, loss = 0.06193067\n",
      "Iteration 4375, loss = 0.06192132\n",
      "Iteration 4376, loss = 0.06191198\n",
      "Iteration 4377, loss = 0.06190263\n",
      "Iteration 4378, loss = 0.06189328\n",
      "Iteration 4379, loss = 0.06188393\n",
      "Iteration 4380, loss = 0.06187457\n",
      "Iteration 4381, loss = 0.06186522\n",
      "Iteration 4382, loss = 0.06185587\n",
      "Iteration 4383, loss = 0.06184651\n",
      "Iteration 4384, loss = 0.06183715\n",
      "Iteration 4385, loss = 0.06182780\n",
      "Iteration 4386, loss = 0.06181844\n",
      "Iteration 4387, loss = 0.06180908\n",
      "Iteration 4388, loss = 0.06179972\n",
      "Iteration 4389, loss = 0.06179035\n",
      "Iteration 4390, loss = 0.06178099\n",
      "Iteration 4391, loss = 0.06177163\n",
      "Iteration 4392, loss = 0.06176226\n",
      "Iteration 4393, loss = 0.06175289\n",
      "Iteration 4394, loss = 0.06174353\n",
      "Iteration 4395, loss = 0.06173416\n",
      "Iteration 4396, loss = 0.06172479\n",
      "Iteration 4397, loss = 0.06171542\n",
      "Iteration 4398, loss = 0.06170604\n",
      "Iteration 4399, loss = 0.06169667\n",
      "Iteration 4400, loss = 0.06168729\n",
      "Iteration 4401, loss = 0.06167792\n",
      "Iteration 4402, loss = 0.06166854\n",
      "Iteration 4403, loss = 0.06165916\n",
      "Iteration 4404, loss = 0.06164978\n",
      "Iteration 4405, loss = 0.06164040\n",
      "Iteration 4406, loss = 0.06163102\n",
      "Iteration 4407, loss = 0.06162164\n",
      "Iteration 4408, loss = 0.06161226\n",
      "Iteration 4409, loss = 0.06160287\n",
      "Iteration 4410, loss = 0.06159348\n",
      "Iteration 4411, loss = 0.06158410\n",
      "Iteration 4412, loss = 0.06157471\n",
      "Iteration 4413, loss = 0.06156532\n",
      "Iteration 4414, loss = 0.06155593\n",
      "Iteration 4415, loss = 0.06154654\n",
      "Iteration 4416, loss = 0.06153714\n",
      "Iteration 4417, loss = 0.06152775\n",
      "Iteration 4418, loss = 0.06151835\n",
      "Iteration 4419, loss = 0.06150896\n",
      "Iteration 4420, loss = 0.06149956\n",
      "Iteration 4421, loss = 0.06149016\n",
      "Iteration 4422, loss = 0.06148076\n",
      "Iteration 4423, loss = 0.06147136\n",
      "Iteration 4424, loss = 0.06146196\n",
      "Iteration 4425, loss = 0.06145256\n",
      "Iteration 4426, loss = 0.06144315\n",
      "Iteration 4427, loss = 0.06143375\n",
      "Iteration 4428, loss = 0.06142434\n",
      "Iteration 4429, loss = 0.06141493\n",
      "Iteration 4430, loss = 0.06140552\n",
      "Iteration 4431, loss = 0.06139611\n",
      "Iteration 4432, loss = 0.06138670\n",
      "Iteration 4433, loss = 0.06137729\n",
      "Iteration 4434, loss = 0.06136788\n",
      "Iteration 4435, loss = 0.06135846\n",
      "Iteration 4436, loss = 0.06134905\n",
      "Iteration 4437, loss = 0.06133963\n",
      "Iteration 4438, loss = 0.06133021\n",
      "Iteration 4439, loss = 0.06132079\n",
      "Iteration 4440, loss = 0.06131137\n",
      "Iteration 4441, loss = 0.06130195\n",
      "Iteration 4442, loss = 0.06129253\n",
      "Iteration 4443, loss = 0.06128310\n",
      "Iteration 4444, loss = 0.06127368\n",
      "Iteration 4445, loss = 0.06126425\n",
      "Iteration 4446, loss = 0.06125482\n",
      "Iteration 4447, loss = 0.06124540\n",
      "Iteration 4448, loss = 0.06123597\n",
      "Iteration 4449, loss = 0.06122653\n",
      "Iteration 4450, loss = 0.06121710\n",
      "Iteration 4451, loss = 0.06120767\n",
      "Iteration 4452, loss = 0.06119824\n",
      "Iteration 4453, loss = 0.06118880\n",
      "Iteration 4454, loss = 0.06117936\n",
      "Iteration 4455, loss = 0.06116993\n",
      "Iteration 4456, loss = 0.06116049\n",
      "Iteration 4457, loss = 0.06115105\n",
      "Iteration 4458, loss = 0.06114161\n",
      "Iteration 4459, loss = 0.06113216\n",
      "Iteration 4460, loss = 0.06112272\n",
      "Iteration 4461, loss = 0.06111328\n",
      "Iteration 4462, loss = 0.06110383\n",
      "Iteration 4463, loss = 0.06109438\n",
      "Iteration 4464, loss = 0.06108494\n",
      "Iteration 4465, loss = 0.06107549\n",
      "Iteration 4466, loss = 0.06106604\n",
      "Iteration 4467, loss = 0.06105658\n",
      "Iteration 4468, loss = 0.06104713\n",
      "Iteration 4469, loss = 0.06103768\n",
      "Iteration 4470, loss = 0.06102822\n",
      "Iteration 4471, loss = 0.06101877\n",
      "Iteration 4472, loss = 0.06100931\n",
      "Iteration 4473, loss = 0.06099985\n",
      "Iteration 4474, loss = 0.06099039\n",
      "Iteration 4475, loss = 0.06098093\n",
      "Iteration 4476, loss = 0.06097147\n",
      "Iteration 4477, loss = 0.06096201\n",
      "Iteration 4478, loss = 0.06095254\n",
      "Iteration 4479, loss = 0.06094308\n",
      "Iteration 4480, loss = 0.06093361\n",
      "Iteration 4481, loss = 0.06092414\n",
      "Iteration 4482, loss = 0.06091467\n",
      "Iteration 4483, loss = 0.06090520\n",
      "Iteration 4484, loss = 0.06089573\n",
      "Iteration 4485, loss = 0.06088626\n",
      "Iteration 4486, loss = 0.06087679\n",
      "Iteration 4487, loss = 0.06086731\n",
      "Iteration 4488, loss = 0.06085783\n",
      "Iteration 4489, loss = 0.06084836\n",
      "Iteration 4490, loss = 0.06083888\n",
      "Iteration 4491, loss = 0.06082940\n",
      "Iteration 4492, loss = 0.06081992\n",
      "Iteration 4493, loss = 0.06081044\n",
      "Iteration 4494, loss = 0.06080095\n",
      "Iteration 4495, loss = 0.06079147\n",
      "Iteration 4496, loss = 0.06078199\n",
      "Iteration 4497, loss = 0.06077250\n",
      "Iteration 4498, loss = 0.06076301\n",
      "Iteration 4499, loss = 0.06075352\n",
      "Iteration 4500, loss = 0.06074403\n",
      "Iteration 4501, loss = 0.06073454\n",
      "Iteration 4502, loss = 0.06072505\n",
      "Iteration 4503, loss = 0.06071556\n",
      "Iteration 4504, loss = 0.06070606\n",
      "Iteration 4505, loss = 0.06069657\n",
      "Iteration 4506, loss = 0.06068707\n",
      "Iteration 4507, loss = 0.06067757\n",
      "Iteration 4508, loss = 0.06066807\n",
      "Iteration 4509, loss = 0.06065857\n",
      "Iteration 4510, loss = 0.06064907\n",
      "Iteration 4511, loss = 0.06063957\n",
      "Iteration 4512, loss = 0.06063006\n",
      "Iteration 4513, loss = 0.06062056\n",
      "Iteration 4514, loss = 0.06061105\n",
      "Iteration 4515, loss = 0.06060154\n",
      "Iteration 4516, loss = 0.06059203\n",
      "Iteration 4517, loss = 0.06058253\n",
      "Iteration 4518, loss = 0.06057301\n",
      "Iteration 4519, loss = 0.06056350\n",
      "Iteration 4520, loss = 0.06055399\n",
      "Iteration 4521, loss = 0.06054447\n",
      "Iteration 4522, loss = 0.06053496\n",
      "Iteration 4523, loss = 0.06052544\n",
      "Iteration 4524, loss = 0.06051592\n",
      "Iteration 4525, loss = 0.06050640\n",
      "Iteration 4526, loss = 0.06049688\n",
      "Iteration 4527, loss = 0.06048736\n",
      "Iteration 4528, loss = 0.06047784\n",
      "Iteration 4529, loss = 0.06046831\n",
      "Iteration 4530, loss = 0.06045879\n",
      "Iteration 4531, loss = 0.06044926\n",
      "Iteration 4532, loss = 0.06043973\n",
      "Iteration 4533, loss = 0.06043021\n",
      "Iteration 4534, loss = 0.06042068\n",
      "Iteration 4535, loss = 0.06041114\n",
      "Iteration 4536, loss = 0.06040161\n",
      "Iteration 4537, loss = 0.06039208\n",
      "Iteration 4538, loss = 0.06038254\n",
      "Iteration 4539, loss = 0.06037301\n",
      "Iteration 4540, loss = 0.06036347\n",
      "Iteration 4541, loss = 0.06035393\n",
      "Iteration 4542, loss = 0.06034439\n",
      "Iteration 4543, loss = 0.06033485\n",
      "Iteration 4544, loss = 0.06032531\n",
      "Iteration 4545, loss = 0.06031577\n",
      "Iteration 4546, loss = 0.06030622\n",
      "Iteration 4547, loss = 0.06029668\n",
      "Iteration 4548, loss = 0.06028713\n",
      "Iteration 4549, loss = 0.06027758\n",
      "Iteration 4550, loss = 0.06026803\n",
      "Iteration 4551, loss = 0.06025848\n",
      "Iteration 4552, loss = 0.06024893\n",
      "Iteration 4553, loss = 0.06023938\n",
      "Iteration 4554, loss = 0.06022982\n",
      "Iteration 4555, loss = 0.06022027\n",
      "Iteration 4556, loss = 0.06021071\n",
      "Iteration 4557, loss = 0.06020115\n",
      "Iteration 4558, loss = 0.06019159\n",
      "Iteration 4559, loss = 0.06018203\n",
      "Iteration 4560, loss = 0.06017247\n",
      "Iteration 4561, loss = 0.06016291\n",
      "Iteration 4562, loss = 0.06015335\n",
      "Iteration 4563, loss = 0.06014378\n",
      "Iteration 4564, loss = 0.06013421\n",
      "Iteration 4565, loss = 0.06012465\n",
      "Iteration 4566, loss = 0.06011508\n",
      "Iteration 4567, loss = 0.06010551\n",
      "Iteration 4568, loss = 0.06009594\n",
      "Iteration 4569, loss = 0.06008636\n",
      "Iteration 4570, loss = 0.06007679\n",
      "Iteration 4571, loss = 0.06006722\n",
      "Iteration 4572, loss = 0.06005764\n",
      "Iteration 4573, loss = 0.06004806\n",
      "Iteration 4574, loss = 0.06003848\n",
      "Iteration 4575, loss = 0.06002890\n",
      "Iteration 4576, loss = 0.06001932\n",
      "Iteration 4577, loss = 0.06000974\n",
      "Iteration 4578, loss = 0.06000016\n",
      "Iteration 4579, loss = 0.05999057\n",
      "Iteration 4580, loss = 0.05998099\n",
      "Iteration 4581, loss = 0.05997140\n",
      "Iteration 4582, loss = 0.05996181\n",
      "Iteration 4583, loss = 0.05995222\n",
      "Iteration 4584, loss = 0.05994263\n",
      "Iteration 4585, loss = 0.05993304\n",
      "Iteration 4586, loss = 0.05992345\n",
      "Iteration 4587, loss = 0.05991385\n",
      "Iteration 4588, loss = 0.05990425\n",
      "Iteration 4589, loss = 0.05989466\n",
      "Iteration 4590, loss = 0.05988506\n",
      "Iteration 4591, loss = 0.05987546\n",
      "Iteration 4592, loss = 0.05986586\n",
      "Iteration 4593, loss = 0.05985626\n",
      "Iteration 4594, loss = 0.05984665\n",
      "Iteration 4595, loss = 0.05983705\n",
      "Iteration 4596, loss = 0.05982744\n",
      "Iteration 4597, loss = 0.05981784\n",
      "Iteration 4598, loss = 0.05980823\n",
      "Iteration 4599, loss = 0.05979862\n",
      "Iteration 4600, loss = 0.05978901\n",
      "Iteration 4601, loss = 0.05977940\n",
      "Iteration 4602, loss = 0.05976978\n",
      "Iteration 4603, loss = 0.05976017\n",
      "Iteration 4604, loss = 0.05975055\n",
      "Iteration 4605, loss = 0.05974093\n",
      "Iteration 4606, loss = 0.05973132\n",
      "Iteration 4607, loss = 0.05972170\n",
      "Iteration 4608, loss = 0.05971207\n",
      "Iteration 4609, loss = 0.05970245\n",
      "Iteration 4610, loss = 0.05969283\n",
      "Iteration 4611, loss = 0.05968320\n",
      "Iteration 4612, loss = 0.05967358\n",
      "Iteration 4613, loss = 0.05966395\n",
      "Iteration 4614, loss = 0.05965432\n",
      "Iteration 4615, loss = 0.05964469\n",
      "Iteration 4616, loss = 0.05963506\n",
      "Iteration 4617, loss = 0.05962543\n",
      "Iteration 4618, loss = 0.05961579\n",
      "Iteration 4619, loss = 0.05960616\n",
      "Iteration 4620, loss = 0.05959652\n",
      "Iteration 4621, loss = 0.05958689\n",
      "Iteration 4622, loss = 0.05957725\n",
      "Iteration 4623, loss = 0.05956761\n",
      "Iteration 4624, loss = 0.05955796\n",
      "Iteration 4625, loss = 0.05954832\n",
      "Iteration 4626, loss = 0.05953868\n",
      "Iteration 4627, loss = 0.05952903\n",
      "Iteration 4628, loss = 0.05951939\n",
      "Iteration 4629, loss = 0.05950974\n",
      "Iteration 4630, loss = 0.05950009\n",
      "Iteration 4631, loss = 0.05949044\n",
      "Iteration 4632, loss = 0.05948079\n",
      "Iteration 4633, loss = 0.05947113\n",
      "Iteration 4634, loss = 0.05946148\n",
      "Iteration 4635, loss = 0.05945182\n",
      "Iteration 4636, loss = 0.05944217\n",
      "Iteration 4637, loss = 0.05943251\n",
      "Iteration 4638, loss = 0.05942285\n",
      "Iteration 4639, loss = 0.05941319\n",
      "Iteration 4640, loss = 0.05940352\n",
      "Iteration 4641, loss = 0.05939386\n",
      "Iteration 4642, loss = 0.05938419\n",
      "Iteration 4643, loss = 0.05937453\n",
      "Iteration 4644, loss = 0.05936486\n",
      "Iteration 4645, loss = 0.05935519\n",
      "Iteration 4646, loss = 0.05934552\n",
      "Iteration 4647, loss = 0.05933585\n",
      "Iteration 4648, loss = 0.05932618\n",
      "Iteration 4649, loss = 0.05931650\n",
      "Iteration 4650, loss = 0.05930683\n",
      "Iteration 4651, loss = 0.05929715\n",
      "Iteration 4652, loss = 0.05928747\n",
      "Iteration 4653, loss = 0.05927779\n",
      "Iteration 4654, loss = 0.05926811\n",
      "Iteration 4655, loss = 0.05925843\n",
      "Iteration 4656, loss = 0.05924875\n",
      "Iteration 4657, loss = 0.05923906\n",
      "Iteration 4658, loss = 0.05922937\n",
      "Iteration 4659, loss = 0.05921969\n",
      "Iteration 4660, loss = 0.05921000\n",
      "Iteration 4661, loss = 0.05920031\n",
      "Iteration 4662, loss = 0.05919062\n",
      "Iteration 4663, loss = 0.05918092\n",
      "Iteration 4664, loss = 0.05917123\n",
      "Iteration 4665, loss = 0.05916153\n",
      "Iteration 4666, loss = 0.05915184\n",
      "Iteration 4667, loss = 0.05914214\n",
      "Iteration 4668, loss = 0.05913244\n",
      "Iteration 4669, loss = 0.05912274\n",
      "Iteration 4670, loss = 0.05911303\n",
      "Iteration 4671, loss = 0.05910333\n",
      "Iteration 4672, loss = 0.05909362\n",
      "Iteration 4673, loss = 0.05908392\n",
      "Iteration 4674, loss = 0.05907421\n",
      "Iteration 4675, loss = 0.05906450\n",
      "Iteration 4676, loss = 0.05905479\n",
      "Iteration 4677, loss = 0.05904508\n",
      "Iteration 4678, loss = 0.05903536\n",
      "Iteration 4679, loss = 0.05902565\n",
      "Iteration 4680, loss = 0.05901593\n",
      "Iteration 4681, loss = 0.05900622\n",
      "Iteration 4682, loss = 0.05899650\n",
      "Iteration 4683, loss = 0.05898678\n",
      "Iteration 4684, loss = 0.05897706\n",
      "Iteration 4685, loss = 0.05896733\n",
      "Iteration 4686, loss = 0.05895761\n",
      "Iteration 4687, loss = 0.05894788\n",
      "Iteration 4688, loss = 0.05893815\n",
      "Iteration 4689, loss = 0.05892843\n",
      "Iteration 4690, loss = 0.05891870\n",
      "Iteration 4691, loss = 0.05890896\n",
      "Iteration 4692, loss = 0.05889923\n",
      "Iteration 4693, loss = 0.05888950\n",
      "Iteration 4694, loss = 0.05887976\n",
      "Iteration 4695, loss = 0.05887002\n",
      "Iteration 4696, loss = 0.05886029\n",
      "Iteration 4697, loss = 0.05885055\n",
      "Iteration 4698, loss = 0.05884080\n",
      "Iteration 4699, loss = 0.05883106\n",
      "Iteration 4700, loss = 0.05882132\n",
      "Iteration 4701, loss = 0.05881157\n",
      "Iteration 4702, loss = 0.05880183\n",
      "Iteration 4703, loss = 0.05879208\n",
      "Iteration 4704, loss = 0.05878233\n",
      "Iteration 4705, loss = 0.05877258\n",
      "Iteration 4706, loss = 0.05876282\n",
      "Iteration 4707, loss = 0.05875307\n",
      "Iteration 4708, loss = 0.05874331\n",
      "Iteration 4709, loss = 0.05873356\n",
      "Iteration 4710, loss = 0.05872380\n",
      "Iteration 4711, loss = 0.05871404\n",
      "Iteration 4712, loss = 0.05870428\n",
      "Iteration 4713, loss = 0.05869452\n",
      "Iteration 4714, loss = 0.05868475\n",
      "Iteration 4715, loss = 0.05867499\n",
      "Iteration 4716, loss = 0.05866522\n",
      "Iteration 4717, loss = 0.05865545\n",
      "Iteration 4718, loss = 0.05864568\n",
      "Iteration 4719, loss = 0.05863591\n",
      "Iteration 4720, loss = 0.05862614\n",
      "Iteration 4721, loss = 0.05861636\n",
      "Iteration 4722, loss = 0.05860659\n",
      "Iteration 4723, loss = 0.05859681\n",
      "Iteration 4724, loss = 0.05858703\n",
      "Iteration 4725, loss = 0.05857725\n",
      "Iteration 4726, loss = 0.05856747\n",
      "Iteration 4727, loss = 0.05855769\n",
      "Iteration 4728, loss = 0.05854790\n",
      "Iteration 4729, loss = 0.05853812\n",
      "Iteration 4730, loss = 0.05852833\n",
      "Iteration 4731, loss = 0.05851854\n",
      "Iteration 4732, loss = 0.05850875\n",
      "Iteration 4733, loss = 0.05849896\n",
      "Iteration 4734, loss = 0.05848917\n",
      "Iteration 4735, loss = 0.05847937\n",
      "Iteration 4736, loss = 0.05846958\n",
      "Iteration 4737, loss = 0.05845978\n",
      "Iteration 4738, loss = 0.05844998\n",
      "Iteration 4739, loss = 0.05844018\n",
      "Iteration 4740, loss = 0.05843038\n",
      "Iteration 4741, loss = 0.05842057\n",
      "Iteration 4742, loss = 0.05841077\n",
      "Iteration 4743, loss = 0.05840096\n",
      "Iteration 4744, loss = 0.05839115\n",
      "Iteration 4745, loss = 0.05838134\n",
      "Iteration 4746, loss = 0.05837153\n",
      "Iteration 4747, loss = 0.05836172\n",
      "Iteration 4748, loss = 0.05835191\n",
      "Iteration 4749, loss = 0.05834209\n",
      "Iteration 4750, loss = 0.05833227\n",
      "Iteration 4751, loss = 0.05832246\n",
      "Iteration 4752, loss = 0.05831264\n",
      "Iteration 4753, loss = 0.05830281\n",
      "Iteration 4754, loss = 0.05829299\n",
      "Iteration 4755, loss = 0.05828317\n",
      "Iteration 4756, loss = 0.05827334\n",
      "Iteration 4757, loss = 0.05826351\n",
      "Iteration 4758, loss = 0.05825368\n",
      "Iteration 4759, loss = 0.05824385\n",
      "Iteration 4760, loss = 0.05823402\n",
      "Iteration 4761, loss = 0.05822419\n",
      "Iteration 4762, loss = 0.05821435\n",
      "Iteration 4763, loss = 0.05820452\n",
      "Iteration 4764, loss = 0.05819468\n",
      "Iteration 4765, loss = 0.05818484\n",
      "Iteration 4766, loss = 0.05817500\n",
      "Iteration 4767, loss = 0.05816515\n",
      "Iteration 4768, loss = 0.05815531\n",
      "Iteration 4769, loss = 0.05814546\n",
      "Iteration 4770, loss = 0.05813562\n",
      "Iteration 4771, loss = 0.05812577\n",
      "Iteration 4772, loss = 0.05811592\n",
      "Iteration 4773, loss = 0.05810607\n",
      "Iteration 4774, loss = 0.05809621\n",
      "Iteration 4775, loss = 0.05808636\n",
      "Iteration 4776, loss = 0.05807650\n",
      "Iteration 4777, loss = 0.05806664\n",
      "Iteration 4778, loss = 0.05805678\n",
      "Iteration 4779, loss = 0.05804692\n",
      "Iteration 4780, loss = 0.05803706\n",
      "Iteration 4781, loss = 0.05802719\n",
      "Iteration 4782, loss = 0.05801733\n",
      "Iteration 4783, loss = 0.05800746\n",
      "Iteration 4784, loss = 0.05799759\n",
      "Iteration 4785, loss = 0.05798772\n",
      "Iteration 4786, loss = 0.05797785\n",
      "Iteration 4787, loss = 0.05796798\n",
      "Iteration 4788, loss = 0.05795810\n",
      "Iteration 4789, loss = 0.05794822\n",
      "Iteration 4790, loss = 0.05793834\n",
      "Iteration 4791, loss = 0.05792846\n",
      "Iteration 4792, loss = 0.05791858\n",
      "Iteration 4793, loss = 0.05790870\n",
      "Iteration 4794, loss = 0.05789881\n",
      "Iteration 4795, loss = 0.05788893\n",
      "Iteration 4796, loss = 0.05787904\n",
      "Iteration 4797, loss = 0.05786915\n",
      "Iteration 4798, loss = 0.05785926\n",
      "Iteration 4799, loss = 0.05784937\n",
      "Iteration 4800, loss = 0.05783947\n",
      "Iteration 4801, loss = 0.05782958\n",
      "Iteration 4802, loss = 0.05781968\n",
      "Iteration 4803, loss = 0.05780978\n",
      "Iteration 4804, loss = 0.05779988\n",
      "Iteration 4805, loss = 0.05778998\n",
      "Iteration 4806, loss = 0.05778007\n",
      "Iteration 4807, loss = 0.05777017\n",
      "Iteration 4808, loss = 0.05776026\n",
      "Iteration 4809, loss = 0.05775035\n",
      "Iteration 4810, loss = 0.05774044\n",
      "Iteration 4811, loss = 0.05773053\n",
      "Iteration 4812, loss = 0.05772062\n",
      "Iteration 4813, loss = 0.05771070\n",
      "Iteration 4814, loss = 0.05770078\n",
      "Iteration 4815, loss = 0.05769087\n",
      "Iteration 4816, loss = 0.05768095\n",
      "Iteration 4817, loss = 0.05767102\n",
      "Iteration 4818, loss = 0.05766110\n",
      "Iteration 4819, loss = 0.05765118\n",
      "Iteration 4820, loss = 0.05764125\n",
      "Iteration 4821, loss = 0.05763132\n",
      "Iteration 4822, loss = 0.05762139\n",
      "Iteration 4823, loss = 0.05761146\n",
      "Iteration 4824, loss = 0.05760153\n",
      "Iteration 4825, loss = 0.05759159\n",
      "Iteration 4826, loss = 0.05758166\n",
      "Iteration 4827, loss = 0.05757172\n",
      "Iteration 4828, loss = 0.05756178\n",
      "Iteration 4829, loss = 0.05755184\n",
      "Iteration 4830, loss = 0.05754189\n",
      "Iteration 4831, loss = 0.05753195\n",
      "Iteration 4832, loss = 0.05752200\n",
      "Iteration 4833, loss = 0.05751205\n",
      "Iteration 4834, loss = 0.05750211\n",
      "Iteration 4835, loss = 0.05749215\n",
      "Iteration 4836, loss = 0.05748220\n",
      "Iteration 4837, loss = 0.05747225\n",
      "Iteration 4838, loss = 0.05746229\n",
      "Iteration 4839, loss = 0.05745233\n",
      "Iteration 4840, loss = 0.05744237\n",
      "Iteration 4841, loss = 0.05743241\n",
      "Iteration 4842, loss = 0.05742245\n",
      "Iteration 4843, loss = 0.05741248\n",
      "Iteration 4844, loss = 0.05740252\n",
      "Iteration 4845, loss = 0.05739255\n",
      "Iteration 4846, loss = 0.05738258\n",
      "Iteration 4847, loss = 0.05737261\n",
      "Iteration 4848, loss = 0.05736264\n",
      "Iteration 4849, loss = 0.05735266\n",
      "Iteration 4850, loss = 0.05734269\n",
      "Iteration 4851, loss = 0.05733271\n",
      "Iteration 4852, loss = 0.05732273\n",
      "Iteration 4853, loss = 0.05731275\n",
      "Iteration 4854, loss = 0.05730276\n",
      "Iteration 4855, loss = 0.05729278\n",
      "Iteration 4856, loss = 0.05728279\n",
      "Iteration 4857, loss = 0.05727280\n",
      "Iteration 4858, loss = 0.05726282\n",
      "Iteration 4859, loss = 0.05725282\n",
      "Iteration 4860, loss = 0.05724283\n",
      "Iteration 4861, loss = 0.05723284\n",
      "Iteration 4862, loss = 0.05722284\n",
      "Iteration 4863, loss = 0.05721284\n",
      "Iteration 4864, loss = 0.05720284\n",
      "Iteration 4865, loss = 0.05719284\n",
      "Iteration 4866, loss = 0.05718284\n",
      "Iteration 4867, loss = 0.05717283\n",
      "Iteration 4868, loss = 0.05716282\n",
      "Iteration 4869, loss = 0.05715282\n",
      "Iteration 4870, loss = 0.05714281\n",
      "Iteration 4871, loss = 0.05713279\n",
      "Iteration 4872, loss = 0.05712278\n",
      "Iteration 4873, loss = 0.05711276\n",
      "Iteration 4874, loss = 0.05710275\n",
      "Iteration 4875, loss = 0.05709273\n",
      "Iteration 4876, loss = 0.05708271\n",
      "Iteration 4877, loss = 0.05707269\n",
      "Iteration 4878, loss = 0.05706266\n",
      "Iteration 4879, loss = 0.05705264\n",
      "Iteration 4880, loss = 0.05704261\n",
      "Iteration 4881, loss = 0.05703258\n",
      "Iteration 4882, loss = 0.05702255\n",
      "Iteration 4883, loss = 0.05701252\n",
      "Iteration 4884, loss = 0.05700248\n",
      "Iteration 4885, loss = 0.05699245\n",
      "Iteration 4886, loss = 0.05698241\n",
      "Iteration 4887, loss = 0.05697237\n",
      "Iteration 4888, loss = 0.05696233\n",
      "Iteration 4889, loss = 0.05695228\n",
      "Iteration 4890, loss = 0.05694224\n",
      "Iteration 4891, loss = 0.05693219\n",
      "Iteration 4892, loss = 0.05692214\n",
      "Iteration 4893, loss = 0.05691209\n",
      "Iteration 4894, loss = 0.05690204\n",
      "Iteration 4895, loss = 0.05689199\n",
      "Iteration 4896, loss = 0.05688193\n",
      "Iteration 4897, loss = 0.05687188\n",
      "Iteration 4898, loss = 0.05686182\n",
      "Iteration 4899, loss = 0.05685176\n",
      "Iteration 4900, loss = 0.05684169\n",
      "Iteration 4901, loss = 0.05683163\n",
      "Iteration 4902, loss = 0.05682156\n",
      "Iteration 4903, loss = 0.05681150\n",
      "Iteration 4904, loss = 0.05680143\n",
      "Iteration 4905, loss = 0.05679136\n",
      "Iteration 4906, loss = 0.05678128\n",
      "Iteration 4907, loss = 0.05677121\n",
      "Iteration 4908, loss = 0.05676113\n",
      "Iteration 4909, loss = 0.05675105\n",
      "Iteration 4910, loss = 0.05674097\n",
      "Iteration 4911, loss = 0.05673089\n",
      "Iteration 4912, loss = 0.05672081\n",
      "Iteration 4913, loss = 0.05671072\n",
      "Iteration 4914, loss = 0.05670063\n",
      "Iteration 4915, loss = 0.05669055\n",
      "Iteration 4916, loss = 0.05668046\n",
      "Iteration 4917, loss = 0.05667036\n",
      "Iteration 4918, loss = 0.05666027\n",
      "Iteration 4919, loss = 0.05665017\n",
      "Iteration 4920, loss = 0.05664007\n",
      "Iteration 4921, loss = 0.05662997\n",
      "Iteration 4922, loss = 0.05661987\n",
      "Iteration 4923, loss = 0.05660977\n",
      "Iteration 4924, loss = 0.05659967\n",
      "Iteration 4925, loss = 0.05658956\n",
      "Iteration 4926, loss = 0.05657945\n",
      "Iteration 4927, loss = 0.05656934\n",
      "Iteration 4928, loss = 0.05655923\n",
      "Iteration 4929, loss = 0.05654911\n",
      "Iteration 4930, loss = 0.05653900\n",
      "Iteration 4931, loss = 0.05652888\n",
      "Iteration 4932, loss = 0.05651876\n",
      "Iteration 4933, loss = 0.05650864\n",
      "Iteration 4934, loss = 0.05649852\n",
      "Iteration 4935, loss = 0.05648839\n",
      "Iteration 4936, loss = 0.05647826\n",
      "Iteration 4937, loss = 0.05646814\n",
      "Iteration 4938, loss = 0.05645801\n",
      "Iteration 4939, loss = 0.05644787\n",
      "Iteration 4940, loss = 0.05643774\n",
      "Iteration 4941, loss = 0.05642760\n",
      "Iteration 4942, loss = 0.05641747\n",
      "Iteration 4943, loss = 0.05640733\n",
      "Iteration 4944, loss = 0.05639719\n",
      "Iteration 4945, loss = 0.05638704\n",
      "Iteration 4946, loss = 0.05637690\n",
      "Iteration 4947, loss = 0.05636675\n",
      "Iteration 4948, loss = 0.05635660\n",
      "Iteration 4949, loss = 0.05634645\n",
      "Iteration 4950, loss = 0.05633630\n",
      "Iteration 4951, loss = 0.05632615\n",
      "Iteration 4952, loss = 0.05631599\n",
      "Iteration 4953, loss = 0.05630583\n",
      "Iteration 4954, loss = 0.05629567\n",
      "Iteration 4955, loss = 0.05628551\n",
      "Iteration 4956, loss = 0.05627535\n",
      "Iteration 4957, loss = 0.05626518\n",
      "Iteration 4958, loss = 0.05625502\n",
      "Iteration 4959, loss = 0.05624485\n",
      "Iteration 4960, loss = 0.05623468\n",
      "Iteration 4961, loss = 0.05622451\n",
      "Iteration 4962, loss = 0.05621433\n",
      "Iteration 4963, loss = 0.05620416\n",
      "Iteration 4964, loss = 0.05619398\n",
      "Iteration 4965, loss = 0.05618380\n",
      "Iteration 4966, loss = 0.05617362\n",
      "Iteration 4967, loss = 0.05616343\n",
      "Iteration 4968, loss = 0.05615325\n",
      "Iteration 4969, loss = 0.05614306\n",
      "Iteration 4970, loss = 0.05613287\n",
      "Iteration 4971, loss = 0.05612268\n",
      "Iteration 4972, loss = 0.05611249\n",
      "Iteration 4973, loss = 0.05610230\n",
      "Iteration 4974, loss = 0.05609210\n",
      "Iteration 4975, loss = 0.05608190\n",
      "Iteration 4976, loss = 0.05607170\n",
      "Iteration 4977, loss = 0.05606150\n",
      "Iteration 4978, loss = 0.05605130\n",
      "Iteration 4979, loss = 0.05604109\n",
      "Iteration 4980, loss = 0.05603088\n",
      "Iteration 4981, loss = 0.05602067\n",
      "Iteration 4982, loss = 0.05601046\n",
      "Iteration 4983, loss = 0.05600025\n",
      "Iteration 4984, loss = 0.05599004\n",
      "Iteration 4985, loss = 0.05597982\n",
      "Iteration 4986, loss = 0.05596960\n",
      "Iteration 4987, loss = 0.05595938\n",
      "Iteration 4988, loss = 0.05594916\n",
      "Iteration 4989, loss = 0.05593893\n",
      "Iteration 4990, loss = 0.05592871\n",
      "Iteration 4991, loss = 0.05591848\n",
      "Iteration 4992, loss = 0.05590825\n",
      "Iteration 4993, loss = 0.05589802\n",
      "Iteration 4994, loss = 0.05588779\n",
      "Iteration 4995, loss = 0.05587755\n",
      "Iteration 4996, loss = 0.05586731\n",
      "Iteration 4997, loss = 0.05585707\n",
      "Iteration 4998, loss = 0.05584683\n",
      "Iteration 4999, loss = 0.05583659\n",
      "Iteration 5000, loss = 0.05582635\n",
      "Iteration 5001, loss = 0.05581610\n",
      "Iteration 5002, loss = 0.05580585\n",
      "Iteration 5003, loss = 0.05579560\n",
      "Iteration 5004, loss = 0.05578535\n",
      "Iteration 5005, loss = 0.05577510\n",
      "Iteration 5006, loss = 0.05576484\n",
      "Iteration 5007, loss = 0.05575458\n",
      "Iteration 5008, loss = 0.05574432\n",
      "Iteration 5009, loss = 0.05573406\n",
      "Iteration 5010, loss = 0.05572380\n",
      "Iteration 5011, loss = 0.05571353\n",
      "Iteration 5012, loss = 0.05570326\n",
      "Iteration 5013, loss = 0.05569300\n",
      "Iteration 5014, loss = 0.05568272\n",
      "Iteration 5015, loss = 0.05567245\n",
      "Iteration 5016, loss = 0.05566218\n",
      "Iteration 5017, loss = 0.05565190\n",
      "Iteration 5018, loss = 0.05564162\n",
      "Iteration 5019, loss = 0.05563134\n",
      "Iteration 5020, loss = 0.05562106\n",
      "Iteration 5021, loss = 0.05561078\n",
      "Iteration 5022, loss = 0.05560049\n",
      "Iteration 5023, loss = 0.05559020\n",
      "Iteration 5024, loss = 0.05557991\n",
      "Iteration 5025, loss = 0.05556962\n",
      "Iteration 5026, loss = 0.05555933\n",
      "Iteration 5027, loss = 0.05554903\n",
      "Iteration 5028, loss = 0.05553874\n",
      "Iteration 5029, loss = 0.05552844\n",
      "Iteration 5030, loss = 0.05551814\n",
      "Iteration 5031, loss = 0.05550783\n",
      "Iteration 5032, loss = 0.05549753\n",
      "Iteration 5033, loss = 0.05548722\n",
      "Iteration 5034, loss = 0.05547691\n",
      "Iteration 5035, loss = 0.05546660\n",
      "Iteration 5036, loss = 0.05545629\n",
      "Iteration 5037, loss = 0.05544598\n",
      "Iteration 5038, loss = 0.05543566\n",
      "Iteration 5039, loss = 0.05542534\n",
      "Iteration 5040, loss = 0.05541502\n",
      "Iteration 5041, loss = 0.05540470\n",
      "Iteration 5042, loss = 0.05539438\n",
      "Iteration 5043, loss = 0.05538405\n",
      "Iteration 5044, loss = 0.05537372\n",
      "Iteration 5045, loss = 0.05536339\n",
      "Iteration 5046, loss = 0.05535306\n",
      "Iteration 5047, loss = 0.05534273\n",
      "Iteration 5048, loss = 0.05533240\n",
      "Iteration 5049, loss = 0.05532206\n",
      "Iteration 5050, loss = 0.05531172\n",
      "Iteration 5051, loss = 0.05530138\n",
      "Iteration 5052, loss = 0.05529104\n",
      "Iteration 5053, loss = 0.05528069\n",
      "Iteration 5054, loss = 0.05527035\n",
      "Iteration 5055, loss = 0.05526000\n",
      "Iteration 5056, loss = 0.05524965\n",
      "Iteration 5057, loss = 0.05523930\n",
      "Iteration 5058, loss = 0.05522894\n",
      "Iteration 5059, loss = 0.05521859\n",
      "Iteration 5060, loss = 0.05520823\n",
      "Iteration 5061, loss = 0.05519787\n",
      "Iteration 5062, loss = 0.05518751\n",
      "Iteration 5063, loss = 0.05517714\n",
      "Iteration 5064, loss = 0.05516678\n",
      "Iteration 5065, loss = 0.05515641\n",
      "Iteration 5066, loss = 0.05514604\n",
      "Iteration 5067, loss = 0.05513567\n",
      "Iteration 5068, loss = 0.05512530\n",
      "Iteration 5069, loss = 0.05511492\n",
      "Iteration 5070, loss = 0.05510455\n",
      "Iteration 5071, loss = 0.05509417\n",
      "Iteration 5072, loss = 0.05508379\n",
      "Iteration 5073, loss = 0.05507341\n",
      "Iteration 5074, loss = 0.05506302\n",
      "Iteration 5075, loss = 0.05505263\n",
      "Iteration 5076, loss = 0.05504225\n",
      "Iteration 5077, loss = 0.05503186\n",
      "Iteration 5078, loss = 0.05502147\n",
      "Iteration 5079, loss = 0.05501107\n",
      "Iteration 5080, loss = 0.05500068\n",
      "Iteration 5081, loss = 0.05499028\n",
      "Iteration 5082, loss = 0.05497988\n",
      "Iteration 5083, loss = 0.05496948\n",
      "Iteration 5084, loss = 0.05495907\n",
      "Iteration 5085, loss = 0.05494867\n",
      "Iteration 5086, loss = 0.05493826\n",
      "Iteration 5087, loss = 0.05492785\n",
      "Iteration 5088, loss = 0.05491744\n",
      "Iteration 5089, loss = 0.05490703\n",
      "Iteration 5090, loss = 0.05489662\n",
      "Iteration 5091, loss = 0.05488620\n",
      "Iteration 5092, loss = 0.05487578\n",
      "Iteration 5093, loss = 0.05486536\n",
      "Iteration 5094, loss = 0.05485494\n",
      "Iteration 5095, loss = 0.05484452\n",
      "Iteration 5096, loss = 0.05483409\n",
      "Iteration 5097, loss = 0.05482366\n",
      "Iteration 5098, loss = 0.05481323\n",
      "Iteration 5099, loss = 0.05480280\n",
      "Iteration 5100, loss = 0.05479237\n",
      "Iteration 5101, loss = 0.05478193\n",
      "Iteration 5102, loss = 0.05477150\n",
      "Iteration 5103, loss = 0.05476106\n",
      "Iteration 5104, loss = 0.05475062\n",
      "Iteration 5105, loss = 0.05474017\n",
      "Iteration 5106, loss = 0.05472973\n",
      "Iteration 5107, loss = 0.05471928\n",
      "Iteration 5108, loss = 0.05470883\n",
      "Iteration 5109, loss = 0.05469838\n",
      "Iteration 5110, loss = 0.05468793\n",
      "Iteration 5111, loss = 0.05467748\n",
      "Iteration 5112, loss = 0.05466702\n",
      "Iteration 5113, loss = 0.05465656\n",
      "Iteration 5114, loss = 0.05464610\n",
      "Iteration 5115, loss = 0.05463564\n",
      "Iteration 5116, loss = 0.05462518\n",
      "Iteration 5117, loss = 0.05461471\n",
      "Iteration 5118, loss = 0.05460424\n",
      "Iteration 5119, loss = 0.05459378\n",
      "Iteration 5120, loss = 0.05458330\n",
      "Iteration 5121, loss = 0.05457283\n",
      "Iteration 5122, loss = 0.05456236\n",
      "Iteration 5123, loss = 0.05455188\n",
      "Iteration 5124, loss = 0.05454140\n",
      "Iteration 5125, loss = 0.05453092\n",
      "Iteration 5126, loss = 0.05452044\n",
      "Iteration 5127, loss = 0.05450996\n",
      "Iteration 5128, loss = 0.05449947\n",
      "Iteration 5129, loss = 0.05448898\n",
      "Iteration 5130, loss = 0.05447849\n",
      "Iteration 5131, loss = 0.05446800\n",
      "Iteration 5132, loss = 0.05445751\n",
      "Iteration 5133, loss = 0.05444701\n",
      "Iteration 5134, loss = 0.05443651\n",
      "Iteration 5135, loss = 0.05442601\n",
      "Iteration 5136, loss = 0.05441551\n",
      "Iteration 5137, loss = 0.05440501\n",
      "Iteration 5138, loss = 0.05439451\n",
      "Iteration 5139, loss = 0.05438400\n",
      "Iteration 5140, loss = 0.05437349\n",
      "Iteration 5141, loss = 0.05436298\n",
      "Iteration 5142, loss = 0.05435247\n",
      "Iteration 5143, loss = 0.05434195\n",
      "Iteration 5144, loss = 0.05433144\n",
      "Iteration 5145, loss = 0.05432092\n",
      "Iteration 5146, loss = 0.05431040\n",
      "Iteration 5147, loss = 0.05429988\n",
      "Iteration 5148, loss = 0.05428936\n",
      "Iteration 5149, loss = 0.05427883\n",
      "Iteration 5150, loss = 0.05426830\n",
      "Iteration 5151, loss = 0.05425778\n",
      "Iteration 5152, loss = 0.05424724\n",
      "Iteration 5153, loss = 0.05423671\n",
      "Iteration 5154, loss = 0.05422618\n",
      "Iteration 5155, loss = 0.05421564\n",
      "Iteration 5156, loss = 0.05420510\n",
      "Iteration 5157, loss = 0.05419456\n",
      "Iteration 5158, loss = 0.05418402\n",
      "Iteration 5159, loss = 0.05417348\n",
      "Iteration 5160, loss = 0.05416293\n",
      "Iteration 5161, loss = 0.05415239\n",
      "Iteration 5162, loss = 0.05414184\n",
      "Iteration 5163, loss = 0.05413129\n",
      "Iteration 5164, loss = 0.05412073\n",
      "Iteration 5165, loss = 0.05411018\n",
      "Iteration 5166, loss = 0.05409962\n",
      "Iteration 5167, loss = 0.05408906\n",
      "Iteration 5168, loss = 0.05407850\n",
      "Iteration 5169, loss = 0.05406794\n",
      "Iteration 5170, loss = 0.05405738\n",
      "Iteration 5171, loss = 0.05404681\n",
      "Iteration 5172, loss = 0.05403624\n",
      "Iteration 5173, loss = 0.05402567\n",
      "Iteration 5174, loss = 0.05401510\n",
      "Iteration 5175, loss = 0.05400453\n",
      "Iteration 5176, loss = 0.05399395\n",
      "Iteration 5177, loss = 0.05398338\n",
      "Iteration 5178, loss = 0.05397280\n",
      "Iteration 5179, loss = 0.05396222\n",
      "Iteration 5180, loss = 0.05395164\n",
      "Iteration 5181, loss = 0.05394105\n",
      "Iteration 5182, loss = 0.05393047\n",
      "Iteration 5183, loss = 0.05391988\n",
      "Iteration 5184, loss = 0.05390929\n",
      "Iteration 5185, loss = 0.05389870\n",
      "Iteration 5186, loss = 0.05388811\n",
      "Iteration 5187, loss = 0.05387751\n",
      "Iteration 5188, loss = 0.05386691\n",
      "Iteration 5189, loss = 0.05385632\n",
      "Iteration 5190, loss = 0.05384572\n",
      "Iteration 5191, loss = 0.05383511\n",
      "Iteration 5192, loss = 0.05382451\n",
      "Iteration 5193, loss = 0.05381390\n",
      "Iteration 5194, loss = 0.05380330\n",
      "Iteration 5195, loss = 0.05379269\n",
      "Iteration 5196, loss = 0.05378208\n",
      "Iteration 5197, loss = 0.05377146\n",
      "Iteration 5198, loss = 0.05376085\n",
      "Iteration 5199, loss = 0.05375023\n",
      "Iteration 5200, loss = 0.05373961\n",
      "Iteration 5201, loss = 0.05372899\n",
      "Iteration 5202, loss = 0.05371837\n",
      "Iteration 5203, loss = 0.05370775\n",
      "Iteration 5204, loss = 0.05369712\n",
      "Iteration 5205, loss = 0.05368650\n",
      "Iteration 5206, loss = 0.05367587\n",
      "Iteration 5207, loss = 0.05366524\n",
      "Iteration 5208, loss = 0.05365460\n",
      "Iteration 5209, loss = 0.05364397\n",
      "Iteration 5210, loss = 0.05363333\n",
      "Iteration 5211, loss = 0.05362269\n",
      "Iteration 5212, loss = 0.05361205\n",
      "Iteration 5213, loss = 0.05360141\n",
      "Iteration 5214, loss = 0.05359077\n",
      "Iteration 5215, loss = 0.05358013\n",
      "Iteration 5216, loss = 0.05356948\n",
      "Iteration 5217, loss = 0.05355883\n",
      "Iteration 5218, loss = 0.05354818\n",
      "Iteration 5219, loss = 0.05353753\n",
      "Iteration 5220, loss = 0.05352687\n",
      "Iteration 5221, loss = 0.05351622\n",
      "Iteration 5222, loss = 0.05350556\n",
      "Iteration 5223, loss = 0.05349490\n",
      "Iteration 5224, loss = 0.05348424\n",
      "Iteration 5225, loss = 0.05347358\n",
      "Iteration 5226, loss = 0.05346291\n",
      "Iteration 5227, loss = 0.05345225\n",
      "Iteration 5228, loss = 0.05344158\n",
      "Iteration 5229, loss = 0.05343091\n",
      "Iteration 5230, loss = 0.05342024\n",
      "Iteration 5231, loss = 0.05340957\n",
      "Iteration 5232, loss = 0.05339889\n",
      "Iteration 5233, loss = 0.05338822\n",
      "Iteration 5234, loss = 0.05337754\n",
      "Iteration 5235, loss = 0.05336686\n",
      "Iteration 5236, loss = 0.05335618\n",
      "Iteration 5237, loss = 0.05334549\n",
      "Iteration 5238, loss = 0.05333481\n",
      "Iteration 5239, loss = 0.05332412\n",
      "Iteration 5240, loss = 0.05331343\n",
      "Iteration 5241, loss = 0.05330274\n",
      "Iteration 5242, loss = 0.05329205\n",
      "Iteration 5243, loss = 0.05328135\n",
      "Iteration 5244, loss = 0.05327066\n",
      "Iteration 5245, loss = 0.05325996\n",
      "Iteration 5246, loss = 0.05324926\n",
      "Iteration 5247, loss = 0.05323856\n",
      "Iteration 5248, loss = 0.05322786\n",
      "Iteration 5249, loss = 0.05321716\n",
      "Iteration 5250, loss = 0.05320645\n",
      "Iteration 5251, loss = 0.05319574\n",
      "Iteration 5252, loss = 0.05318503\n",
      "Iteration 5253, loss = 0.05317432\n",
      "Iteration 5254, loss = 0.05316361\n",
      "Iteration 5255, loss = 0.05315290\n",
      "Iteration 5256, loss = 0.05314218\n",
      "Iteration 5257, loss = 0.05313146\n",
      "Iteration 5258, loss = 0.05312074\n",
      "Iteration 5259, loss = 0.05311002\n",
      "Iteration 5260, loss = 0.05309930\n",
      "Iteration 5261, loss = 0.05308858\n",
      "Iteration 5262, loss = 0.05307785\n",
      "Iteration 5263, loss = 0.05306712\n",
      "Iteration 5264, loss = 0.05305639\n",
      "Iteration 5265, loss = 0.05304566\n",
      "Iteration 5266, loss = 0.05303493\n",
      "Iteration 5267, loss = 0.05302419\n",
      "Iteration 5268, loss = 0.05301346\n",
      "Iteration 5269, loss = 0.05300272\n",
      "Iteration 5270, loss = 0.05299198\n",
      "Iteration 5271, loss = 0.05298124\n",
      "Iteration 5272, loss = 0.05297050\n",
      "Iteration 5273, loss = 0.05295975\n",
      "Iteration 5274, loss = 0.05294901\n",
      "Iteration 5275, loss = 0.05293826\n",
      "Iteration 5276, loss = 0.05292751\n",
      "Iteration 5277, loss = 0.05291676\n",
      "Iteration 5278, loss = 0.05290601\n",
      "Iteration 5279, loss = 0.05289525\n",
      "Iteration 5280, loss = 0.05288450\n",
      "Iteration 5281, loss = 0.05287374\n",
      "Iteration 5282, loss = 0.05286298\n",
      "Iteration 5283, loss = 0.05285222\n",
      "Iteration 5284, loss = 0.05284146\n",
      "Iteration 5285, loss = 0.05283069\n",
      "Iteration 5286, loss = 0.05281993\n",
      "Iteration 5287, loss = 0.05280916\n",
      "Iteration 5288, loss = 0.05279839\n",
      "Iteration 5289, loss = 0.05278762\n",
      "Iteration 5290, loss = 0.05277685\n",
      "Iteration 5291, loss = 0.05276608\n",
      "Iteration 5292, loss = 0.05275530\n",
      "Iteration 5293, loss = 0.05274452\n",
      "Iteration 5294, loss = 0.05273374\n",
      "Iteration 5295, loss = 0.05272296\n",
      "Iteration 5296, loss = 0.05271218\n",
      "Iteration 5297, loss = 0.05270140\n",
      "Iteration 5298, loss = 0.05269062\n",
      "Iteration 5299, loss = 0.05267983\n",
      "Iteration 5300, loss = 0.05266904\n",
      "Iteration 5301, loss = 0.05265825\n",
      "Iteration 5302, loss = 0.05264746\n",
      "Iteration 5303, loss = 0.05263667\n",
      "Iteration 5304, loss = 0.05262587\n",
      "Iteration 5305, loss = 0.05261508\n",
      "Iteration 5306, loss = 0.05260428\n",
      "Iteration 5307, loss = 0.05259348\n",
      "Iteration 5308, loss = 0.05258268\n",
      "Iteration 5309, loss = 0.05257188\n",
      "Iteration 5310, loss = 0.05256107\n",
      "Iteration 5311, loss = 0.05255027\n",
      "Iteration 5312, loss = 0.05253946\n",
      "Iteration 5313, loss = 0.05252865\n",
      "Iteration 5314, loss = 0.05251784\n",
      "Iteration 5315, loss = 0.05250703\n",
      "Iteration 5316, loss = 0.05249622\n",
      "Iteration 5317, loss = 0.05248540\n",
      "Iteration 5318, loss = 0.05247459\n",
      "Iteration 5319, loss = 0.05246377\n",
      "Iteration 5320, loss = 0.05245295\n",
      "Iteration 5321, loss = 0.05244213\n",
      "Iteration 5322, loss = 0.05243131\n",
      "Iteration 5323, loss = 0.05242048\n",
      "Iteration 5324, loss = 0.05240966\n",
      "Iteration 5325, loss = 0.05239883\n",
      "Iteration 5326, loss = 0.05238800\n",
      "Iteration 5327, loss = 0.05237717\n",
      "Iteration 5328, loss = 0.05236634\n",
      "Iteration 5329, loss = 0.05235551\n",
      "Iteration 5330, loss = 0.05234467\n",
      "Iteration 5331, loss = 0.05233384\n",
      "Iteration 5332, loss = 0.05232300\n",
      "Iteration 5333, loss = 0.05231216\n",
      "Iteration 5334, loss = 0.05230132\n",
      "Iteration 5335, loss = 0.05229048\n",
      "Iteration 5336, loss = 0.05227964\n",
      "Iteration 5337, loss = 0.05226879\n",
      "Iteration 5338, loss = 0.05225795\n",
      "Iteration 5339, loss = 0.05224710\n",
      "Iteration 5340, loss = 0.05223625\n",
      "Iteration 5341, loss = 0.05222540\n",
      "Iteration 5342, loss = 0.05221454\n",
      "Iteration 5343, loss = 0.05220369\n",
      "Iteration 5344, loss = 0.05219284\n",
      "Iteration 5345, loss = 0.05218198\n",
      "Iteration 5346, loss = 0.05217112\n",
      "Iteration 5347, loss = 0.05216026\n",
      "Iteration 5348, loss = 0.05214940\n",
      "Iteration 5349, loss = 0.05213854\n",
      "Iteration 5350, loss = 0.05212767\n",
      "Iteration 5351, loss = 0.05211681\n",
      "Iteration 5352, loss = 0.05210594\n",
      "Iteration 5353, loss = 0.05209507\n",
      "Iteration 5354, loss = 0.05208420\n",
      "Iteration 5355, loss = 0.05207333\n",
      "Iteration 5356, loss = 0.05206246\n",
      "Iteration 5357, loss = 0.05205159\n",
      "Iteration 5358, loss = 0.05204071\n",
      "Iteration 5359, loss = 0.05202983\n",
      "Iteration 5360, loss = 0.05201896\n",
      "Iteration 5361, loss = 0.05200808\n",
      "Iteration 5362, loss = 0.05199719\n",
      "Iteration 5363, loss = 0.05198631\n",
      "Iteration 5364, loss = 0.05197543\n",
      "Iteration 5365, loss = 0.05196454\n",
      "Iteration 5366, loss = 0.05195366\n",
      "Iteration 5367, loss = 0.05194277\n",
      "Iteration 5368, loss = 0.05193188\n",
      "Iteration 5369, loss = 0.05192099\n",
      "Iteration 5370, loss = 0.05191010\n",
      "Iteration 5371, loss = 0.05189920\n",
      "Iteration 5372, loss = 0.05188831\n",
      "Iteration 5373, loss = 0.05187741\n",
      "Iteration 5374, loss = 0.05186651\n",
      "Iteration 5375, loss = 0.05185561\n",
      "Iteration 5376, loss = 0.05184471\n",
      "Iteration 5377, loss = 0.05183381\n",
      "Iteration 5378, loss = 0.05182291\n",
      "Iteration 5379, loss = 0.05181200\n",
      "Iteration 5380, loss = 0.05180110\n",
      "Iteration 5381, loss = 0.05179019\n",
      "Iteration 5382, loss = 0.05177928\n",
      "Iteration 5383, loss = 0.05176837\n",
      "Iteration 5384, loss = 0.05175746\n",
      "Iteration 5385, loss = 0.05174655\n",
      "Iteration 5386, loss = 0.05173563\n",
      "Iteration 5387, loss = 0.05172472\n",
      "Iteration 5388, loss = 0.05171380\n",
      "Iteration 5389, loss = 0.05170288\n",
      "Iteration 5390, loss = 0.05169196\n",
      "Iteration 5391, loss = 0.05168104\n",
      "Iteration 5392, loss = 0.05167012\n",
      "Iteration 5393, loss = 0.05165920\n",
      "Iteration 5394, loss = 0.05164827\n",
      "Iteration 5395, loss = 0.05163735\n",
      "Iteration 5396, loss = 0.05162642\n",
      "Iteration 5397, loss = 0.05161549\n",
      "Iteration 5398, loss = 0.05160456\n",
      "Iteration 5399, loss = 0.05159363\n",
      "Iteration 5400, loss = 0.05158270\n",
      "Iteration 5401, loss = 0.05157176\n",
      "Iteration 5402, loss = 0.05156083\n",
      "Iteration 5403, loss = 0.05154989\n",
      "Iteration 5404, loss = 0.05153896\n",
      "Iteration 5405, loss = 0.05152802\n",
      "Iteration 5406, loss = 0.05151708\n",
      "Iteration 5407, loss = 0.05150614\n",
      "Iteration 5408, loss = 0.05149519\n",
      "Iteration 5409, loss = 0.05148425\n",
      "Iteration 5410, loss = 0.05147330\n",
      "Iteration 5411, loss = 0.05146236\n",
      "Iteration 5412, loss = 0.05145141\n",
      "Iteration 5413, loss = 0.05144046\n",
      "Iteration 5414, loss = 0.05142951\n",
      "Iteration 5415, loss = 0.05141856\n",
      "Iteration 5416, loss = 0.05140761\n",
      "Iteration 5417, loss = 0.05139665\n",
      "Iteration 5418, loss = 0.05138570\n",
      "Iteration 5419, loss = 0.05137474\n",
      "Iteration 5420, loss = 0.05136378\n",
      "Iteration 5421, loss = 0.05135283\n",
      "Iteration 5422, loss = 0.05134187\n",
      "Iteration 5423, loss = 0.05133090\n",
      "Iteration 5424, loss = 0.05131994\n",
      "Iteration 5425, loss = 0.05130898\n",
      "Iteration 5426, loss = 0.05129801\n",
      "Iteration 5427, loss = 0.05128705\n",
      "Iteration 5428, loss = 0.05127608\n",
      "Iteration 5429, loss = 0.05126511\n",
      "Iteration 5430, loss = 0.05125414\n",
      "Iteration 5431, loss = 0.05124317\n",
      "Iteration 5432, loss = 0.05123220\n",
      "Iteration 5433, loss = 0.05122123\n",
      "Iteration 5434, loss = 0.05121025\n",
      "Iteration 5435, loss = 0.05119928\n",
      "Iteration 5436, loss = 0.05118830\n",
      "Iteration 5437, loss = 0.05117732\n",
      "Iteration 5438, loss = 0.05116634\n",
      "Iteration 5439, loss = 0.05115536\n",
      "Iteration 5440, loss = 0.05114438\n",
      "Iteration 5441, loss = 0.05113340\n",
      "Iteration 5442, loss = 0.05112242\n",
      "Iteration 5443, loss = 0.05111143\n",
      "Iteration 5444, loss = 0.05110045\n",
      "Iteration 5445, loss = 0.05108946\n",
      "Iteration 5446, loss = 0.05107847\n",
      "Iteration 5447, loss = 0.05106748\n",
      "Iteration 5448, loss = 0.05105649\n",
      "Iteration 5449, loss = 0.05104550\n",
      "Iteration 5450, loss = 0.05103451\n",
      "Iteration 5451, loss = 0.05102351\n",
      "Iteration 5452, loss = 0.05101252\n",
      "Iteration 5453, loss = 0.05100152\n",
      "Iteration 5454, loss = 0.05099053\n",
      "Iteration 5455, loss = 0.05097953\n",
      "Iteration 5456, loss = 0.05096853\n",
      "Iteration 5457, loss = 0.05095753\n",
      "Iteration 5458, loss = 0.05094653\n",
      "Iteration 5459, loss = 0.05093553\n",
      "Iteration 5460, loss = 0.05092452\n",
      "Iteration 5461, loss = 0.05091352\n",
      "Iteration 5462, loss = 0.05090251\n",
      "Iteration 5463, loss = 0.05089150\n",
      "Iteration 5464, loss = 0.05088050\n",
      "Iteration 5465, loss = 0.05086949\n",
      "Iteration 5466, loss = 0.05085848\n",
      "Iteration 5467, loss = 0.05084747\n",
      "Iteration 5468, loss = 0.05083645\n",
      "Iteration 5469, loss = 0.05082544\n",
      "Iteration 5470, loss = 0.05081443\n",
      "Iteration 5471, loss = 0.05080341\n",
      "Iteration 5472, loss = 0.05079240\n",
      "Iteration 5473, loss = 0.05078138\n",
      "Iteration 5474, loss = 0.05077036\n",
      "Iteration 5475, loss = 0.05075934\n",
      "Iteration 5476, loss = 0.05074832\n",
      "Iteration 5477, loss = 0.05073730\n",
      "Iteration 5478, loss = 0.05072628\n",
      "Iteration 5479, loss = 0.05071525\n",
      "Iteration 5480, loss = 0.05070423\n",
      "Iteration 5481, loss = 0.05069320\n",
      "Iteration 5482, loss = 0.05068218\n",
      "Iteration 5483, loss = 0.05067115\n",
      "Iteration 5484, loss = 0.05066012\n",
      "Iteration 5485, loss = 0.05064909\n",
      "Iteration 5486, loss = 0.05063806\n",
      "Iteration 5487, loss = 0.05062703\n",
      "Iteration 5488, loss = 0.05061600\n",
      "Iteration 5489, loss = 0.05060496\n",
      "Iteration 5490, loss = 0.05059393\n",
      "Iteration 5491, loss = 0.05058289\n",
      "Iteration 5492, loss = 0.05057186\n",
      "Iteration 5493, loss = 0.05056082\n",
      "Iteration 5494, loss = 0.05054978\n",
      "Iteration 5495, loss = 0.05053874\n",
      "Iteration 5496, loss = 0.05052770\n",
      "Iteration 5497, loss = 0.05051666\n",
      "Iteration 5498, loss = 0.05050562\n",
      "Iteration 5499, loss = 0.05049458\n",
      "Iteration 5500, loss = 0.05048353\n",
      "Iteration 5501, loss = 0.05047249\n",
      "Iteration 5502, loss = 0.05046144\n",
      "Iteration 5503, loss = 0.05045040\n",
      "Iteration 5504, loss = 0.05043935\n",
      "Iteration 5505, loss = 0.05042830\n",
      "Iteration 5506, loss = 0.05041725\n",
      "Iteration 5507, loss = 0.05040620\n",
      "Iteration 5508, loss = 0.05039515\n",
      "Iteration 5509, loss = 0.05038410\n",
      "Iteration 5510, loss = 0.05037304\n",
      "Iteration 5511, loss = 0.05036199\n",
      "Iteration 5512, loss = 0.05035094\n",
      "Iteration 5513, loss = 0.05033988\n",
      "Iteration 5514, loss = 0.05032882\n",
      "Iteration 5515, loss = 0.05031777\n",
      "Iteration 5516, loss = 0.05030671\n",
      "Iteration 5517, loss = 0.05029565\n",
      "Iteration 5518, loss = 0.05028459\n",
      "Iteration 5519, loss = 0.05027353\n",
      "Iteration 5520, loss = 0.05026247\n",
      "Iteration 5521, loss = 0.05025140\n",
      "Iteration 5522, loss = 0.05024034\n",
      "Iteration 5523, loss = 0.05022928\n",
      "Iteration 5524, loss = 0.05021821\n",
      "Iteration 5525, loss = 0.05020714\n",
      "Iteration 5526, loss = 0.05019608\n",
      "Iteration 5527, loss = 0.05018501\n",
      "Iteration 5528, loss = 0.05017394\n",
      "Iteration 5529, loss = 0.05016287\n",
      "Iteration 5530, loss = 0.05015180\n",
      "Iteration 5531, loss = 0.05014073\n",
      "Iteration 5532, loss = 0.05012966\n",
      "Iteration 5533, loss = 0.05011859\n",
      "Iteration 5534, loss = 0.05010751\n",
      "Iteration 5535, loss = 0.05009644\n",
      "Iteration 5536, loss = 0.05008536\n",
      "Iteration 5537, loss = 0.05007429\n",
      "Iteration 5538, loss = 0.05006321\n",
      "Iteration 5539, loss = 0.05005213\n",
      "Iteration 5540, loss = 0.05004105\n",
      "Iteration 5541, loss = 0.05002998\n",
      "Iteration 5542, loss = 0.05001890\n",
      "Iteration 5543, loss = 0.05000782\n",
      "Iteration 5544, loss = 0.04999673\n",
      "Iteration 5545, loss = 0.04998565\n",
      "Iteration 5546, loss = 0.04997457\n",
      "Iteration 5547, loss = 0.04996349\n",
      "Iteration 5548, loss = 0.04995240\n",
      "Iteration 5549, loss = 0.04994132\n",
      "Iteration 5550, loss = 0.04993023\n",
      "Iteration 5551, loss = 0.04991914\n",
      "Iteration 5552, loss = 0.04990806\n",
      "Iteration 5553, loss = 0.04989697\n",
      "Iteration 5554, loss = 0.04988588\n",
      "Iteration 5555, loss = 0.04987479\n",
      "Iteration 5556, loss = 0.04986370\n",
      "Iteration 5557, loss = 0.04985261\n",
      "Iteration 5558, loss = 0.04984152\n",
      "Iteration 5559, loss = 0.04983042\n",
      "Iteration 5560, loss = 0.04981933\n",
      "Iteration 5561, loss = 0.04980824\n",
      "Iteration 5562, loss = 0.04979714\n",
      "Iteration 5563, loss = 0.04978605\n",
      "Iteration 5564, loss = 0.04977495\n",
      "Iteration 5565, loss = 0.04976385\n",
      "Iteration 5566, loss = 0.04975276\n",
      "Iteration 5567, loss = 0.04974166\n",
      "Iteration 5568, loss = 0.04973056\n",
      "Iteration 5569, loss = 0.04971946\n",
      "Iteration 5570, loss = 0.04970836\n",
      "Iteration 5571, loss = 0.04969726\n",
      "Iteration 5572, loss = 0.04968616\n",
      "Iteration 5573, loss = 0.04967505\n",
      "Iteration 5574, loss = 0.04966395\n",
      "Iteration 5575, loss = 0.04965285\n",
      "Iteration 5576, loss = 0.04964174\n",
      "Iteration 5577, loss = 0.04963064\n",
      "Iteration 5578, loss = 0.04961953\n",
      "Iteration 5579, loss = 0.04960843\n",
      "Iteration 5580, loss = 0.04959732\n",
      "Iteration 5581, loss = 0.04958621\n",
      "Iteration 5582, loss = 0.04957511\n",
      "Iteration 5583, loss = 0.04956400\n",
      "Iteration 5584, loss = 0.04955289\n",
      "Iteration 5585, loss = 0.04954178\n",
      "Iteration 5586, loss = 0.04953067\n",
      "Iteration 5587, loss = 0.04951956\n",
      "Iteration 5588, loss = 0.04950845\n",
      "Iteration 5589, loss = 0.04949733\n",
      "Iteration 5590, loss = 0.04948622\n",
      "Iteration 5591, loss = 0.04947511\n",
      "Iteration 5592, loss = 0.04946399\n",
      "Iteration 5593, loss = 0.04945288\n",
      "Iteration 5594, loss = 0.04944176\n",
      "Iteration 5595, loss = 0.04943065\n",
      "Iteration 5596, loss = 0.04941953\n",
      "Iteration 5597, loss = 0.04940841\n",
      "Iteration 5598, loss = 0.04939730\n",
      "Iteration 5599, loss = 0.04938618\n",
      "Iteration 5600, loss = 0.04937506\n",
      "Iteration 5601, loss = 0.04936394\n",
      "Iteration 5602, loss = 0.04935282\n",
      "Iteration 5603, loss = 0.04934170\n",
      "Iteration 5604, loss = 0.04933058\n",
      "Iteration 5605, loss = 0.04931946\n",
      "Iteration 5606, loss = 0.04930834\n",
      "Iteration 5607, loss = 0.04929721\n",
      "Iteration 5608, loss = 0.04928609\n",
      "Iteration 5609, loss = 0.04927497\n",
      "Iteration 5610, loss = 0.04926384\n",
      "Iteration 5611, loss = 0.04925272\n",
      "Iteration 5612, loss = 0.04924159\n",
      "Iteration 5613, loss = 0.04923047\n",
      "Iteration 5614, loss = 0.04921934\n",
      "Iteration 5615, loss = 0.04920821\n",
      "Iteration 5616, loss = 0.04919709\n",
      "Iteration 5617, loss = 0.04918596\n",
      "Iteration 5618, loss = 0.04917483\n",
      "Iteration 5619, loss = 0.04916370\n",
      "Iteration 5620, loss = 0.04915257\n",
      "Iteration 5621, loss = 0.04914144\n",
      "Iteration 5622, loss = 0.04913031\n",
      "Iteration 5623, loss = 0.04911918\n",
      "Iteration 5624, loss = 0.04910805\n",
      "Iteration 5625, loss = 0.04909692\n",
      "Iteration 5626, loss = 0.04908579\n",
      "Iteration 5627, loss = 0.04907466\n",
      "Iteration 5628, loss = 0.04906352\n",
      "Iteration 5629, loss = 0.04905239\n",
      "Iteration 5630, loss = 0.04904126\n",
      "Iteration 5631, loss = 0.04903012\n",
      "Iteration 5632, loss = 0.04901899\n",
      "Iteration 5633, loss = 0.04900785\n",
      "Iteration 5634, loss = 0.04899672\n",
      "Iteration 5635, loss = 0.04898558\n",
      "Iteration 5636, loss = 0.04897444\n",
      "Iteration 5637, loss = 0.04896331\n",
      "Iteration 5638, loss = 0.04895217\n",
      "Iteration 5639, loss = 0.04894103\n",
      "Iteration 5640, loss = 0.04892989\n",
      "Iteration 5641, loss = 0.04891875\n",
      "Iteration 5642, loss = 0.04890761\n",
      "Iteration 5643, loss = 0.04889648\n",
      "Iteration 5644, loss = 0.04888534\n",
      "Iteration 5645, loss = 0.04887420\n",
      "Iteration 5646, loss = 0.04886305\n",
      "Iteration 5647, loss = 0.04885191\n",
      "Iteration 5648, loss = 0.04884077\n",
      "Iteration 5649, loss = 0.04882963\n",
      "Iteration 5650, loss = 0.04881849\n",
      "Iteration 5651, loss = 0.04880735\n",
      "Iteration 5652, loss = 0.04879620\n",
      "Iteration 5653, loss = 0.04878506\n",
      "Iteration 5654, loss = 0.04877392\n",
      "Iteration 5655, loss = 0.04876277\n",
      "Iteration 5656, loss = 0.04875163\n",
      "Iteration 5657, loss = 0.04874048\n",
      "Iteration 5658, loss = 0.04872934\n",
      "Iteration 5659, loss = 0.04871819\n",
      "Iteration 5660, loss = 0.04870705\n",
      "Iteration 5661, loss = 0.04869590\n",
      "Iteration 5662, loss = 0.04868475\n",
      "Iteration 5663, loss = 0.04867361\n",
      "Iteration 5664, loss = 0.04866246\n",
      "Iteration 5665, loss = 0.04865131\n",
      "Iteration 5666, loss = 0.04864016\n",
      "Iteration 5667, loss = 0.04862901\n",
      "Iteration 5668, loss = 0.04861787\n",
      "Iteration 5669, loss = 0.04860672\n",
      "Iteration 5670, loss = 0.04859557\n",
      "Iteration 5671, loss = 0.04858442\n",
      "Iteration 5672, loss = 0.04857327\n",
      "Iteration 5673, loss = 0.04856212\n",
      "Iteration 5674, loss = 0.04855097\n",
      "Iteration 5675, loss = 0.04853982\n",
      "Iteration 5676, loss = 0.04852867\n",
      "Iteration 5677, loss = 0.04851752\n",
      "Iteration 5678, loss = 0.04850636\n",
      "Iteration 5679, loss = 0.04849521\n",
      "Iteration 5680, loss = 0.04848406\n",
      "Iteration 5681, loss = 0.04847291\n",
      "Iteration 5682, loss = 0.04846176\n",
      "Iteration 5683, loss = 0.04845060\n",
      "Iteration 5684, loss = 0.04843945\n",
      "Iteration 5685, loss = 0.04842830\n",
      "Iteration 5686, loss = 0.04841714\n",
      "Iteration 5687, loss = 0.04840599\n",
      "Iteration 5688, loss = 0.04839483\n",
      "Iteration 5689, loss = 0.04838368\n",
      "Iteration 5690, loss = 0.04837253\n",
      "Iteration 5691, loss = 0.04836137\n",
      "Iteration 5692, loss = 0.04835022\n",
      "Iteration 5693, loss = 0.04833906\n",
      "Iteration 5694, loss = 0.04832790\n",
      "Iteration 5695, loss = 0.04831675\n",
      "Iteration 5696, loss = 0.04830559\n",
      "Iteration 5697, loss = 0.04829444\n",
      "Iteration 5698, loss = 0.04828328\n",
      "Iteration 5699, loss = 0.04827212\n",
      "Iteration 5700, loss = 0.04826097\n",
      "Iteration 5701, loss = 0.04824981\n",
      "Iteration 5702, loss = 0.04823865\n",
      "Iteration 5703, loss = 0.04822749\n",
      "Iteration 5704, loss = 0.04821634\n",
      "Iteration 5705, loss = 0.04820518\n",
      "Iteration 5706, loss = 0.04819402\n",
      "Iteration 5707, loss = 0.04818286\n",
      "Iteration 5708, loss = 0.04817170\n",
      "Iteration 5709, loss = 0.04816054\n",
      "Iteration 5710, loss = 0.04814939\n",
      "Iteration 5711, loss = 0.04813823\n",
      "Iteration 5712, loss = 0.04812707\n",
      "Iteration 5713, loss = 0.04811591\n",
      "Iteration 5714, loss = 0.04810475\n",
      "Iteration 5715, loss = 0.04809359\n",
      "Iteration 5716, loss = 0.04808243\n",
      "Iteration 5717, loss = 0.04807127\n",
      "Iteration 5718, loss = 0.04806011\n",
      "Iteration 5719, loss = 0.04804895\n",
      "Iteration 5720, loss = 0.04803779\n",
      "Iteration 5721, loss = 0.04802663\n",
      "Iteration 5722, loss = 0.04801547\n",
      "Iteration 5723, loss = 0.04800431\n",
      "Iteration 5724, loss = 0.04799315\n",
      "Iteration 5725, loss = 0.04798198\n",
      "Iteration 5726, loss = 0.04797082\n",
      "Iteration 5727, loss = 0.04795966\n",
      "Iteration 5728, loss = 0.04794850\n",
      "Iteration 5729, loss = 0.04793734\n",
      "Iteration 5730, loss = 0.04792618\n",
      "Iteration 5731, loss = 0.04791502\n",
      "Iteration 5732, loss = 0.04790385\n",
      "Iteration 5733, loss = 0.04789269\n",
      "Iteration 5734, loss = 0.04788153\n",
      "Iteration 5735, loss = 0.04787037\n",
      "Iteration 5736, loss = 0.04785921\n",
      "Iteration 5737, loss = 0.04784804\n",
      "Iteration 5738, loss = 0.04783688\n",
      "Iteration 5739, loss = 0.04782572\n",
      "Iteration 5740, loss = 0.04781456\n",
      "Iteration 5741, loss = 0.04780339\n",
      "Iteration 5742, loss = 0.04779223\n",
      "Iteration 5743, loss = 0.04778107\n",
      "Iteration 5744, loss = 0.04776990\n",
      "Iteration 5745, loss = 0.04775874\n",
      "Iteration 5746, loss = 0.04774758\n",
      "Iteration 5747, loss = 0.04773642\n",
      "Iteration 5748, loss = 0.04772525\n",
      "Iteration 5749, loss = 0.04771409\n",
      "Iteration 5750, loss = 0.04770293\n",
      "Iteration 5751, loss = 0.04769176\n",
      "Iteration 5752, loss = 0.04768060\n",
      "Iteration 5753, loss = 0.04766944\n",
      "Iteration 5754, loss = 0.04765827\n",
      "Iteration 5755, loss = 0.04764711\n",
      "Iteration 5756, loss = 0.04763595\n",
      "Iteration 5757, loss = 0.04762478\n",
      "Iteration 5758, loss = 0.04761362\n",
      "Iteration 5759, loss = 0.04760246\n",
      "Iteration 5760, loss = 0.04759129\n",
      "Iteration 5761, loss = 0.04758013\n",
      "Iteration 5762, loss = 0.04756897\n",
      "Iteration 5763, loss = 0.04755780\n",
      "Iteration 5764, loss = 0.04754664\n",
      "Iteration 5765, loss = 0.04753548\n",
      "Iteration 5766, loss = 0.04752431\n",
      "Iteration 5767, loss = 0.04751315\n",
      "Iteration 5768, loss = 0.04750199\n",
      "Iteration 5769, loss = 0.04749082\n",
      "Iteration 5770, loss = 0.04747966\n",
      "Iteration 5771, loss = 0.04746849\n",
      "Iteration 5772, loss = 0.04745733\n",
      "Iteration 5773, loss = 0.04744617\n",
      "Iteration 5774, loss = 0.04743500\n",
      "Iteration 5775, loss = 0.04742384\n",
      "Iteration 5776, loss = 0.04741268\n",
      "Iteration 5777, loss = 0.04740151\n",
      "Iteration 5778, loss = 0.04739035\n",
      "Iteration 5779, loss = 0.04737919\n",
      "Iteration 5780, loss = 0.04736803\n",
      "Iteration 5781, loss = 0.04735686\n",
      "Iteration 5782, loss = 0.04734570\n",
      "Iteration 5783, loss = 0.04733454\n",
      "Iteration 5784, loss = 0.04732337\n",
      "Iteration 5785, loss = 0.04731221\n",
      "Iteration 5786, loss = 0.04730105\n",
      "Iteration 5787, loss = 0.04728988\n",
      "Iteration 5788, loss = 0.04727872\n",
      "Iteration 5789, loss = 0.04726756\n",
      "Iteration 5790, loss = 0.04725640\n",
      "Iteration 5791, loss = 0.04724523\n",
      "Iteration 5792, loss = 0.04723407\n",
      "Iteration 5793, loss = 0.04722291\n",
      "Iteration 5794, loss = 0.04721175\n",
      "Iteration 5795, loss = 0.04720059\n",
      "Iteration 5796, loss = 0.04718942\n",
      "Iteration 5797, loss = 0.04717826\n",
      "Iteration 5798, loss = 0.04716710\n",
      "Iteration 5799, loss = 0.04715594\n",
      "Iteration 5800, loss = 0.04714478\n",
      "Iteration 5801, loss = 0.04713362\n",
      "Iteration 5802, loss = 0.04712245\n",
      "Iteration 5803, loss = 0.04711129\n",
      "Iteration 5804, loss = 0.04710013\n",
      "Iteration 5805, loss = 0.04708897\n",
      "Iteration 5806, loss = 0.04707781\n",
      "Iteration 5807, loss = 0.04706665\n",
      "Iteration 5808, loss = 0.04705549\n",
      "Iteration 5809, loss = 0.04704433\n",
      "Iteration 5810, loss = 0.04703317\n",
      "Iteration 5811, loss = 0.04702201\n",
      "Iteration 5812, loss = 0.04701085\n",
      "Iteration 5813, loss = 0.04699969\n",
      "Iteration 5814, loss = 0.04698853\n",
      "Iteration 5815, loss = 0.04697737\n",
      "Iteration 5816, loss = 0.04696621\n",
      "Iteration 5817, loss = 0.04695505\n",
      "Iteration 5818, loss = 0.04694389\n",
      "Iteration 5819, loss = 0.04693273\n",
      "Iteration 5820, loss = 0.04692157\n",
      "Iteration 5821, loss = 0.04691041\n",
      "Iteration 5822, loss = 0.04689925\n",
      "Iteration 5823, loss = 0.04688810\n",
      "Iteration 5824, loss = 0.04687694\n",
      "Iteration 5825, loss = 0.04686578\n",
      "Iteration 5826, loss = 0.04685462\n",
      "Iteration 5827, loss = 0.04684347\n",
      "Iteration 5828, loss = 0.04683231\n",
      "Iteration 5829, loss = 0.04682115\n",
      "Iteration 5830, loss = 0.04680999\n",
      "Iteration 5831, loss = 0.04679884\n",
      "Iteration 5832, loss = 0.04678768\n",
      "Iteration 5833, loss = 0.04677652\n",
      "Iteration 5834, loss = 0.04676537\n",
      "Iteration 5835, loss = 0.04675421\n",
      "Iteration 5836, loss = 0.04674306\n",
      "Iteration 5837, loss = 0.04673190\n",
      "Iteration 5838, loss = 0.04672074\n",
      "Iteration 5839, loss = 0.04670959\n",
      "Iteration 5840, loss = 0.04669843\n",
      "Iteration 5841, loss = 0.04668728\n",
      "Iteration 5842, loss = 0.04667612\n",
      "Iteration 5843, loss = 0.04666497\n",
      "Iteration 5844, loss = 0.04665382\n",
      "Iteration 5845, loss = 0.04664266\n",
      "Iteration 5846, loss = 0.04663151\n",
      "Iteration 5847, loss = 0.04662036\n",
      "Iteration 5848, loss = 0.04660920\n",
      "Iteration 5849, loss = 0.04659805\n",
      "Iteration 5850, loss = 0.04658690\n",
      "Iteration 5851, loss = 0.04657574\n",
      "Iteration 5852, loss = 0.04656459\n",
      "Iteration 5853, loss = 0.04655344\n",
      "Iteration 5854, loss = 0.04654229\n",
      "Iteration 5855, loss = 0.04653114\n",
      "Iteration 5856, loss = 0.04651999\n",
      "Iteration 5857, loss = 0.04650884\n",
      "Iteration 5858, loss = 0.04649769\n",
      "Iteration 5859, loss = 0.04648653\n",
      "Iteration 5860, loss = 0.04647538\n",
      "Iteration 5861, loss = 0.04646424\n",
      "Iteration 5862, loss = 0.04645309\n",
      "Iteration 5863, loss = 0.04644194\n",
      "Iteration 5864, loss = 0.04643079\n",
      "Iteration 5865, loss = 0.04641964\n",
      "Iteration 5866, loss = 0.04640849\n",
      "Iteration 5867, loss = 0.04639734\n",
      "Iteration 5868, loss = 0.04638619\n",
      "Iteration 5869, loss = 0.04637505\n",
      "Iteration 5870, loss = 0.04636390\n",
      "Iteration 5871, loss = 0.04635275\n",
      "Iteration 5872, loss = 0.04634161\n",
      "Iteration 5873, loss = 0.04633046\n",
      "Iteration 5874, loss = 0.04631932\n",
      "Iteration 5875, loss = 0.04630817\n",
      "Iteration 5876, loss = 0.04629702\n",
      "Iteration 5877, loss = 0.04628588\n",
      "Iteration 5878, loss = 0.04627473\n",
      "Iteration 5879, loss = 0.04626359\n",
      "Iteration 5880, loss = 0.04625245\n",
      "Iteration 5881, loss = 0.04624130\n",
      "Iteration 5882, loss = 0.04623016\n",
      "Iteration 5883, loss = 0.04621902\n",
      "Iteration 5884, loss = 0.04620787\n",
      "Iteration 5885, loss = 0.04619673\n",
      "Iteration 5886, loss = 0.04618559\n",
      "Iteration 5887, loss = 0.04617445\n",
      "Iteration 5888, loss = 0.04616331\n",
      "Iteration 5889, loss = 0.04615217\n",
      "Iteration 5890, loss = 0.04614103\n",
      "Iteration 5891, loss = 0.04612989\n",
      "Iteration 5892, loss = 0.04611875\n",
      "Iteration 5893, loss = 0.04610761\n",
      "Iteration 5894, loss = 0.04609647\n",
      "Iteration 5895, loss = 0.04608533\n",
      "Iteration 5896, loss = 0.04607419\n",
      "Iteration 5897, loss = 0.04606305\n",
      "Iteration 5898, loss = 0.04605192\n",
      "Iteration 5899, loss = 0.04604078\n",
      "Iteration 5900, loss = 0.04602964\n",
      "Iteration 5901, loss = 0.04601850\n",
      "Iteration 5902, loss = 0.04600737\n",
      "Iteration 5903, loss = 0.04599623\n",
      "Iteration 5904, loss = 0.04598510\n",
      "Iteration 5905, loss = 0.04597396\n",
      "Iteration 5906, loss = 0.04596283\n",
      "Iteration 5907, loss = 0.04595169\n",
      "Iteration 5908, loss = 0.04594056\n",
      "Iteration 5909, loss = 0.04592943\n",
      "Iteration 5910, loss = 0.04591829\n",
      "Iteration 5911, loss = 0.04590716\n",
      "Iteration 5912, loss = 0.04589603\n",
      "Iteration 5913, loss = 0.04588490\n",
      "Iteration 5914, loss = 0.04587377\n",
      "Iteration 5915, loss = 0.04586264\n",
      "Iteration 5916, loss = 0.04585151\n",
      "Iteration 5917, loss = 0.04584038\n",
      "Iteration 5918, loss = 0.04582925\n",
      "Iteration 5919, loss = 0.04581812\n",
      "Iteration 5920, loss = 0.04580699\n",
      "Iteration 5921, loss = 0.04579586\n",
      "Iteration 5922, loss = 0.04578473\n",
      "Iteration 5923, loss = 0.04577361\n",
      "Iteration 5924, loss = 0.04576248\n",
      "Iteration 5925, loss = 0.04575135\n",
      "Iteration 5926, loss = 0.04574023\n",
      "Iteration 5927, loss = 0.04572910\n",
      "Iteration 5928, loss = 0.04571798\n",
      "Iteration 5929, loss = 0.04570685\n",
      "Iteration 5930, loss = 0.04569573\n",
      "Iteration 5931, loss = 0.04568460\n",
      "Iteration 5932, loss = 0.04567348\n",
      "Iteration 5933, loss = 0.04566236\n",
      "Iteration 5934, loss = 0.04565124\n",
      "Iteration 5935, loss = 0.04564011\n",
      "Iteration 5936, loss = 0.04562899\n",
      "Iteration 5937, loss = 0.04561787\n",
      "Iteration 5938, loss = 0.04560675\n",
      "Iteration 5939, loss = 0.04559563\n",
      "Iteration 5940, loss = 0.04558451\n",
      "Iteration 5941, loss = 0.04557339\n",
      "Iteration 5942, loss = 0.04556227\n",
      "Iteration 5943, loss = 0.04555116\n",
      "Iteration 5944, loss = 0.04554004\n",
      "Iteration 5945, loss = 0.04552892\n",
      "Iteration 5946, loss = 0.04551781\n",
      "Iteration 5947, loss = 0.04550669\n",
      "Iteration 5948, loss = 0.04549557\n",
      "Iteration 5949, loss = 0.04548446\n",
      "Iteration 5950, loss = 0.04547334\n",
      "Iteration 5951, loss = 0.04546223\n",
      "Iteration 5952, loss = 0.04545112\n",
      "Iteration 5953, loss = 0.04544000\n",
      "Iteration 5954, loss = 0.04542889\n",
      "Iteration 5955, loss = 0.04541778\n",
      "Iteration 5956, loss = 0.04540667\n",
      "Iteration 5957, loss = 0.04539556\n",
      "Iteration 5958, loss = 0.04538445\n",
      "Iteration 5959, loss = 0.04537334\n",
      "Iteration 5960, loss = 0.04536223\n",
      "Iteration 5961, loss = 0.04535112\n",
      "Iteration 5962, loss = 0.04534001\n",
      "Iteration 5963, loss = 0.04532891\n",
      "Iteration 5964, loss = 0.04531780\n",
      "Iteration 5965, loss = 0.04530669\n",
      "Iteration 5966, loss = 0.04529559\n",
      "Iteration 5967, loss = 0.04528448\n",
      "Iteration 5968, loss = 0.04527338\n",
      "Iteration 5969, loss = 0.04526227\n",
      "Iteration 5970, loss = 0.04525117\n",
      "Iteration 5971, loss = 0.04524007\n",
      "Iteration 5972, loss = 0.04522896\n",
      "Iteration 5973, loss = 0.04521786\n",
      "Iteration 5974, loss = 0.04520676\n",
      "Iteration 5975, loss = 0.04519566\n",
      "Iteration 5976, loss = 0.04518456\n",
      "Iteration 5977, loss = 0.04517346\n",
      "Iteration 5978, loss = 0.04516236\n",
      "Iteration 5979, loss = 0.04515126\n",
      "Iteration 5980, loss = 0.04514016\n",
      "Iteration 5981, loss = 0.04512907\n",
      "Iteration 5982, loss = 0.04511797\n",
      "Iteration 5983, loss = 0.04510687\n",
      "Iteration 5984, loss = 0.04509578\n",
      "Iteration 5985, loss = 0.04508468\n",
      "Iteration 5986, loss = 0.04507359\n",
      "Iteration 5987, loss = 0.04506250\n",
      "Iteration 5988, loss = 0.04505140\n",
      "Iteration 5989, loss = 0.04504031\n",
      "Iteration 5990, loss = 0.04502922\n",
      "Iteration 5991, loss = 0.04501813\n",
      "Iteration 5992, loss = 0.04500704\n",
      "Iteration 5993, loss = 0.04499595\n",
      "Iteration 5994, loss = 0.04498486\n",
      "Iteration 5995, loss = 0.04497377\n",
      "Iteration 5996, loss = 0.04496268\n",
      "Iteration 5997, loss = 0.04495159\n",
      "Iteration 5998, loss = 0.04494051\n",
      "Iteration 5999, loss = 0.04492942\n",
      "Iteration 6000, loss = 0.04491833\n",
      "Iteration 6001, loss = 0.04490725\n",
      "Iteration 6002, loss = 0.04489617\n",
      "Iteration 6003, loss = 0.04488508\n",
      "Iteration 6004, loss = 0.04487400\n",
      "Iteration 6005, loss = 0.04486292\n",
      "Iteration 6006, loss = 0.04485183\n",
      "Iteration 6007, loss = 0.04484075\n",
      "Iteration 6008, loss = 0.04482967\n",
      "Iteration 6009, loss = 0.04481859\n",
      "Iteration 6010, loss = 0.04480751\n",
      "Iteration 6011, loss = 0.04479644\n",
      "Iteration 6012, loss = 0.04478536\n",
      "Iteration 6013, loss = 0.04477428\n",
      "Iteration 6014, loss = 0.04476321\n",
      "Iteration 6015, loss = 0.04475213\n",
      "Iteration 6016, loss = 0.04474106\n",
      "Iteration 6017, loss = 0.04472998\n",
      "Iteration 6018, loss = 0.04471891\n",
      "Iteration 6019, loss = 0.04470783\n",
      "Iteration 6020, loss = 0.04469676\n",
      "Iteration 6021, loss = 0.04468569\n",
      "Iteration 6022, loss = 0.04467462\n",
      "Iteration 6023, loss = 0.04466355\n",
      "Iteration 6024, loss = 0.04465248\n",
      "Iteration 6025, loss = 0.04464141\n",
      "Iteration 6026, loss = 0.04463035\n",
      "Iteration 6027, loss = 0.04461928\n",
      "Iteration 6028, loss = 0.04460821\n",
      "Iteration 6029, loss = 0.04459715\n",
      "Iteration 6030, loss = 0.04458608\n",
      "Iteration 6031, loss = 0.04457502\n",
      "Iteration 6032, loss = 0.04456395\n",
      "Iteration 6033, loss = 0.04455289\n",
      "Iteration 6034, loss = 0.04454183\n",
      "Iteration 6035, loss = 0.04453077\n",
      "Iteration 6036, loss = 0.04451971\n",
      "Iteration 6037, loss = 0.04450865\n",
      "Iteration 6038, loss = 0.04449759\n",
      "Iteration 6039, loss = 0.04448653\n",
      "Iteration 6040, loss = 0.04447547\n",
      "Iteration 6041, loss = 0.04446441\n",
      "Iteration 6042, loss = 0.04445336\n",
      "Iteration 6043, loss = 0.04444230\n",
      "Iteration 6044, loss = 0.04443125\n",
      "Iteration 6045, loss = 0.04442019\n",
      "Iteration 6046, loss = 0.04440914\n",
      "Iteration 6047, loss = 0.04439809\n",
      "Iteration 6048, loss = 0.04438704\n",
      "Iteration 6049, loss = 0.04437599\n",
      "Iteration 6050, loss = 0.04436494\n",
      "Iteration 6051, loss = 0.04435389\n",
      "Iteration 6052, loss = 0.04434284\n",
      "Iteration 6053, loss = 0.04433179\n",
      "Iteration 6054, loss = 0.04432075\n",
      "Iteration 6055, loss = 0.04430970\n",
      "Iteration 6056, loss = 0.04429865\n",
      "Iteration 6057, loss = 0.04428761\n",
      "Iteration 6058, loss = 0.04427657\n",
      "Iteration 6059, loss = 0.04426552\n",
      "Iteration 6060, loss = 0.04425448\n",
      "Iteration 6061, loss = 0.04424344\n",
      "Iteration 6062, loss = 0.04423240\n",
      "Iteration 6063, loss = 0.04422136\n",
      "Iteration 6064, loss = 0.04421032\n",
      "Iteration 6065, loss = 0.04419928\n",
      "Iteration 6066, loss = 0.04418825\n",
      "Iteration 6067, loss = 0.04417721\n",
      "Iteration 6068, loss = 0.04416618\n",
      "Iteration 6069, loss = 0.04415514\n",
      "Iteration 6070, loss = 0.04414411\n",
      "Iteration 6071, loss = 0.04413307\n",
      "Iteration 6072, loss = 0.04412204\n",
      "Iteration 6073, loss = 0.04411101\n",
      "Iteration 6074, loss = 0.04409998\n",
      "Iteration 6075, loss = 0.04408895\n",
      "Iteration 6076, loss = 0.04407792\n",
      "Iteration 6077, loss = 0.04406689\n",
      "Iteration 6078, loss = 0.04405587\n",
      "Iteration 6079, loss = 0.04404484\n",
      "Iteration 6080, loss = 0.04403382\n",
      "Iteration 6081, loss = 0.04402279\n",
      "Iteration 6082, loss = 0.04401177\n",
      "Iteration 6083, loss = 0.04400075\n",
      "Iteration 6084, loss = 0.04398972\n",
      "Iteration 6085, loss = 0.04397870\n",
      "Iteration 6086, loss = 0.04396768\n",
      "Iteration 6087, loss = 0.04395666\n",
      "Iteration 6088, loss = 0.04394565\n",
      "Iteration 6089, loss = 0.04393463\n",
      "Iteration 6090, loss = 0.04392361\n",
      "Iteration 6091, loss = 0.04391260\n",
      "Iteration 6092, loss = 0.04390158\n",
      "Iteration 6093, loss = 0.04389057\n",
      "Iteration 6094, loss = 0.04387956\n",
      "Iteration 6095, loss = 0.04386854\n",
      "Iteration 6096, loss = 0.04385753\n",
      "Iteration 6097, loss = 0.04384652\n",
      "Iteration 6098, loss = 0.04383551\n",
      "Iteration 6099, loss = 0.04382451\n",
      "Iteration 6100, loss = 0.04381350\n",
      "Iteration 6101, loss = 0.04380249\n",
      "Iteration 6102, loss = 0.04379149\n",
      "Iteration 6103, loss = 0.04378048\n",
      "Iteration 6104, loss = 0.04376948\n",
      "Iteration 6105, loss = 0.04375848\n",
      "Iteration 6106, loss = 0.04374748\n",
      "Iteration 6107, loss = 0.04373648\n",
      "Iteration 6108, loss = 0.04372548\n",
      "Iteration 6109, loss = 0.04371448\n",
      "Iteration 6110, loss = 0.04370348\n",
      "Iteration 6111, loss = 0.04369248\n",
      "Iteration 6112, loss = 0.04368149\n",
      "Iteration 6113, loss = 0.04367049\n",
      "Iteration 6114, loss = 0.04365950\n",
      "Iteration 6115, loss = 0.04364851\n",
      "Iteration 6116, loss = 0.04363751\n",
      "Iteration 6117, loss = 0.04362652\n",
      "Iteration 6118, loss = 0.04361553\n",
      "Iteration 6119, loss = 0.04360454\n",
      "Iteration 6120, loss = 0.04359356\n",
      "Iteration 6121, loss = 0.04358257\n",
      "Iteration 6122, loss = 0.04357158\n",
      "Iteration 6123, loss = 0.04356060\n",
      "Iteration 6124, loss = 0.04354961\n",
      "Iteration 6125, loss = 0.04353863\n",
      "Iteration 6126, loss = 0.04352765\n",
      "Iteration 6127, loss = 0.04351667\n",
      "Iteration 6128, loss = 0.04350569\n",
      "Iteration 6129, loss = 0.04349471\n",
      "Iteration 6130, loss = 0.04348373\n",
      "Iteration 6131, loss = 0.04347275\n",
      "Iteration 6132, loss = 0.04346178\n",
      "Iteration 6133, loss = 0.04345080\n",
      "Iteration 6134, loss = 0.04343983\n",
      "Iteration 6135, loss = 0.04342886\n",
      "Iteration 6136, loss = 0.04341789\n",
      "Iteration 6137, loss = 0.04340692\n",
      "Iteration 6138, loss = 0.04339595\n",
      "Iteration 6139, loss = 0.04338498\n",
      "Iteration 6140, loss = 0.04337401\n",
      "Iteration 6141, loss = 0.04336304\n",
      "Iteration 6142, loss = 0.04335208\n",
      "Iteration 6143, loss = 0.04334111\n",
      "Iteration 6144, loss = 0.04333015\n",
      "Iteration 6145, loss = 0.04331919\n",
      "Iteration 6146, loss = 0.04330823\n",
      "Iteration 6147, loss = 0.04329727\n",
      "Iteration 6148, loss = 0.04328631\n",
      "Iteration 6149, loss = 0.04327535\n",
      "Iteration 6150, loss = 0.04326439\n",
      "Iteration 6151, loss = 0.04325344\n",
      "Iteration 6152, loss = 0.04324248\n",
      "Iteration 6153, loss = 0.04323153\n",
      "Iteration 6154, loss = 0.04322058\n",
      "Iteration 6155, loss = 0.04320963\n",
      "Iteration 6156, loss = 0.04319868\n",
      "Iteration 6157, loss = 0.04318773\n",
      "Iteration 6158, loss = 0.04317678\n",
      "Iteration 6159, loss = 0.04316583\n",
      "Iteration 6160, loss = 0.04315489\n",
      "Iteration 6161, loss = 0.04314394\n",
      "Iteration 6162, loss = 0.04313300\n",
      "Iteration 6163, loss = 0.04312206\n",
      "Iteration 6164, loss = 0.04311112\n",
      "Iteration 6165, loss = 0.04310018\n",
      "Iteration 6166, loss = 0.04308924\n",
      "Iteration 6167, loss = 0.04307830\n",
      "Iteration 6168, loss = 0.04306736\n",
      "Iteration 6169, loss = 0.04305643\n",
      "Iteration 6170, loss = 0.04304550\n",
      "Iteration 6171, loss = 0.04303456\n",
      "Iteration 6172, loss = 0.04302363\n",
      "Iteration 6173, loss = 0.04301270\n",
      "Iteration 6174, loss = 0.04300177\n",
      "Iteration 6175, loss = 0.04299084\n",
      "Iteration 6176, loss = 0.04297991\n",
      "Iteration 6177, loss = 0.04296899\n",
      "Iteration 6178, loss = 0.04295806\n",
      "Iteration 6179, loss = 0.04294714\n",
      "Iteration 6180, loss = 0.04293622\n",
      "Iteration 6181, loss = 0.04292530\n",
      "Iteration 6182, loss = 0.04291438\n",
      "Iteration 6183, loss = 0.04290346\n",
      "Iteration 6184, loss = 0.04289254\n",
      "Iteration 6185, loss = 0.04288162\n",
      "Iteration 6186, loss = 0.04287071\n",
      "Iteration 6187, loss = 0.04285980\n",
      "Iteration 6188, loss = 0.04284888\n",
      "Iteration 6189, loss = 0.04283797\n",
      "Iteration 6190, loss = 0.04282706\n",
      "Iteration 6191, loss = 0.04281615\n",
      "Iteration 6192, loss = 0.04280524\n",
      "Iteration 6193, loss = 0.04279434\n",
      "Iteration 6194, loss = 0.04278343\n",
      "Iteration 6195, loss = 0.04277253\n",
      "Iteration 6196, loss = 0.04276163\n",
      "Iteration 6197, loss = 0.04275072\n",
      "Iteration 6198, loss = 0.04273982\n",
      "Iteration 6199, loss = 0.04272893\n",
      "Iteration 6200, loss = 0.04271803\n",
      "Iteration 6201, loss = 0.04270713\n",
      "Iteration 6202, loss = 0.04269624\n",
      "Iteration 6203, loss = 0.04268534\n",
      "Iteration 6204, loss = 0.04267445\n",
      "Iteration 6205, loss = 0.04266356\n",
      "Iteration 6206, loss = 0.04265267\n",
      "Iteration 6207, loss = 0.04264178\n",
      "Iteration 6208, loss = 0.04263089\n",
      "Iteration 6209, loss = 0.04262001\n",
      "Iteration 6210, loss = 0.04260912\n",
      "Iteration 6211, loss = 0.04259824\n",
      "Iteration 6212, loss = 0.04258736\n",
      "Iteration 6213, loss = 0.04257647\n",
      "Iteration 6214, loss = 0.04256560\n",
      "Iteration 6215, loss = 0.04255472\n",
      "Iteration 6216, loss = 0.04254384\n",
      "Iteration 6217, loss = 0.04253296\n",
      "Iteration 6218, loss = 0.04252209\n",
      "Iteration 6219, loss = 0.04251122\n",
      "Iteration 6220, loss = 0.04250035\n",
      "Iteration 6221, loss = 0.04248948\n",
      "Iteration 6222, loss = 0.04247861\n",
      "Iteration 6223, loss = 0.04246774\n",
      "Iteration 6224, loss = 0.04245687\n",
      "Iteration 6225, loss = 0.04244601\n",
      "Iteration 6226, loss = 0.04243514\n",
      "Iteration 6227, loss = 0.04242428\n",
      "Iteration 6228, loss = 0.04241342\n",
      "Iteration 6229, loss = 0.04240256\n",
      "Iteration 6230, loss = 0.04239170\n",
      "Iteration 6231, loss = 0.04238085\n",
      "Iteration 6232, loss = 0.04236999\n",
      "Iteration 6233, loss = 0.04235914\n",
      "Iteration 6234, loss = 0.04234829\n",
      "Iteration 6235, loss = 0.04233743\n",
      "Iteration 6236, loss = 0.04232658\n",
      "Iteration 6237, loss = 0.04231574\n",
      "Iteration 6238, loss = 0.04230489\n",
      "Iteration 6239, loss = 0.04229404\n",
      "Iteration 6240, loss = 0.04228320\n",
      "Iteration 6241, loss = 0.04227236\n",
      "Iteration 6242, loss = 0.04226152\n",
      "Iteration 6243, loss = 0.04225068\n",
      "Iteration 6244, loss = 0.04223984\n",
      "Iteration 6245, loss = 0.04222900\n",
      "Iteration 6246, loss = 0.04221817\n",
      "Iteration 6247, loss = 0.04220733\n",
      "Iteration 6248, loss = 0.04219650\n",
      "Iteration 6249, loss = 0.04218567\n",
      "Iteration 6250, loss = 0.04217484\n",
      "Iteration 6251, loss = 0.04216401\n",
      "Iteration 6252, loss = 0.04215318\n",
      "Iteration 6253, loss = 0.04214236\n",
      "Iteration 6254, loss = 0.04213153\n",
      "Iteration 6255, loss = 0.04212071\n",
      "Iteration 6256, loss = 0.04210989\n",
      "Iteration 6257, loss = 0.04209907\n",
      "Iteration 6258, loss = 0.04208825\n",
      "Iteration 6259, loss = 0.04207744\n",
      "Iteration 6260, loss = 0.04206662\n",
      "Iteration 6261, loss = 0.04205581\n",
      "Iteration 6262, loss = 0.04204499\n",
      "Iteration 6263, loss = 0.04203418\n",
      "Iteration 6264, loss = 0.04202338\n",
      "Iteration 6265, loss = 0.04201257\n",
      "Iteration 6266, loss = 0.04200176\n",
      "Iteration 6267, loss = 0.04199096\n",
      "Iteration 6268, loss = 0.04198015\n",
      "Iteration 6269, loss = 0.04196935\n",
      "Iteration 6270, loss = 0.04195855\n",
      "Iteration 6271, loss = 0.04194775\n",
      "Iteration 6272, loss = 0.04193696\n",
      "Iteration 6273, loss = 0.04192616\n",
      "Iteration 6274, loss = 0.04191537\n",
      "Iteration 6275, loss = 0.04190457\n",
      "Iteration 6276, loss = 0.04189378\n",
      "Iteration 6277, loss = 0.04188299\n",
      "Iteration 6278, loss = 0.04187221\n",
      "Iteration 6279, loss = 0.04186142\n",
      "Iteration 6280, loss = 0.04185064\n",
      "Iteration 6281, loss = 0.04183985\n",
      "Iteration 6282, loss = 0.04182907\n",
      "Iteration 6283, loss = 0.04181829\n",
      "Iteration 6284, loss = 0.04180751\n",
      "Iteration 6285, loss = 0.04179674\n",
      "Iteration 6286, loss = 0.04178596\n",
      "Iteration 6287, loss = 0.04177519\n",
      "Iteration 6288, loss = 0.04176442\n",
      "Iteration 6289, loss = 0.04175365\n",
      "Iteration 6290, loss = 0.04174288\n",
      "Iteration 6291, loss = 0.04173211\n",
      "Iteration 6292, loss = 0.04172134\n",
      "Iteration 6293, loss = 0.04171058\n",
      "Iteration 6294, loss = 0.04169982\n",
      "Iteration 6295, loss = 0.04168906\n",
      "Iteration 6296, loss = 0.04167830\n",
      "Iteration 6297, loss = 0.04166754\n",
      "Iteration 6298, loss = 0.04165678\n",
      "Iteration 6299, loss = 0.04164603\n",
      "Iteration 6300, loss = 0.04163528\n",
      "Iteration 6301, loss = 0.04162453\n",
      "Iteration 6302, loss = 0.04161378\n",
      "Iteration 6303, loss = 0.04160303\n",
      "Iteration 6304, loss = 0.04159228\n",
      "Iteration 6305, loss = 0.04158154\n",
      "Iteration 6306, loss = 0.04157080\n",
      "Iteration 6307, loss = 0.04156005\n",
      "Iteration 6308, loss = 0.04154931\n",
      "Iteration 6309, loss = 0.04153858\n",
      "Iteration 6310, loss = 0.04152784\n",
      "Iteration 6311, loss = 0.04151711\n",
      "Iteration 6312, loss = 0.04150637\n",
      "Iteration 6313, loss = 0.04149564\n",
      "Iteration 6314, loss = 0.04148491\n",
      "Iteration 6315, loss = 0.04147418\n",
      "Iteration 6316, loss = 0.04146346\n",
      "Iteration 6317, loss = 0.04145273\n",
      "Iteration 6318, loss = 0.04144201\n",
      "Iteration 6319, loss = 0.04143129\n",
      "Iteration 6320, loss = 0.04142057\n",
      "Iteration 6321, loss = 0.04140985\n",
      "Iteration 6322, loss = 0.04139914\n",
      "Iteration 6323, loss = 0.04138842\n",
      "Iteration 6324, loss = 0.04137771\n",
      "Iteration 6325, loss = 0.04136700\n",
      "Iteration 6326, loss = 0.04135629\n",
      "Iteration 6327, loss = 0.04134558\n",
      "Iteration 6328, loss = 0.04133488\n",
      "Iteration 6329, loss = 0.04132417\n",
      "Iteration 6330, loss = 0.04131347\n",
      "Iteration 6331, loss = 0.04130277\n",
      "Iteration 6332, loss = 0.04129207\n",
      "Iteration 6333, loss = 0.04128137\n",
      "Iteration 6334, loss = 0.04127068\n",
      "Iteration 6335, loss = 0.04125999\n",
      "Iteration 6336, loss = 0.04124929\n",
      "Iteration 6337, loss = 0.04123860\n",
      "Iteration 6338, loss = 0.04122792\n",
      "Iteration 6339, loss = 0.04121723\n",
      "Iteration 6340, loss = 0.04120654\n",
      "Iteration 6341, loss = 0.04119586\n",
      "Iteration 6342, loss = 0.04118518\n",
      "Iteration 6343, loss = 0.04117450\n",
      "Iteration 6344, loss = 0.04116382\n",
      "Iteration 6345, loss = 0.04115315\n",
      "Iteration 6346, loss = 0.04114247\n",
      "Iteration 6347, loss = 0.04113180\n",
      "Iteration 6348, loss = 0.04112113\n",
      "Iteration 6349, loss = 0.04111046\n",
      "Iteration 6350, loss = 0.04109980\n",
      "Iteration 6351, loss = 0.04108913\n",
      "Iteration 6352, loss = 0.04107847\n",
      "Iteration 6353, loss = 0.04106781\n",
      "Iteration 6354, loss = 0.04105715\n",
      "Iteration 6355, loss = 0.04104649\n",
      "Iteration 6356, loss = 0.04103583\n",
      "Iteration 6357, loss = 0.04102518\n",
      "Iteration 6358, loss = 0.04101453\n",
      "Iteration 6359, loss = 0.04100388\n",
      "Iteration 6360, loss = 0.04099323\n",
      "Iteration 6361, loss = 0.04098258\n",
      "Iteration 6362, loss = 0.04097194\n",
      "Iteration 6363, loss = 0.04096129\n",
      "Iteration 6364, loss = 0.04095065\n",
      "Iteration 6365, loss = 0.04094001\n",
      "Iteration 6366, loss = 0.04092937\n",
      "Iteration 6367, loss = 0.04091874\n",
      "Iteration 6368, loss = 0.04090810\n",
      "Iteration 6369, loss = 0.04089747\n",
      "Iteration 6370, loss = 0.04088684\n",
      "Iteration 6371, loss = 0.04087621\n",
      "Iteration 6372, loss = 0.04086559\n",
      "Iteration 6373, loss = 0.04085496\n",
      "Iteration 6374, loss = 0.04084434\n",
      "Iteration 6375, loss = 0.04083372\n",
      "Iteration 6376, loss = 0.04082310\n",
      "Iteration 6377, loss = 0.04081248\n",
      "Iteration 6378, loss = 0.04080187\n",
      "Iteration 6379, loss = 0.04079125\n",
      "Iteration 6380, loss = 0.04078064\n",
      "Iteration 6381, loss = 0.04077003\n",
      "Iteration 6382, loss = 0.04075943\n",
      "Iteration 6383, loss = 0.04074882\n",
      "Iteration 6384, loss = 0.04073822\n",
      "Iteration 6385, loss = 0.04072761\n",
      "Iteration 6386, loss = 0.04071701\n",
      "Iteration 6387, loss = 0.04070642\n",
      "Iteration 6388, loss = 0.04069582\n",
      "Iteration 6389, loss = 0.04068523\n",
      "Iteration 6390, loss = 0.04067463\n",
      "Iteration 6391, loss = 0.04066404\n",
      "Iteration 6392, loss = 0.04065345\n",
      "Iteration 6393, loss = 0.04064287\n",
      "Iteration 6394, loss = 0.04063228\n",
      "Iteration 6395, loss = 0.04062170\n",
      "Iteration 6396, loss = 0.04061112\n",
      "Iteration 6397, loss = 0.04060054\n",
      "Iteration 6398, loss = 0.04058996\n",
      "Iteration 6399, loss = 0.04057939\n",
      "Iteration 6400, loss = 0.04056882\n",
      "Iteration 6401, loss = 0.04055825\n",
      "Iteration 6402, loss = 0.04054768\n",
      "Iteration 6403, loss = 0.04053711\n",
      "Iteration 6404, loss = 0.04052654\n",
      "Iteration 6405, loss = 0.04051598\n",
      "Iteration 6406, loss = 0.04050542\n",
      "Iteration 6407, loss = 0.04049486\n",
      "Iteration 6408, loss = 0.04048430\n",
      "Iteration 6409, loss = 0.04047375\n",
      "Iteration 6410, loss = 0.04046319\n",
      "Iteration 6411, loss = 0.04045264\n",
      "Iteration 6412, loss = 0.04044209\n",
      "Iteration 6413, loss = 0.04043155\n",
      "Iteration 6414, loss = 0.04042100\n",
      "Iteration 6415, loss = 0.04041046\n",
      "Iteration 6416, loss = 0.04039992\n",
      "Iteration 6417, loss = 0.04038938\n",
      "Iteration 6418, loss = 0.04037884\n",
      "Iteration 6419, loss = 0.04036830\n",
      "Iteration 6420, loss = 0.04035777\n",
      "Iteration 6421, loss = 0.04034724\n",
      "Iteration 6422, loss = 0.04033671\n",
      "Iteration 6423, loss = 0.04032618\n",
      "Iteration 6424, loss = 0.04031566\n",
      "Iteration 6425, loss = 0.04030513\n",
      "Iteration 6426, loss = 0.04029461\n",
      "Iteration 6427, loss = 0.04028409\n",
      "Iteration 6428, loss = 0.04027358\n",
      "Iteration 6429, loss = 0.04026306\n",
      "Iteration 6430, loss = 0.04025255\n",
      "Iteration 6431, loss = 0.04024204\n",
      "Iteration 6432, loss = 0.04023153\n",
      "Iteration 6433, loss = 0.04022102\n",
      "Iteration 6434, loss = 0.04021051\n",
      "Iteration 6435, loss = 0.04020001\n",
      "Iteration 6436, loss = 0.04018951\n",
      "Iteration 6437, loss = 0.04017901\n",
      "Iteration 6438, loss = 0.04016851\n",
      "Iteration 6439, loss = 0.04015802\n",
      "Iteration 6440, loss = 0.04014753\n",
      "Iteration 6441, loss = 0.04013704\n",
      "Iteration 6442, loss = 0.04012655\n",
      "Iteration 6443, loss = 0.04011606\n",
      "Iteration 6444, loss = 0.04010558\n",
      "Iteration 6445, loss = 0.04009509\n",
      "Iteration 6446, loss = 0.04008461\n",
      "Iteration 6447, loss = 0.04007413\n",
      "Iteration 6448, loss = 0.04006366\n",
      "Iteration 6449, loss = 0.04005318\n",
      "Iteration 6450, loss = 0.04004271\n",
      "Iteration 6451, loss = 0.04003224\n",
      "Iteration 6452, loss = 0.04002177\n",
      "Iteration 6453, loss = 0.04001131\n",
      "Iteration 6454, loss = 0.04000084\n",
      "Iteration 6455, loss = 0.03999038\n",
      "Iteration 6456, loss = 0.03997992\n",
      "Iteration 6457, loss = 0.03996946\n",
      "Iteration 6458, loss = 0.03995901\n",
      "Iteration 6459, loss = 0.03994855\n",
      "Iteration 6460, loss = 0.03993810\n",
      "Iteration 6461, loss = 0.03992765\n",
      "Iteration 6462, loss = 0.03991721\n",
      "Iteration 6463, loss = 0.03990676\n",
      "Iteration 6464, loss = 0.03989632\n",
      "Iteration 6465, loss = 0.03988588\n",
      "Iteration 6466, loss = 0.03987544\n",
      "Iteration 6467, loss = 0.03986500\n",
      "Iteration 6468, loss = 0.03985457\n",
      "Iteration 6469, loss = 0.03984414\n",
      "Iteration 6470, loss = 0.03983371\n",
      "Iteration 6471, loss = 0.03982328\n",
      "Iteration 6472, loss = 0.03981285\n",
      "Iteration 6473, loss = 0.03980243\n",
      "Iteration 6474, loss = 0.03979201\n",
      "Iteration 6475, loss = 0.03978159\n",
      "Iteration 6476, loss = 0.03977117\n",
      "Iteration 6477, loss = 0.03976075\n",
      "Iteration 6478, loss = 0.03975034\n",
      "Iteration 6479, loss = 0.03973993\n",
      "Iteration 6480, loss = 0.03972952\n",
      "Iteration 6481, loss = 0.03971911\n",
      "Iteration 6482, loss = 0.03970871\n",
      "Iteration 6483, loss = 0.03969831\n",
      "Iteration 6484, loss = 0.03968791\n",
      "Iteration 6485, loss = 0.03967751\n",
      "Iteration 6486, loss = 0.03966711\n",
      "Iteration 6487, loss = 0.03965672\n",
      "Iteration 6488, loss = 0.03964633\n",
      "Iteration 6489, loss = 0.03963594\n",
      "Iteration 6490, loss = 0.03962555\n",
      "Iteration 6491, loss = 0.03961516\n",
      "Iteration 6492, loss = 0.03960478\n",
      "Iteration 6493, loss = 0.03959440\n",
      "Iteration 6494, loss = 0.03958402\n",
      "Iteration 6495, loss = 0.03957364\n",
      "Iteration 6496, loss = 0.03956327\n",
      "Iteration 6497, loss = 0.03955290\n",
      "Iteration 6498, loss = 0.03954253\n",
      "Iteration 6499, loss = 0.03953216\n",
      "Iteration 6500, loss = 0.03952179\n",
      "Iteration 6501, loss = 0.03951143\n",
      "Iteration 6502, loss = 0.03950107\n",
      "Iteration 6503, loss = 0.03949071\n",
      "Iteration 6504, loss = 0.03948035\n",
      "Iteration 6505, loss = 0.03947000\n",
      "Iteration 6506, loss = 0.03945964\n",
      "Iteration 6507, loss = 0.03944929\n",
      "Iteration 6508, loss = 0.03943895\n",
      "Iteration 6509, loss = 0.03942860\n",
      "Iteration 6510, loss = 0.03941826\n",
      "Iteration 6511, loss = 0.03940791\n",
      "Iteration 6512, loss = 0.03939757\n",
      "Iteration 6513, loss = 0.03938724\n",
      "Iteration 6514, loss = 0.03937690\n",
      "Iteration 6515, loss = 0.03936657\n",
      "Iteration 6516, loss = 0.03935624\n",
      "Iteration 6517, loss = 0.03934591\n",
      "Iteration 6518, loss = 0.03933558\n",
      "Iteration 6519, loss = 0.03932526\n",
      "Iteration 6520, loss = 0.03931494\n",
      "Iteration 6521, loss = 0.03930462\n",
      "Iteration 6522, loss = 0.03929430\n",
      "Iteration 6523, loss = 0.03928398\n",
      "Iteration 6524, loss = 0.03927367\n",
      "Iteration 6525, loss = 0.03926336\n",
      "Iteration 6526, loss = 0.03925305\n",
      "Iteration 6527, loss = 0.03924275\n",
      "Iteration 6528, loss = 0.03923244\n",
      "Iteration 6529, loss = 0.03922214\n",
      "Iteration 6530, loss = 0.03921184\n",
      "Iteration 6531, loss = 0.03920154\n",
      "Iteration 6532, loss = 0.03919125\n",
      "Iteration 6533, loss = 0.03918095\n",
      "Iteration 6534, loss = 0.03917066\n",
      "Iteration 6535, loss = 0.03916038\n",
      "Iteration 6536, loss = 0.03915009\n",
      "Iteration 6537, loss = 0.03913981\n",
      "Iteration 6538, loss = 0.03912952\n",
      "Iteration 6539, loss = 0.03911924\n",
      "Iteration 6540, loss = 0.03910897\n",
      "Iteration 6541, loss = 0.03909869\n",
      "Iteration 6542, loss = 0.03908842\n",
      "Iteration 6543, loss = 0.03907815\n",
      "Iteration 6544, loss = 0.03906788\n",
      "Iteration 6545, loss = 0.03905761\n",
      "Iteration 6546, loss = 0.03904735\n",
      "Iteration 6547, loss = 0.03903709\n",
      "Iteration 6548, loss = 0.03902683\n",
      "Iteration 6549, loss = 0.03901657\n",
      "Iteration 6550, loss = 0.03900632\n",
      "Iteration 6551, loss = 0.03899607\n",
      "Iteration 6552, loss = 0.03898582\n",
      "Iteration 6553, loss = 0.03897557\n",
      "Iteration 6554, loss = 0.03896532\n",
      "Iteration 6555, loss = 0.03895508\n",
      "Iteration 6556, loss = 0.03894484\n",
      "Iteration 6557, loss = 0.03893460\n",
      "Iteration 6558, loss = 0.03892436\n",
      "Iteration 6559, loss = 0.03891413\n",
      "Iteration 6560, loss = 0.03890390\n",
      "Iteration 6561, loss = 0.03889367\n",
      "Iteration 6562, loss = 0.03888344\n",
      "Iteration 6563, loss = 0.03887321\n",
      "Iteration 6564, loss = 0.03886299\n",
      "Iteration 6565, loss = 0.03885277\n",
      "Iteration 6566, loss = 0.03884255\n",
      "Iteration 6567, loss = 0.03883234\n",
      "Iteration 6568, loss = 0.03882212\n",
      "Iteration 6569, loss = 0.03881191\n",
      "Iteration 6570, loss = 0.03880170\n",
      "Iteration 6571, loss = 0.03879150\n",
      "Iteration 6572, loss = 0.03878129\n",
      "Iteration 6573, loss = 0.03877109\n",
      "Iteration 6574, loss = 0.03876089\n",
      "Iteration 6575, loss = 0.03875069\n",
      "Iteration 6576, loss = 0.03874050\n",
      "Iteration 6577, loss = 0.03873030\n",
      "Iteration 6578, loss = 0.03872011\n",
      "Iteration 6579, loss = 0.03870992\n",
      "Iteration 6580, loss = 0.03869974\n",
      "Iteration 6581, loss = 0.03868955\n",
      "Iteration 6582, loss = 0.03867937\n",
      "Iteration 6583, loss = 0.03866919\n",
      "Iteration 6584, loss = 0.03865902\n",
      "Iteration 6585, loss = 0.03864884\n",
      "Iteration 6586, loss = 0.03863867\n",
      "Iteration 6587, loss = 0.03862850\n",
      "Iteration 6588, loss = 0.03861833\n",
      "Iteration 6589, loss = 0.03860817\n",
      "Iteration 6590, loss = 0.03859801\n",
      "Iteration 6591, loss = 0.03858785\n",
      "Iteration 6592, loss = 0.03857769\n",
      "Iteration 6593, loss = 0.03856753\n",
      "Iteration 6594, loss = 0.03855738\n",
      "Iteration 6595, loss = 0.03854723\n",
      "Iteration 6596, loss = 0.03853708\n",
      "Iteration 6597, loss = 0.03852693\n",
      "Iteration 6598, loss = 0.03851679\n",
      "Iteration 6599, loss = 0.03850665\n",
      "Iteration 6600, loss = 0.03849651\n",
      "Iteration 6601, loss = 0.03848637\n",
      "Iteration 6602, loss = 0.03847624\n",
      "Iteration 6603, loss = 0.03846610\n",
      "Iteration 6604, loss = 0.03845597\n",
      "Iteration 6605, loss = 0.03844585\n",
      "Iteration 6606, loss = 0.03843572\n",
      "Iteration 6607, loss = 0.03842560\n",
      "Iteration 6608, loss = 0.03841548\n",
      "Iteration 6609, loss = 0.03840536\n",
      "Iteration 6610, loss = 0.03839524\n",
      "Iteration 6611, loss = 0.03838513\n",
      "Iteration 6612, loss = 0.03837502\n",
      "Iteration 6613, loss = 0.03836491\n",
      "Iteration 6614, loss = 0.03835480\n",
      "Iteration 6615, loss = 0.03834470\n",
      "Iteration 6616, loss = 0.03833460\n",
      "Iteration 6617, loss = 0.03832450\n",
      "Iteration 6618, loss = 0.03831440\n",
      "Iteration 6619, loss = 0.03830431\n",
      "Iteration 6620, loss = 0.03829422\n",
      "Iteration 6621, loss = 0.03828413\n",
      "Iteration 6622, loss = 0.03827404\n",
      "Iteration 6623, loss = 0.03826395\n",
      "Iteration 6624, loss = 0.03825387\n",
      "Iteration 6625, loss = 0.03824379\n",
      "Iteration 6626, loss = 0.03823371\n",
      "Iteration 6627, loss = 0.03822364\n",
      "Iteration 6628, loss = 0.03821357\n",
      "Iteration 6629, loss = 0.03820350\n",
      "Iteration 6630, loss = 0.03819343\n",
      "Iteration 6631, loss = 0.03818336\n",
      "Iteration 6632, loss = 0.03817330\n",
      "Iteration 6633, loss = 0.03816324\n",
      "Iteration 6634, loss = 0.03815318\n",
      "Iteration 6635, loss = 0.03814312\n",
      "Iteration 6636, loss = 0.03813307\n",
      "Iteration 6637, loss = 0.03812302\n",
      "Iteration 6638, loss = 0.03811297\n",
      "Iteration 6639, loss = 0.03810293\n",
      "Iteration 6640, loss = 0.03809288\n",
      "Iteration 6641, loss = 0.03808284\n",
      "Iteration 6642, loss = 0.03807280\n",
      "Iteration 6643, loss = 0.03806277\n",
      "Iteration 6644, loss = 0.03805273\n",
      "Iteration 6645, loss = 0.03804270\n",
      "Iteration 6646, loss = 0.03803267\n",
      "Iteration 6647, loss = 0.03802264\n",
      "Iteration 6648, loss = 0.03801262\n",
      "Iteration 6649, loss = 0.03800260\n",
      "Iteration 6650, loss = 0.03799258\n",
      "Iteration 6651, loss = 0.03798256\n",
      "Iteration 6652, loss = 0.03797255\n",
      "Iteration 6653, loss = 0.03796254\n",
      "Iteration 6654, loss = 0.03795253\n",
      "Iteration 6655, loss = 0.03794252\n",
      "Iteration 6656, loss = 0.03793251\n",
      "Iteration 6657, loss = 0.03792251\n",
      "Iteration 6658, loss = 0.03791251\n",
      "Iteration 6659, loss = 0.03790252\n",
      "Iteration 6660, loss = 0.03789252\n",
      "Iteration 6661, loss = 0.03788253\n",
      "Iteration 6662, loss = 0.03787254\n",
      "Iteration 6663, loss = 0.03786255\n",
      "Iteration 6664, loss = 0.03785257\n",
      "Iteration 6665, loss = 0.03784259\n",
      "Iteration 6666, loss = 0.03783261\n",
      "Iteration 6667, loss = 0.03782263\n",
      "Iteration 6668, loss = 0.03781265\n",
      "Iteration 6669, loss = 0.03780268\n",
      "Iteration 6670, loss = 0.03779271\n",
      "Iteration 6671, loss = 0.03778274\n",
      "Iteration 6672, loss = 0.03777278\n",
      "Iteration 6673, loss = 0.03776282\n",
      "Iteration 6674, loss = 0.03775286\n",
      "Iteration 6675, loss = 0.03774290\n",
      "Iteration 6676, loss = 0.03773295\n",
      "Iteration 6677, loss = 0.03772299\n",
      "Iteration 6678, loss = 0.03771304\n",
      "Iteration 6679, loss = 0.03770310\n",
      "Iteration 6680, loss = 0.03769315\n",
      "Iteration 6681, loss = 0.03768321\n",
      "Iteration 6682, loss = 0.03767327\n",
      "Iteration 6683, loss = 0.03766333\n",
      "Iteration 6684, loss = 0.03765340\n",
      "Iteration 6685, loss = 0.03764347\n",
      "Iteration 6686, loss = 0.03763354\n",
      "Iteration 6687, loss = 0.03762361\n",
      "Iteration 6688, loss = 0.03761369\n",
      "Iteration 6689, loss = 0.03760377\n",
      "Iteration 6690, loss = 0.03759385\n",
      "Iteration 6691, loss = 0.03758393\n",
      "Iteration 6692, loss = 0.03757402\n",
      "Iteration 6693, loss = 0.03756411\n",
      "Iteration 6694, loss = 0.03755420\n",
      "Iteration 6695, loss = 0.03754429\n",
      "Iteration 6696, loss = 0.03753439\n",
      "Iteration 6697, loss = 0.03752449\n",
      "Iteration 6698, loss = 0.03751459\n",
      "Iteration 6699, loss = 0.03750469\n",
      "Iteration 6700, loss = 0.03749480\n",
      "Iteration 6701, loss = 0.03748491\n",
      "Iteration 6702, loss = 0.03747502\n",
      "Iteration 6703, loss = 0.03746514\n",
      "Iteration 6704, loss = 0.03745525\n",
      "Iteration 6705, loss = 0.03744537\n",
      "Iteration 6706, loss = 0.03743550\n",
      "Iteration 6707, loss = 0.03742562\n",
      "Iteration 6708, loss = 0.03741575\n",
      "Iteration 6709, loss = 0.03740588\n",
      "Iteration 6710, loss = 0.03739601\n",
      "Iteration 6711, loss = 0.03738615\n",
      "Iteration 6712, loss = 0.03737629\n",
      "Iteration 6713, loss = 0.03736643\n",
      "Iteration 6714, loss = 0.03735657\n",
      "Iteration 6715, loss = 0.03734672\n",
      "Iteration 6716, loss = 0.03733687\n",
      "Iteration 6717, loss = 0.03732702\n",
      "Iteration 6718, loss = 0.03731717\n",
      "Iteration 6719, loss = 0.03730733\n",
      "Iteration 6720, loss = 0.03729749\n",
      "Iteration 6721, loss = 0.03728765\n",
      "Iteration 6722, loss = 0.03727782\n",
      "Iteration 6723, loss = 0.03726798\n",
      "Iteration 6724, loss = 0.03725815\n",
      "Iteration 6725, loss = 0.03724833\n",
      "Iteration 6726, loss = 0.03723850\n",
      "Iteration 6727, loss = 0.03722868\n",
      "Iteration 6728, loss = 0.03721886\n",
      "Iteration 6729, loss = 0.03720904\n",
      "Iteration 6730, loss = 0.03719923\n",
      "Iteration 6731, loss = 0.03718942\n",
      "Iteration 6732, loss = 0.03717961\n",
      "Iteration 6733, loss = 0.03716981\n",
      "Iteration 6734, loss = 0.03716000\n",
      "Iteration 6735, loss = 0.03715020\n",
      "Iteration 6736, loss = 0.03714041\n",
      "Iteration 6737, loss = 0.03713061\n",
      "Iteration 6738, loss = 0.03712082\n",
      "Iteration 6739, loss = 0.03711103\n",
      "Iteration 6740, loss = 0.03710125\n",
      "Iteration 6741, loss = 0.03709146\n",
      "Iteration 6742, loss = 0.03708168\n",
      "Iteration 6743, loss = 0.03707190\n",
      "Iteration 6744, loss = 0.03706213\n",
      "Iteration 6745, loss = 0.03705235\n",
      "Iteration 6746, loss = 0.03704259\n",
      "Iteration 6747, loss = 0.03703282\n",
      "Iteration 6748, loss = 0.03702305\n",
      "Iteration 6749, loss = 0.03701329\n",
      "Iteration 6750, loss = 0.03700353\n",
      "Iteration 6751, loss = 0.03699378\n",
      "Iteration 6752, loss = 0.03698403\n",
      "Iteration 6753, loss = 0.03697427\n",
      "Iteration 6754, loss = 0.03696453\n",
      "Iteration 6755, loss = 0.03695478\n",
      "Iteration 6756, loss = 0.03694504\n",
      "Iteration 6757, loss = 0.03693530\n",
      "Iteration 6758, loss = 0.03692557\n",
      "Iteration 6759, loss = 0.03691583\n",
      "Iteration 6760, loss = 0.03690610\n",
      "Iteration 6761, loss = 0.03689637\n",
      "Iteration 6762, loss = 0.03688665\n",
      "Iteration 6763, loss = 0.03687693\n",
      "Iteration 6764, loss = 0.03686721\n",
      "Iteration 6765, loss = 0.03685749\n",
      "Iteration 6766, loss = 0.03684778\n",
      "Iteration 6767, loss = 0.03683807\n",
      "Iteration 6768, loss = 0.03682836\n",
      "Iteration 6769, loss = 0.03681866\n",
      "Iteration 6770, loss = 0.03680895\n",
      "Iteration 6771, loss = 0.03679926\n",
      "Iteration 6772, loss = 0.03678956\n",
      "Iteration 6773, loss = 0.03677987\n",
      "Iteration 6774, loss = 0.03677018\n",
      "Iteration 6775, loss = 0.03676049\n",
      "Iteration 6776, loss = 0.03675080\n",
      "Iteration 6777, loss = 0.03674112\n",
      "Iteration 6778, loss = 0.03673144\n",
      "Iteration 6779, loss = 0.03672177\n",
      "Iteration 6780, loss = 0.03671209\n",
      "Iteration 6781, loss = 0.03670242\n",
      "Iteration 6782, loss = 0.03669276\n",
      "Iteration 6783, loss = 0.03668309\n",
      "Iteration 6784, loss = 0.03667343\n",
      "Iteration 6785, loss = 0.03666377\n",
      "Iteration 6786, loss = 0.03665412\n",
      "Iteration 6787, loss = 0.03664447\n",
      "Iteration 6788, loss = 0.03663482\n",
      "Iteration 6789, loss = 0.03662517\n",
      "Iteration 6790, loss = 0.03661553\n",
      "Iteration 6791, loss = 0.03660589\n",
      "Iteration 6792, loss = 0.03659625\n",
      "Iteration 6793, loss = 0.03658662\n",
      "Iteration 6794, loss = 0.03657698\n",
      "Iteration 6795, loss = 0.03656736\n",
      "Iteration 6796, loss = 0.03655773\n",
      "Iteration 6797, loss = 0.03654811\n",
      "Iteration 6798, loss = 0.03653849\n",
      "Iteration 6799, loss = 0.03652887\n",
      "Iteration 6800, loss = 0.03651926\n",
      "Iteration 6801, loss = 0.03650965\n",
      "Iteration 6802, loss = 0.03650004\n",
      "Iteration 6803, loss = 0.03649044\n",
      "Iteration 6804, loss = 0.03648084\n",
      "Iteration 6805, loss = 0.03647124\n",
      "Iteration 6806, loss = 0.03646164\n",
      "Iteration 6807, loss = 0.03645205\n",
      "Iteration 6808, loss = 0.03644246\n",
      "Iteration 6809, loss = 0.03643288\n",
      "Iteration 6810, loss = 0.03642329\n",
      "Iteration 6811, loss = 0.03641371\n",
      "Iteration 6812, loss = 0.03640414\n",
      "Iteration 6813, loss = 0.03639456\n",
      "Iteration 6814, loss = 0.03638499\n",
      "Iteration 6815, loss = 0.03637542\n",
      "Iteration 6816, loss = 0.03636586\n",
      "Iteration 6817, loss = 0.03635630\n",
      "Iteration 6818, loss = 0.03634674\n",
      "Iteration 6819, loss = 0.03633719\n",
      "Iteration 6820, loss = 0.03632763\n",
      "Iteration 6821, loss = 0.03631809\n",
      "Iteration 6822, loss = 0.03630854\n",
      "Iteration 6823, loss = 0.03629900\n",
      "Iteration 6824, loss = 0.03628946\n",
      "Iteration 6825, loss = 0.03627992\n",
      "Iteration 6826, loss = 0.03627039\n",
      "Iteration 6827, loss = 0.03626086\n",
      "Iteration 6828, loss = 0.03625133\n",
      "Iteration 6829, loss = 0.03624181\n",
      "Iteration 6830, loss = 0.03623229\n",
      "Iteration 6831, loss = 0.03622277\n",
      "Iteration 6832, loss = 0.03621326\n",
      "Iteration 6833, loss = 0.03620375\n",
      "Iteration 6834, loss = 0.03619424\n",
      "Iteration 6835, loss = 0.03618473\n",
      "Iteration 6836, loss = 0.03617523\n",
      "Iteration 6837, loss = 0.03616574\n",
      "Iteration 6838, loss = 0.03615624\n",
      "Iteration 6839, loss = 0.03614675\n",
      "Iteration 6840, loss = 0.03613726\n",
      "Iteration 6841, loss = 0.03612778\n",
      "Iteration 6842, loss = 0.03611829\n",
      "Iteration 6843, loss = 0.03610881\n",
      "Iteration 6844, loss = 0.03609934\n",
      "Iteration 6845, loss = 0.03608987\n",
      "Iteration 6846, loss = 0.03608040\n",
      "Iteration 6847, loss = 0.03607093\n",
      "Iteration 6848, loss = 0.03606147\n",
      "Iteration 6849, loss = 0.03605201\n",
      "Iteration 6850, loss = 0.03604256\n",
      "Iteration 6851, loss = 0.03603310\n",
      "Iteration 6852, loss = 0.03602365\n",
      "Iteration 6853, loss = 0.03601421\n",
      "Iteration 6854, loss = 0.03600476\n",
      "Iteration 6855, loss = 0.03599533\n",
      "Iteration 6856, loss = 0.03598589\n",
      "Iteration 6857, loss = 0.03597646\n",
      "Iteration 6858, loss = 0.03596703\n",
      "Iteration 6859, loss = 0.03595760\n",
      "Iteration 6860, loss = 0.03594818\n",
      "Iteration 6861, loss = 0.03593876\n",
      "Iteration 6862, loss = 0.03592934\n",
      "Iteration 6863, loss = 0.03591993\n",
      "Iteration 6864, loss = 0.03591052\n",
      "Iteration 6865, loss = 0.03590111\n",
      "Iteration 6866, loss = 0.03589171\n",
      "Iteration 6867, loss = 0.03588231\n",
      "Iteration 6868, loss = 0.03587292\n",
      "Iteration 6869, loss = 0.03586352\n",
      "Iteration 6870, loss = 0.03585413\n",
      "Iteration 6871, loss = 0.03584475\n",
      "Iteration 6872, loss = 0.03583536\n",
      "Iteration 6873, loss = 0.03582599\n",
      "Iteration 6874, loss = 0.03581661\n",
      "Iteration 6875, loss = 0.03580724\n",
      "Iteration 6876, loss = 0.03579787\n",
      "Iteration 6877, loss = 0.03578850\n",
      "Iteration 6878, loss = 0.03577914\n",
      "Iteration 6879, loss = 0.03576978\n",
      "Iteration 6880, loss = 0.03576043\n",
      "Iteration 6881, loss = 0.03575107\n",
      "Iteration 6882, loss = 0.03574172\n",
      "Iteration 6883, loss = 0.03573238\n",
      "Iteration 6884, loss = 0.03572304\n",
      "Iteration 6885, loss = 0.03571370\n",
      "Iteration 6886, loss = 0.03570436\n",
      "Iteration 6887, loss = 0.03569503\n",
      "Iteration 6888, loss = 0.03568571\n",
      "Iteration 6889, loss = 0.03567638\n",
      "Iteration 6890, loss = 0.03566706\n",
      "Iteration 6891, loss = 0.03565774\n",
      "Iteration 6892, loss = 0.03564843\n",
      "Iteration 6893, loss = 0.03563912\n",
      "Iteration 6894, loss = 0.03562981\n",
      "Iteration 6895, loss = 0.03562051\n",
      "Iteration 6896, loss = 0.03561121\n",
      "Iteration 6897, loss = 0.03560191\n",
      "Iteration 6898, loss = 0.03559262\n",
      "Iteration 6899, loss = 0.03558333\n",
      "Iteration 6900, loss = 0.03557404\n",
      "Iteration 6901, loss = 0.03556476\n",
      "Iteration 6902, loss = 0.03555548\n",
      "Iteration 6903, loss = 0.03554620\n",
      "Iteration 6904, loss = 0.03553693\n",
      "Iteration 6905, loss = 0.03552766\n",
      "Iteration 6906, loss = 0.03551840\n",
      "Iteration 6907, loss = 0.03550914\n",
      "Iteration 6908, loss = 0.03549988\n",
      "Iteration 6909, loss = 0.03549062\n",
      "Iteration 6910, loss = 0.03548137\n",
      "Iteration 6911, loss = 0.03547212\n",
      "Iteration 6912, loss = 0.03546288\n",
      "Iteration 6913, loss = 0.03545364\n",
      "Iteration 6914, loss = 0.03544440\n",
      "Iteration 6915, loss = 0.03543517\n",
      "Iteration 6916, loss = 0.03542594\n",
      "Iteration 6917, loss = 0.03541671\n",
      "Iteration 6918, loss = 0.03540749\n",
      "Iteration 6919, loss = 0.03539827\n",
      "Iteration 6920, loss = 0.03538906\n",
      "Iteration 6921, loss = 0.03537985\n",
      "Iteration 6922, loss = 0.03537064\n",
      "Iteration 6923, loss = 0.03536143\n",
      "Iteration 6924, loss = 0.03535223\n",
      "Iteration 6925, loss = 0.03534303\n",
      "Iteration 6926, loss = 0.03533384\n",
      "Iteration 6927, loss = 0.03532465\n",
      "Iteration 6928, loss = 0.03531546\n",
      "Iteration 6929, loss = 0.03530628\n",
      "Iteration 6930, loss = 0.03529710\n",
      "Iteration 6931, loss = 0.03528793\n",
      "Iteration 6932, loss = 0.03527875\n",
      "Iteration 6933, loss = 0.03526959\n",
      "Iteration 6934, loss = 0.03526042\n",
      "Iteration 6935, loss = 0.03525126\n",
      "Iteration 6936, loss = 0.03524210\n",
      "Iteration 6937, loss = 0.03523295\n",
      "Iteration 6938, loss = 0.03522380\n",
      "Iteration 6939, loss = 0.03521465\n",
      "Iteration 6940, loss = 0.03520551\n",
      "Iteration 6941, loss = 0.03519637\n",
      "Iteration 6942, loss = 0.03518723\n",
      "Iteration 6943, loss = 0.03517810\n",
      "Iteration 6944, loss = 0.03516897\n",
      "Iteration 6945, loss = 0.03515985\n",
      "Iteration 6946, loss = 0.03515073\n",
      "Iteration 6947, loss = 0.03514161\n",
      "Iteration 6948, loss = 0.03513250\n",
      "Iteration 6949, loss = 0.03512339\n",
      "Iteration 6950, loss = 0.03511428\n",
      "Iteration 6951, loss = 0.03510518\n",
      "Iteration 6952, loss = 0.03509608\n",
      "Iteration 6953, loss = 0.03508698\n",
      "Iteration 6954, loss = 0.03507789\n",
      "Iteration 6955, loss = 0.03506880\n",
      "Iteration 6956, loss = 0.03505972\n",
      "Iteration 6957, loss = 0.03505064\n",
      "Iteration 6958, loss = 0.03504156\n",
      "Iteration 6959, loss = 0.03503249\n",
      "Iteration 6960, loss = 0.03502342\n",
      "Iteration 6961, loss = 0.03501436\n",
      "Iteration 6962, loss = 0.03500529\n",
      "Iteration 6963, loss = 0.03499624\n",
      "Iteration 6964, loss = 0.03498718\n",
      "Iteration 6965, loss = 0.03497813\n",
      "Iteration 6966, loss = 0.03496908\n",
      "Iteration 6967, loss = 0.03496004\n",
      "Iteration 6968, loss = 0.03495100\n",
      "Iteration 6969, loss = 0.03494196\n",
      "Iteration 6970, loss = 0.03493293\n",
      "Iteration 6971, loss = 0.03492390\n",
      "Iteration 6972, loss = 0.03491488\n",
      "Iteration 6973, loss = 0.03490586\n",
      "Iteration 6974, loss = 0.03489684\n",
      "Iteration 6975, loss = 0.03488783\n",
      "Iteration 6976, loss = 0.03487882\n",
      "Iteration 6977, loss = 0.03486981\n",
      "Iteration 6978, loss = 0.03486081\n",
      "Iteration 6979, loss = 0.03485181\n",
      "Iteration 6980, loss = 0.03484282\n",
      "Iteration 6981, loss = 0.03483383\n",
      "Iteration 6982, loss = 0.03482484\n",
      "Iteration 6983, loss = 0.03481586\n",
      "Iteration 6984, loss = 0.03480688\n",
      "Iteration 6985, loss = 0.03479790\n",
      "Iteration 6986, loss = 0.03478893\n",
      "Iteration 6987, loss = 0.03477996\n",
      "Iteration 6988, loss = 0.03477100\n",
      "Iteration 6989, loss = 0.03476204\n",
      "Iteration 6990, loss = 0.03475308\n",
      "Iteration 6991, loss = 0.03474413\n",
      "Iteration 6992, loss = 0.03473518\n",
      "Iteration 6993, loss = 0.03472623\n",
      "Iteration 6994, loss = 0.03471729\n",
      "Iteration 6995, loss = 0.03470835\n",
      "Iteration 6996, loss = 0.03469942\n",
      "Iteration 6997, loss = 0.03469049\n",
      "Iteration 6998, loss = 0.03468156\n",
      "Iteration 6999, loss = 0.03467264\n",
      "Iteration 7000, loss = 0.03466372\n",
      "Iteration 7001, loss = 0.03465481\n",
      "Iteration 7002, loss = 0.03464590\n",
      "Iteration 7003, loss = 0.03463699\n",
      "Iteration 7004, loss = 0.03462808\n",
      "Iteration 7005, loss = 0.03461919\n",
      "Iteration 7006, loss = 0.03461029\n",
      "Iteration 7007, loss = 0.03460140\n",
      "Iteration 7008, loss = 0.03459251\n",
      "Iteration 7009, loss = 0.03458362\n",
      "Iteration 7010, loss = 0.03457474\n",
      "Iteration 7011, loss = 0.03456587\n",
      "Iteration 7012, loss = 0.03455699\n",
      "Iteration 7013, loss = 0.03454812\n",
      "Iteration 7014, loss = 0.03453926\n",
      "Iteration 7015, loss = 0.03453040\n",
      "Iteration 7016, loss = 0.03452154\n",
      "Iteration 7017, loss = 0.03451269\n",
      "Iteration 7018, loss = 0.03450384\n",
      "Iteration 7019, loss = 0.03449499\n",
      "Iteration 7020, loss = 0.03448615\n",
      "Iteration 7021, loss = 0.03447731\n",
      "Iteration 7022, loss = 0.03446847\n",
      "Iteration 7023, loss = 0.03445964\n",
      "Iteration 7024, loss = 0.03445082\n",
      "Iteration 7025, loss = 0.03444199\n",
      "Iteration 7026, loss = 0.03443317\n",
      "Iteration 7027, loss = 0.03442436\n",
      "Iteration 7028, loss = 0.03441555\n",
      "Iteration 7029, loss = 0.03440674\n",
      "Iteration 7030, loss = 0.03439793\n",
      "Iteration 7031, loss = 0.03438913\n",
      "Iteration 7032, loss = 0.03438034\n",
      "Iteration 7033, loss = 0.03437154\n",
      "Iteration 7034, loss = 0.03436276\n",
      "Iteration 7035, loss = 0.03435397\n",
      "Iteration 7036, loss = 0.03434519\n",
      "Iteration 7037, loss = 0.03433641\n",
      "Iteration 7038, loss = 0.03432764\n",
      "Iteration 7039, loss = 0.03431887\n",
      "Iteration 7040, loss = 0.03431011\n",
      "Iteration 7041, loss = 0.03430134\n",
      "Iteration 7042, loss = 0.03429259\n",
      "Iteration 7043, loss = 0.03428383\n",
      "Iteration 7044, loss = 0.03427508\n",
      "Iteration 7045, loss = 0.03426634\n",
      "Iteration 7046, loss = 0.03425759\n",
      "Iteration 7047, loss = 0.03424886\n",
      "Iteration 7048, loss = 0.03424012\n",
      "Iteration 7049, loss = 0.03423139\n",
      "Iteration 7050, loss = 0.03422266\n",
      "Iteration 7051, loss = 0.03421394\n",
      "Iteration 7052, loss = 0.03420522\n",
      "Iteration 7053, loss = 0.03419651\n",
      "Iteration 7054, loss = 0.03418780\n",
      "Iteration 7055, loss = 0.03417909\n",
      "Iteration 7056, loss = 0.03417038\n",
      "Iteration 7057, loss = 0.03416168\n",
      "Iteration 7058, loss = 0.03415299\n",
      "Iteration 7059, loss = 0.03414430\n",
      "Iteration 7060, loss = 0.03413561\n",
      "Iteration 7061, loss = 0.03412692\n",
      "Iteration 7062, loss = 0.03411824\n",
      "Iteration 7063, loss = 0.03410957\n",
      "Iteration 7064, loss = 0.03410090\n",
      "Iteration 7065, loss = 0.03409223\n",
      "Iteration 7066, loss = 0.03408356\n",
      "Iteration 7067, loss = 0.03407490\n",
      "Iteration 7068, loss = 0.03406624\n",
      "Iteration 7069, loss = 0.03405759\n",
      "Iteration 7070, loss = 0.03404894\n",
      "Iteration 7071, loss = 0.03404030\n",
      "Iteration 7072, loss = 0.03403165\n",
      "Iteration 7073, loss = 0.03402302\n",
      "Iteration 7074, loss = 0.03401438\n",
      "Iteration 7075, loss = 0.03400575\n",
      "Iteration 7076, loss = 0.03399713\n",
      "Iteration 7077, loss = 0.03398851\n",
      "Iteration 7078, loss = 0.03397989\n",
      "Iteration 7079, loss = 0.03397127\n",
      "Iteration 7080, loss = 0.03396266\n",
      "Iteration 7081, loss = 0.03395406\n",
      "Iteration 7082, loss = 0.03394545\n",
      "Iteration 7083, loss = 0.03393685\n",
      "Iteration 7084, loss = 0.03392826\n",
      "Iteration 7085, loss = 0.03391967\n",
      "Iteration 7086, loss = 0.03391108\n",
      "Iteration 7087, loss = 0.03390250\n",
      "Iteration 7088, loss = 0.03389392\n",
      "Iteration 7089, loss = 0.03388534\n",
      "Iteration 7090, loss = 0.03387677\n",
      "Iteration 7091, loss = 0.03386821\n",
      "Iteration 7092, loss = 0.03385964\n",
      "Iteration 7093, loss = 0.03385108\n",
      "Iteration 7094, loss = 0.03384253\n",
      "Iteration 7095, loss = 0.03383397\n",
      "Iteration 7096, loss = 0.03382543\n",
      "Iteration 7097, loss = 0.03381688\n",
      "Iteration 7098, loss = 0.03380834\n",
      "Iteration 7099, loss = 0.03379981\n",
      "Iteration 7100, loss = 0.03379127\n",
      "Iteration 7101, loss = 0.03378274\n",
      "Iteration 7102, loss = 0.03377422\n",
      "Iteration 7103, loss = 0.03376570\n",
      "Iteration 7104, loss = 0.03375718\n",
      "Iteration 7105, loss = 0.03374867\n",
      "Iteration 7106, loss = 0.03374016\n",
      "Iteration 7107, loss = 0.03373165\n",
      "Iteration 7108, loss = 0.03372315\n",
      "Iteration 7109, loss = 0.03371466\n",
      "Iteration 7110, loss = 0.03370616\n",
      "Iteration 7111, loss = 0.03369767\n",
      "Iteration 7112, loss = 0.03368919\n",
      "Iteration 7113, loss = 0.03368071\n",
      "Iteration 7114, loss = 0.03367223\n",
      "Iteration 7115, loss = 0.03366375\n",
      "Iteration 7116, loss = 0.03365528\n",
      "Iteration 7117, loss = 0.03364682\n",
      "Iteration 7118, loss = 0.03363836\n",
      "Iteration 7119, loss = 0.03362990\n",
      "Iteration 7120, loss = 0.03362144\n",
      "Iteration 7121, loss = 0.03361299\n",
      "Iteration 7122, loss = 0.03360455\n",
      "Iteration 7123, loss = 0.03359610\n",
      "Iteration 7124, loss = 0.03358766\n",
      "Iteration 7125, loss = 0.03357923\n",
      "Iteration 7126, loss = 0.03357080\n",
      "Iteration 7127, loss = 0.03356237\n",
      "Iteration 7128, loss = 0.03355395\n",
      "Iteration 7129, loss = 0.03354553\n",
      "Iteration 7130, loss = 0.03353711\n",
      "Iteration 7131, loss = 0.03352870\n",
      "Iteration 7132, loss = 0.03352029\n",
      "Iteration 7133, loss = 0.03351189\n",
      "Iteration 7134, loss = 0.03350349\n",
      "Iteration 7135, loss = 0.03349509\n",
      "Iteration 7136, loss = 0.03348670\n",
      "Iteration 7137, loss = 0.03347831\n",
      "Iteration 7138, loss = 0.03346993\n",
      "Iteration 7139, loss = 0.03346155\n",
      "Iteration 7140, loss = 0.03345317\n",
      "Iteration 7141, loss = 0.03344480\n",
      "Iteration 7142, loss = 0.03343643\n",
      "Iteration 7143, loss = 0.03342807\n",
      "Iteration 7144, loss = 0.03341971\n",
      "Iteration 7145, loss = 0.03341135\n",
      "Iteration 7146, loss = 0.03340300\n",
      "Iteration 7147, loss = 0.03339465\n",
      "Iteration 7148, loss = 0.03338630\n",
      "Iteration 7149, loss = 0.03337796\n",
      "Iteration 7150, loss = 0.03336963\n",
      "Iteration 7151, loss = 0.03336129\n",
      "Iteration 7152, loss = 0.03335296\n",
      "Iteration 7153, loss = 0.03334464\n",
      "Iteration 7154, loss = 0.03333632\n",
      "Iteration 7155, loss = 0.03332800\n",
      "Iteration 7156, loss = 0.03331969\n",
      "Iteration 7157, loss = 0.03331138\n",
      "Iteration 7158, loss = 0.03330307\n",
      "Iteration 7159, loss = 0.03329477\n",
      "Iteration 7160, loss = 0.03328647\n",
      "Iteration 7161, loss = 0.03327818\n",
      "Iteration 7162, loss = 0.03326989\n",
      "Iteration 7163, loss = 0.03326160\n",
      "Iteration 7164, loss = 0.03325332\n",
      "Iteration 7165, loss = 0.03324504\n",
      "Iteration 7166, loss = 0.03323676\n",
      "Iteration 7167, loss = 0.03322849\n",
      "Iteration 7168, loss = 0.03322023\n",
      "Iteration 7169, loss = 0.03321196\n",
      "Iteration 7170, loss = 0.03320370\n",
      "Iteration 7171, loss = 0.03319545\n",
      "Iteration 7172, loss = 0.03318720\n",
      "Iteration 7173, loss = 0.03317895\n",
      "Iteration 7174, loss = 0.03317071\n",
      "Iteration 7175, loss = 0.03316247\n",
      "Iteration 7176, loss = 0.03315423\n",
      "Iteration 7177, loss = 0.03314600\n",
      "Iteration 7178, loss = 0.03313777\n",
      "Iteration 7179, loss = 0.03312955\n",
      "Iteration 7180, loss = 0.03312133\n",
      "Iteration 7181, loss = 0.03311311\n",
      "Iteration 7182, loss = 0.03310490\n",
      "Iteration 7183, loss = 0.03309669\n",
      "Iteration 7184, loss = 0.03308849\n",
      "Iteration 7185, loss = 0.03308029\n",
      "Iteration 7186, loss = 0.03307209\n",
      "Iteration 7187, loss = 0.03306390\n",
      "Iteration 7188, loss = 0.03305571\n",
      "Iteration 7189, loss = 0.03304753\n",
      "Iteration 7190, loss = 0.03303935\n",
      "Iteration 7191, loss = 0.03303117\n",
      "Iteration 7192, loss = 0.03302300\n",
      "Iteration 7193, loss = 0.03301483\n",
      "Iteration 7194, loss = 0.03300666\n",
      "Iteration 7195, loss = 0.03299850\n",
      "Iteration 7196, loss = 0.03299035\n",
      "Iteration 7197, loss = 0.03298219\n",
      "Iteration 7198, loss = 0.03297404\n",
      "Iteration 7199, loss = 0.03296590\n",
      "Iteration 7200, loss = 0.03295776\n",
      "Iteration 7201, loss = 0.03294962\n",
      "Iteration 7202, loss = 0.03294149\n",
      "Iteration 7203, loss = 0.03293336\n",
      "Iteration 7204, loss = 0.03292523\n",
      "Iteration 7205, loss = 0.03291711\n",
      "Iteration 7206, loss = 0.03290899\n",
      "Iteration 7207, loss = 0.03290088\n",
      "Iteration 7208, loss = 0.03289277\n",
      "Iteration 7209, loss = 0.03288466\n",
      "Iteration 7210, loss = 0.03287656\n",
      "Iteration 7211, loss = 0.03286846\n",
      "Iteration 7212, loss = 0.03286037\n",
      "Iteration 7213, loss = 0.03285228\n",
      "Iteration 7214, loss = 0.03284419\n",
      "Iteration 7215, loss = 0.03283611\n",
      "Iteration 7216, loss = 0.03282803\n",
      "Iteration 7217, loss = 0.03281995\n",
      "Iteration 7218, loss = 0.03281188\n",
      "Iteration 7219, loss = 0.03280381\n",
      "Iteration 7220, loss = 0.03279575\n",
      "Iteration 7221, loss = 0.03278769\n",
      "Iteration 7222, loss = 0.03277964\n",
      "Iteration 7223, loss = 0.03277159\n",
      "Iteration 7224, loss = 0.03276354\n",
      "Iteration 7225, loss = 0.03275550\n",
      "Iteration 7226, loss = 0.03274746\n",
      "Iteration 7227, loss = 0.03273942\n",
      "Iteration 7228, loss = 0.03273139\n",
      "Iteration 7229, loss = 0.03272336\n",
      "Iteration 7230, loss = 0.03271534\n",
      "Iteration 7231, loss = 0.03270732\n",
      "Iteration 7232, loss = 0.03269930\n",
      "Iteration 7233, loss = 0.03269129\n",
      "Iteration 7234, loss = 0.03268328\n",
      "Iteration 7235, loss = 0.03267528\n",
      "Iteration 7236, loss = 0.03266728\n",
      "Iteration 7237, loss = 0.03265928\n",
      "Iteration 7238, loss = 0.03265129\n",
      "Iteration 7239, loss = 0.03264330\n",
      "Iteration 7240, loss = 0.03263532\n",
      "Iteration 7241, loss = 0.03262733\n",
      "Iteration 7242, loss = 0.03261936\n",
      "Iteration 7243, loss = 0.03261139\n",
      "Iteration 7244, loss = 0.03260342\n",
      "Iteration 7245, loss = 0.03259545\n",
      "Iteration 7246, loss = 0.03258749\n",
      "Iteration 7247, loss = 0.03257953\n",
      "Iteration 7248, loss = 0.03257158\n",
      "Iteration 7249, loss = 0.03256363\n",
      "Iteration 7250, loss = 0.03255569\n",
      "Iteration 7251, loss = 0.03254774\n",
      "Iteration 7252, loss = 0.03253981\n",
      "Iteration 7253, loss = 0.03253187\n",
      "Iteration 7254, loss = 0.03252394\n",
      "Iteration 7255, loss = 0.03251602\n",
      "Iteration 7256, loss = 0.03250810\n",
      "Iteration 7257, loss = 0.03250018\n",
      "Iteration 7258, loss = 0.03249227\n",
      "Iteration 7259, loss = 0.03248436\n",
      "Iteration 7260, loss = 0.03247645\n",
      "Iteration 7261, loss = 0.03246855\n",
      "Iteration 7262, loss = 0.03246065\n",
      "Iteration 7263, loss = 0.03245276\n",
      "Iteration 7264, loss = 0.03244487\n",
      "Iteration 7265, loss = 0.03243698\n",
      "Iteration 7266, loss = 0.03242910\n",
      "Iteration 7267, loss = 0.03242122\n",
      "Iteration 7268, loss = 0.03241334\n",
      "Iteration 7269, loss = 0.03240547\n",
      "Iteration 7270, loss = 0.03239761\n",
      "Iteration 7271, loss = 0.03238974\n",
      "Iteration 7272, loss = 0.03238189\n",
      "Iteration 7273, loss = 0.03237403\n",
      "Iteration 7274, loss = 0.03236618\n",
      "Iteration 7275, loss = 0.03235833\n",
      "Iteration 7276, loss = 0.03235049\n",
      "Iteration 7277, loss = 0.03234265\n",
      "Iteration 7278, loss = 0.03233482\n",
      "Iteration 7279, loss = 0.03232698\n",
      "Iteration 7280, loss = 0.03231916\n",
      "Iteration 7281, loss = 0.03231133\n",
      "Iteration 7282, loss = 0.03230351\n",
      "Iteration 7283, loss = 0.03229570\n",
      "Iteration 7284, loss = 0.03228789\n",
      "Iteration 7285, loss = 0.03228008\n",
      "Iteration 7286, loss = 0.03227228\n",
      "Iteration 7287, loss = 0.03226448\n",
      "Iteration 7288, loss = 0.03225668\n",
      "Iteration 7289, loss = 0.03224889\n",
      "Iteration 7290, loss = 0.03224110\n",
      "Iteration 7291, loss = 0.03223332\n",
      "Iteration 7292, loss = 0.03222554\n",
      "Iteration 7293, loss = 0.03221776\n",
      "Iteration 7294, loss = 0.03220999\n",
      "Iteration 7295, loss = 0.03220222\n",
      "Iteration 7296, loss = 0.03219446\n",
      "Iteration 7297, loss = 0.03218670\n",
      "Iteration 7298, loss = 0.03217894\n",
      "Iteration 7299, loss = 0.03217119\n",
      "Iteration 7300, loss = 0.03216344\n",
      "Iteration 7301, loss = 0.03215570\n",
      "Iteration 7302, loss = 0.03214796\n",
      "Iteration 7303, loss = 0.03214022\n",
      "Iteration 7304, loss = 0.03213249\n",
      "Iteration 7305, loss = 0.03212476\n",
      "Iteration 7306, loss = 0.03211703\n",
      "Iteration 7307, loss = 0.03210931\n",
      "Iteration 7308, loss = 0.03210159\n",
      "Iteration 7309, loss = 0.03209388\n",
      "Iteration 7310, loss = 0.03208617\n",
      "Iteration 7311, loss = 0.03207847\n",
      "Iteration 7312, loss = 0.03207077\n",
      "Iteration 7313, loss = 0.03206307\n",
      "Iteration 7314, loss = 0.03205538\n",
      "Iteration 7315, loss = 0.03204769\n",
      "Iteration 7316, loss = 0.03204000\n",
      "Iteration 7317, loss = 0.03203232\n",
      "Iteration 7318, loss = 0.03202464\n",
      "Iteration 7319, loss = 0.03201697\n",
      "Iteration 7320, loss = 0.03200930\n",
      "Iteration 7321, loss = 0.03200163\n",
      "Iteration 7322, loss = 0.03199397\n",
      "Iteration 7323, loss = 0.03198631\n",
      "Iteration 7324, loss = 0.03197866\n",
      "Iteration 7325, loss = 0.03197101\n",
      "Iteration 7326, loss = 0.03196337\n",
      "Iteration 7327, loss = 0.03195572\n",
      "Iteration 7328, loss = 0.03194809\n",
      "Iteration 7329, loss = 0.03194045\n",
      "Iteration 7330, loss = 0.03193282\n",
      "Iteration 7331, loss = 0.03192520\n",
      "Iteration 7332, loss = 0.03191757\n",
      "Iteration 7333, loss = 0.03190996\n",
      "Iteration 7334, loss = 0.03190234\n",
      "Iteration 7335, loss = 0.03189473\n",
      "Iteration 7336, loss = 0.03188712\n",
      "Iteration 7337, loss = 0.03187952\n",
      "Iteration 7338, loss = 0.03187192\n",
      "Iteration 7339, loss = 0.03186433\n",
      "Iteration 7340, loss = 0.03185674\n",
      "Iteration 7341, loss = 0.03184915\n",
      "Iteration 7342, loss = 0.03184157\n",
      "Iteration 7343, loss = 0.03183399\n",
      "Iteration 7344, loss = 0.03182642\n",
      "Iteration 7345, loss = 0.03181885\n",
      "Iteration 7346, loss = 0.03181128\n",
      "Iteration 7347, loss = 0.03180372\n",
      "Iteration 7348, loss = 0.03179616\n",
      "Iteration 7349, loss = 0.03178860\n",
      "Iteration 7350, loss = 0.03178105\n",
      "Iteration 7351, loss = 0.03177350\n",
      "Iteration 7352, loss = 0.03176596\n",
      "Iteration 7353, loss = 0.03175842\n",
      "Iteration 7354, loss = 0.03175089\n",
      "Iteration 7355, loss = 0.03174336\n",
      "Iteration 7356, loss = 0.03173583\n",
      "Iteration 7357, loss = 0.03172831\n",
      "Iteration 7358, loss = 0.03172079\n",
      "Iteration 7359, loss = 0.03171327\n",
      "Iteration 7360, loss = 0.03170576\n",
      "Iteration 7361, loss = 0.03169825\n",
      "Iteration 7362, loss = 0.03169075\n",
      "Iteration 7363, loss = 0.03168325\n",
      "Iteration 7364, loss = 0.03167575\n",
      "Iteration 7365, loss = 0.03166826\n",
      "Iteration 7366, loss = 0.03166077\n",
      "Iteration 7367, loss = 0.03165329\n",
      "Iteration 7368, loss = 0.03164581\n",
      "Iteration 7369, loss = 0.03163834\n",
      "Iteration 7370, loss = 0.03163086\n",
      "Iteration 7371, loss = 0.03162340\n",
      "Iteration 7372, loss = 0.03161593\n",
      "Iteration 7373, loss = 0.03160847\n",
      "Iteration 7374, loss = 0.03160102\n",
      "Iteration 7375, loss = 0.03159356\n",
      "Iteration 7376, loss = 0.03158612\n",
      "Iteration 7377, loss = 0.03157867\n",
      "Iteration 7378, loss = 0.03157123\n",
      "Iteration 7379, loss = 0.03156380\n",
      "Iteration 7380, loss = 0.03155636\n",
      "Iteration 7381, loss = 0.03154894\n",
      "Iteration 7382, loss = 0.03154151\n",
      "Iteration 7383, loss = 0.03153409\n",
      "Iteration 7384, loss = 0.03152667\n",
      "Iteration 7385, loss = 0.03151926\n",
      "Iteration 7386, loss = 0.03151185\n",
      "Iteration 7387, loss = 0.03150445\n",
      "Iteration 7388, loss = 0.03149705\n",
      "Iteration 7389, loss = 0.03148965\n",
      "Iteration 7390, loss = 0.03148226\n",
      "Iteration 7391, loss = 0.03147487\n",
      "Iteration 7392, loss = 0.03146749\n",
      "Iteration 7393, loss = 0.03146011\n",
      "Iteration 7394, loss = 0.03145273\n",
      "Iteration 7395, loss = 0.03144536\n",
      "Iteration 7396, loss = 0.03143799\n",
      "Iteration 7397, loss = 0.03143062\n",
      "Iteration 7398, loss = 0.03142326\n",
      "Iteration 7399, loss = 0.03141591\n",
      "Iteration 7400, loss = 0.03140855\n",
      "Iteration 7401, loss = 0.03140120\n",
      "Iteration 7402, loss = 0.03139386\n",
      "Iteration 7403, loss = 0.03138652\n",
      "Iteration 7404, loss = 0.03137918\n",
      "Iteration 7405, loss = 0.03137185\n",
      "Iteration 7406, loss = 0.03136452\n",
      "Iteration 7407, loss = 0.03135719\n",
      "Iteration 7408, loss = 0.03134987\n",
      "Iteration 7409, loss = 0.03134255\n",
      "Iteration 7410, loss = 0.03133524\n",
      "Iteration 7411, loss = 0.03132793\n",
      "Iteration 7412, loss = 0.03132063\n",
      "Iteration 7413, loss = 0.03131332\n",
      "Iteration 7414, loss = 0.03130603\n",
      "Iteration 7415, loss = 0.03129873\n",
      "Iteration 7416, loss = 0.03129144\n",
      "Iteration 7417, loss = 0.03128416\n",
      "Iteration 7418, loss = 0.03127688\n",
      "Iteration 7419, loss = 0.03126960\n",
      "Iteration 7420, loss = 0.03126233\n",
      "Iteration 7421, loss = 0.03125506\n",
      "Iteration 7422, loss = 0.03124779\n",
      "Iteration 7423, loss = 0.03124053\n",
      "Iteration 7424, loss = 0.03123327\n",
      "Iteration 7425, loss = 0.03122602\n",
      "Iteration 7426, loss = 0.03121877\n",
      "Iteration 7427, loss = 0.03121152\n",
      "Iteration 7428, loss = 0.03120428\n",
      "Iteration 7429, loss = 0.03119704\n",
      "Iteration 7430, loss = 0.03118981\n",
      "Iteration 7431, loss = 0.03118258\n",
      "Iteration 7432, loss = 0.03117535\n",
      "Iteration 7433, loss = 0.03116813\n",
      "Iteration 7434, loss = 0.03116091\n",
      "Iteration 7435, loss = 0.03115370\n",
      "Iteration 7436, loss = 0.03114649\n",
      "Iteration 7437, loss = 0.03113928\n",
      "Iteration 7438, loss = 0.03113208\n",
      "Iteration 7439, loss = 0.03112488\n",
      "Iteration 7440, loss = 0.03111768\n",
      "Iteration 7441, loss = 0.03111049\n",
      "Iteration 7442, loss = 0.03110331\n",
      "Iteration 7443, loss = 0.03109612\n",
      "Iteration 7444, loss = 0.03108895\n",
      "Iteration 7445, loss = 0.03108177\n",
      "Iteration 7446, loss = 0.03107460\n",
      "Iteration 7447, loss = 0.03106743\n",
      "Iteration 7448, loss = 0.03106027\n",
      "Iteration 7449, loss = 0.03105311\n",
      "Iteration 7450, loss = 0.03104596\n",
      "Iteration 7451, loss = 0.03103881\n",
      "Iteration 7452, loss = 0.03103166\n",
      "Iteration 7453, loss = 0.03102452\n",
      "Iteration 7454, loss = 0.03101738\n",
      "Iteration 7455, loss = 0.03101024\n",
      "Iteration 7456, loss = 0.03100311\n",
      "Iteration 7457, loss = 0.03099598\n",
      "Iteration 7458, loss = 0.03098886\n",
      "Iteration 7459, loss = 0.03098174\n",
      "Iteration 7460, loss = 0.03097463\n",
      "Iteration 7461, loss = 0.03096752\n",
      "Iteration 7462, loss = 0.03096041\n",
      "Iteration 7463, loss = 0.03095330\n",
      "Iteration 7464, loss = 0.03094620\n",
      "Iteration 7465, loss = 0.03093911\n",
      "Iteration 7466, loss = 0.03093202\n",
      "Iteration 7467, loss = 0.03092493\n",
      "Iteration 7468, loss = 0.03091785\n",
      "Iteration 7469, loss = 0.03091077\n",
      "Iteration 7470, loss = 0.03090369\n",
      "Iteration 7471, loss = 0.03089662\n",
      "Iteration 7472, loss = 0.03088955\n",
      "Iteration 7473, loss = 0.03088249\n",
      "Iteration 7474, loss = 0.03087543\n",
      "Iteration 7475, loss = 0.03086837\n",
      "Iteration 7476, loss = 0.03086132\n",
      "Iteration 7477, loss = 0.03085427\n",
      "Iteration 7478, loss = 0.03084723\n",
      "Iteration 7479, loss = 0.03084019\n",
      "Iteration 7480, loss = 0.03083315\n",
      "Iteration 7481, loss = 0.03082612\n",
      "Iteration 7482, loss = 0.03081909\n",
      "Iteration 7483, loss = 0.03081207\n",
      "Iteration 7484, loss = 0.03080504\n",
      "Iteration 7485, loss = 0.03079803\n",
      "Iteration 7486, loss = 0.03079102\n",
      "Iteration 7487, loss = 0.03078401\n",
      "Iteration 7488, loss = 0.03077700\n",
      "Iteration 7489, loss = 0.03077000\n",
      "Iteration 7490, loss = 0.03076300\n",
      "Iteration 7491, loss = 0.03075601\n",
      "Iteration 7492, loss = 0.03074902\n",
      "Iteration 7493, loss = 0.03074204\n",
      "Iteration 7494, loss = 0.03073506\n",
      "Iteration 7495, loss = 0.03072808\n",
      "Iteration 7496, loss = 0.03072111\n",
      "Iteration 7497, loss = 0.03071414\n",
      "Iteration 7498, loss = 0.03070717\n",
      "Iteration 7499, loss = 0.03070021\n",
      "Iteration 7500, loss = 0.03069325\n",
      "Iteration 7501, loss = 0.03068630\n",
      "Iteration 7502, loss = 0.03067935\n",
      "Iteration 7503, loss = 0.03067240\n",
      "Iteration 7504, loss = 0.03066546\n",
      "Iteration 7505, loss = 0.03065852\n",
      "Iteration 7506, loss = 0.03065159\n",
      "Iteration 7507, loss = 0.03064466\n",
      "Iteration 7508, loss = 0.03063774\n",
      "Iteration 7509, loss = 0.03063081\n",
      "Iteration 7510, loss = 0.03062389\n",
      "Iteration 7511, loss = 0.03061698\n",
      "Iteration 7512, loss = 0.03061007\n",
      "Iteration 7513, loss = 0.03060316\n",
      "Iteration 7514, loss = 0.03059626\n",
      "Iteration 7515, loss = 0.03058936\n",
      "Iteration 7516, loss = 0.03058247\n",
      "Iteration 7517, loss = 0.03057558\n",
      "Iteration 7518, loss = 0.03056869\n",
      "Iteration 7519, loss = 0.03056181\n",
      "Iteration 7520, loss = 0.03055493\n",
      "Iteration 7521, loss = 0.03054806\n",
      "Iteration 7522, loss = 0.03054118\n",
      "Iteration 7523, loss = 0.03053432\n",
      "Iteration 7524, loss = 0.03052745\n",
      "Iteration 7525, loss = 0.03052059\n",
      "Iteration 7526, loss = 0.03051374\n",
      "Iteration 7527, loss = 0.03050689\n",
      "Iteration 7528, loss = 0.03050004\n",
      "Iteration 7529, loss = 0.03049320\n",
      "Iteration 7530, loss = 0.03048636\n",
      "Iteration 7531, loss = 0.03047952\n",
      "Iteration 7532, loss = 0.03047269\n",
      "Iteration 7533, loss = 0.03046586\n",
      "Iteration 7534, loss = 0.03045904\n",
      "Iteration 7535, loss = 0.03045222\n",
      "Iteration 7536, loss = 0.03044540\n",
      "Iteration 7537, loss = 0.03043859\n",
      "Iteration 7538, loss = 0.03043178\n",
      "Iteration 7539, loss = 0.03042498\n",
      "Iteration 7540, loss = 0.03041818\n",
      "Iteration 7541, loss = 0.03041138\n",
      "Iteration 7542, loss = 0.03040459\n",
      "Iteration 7543, loss = 0.03039780\n",
      "Iteration 7544, loss = 0.03039101\n",
      "Iteration 7545, loss = 0.03038423\n",
      "Iteration 7546, loss = 0.03037746\n",
      "Iteration 7547, loss = 0.03037068\n",
      "Iteration 7548, loss = 0.03036391\n",
      "Iteration 7549, loss = 0.03035715\n",
      "Iteration 7550, loss = 0.03035039\n",
      "Iteration 7551, loss = 0.03034363\n",
      "Iteration 7552, loss = 0.03033688\n",
      "Iteration 7553, loss = 0.03033013\n",
      "Iteration 7554, loss = 0.03032338\n",
      "Iteration 7555, loss = 0.03031664\n",
      "Iteration 7556, loss = 0.03030990\n",
      "Iteration 7557, loss = 0.03030317\n",
      "Iteration 7558, loss = 0.03029644\n",
      "Iteration 7559, loss = 0.03028971\n",
      "Iteration 7560, loss = 0.03028299\n",
      "Iteration 7561, loss = 0.03027627\n",
      "Iteration 7562, loss = 0.03026955\n",
      "Iteration 7563, loss = 0.03026284\n",
      "Iteration 7564, loss = 0.03025613\n",
      "Iteration 7565, loss = 0.03024943\n",
      "Iteration 7566, loss = 0.03024273\n",
      "Iteration 7567, loss = 0.03023604\n",
      "Iteration 7568, loss = 0.03022935\n",
      "Iteration 7569, loss = 0.03022266\n",
      "Iteration 7570, loss = 0.03021597\n",
      "Iteration 7571, loss = 0.03020930\n",
      "Iteration 7572, loss = 0.03020262\n",
      "Iteration 7573, loss = 0.03019595\n",
      "Iteration 7574, loss = 0.03018928\n",
      "Iteration 7575, loss = 0.03018261\n",
      "Iteration 7576, loss = 0.03017595\n",
      "Iteration 7577, loss = 0.03016930\n",
      "Iteration 7578, loss = 0.03016265\n",
      "Iteration 7579, loss = 0.03015600\n",
      "Iteration 7580, loss = 0.03014935\n",
      "Iteration 7581, loss = 0.03014271\n",
      "Iteration 7582, loss = 0.03013607\n",
      "Iteration 7583, loss = 0.03012944\n",
      "Iteration 7584, loss = 0.03012281\n",
      "Iteration 7585, loss = 0.03011618\n",
      "Iteration 7586, loss = 0.03010956\n",
      "Iteration 7587, loss = 0.03010295\n",
      "Iteration 7588, loss = 0.03009633\n",
      "Iteration 7589, loss = 0.03008972\n",
      "Iteration 7590, loss = 0.03008311\n",
      "Iteration 7591, loss = 0.03007651\n",
      "Iteration 7592, loss = 0.03006991\n",
      "Iteration 7593, loss = 0.03006332\n",
      "Iteration 7594, loss = 0.03005673\n",
      "Iteration 7595, loss = 0.03005014\n",
      "Iteration 7596, loss = 0.03004356\n",
      "Iteration 7597, loss = 0.03003698\n",
      "Iteration 7598, loss = 0.03003040\n",
      "Iteration 7599, loss = 0.03002383\n",
      "Iteration 7600, loss = 0.03001726\n",
      "Iteration 7601, loss = 0.03001070\n",
      "Iteration 7602, loss = 0.03000414\n",
      "Iteration 7603, loss = 0.02999758\n",
      "Iteration 7604, loss = 0.02999103\n",
      "Iteration 7605, loss = 0.02998448\n",
      "Iteration 7606, loss = 0.02997794\n",
      "Iteration 7607, loss = 0.02997140\n",
      "Iteration 7608, loss = 0.02996486\n",
      "Iteration 7609, loss = 0.02995833\n",
      "Iteration 7610, loss = 0.02995180\n",
      "Iteration 7611, loss = 0.02994527\n",
      "Iteration 7612, loss = 0.02993875\n",
      "Iteration 7613, loss = 0.02993223\n",
      "Iteration 7614, loss = 0.02992572\n",
      "Iteration 7615, loss = 0.02991921\n",
      "Iteration 7616, loss = 0.02991270\n",
      "Iteration 7617, loss = 0.02990620\n",
      "Iteration 7618, loss = 0.02989970\n",
      "Iteration 7619, loss = 0.02989321\n",
      "Iteration 7620, loss = 0.02988671\n",
      "Iteration 7621, loss = 0.02988023\n",
      "Iteration 7622, loss = 0.02987374\n",
      "Iteration 7623, loss = 0.02986726\n",
      "Iteration 7624, loss = 0.02986079\n",
      "Iteration 7625, loss = 0.02985432\n",
      "Iteration 7626, loss = 0.02984785\n",
      "Iteration 7627, loss = 0.02984138\n",
      "Iteration 7628, loss = 0.02983492\n",
      "Iteration 7629, loss = 0.02982847\n",
      "Iteration 7630, loss = 0.02982201\n",
      "Iteration 7631, loss = 0.02981556\n",
      "Iteration 7632, loss = 0.02980912\n",
      "Iteration 7633, loss = 0.02980268\n",
      "Iteration 7634, loss = 0.02979624\n",
      "Iteration 7635, loss = 0.02978980\n",
      "Iteration 7636, loss = 0.02978337\n",
      "Iteration 7637, loss = 0.02977695\n",
      "Iteration 7638, loss = 0.02977053\n",
      "Iteration 7639, loss = 0.02976411\n",
      "Iteration 7640, loss = 0.02975769\n",
      "Iteration 7641, loss = 0.02975128\n",
      "Iteration 7642, loss = 0.02974487\n",
      "Iteration 7643, loss = 0.02973847\n",
      "Iteration 7644, loss = 0.02973207\n",
      "Iteration 7645, loss = 0.02972567\n",
      "Iteration 7646, loss = 0.02971928\n",
      "Iteration 7647, loss = 0.02971289\n",
      "Iteration 7648, loss = 0.02970651\n",
      "Iteration 7649, loss = 0.02970013\n",
      "Iteration 7650, loss = 0.02969375\n",
      "Iteration 7651, loss = 0.02968738\n",
      "Iteration 7652, loss = 0.02968101\n",
      "Iteration 7653, loss = 0.02967464\n",
      "Iteration 7654, loss = 0.02966828\n",
      "Iteration 7655, loss = 0.02966192\n",
      "Iteration 7656, loss = 0.02965557\n",
      "Iteration 7657, loss = 0.02964922\n",
      "Iteration 7658, loss = 0.02964287\n",
      "Iteration 7659, loss = 0.02963653\n",
      "Iteration 7660, loss = 0.02963019\n",
      "Iteration 7661, loss = 0.02962385\n",
      "Iteration 7662, loss = 0.02961752\n",
      "Iteration 7663, loss = 0.02961119\n",
      "Iteration 7664, loss = 0.02960487\n",
      "Iteration 7665, loss = 0.02959855\n",
      "Iteration 7666, loss = 0.02959223\n",
      "Iteration 7667, loss = 0.02958591\n",
      "Iteration 7668, loss = 0.02957961\n",
      "Iteration 7669, loss = 0.02957330\n",
      "Iteration 7670, loss = 0.02956700\n",
      "Iteration 7671, loss = 0.02956070\n",
      "Iteration 7672, loss = 0.02955441\n",
      "Iteration 7673, loss = 0.02954811\n",
      "Iteration 7674, loss = 0.02954183\n",
      "Iteration 7675, loss = 0.02953554\n",
      "Iteration 7676, loss = 0.02952926\n",
      "Iteration 7677, loss = 0.02952299\n",
      "Iteration 7678, loss = 0.02951672\n",
      "Iteration 7679, loss = 0.02951045\n",
      "Iteration 7680, loss = 0.02950418\n",
      "Iteration 7681, loss = 0.02949792\n",
      "Iteration 7682, loss = 0.02949167\n",
      "Iteration 7683, loss = 0.02948541\n",
      "Iteration 7684, loss = 0.02947916\n",
      "Iteration 7685, loss = 0.02947292\n",
      "Iteration 7686, loss = 0.02946667\n",
      "Iteration 7687, loss = 0.02946044\n",
      "Iteration 7688, loss = 0.02945420\n",
      "Iteration 7689, loss = 0.02944797\n",
      "Iteration 7690, loss = 0.02944174\n",
      "Iteration 7691, loss = 0.02943552\n",
      "Iteration 7692, loss = 0.02942930\n",
      "Iteration 7693, loss = 0.02942308\n",
      "Iteration 7694, loss = 0.02941687\n",
      "Iteration 7695, loss = 0.02941066\n",
      "Iteration 7696, loss = 0.02940446\n",
      "Iteration 7697, loss = 0.02939826\n",
      "Iteration 7698, loss = 0.02939206\n",
      "Iteration 7699, loss = 0.02938586\n",
      "Iteration 7700, loss = 0.02937967\n",
      "Iteration 7701, loss = 0.02937349\n",
      "Iteration 7702, loss = 0.02936730\n",
      "Iteration 7703, loss = 0.02936112\n",
      "Iteration 7704, loss = 0.02935495\n",
      "Iteration 7705, loss = 0.02934878\n",
      "Iteration 7706, loss = 0.02934261\n",
      "Iteration 7707, loss = 0.02933644\n",
      "Iteration 7708, loss = 0.02933028\n",
      "Iteration 7709, loss = 0.02932413\n",
      "Iteration 7710, loss = 0.02931797\n",
      "Iteration 7711, loss = 0.02931182\n",
      "Iteration 7712, loss = 0.02930568\n",
      "Iteration 7713, loss = 0.02929953\n",
      "Iteration 7714, loss = 0.02929340\n",
      "Iteration 7715, loss = 0.02928726\n",
      "Iteration 7716, loss = 0.02928113\n",
      "Iteration 7717, loss = 0.02927500\n",
      "Iteration 7718, loss = 0.02926888\n",
      "Iteration 7719, loss = 0.02926276\n",
      "Iteration 7720, loss = 0.02925664\n",
      "Iteration 7721, loss = 0.02925053\n",
      "Iteration 7722, loss = 0.02924442\n",
      "Iteration 7723, loss = 0.02923831\n",
      "Iteration 7724, loss = 0.02923221\n",
      "Iteration 7725, loss = 0.02922611\n",
      "Iteration 7726, loss = 0.02922002\n",
      "Iteration 7727, loss = 0.02921392\n",
      "Iteration 7728, loss = 0.02920784\n",
      "Iteration 7729, loss = 0.02920175\n",
      "Iteration 7730, loss = 0.02919567\n",
      "Iteration 7731, loss = 0.02918960\n",
      "Iteration 7732, loss = 0.02918352\n",
      "Iteration 7733, loss = 0.02917745\n",
      "Iteration 7734, loss = 0.02917139\n",
      "Iteration 7735, loss = 0.02916533\n",
      "Iteration 7736, loss = 0.02915927\n",
      "Iteration 7737, loss = 0.02915321\n",
      "Iteration 7738, loss = 0.02914716\n",
      "Iteration 7739, loss = 0.02914111\n",
      "Iteration 7740, loss = 0.02913507\n",
      "Iteration 7741, loss = 0.02912903\n",
      "Iteration 7742, loss = 0.02912299\n",
      "Iteration 7743, loss = 0.02911696\n",
      "Iteration 7744, loss = 0.02911093\n",
      "Iteration 7745, loss = 0.02910490\n",
      "Iteration 7746, loss = 0.02909888\n",
      "Iteration 7747, loss = 0.02909286\n",
      "Iteration 7748, loss = 0.02908685\n",
      "Iteration 7749, loss = 0.02908084\n",
      "Iteration 7750, loss = 0.02907483\n",
      "Iteration 7751, loss = 0.02906882\n",
      "Iteration 7752, loss = 0.02906282\n",
      "Iteration 7753, loss = 0.02905683\n",
      "Iteration 7754, loss = 0.02905083\n",
      "Iteration 7755, loss = 0.02904484\n",
      "Iteration 7756, loss = 0.02903886\n",
      "Iteration 7757, loss = 0.02903287\n",
      "Iteration 7758, loss = 0.02902689\n",
      "Iteration 7759, loss = 0.02902092\n",
      "Iteration 7760, loss = 0.02901495\n",
      "Iteration 7761, loss = 0.02900898\n",
      "Iteration 7762, loss = 0.02900301\n",
      "Iteration 7763, loss = 0.02899705\n",
      "Iteration 7764, loss = 0.02899109\n",
      "Iteration 7765, loss = 0.02898514\n",
      "Iteration 7766, loss = 0.02897919\n",
      "Iteration 7767, loss = 0.02897324\n",
      "Iteration 7768, loss = 0.02896730\n",
      "Iteration 7769, loss = 0.02896136\n",
      "Iteration 7770, loss = 0.02895542\n",
      "Iteration 7771, loss = 0.02894949\n",
      "Iteration 7772, loss = 0.02894356\n",
      "Iteration 7773, loss = 0.02893763\n",
      "Iteration 7774, loss = 0.02893171\n",
      "Iteration 7775, loss = 0.02892579\n",
      "Iteration 7776, loss = 0.02891988\n",
      "Iteration 7777, loss = 0.02891397\n",
      "Iteration 7778, loss = 0.02890806\n",
      "Iteration 7779, loss = 0.02890215\n",
      "Iteration 7780, loss = 0.02889625\n",
      "Iteration 7781, loss = 0.02889036\n",
      "Iteration 7782, loss = 0.02888446\n",
      "Iteration 7783, loss = 0.02887857\n",
      "Iteration 7784, loss = 0.02887269\n",
      "Iteration 7785, loss = 0.02886680\n",
      "Iteration 7786, loss = 0.02886092\n",
      "Iteration 7787, loss = 0.02885505\n",
      "Iteration 7788, loss = 0.02884917\n",
      "Iteration 7789, loss = 0.02884331\n",
      "Iteration 7790, loss = 0.02883744\n",
      "Iteration 7791, loss = 0.02883158\n",
      "Iteration 7792, loss = 0.02882572\n",
      "Iteration 7793, loss = 0.02881986\n",
      "Iteration 7794, loss = 0.02881401\n",
      "Iteration 7795, loss = 0.02880816\n",
      "Iteration 7796, loss = 0.02880232\n",
      "Iteration 7797, loss = 0.02879648\n",
      "Iteration 7798, loss = 0.02879064\n",
      "Iteration 7799, loss = 0.02878481\n",
      "Iteration 7800, loss = 0.02877898\n",
      "Iteration 7801, loss = 0.02877315\n",
      "Iteration 7802, loss = 0.02876733\n",
      "Iteration 7803, loss = 0.02876151\n",
      "Iteration 7804, loss = 0.02875569\n",
      "Iteration 7805, loss = 0.02874988\n",
      "Iteration 7806, loss = 0.02874407\n",
      "Iteration 7807, loss = 0.02873826\n",
      "Iteration 7808, loss = 0.02873246\n",
      "Iteration 7809, loss = 0.02872666\n",
      "Iteration 7810, loss = 0.02872086\n",
      "Iteration 7811, loss = 0.02871507\n",
      "Iteration 7812, loss = 0.02870928\n",
      "Iteration 7813, loss = 0.02870350\n",
      "Iteration 7814, loss = 0.02869771\n",
      "Iteration 7815, loss = 0.02869194\n",
      "Iteration 7816, loss = 0.02868616\n",
      "Iteration 7817, loss = 0.02868039\n",
      "Iteration 7818, loss = 0.02867462\n",
      "Iteration 7819, loss = 0.02866886\n",
      "Iteration 7820, loss = 0.02866310\n",
      "Iteration 7821, loss = 0.02865734\n",
      "Iteration 7822, loss = 0.02865158\n",
      "Iteration 7823, loss = 0.02864583\n",
      "Iteration 7824, loss = 0.02864009\n",
      "Iteration 7825, loss = 0.02863434\n",
      "Iteration 7826, loss = 0.02862860\n",
      "Iteration 7827, loss = 0.02862287\n",
      "Iteration 7828, loss = 0.02861713\n",
      "Iteration 7829, loss = 0.02861140\n",
      "Iteration 7830, loss = 0.02860567\n",
      "Iteration 7831, loss = 0.02859995\n",
      "Iteration 7832, loss = 0.02859423\n",
      "Iteration 7833, loss = 0.02858852\n",
      "Iteration 7834, loss = 0.02858280\n",
      "Iteration 7835, loss = 0.02857709\n",
      "Iteration 7836, loss = 0.02857139\n",
      "Iteration 7837, loss = 0.02856568\n",
      "Iteration 7838, loss = 0.02855998\n",
      "Iteration 7839, loss = 0.02855429\n",
      "Iteration 7840, loss = 0.02854860\n",
      "Iteration 7841, loss = 0.02854291\n",
      "Iteration 7842, loss = 0.02853722\n",
      "Iteration 7843, loss = 0.02853154\n",
      "Iteration 7844, loss = 0.02852586\n",
      "Iteration 7845, loss = 0.02852018\n",
      "Iteration 7846, loss = 0.02851451\n",
      "Iteration 7847, loss = 0.02850884\n",
      "Iteration 7848, loss = 0.02850318\n",
      "Iteration 7849, loss = 0.02849751\n",
      "Iteration 7850, loss = 0.02849186\n",
      "Iteration 7851, loss = 0.02848620\n",
      "Iteration 7852, loss = 0.02848055\n",
      "Iteration 7853, loss = 0.02847490\n",
      "Iteration 7854, loss = 0.02846925\n",
      "Iteration 7855, loss = 0.02846361\n",
      "Iteration 7856, loss = 0.02845797\n",
      "Iteration 7857, loss = 0.02845234\n",
      "Iteration 7858, loss = 0.02844671\n",
      "Iteration 7859, loss = 0.02844108\n",
      "Iteration 7860, loss = 0.02843545\n",
      "Iteration 7861, loss = 0.02842983\n",
      "Iteration 7862, loss = 0.02842421\n",
      "Iteration 7863, loss = 0.02841860\n",
      "Iteration 7864, loss = 0.02841299\n",
      "Iteration 7865, loss = 0.02840738\n",
      "Iteration 7866, loss = 0.02840177\n",
      "Iteration 7867, loss = 0.02839617\n",
      "Iteration 7868, loss = 0.02839057\n",
      "Iteration 7869, loss = 0.02838498\n",
      "Iteration 7870, loss = 0.02837939\n",
      "Iteration 7871, loss = 0.02837380\n",
      "Iteration 7872, loss = 0.02836821\n",
      "Iteration 7873, loss = 0.02836263\n",
      "Iteration 7874, loss = 0.02835705\n",
      "Iteration 7875, loss = 0.02835148\n",
      "Iteration 7876, loss = 0.02834591\n",
      "Iteration 7877, loss = 0.02834034\n",
      "Iteration 7878, loss = 0.02833477\n",
      "Iteration 7879, loss = 0.02832921\n",
      "Iteration 7880, loss = 0.02832365\n",
      "Iteration 7881, loss = 0.02831810\n",
      "Iteration 7882, loss = 0.02831254\n",
      "Iteration 7883, loss = 0.02830700\n",
      "Iteration 7884, loss = 0.02830145\n",
      "Iteration 7885, loss = 0.02829591\n",
      "Iteration 7886, loss = 0.02829037\n",
      "Iteration 7887, loss = 0.02828483\n",
      "Iteration 7888, loss = 0.02827930\n",
      "Iteration 7889, loss = 0.02827377\n",
      "Iteration 7890, loss = 0.02826825\n",
      "Iteration 7891, loss = 0.02826272\n",
      "Iteration 7892, loss = 0.02825720\n",
      "Iteration 7893, loss = 0.02825169\n",
      "Iteration 7894, loss = 0.02824618\n",
      "Iteration 7895, loss = 0.02824067\n",
      "Iteration 7896, loss = 0.02823516\n",
      "Iteration 7897, loss = 0.02822966\n",
      "Iteration 7898, loss = 0.02822416\n",
      "Iteration 7899, loss = 0.02821866\n",
      "Iteration 7900, loss = 0.02821317\n",
      "Iteration 7901, loss = 0.02820768\n",
      "Iteration 7902, loss = 0.02820219\n",
      "Iteration 7903, loss = 0.02819671\n",
      "Iteration 7904, loss = 0.02819123\n",
      "Iteration 7905, loss = 0.02818575\n",
      "Iteration 7906, loss = 0.02818028\n",
      "Iteration 7907, loss = 0.02817481\n",
      "Iteration 7908, loss = 0.02816934\n",
      "Iteration 7909, loss = 0.02816388\n",
      "Iteration 7910, loss = 0.02815841\n",
      "Iteration 7911, loss = 0.02815296\n",
      "Iteration 7912, loss = 0.02814750\n",
      "Iteration 7913, loss = 0.02814205\n",
      "Iteration 7914, loss = 0.02813660\n",
      "Iteration 7915, loss = 0.02813116\n",
      "Iteration 7916, loss = 0.02812572\n",
      "Iteration 7917, loss = 0.02812028\n",
      "Iteration 7918, loss = 0.02811484\n",
      "Iteration 7919, loss = 0.02810941\n",
      "Iteration 7920, loss = 0.02810398\n",
      "Iteration 7921, loss = 0.02809856\n",
      "Iteration 7922, loss = 0.02809313\n",
      "Iteration 7923, loss = 0.02808772\n",
      "Iteration 7924, loss = 0.02808230\n",
      "Iteration 7925, loss = 0.02807689\n",
      "Iteration 7926, loss = 0.02807148\n",
      "Iteration 7927, loss = 0.02806607\n",
      "Iteration 7928, loss = 0.02806067\n",
      "Iteration 7929, loss = 0.02805527\n",
      "Iteration 7930, loss = 0.02804987\n",
      "Iteration 7931, loss = 0.02804447\n",
      "Iteration 7932, loss = 0.02803908\n",
      "Iteration 7933, loss = 0.02803370\n",
      "Iteration 7934, loss = 0.02802831\n",
      "Iteration 7935, loss = 0.02802293\n",
      "Iteration 7936, loss = 0.02801755\n",
      "Iteration 7937, loss = 0.02801218\n",
      "Iteration 7938, loss = 0.02800681\n",
      "Iteration 7939, loss = 0.02800144\n",
      "Iteration 7940, loss = 0.02799607\n",
      "Iteration 7941, loss = 0.02799071\n",
      "Iteration 7942, loss = 0.02798535\n",
      "Iteration 7943, loss = 0.02797999\n",
      "Iteration 7944, loss = 0.02797464\n",
      "Iteration 7945, loss = 0.02796929\n",
      "Iteration 7946, loss = 0.02796394\n",
      "Iteration 7947, loss = 0.02795860\n",
      "Iteration 7948, loss = 0.02795326\n",
      "Iteration 7949, loss = 0.02794792\n",
      "Iteration 7950, loss = 0.02794259\n",
      "Iteration 7951, loss = 0.02793725\n",
      "Iteration 7952, loss = 0.02793193\n",
      "Iteration 7953, loss = 0.02792660\n",
      "Iteration 7954, loss = 0.02792128\n",
      "Iteration 7955, loss = 0.02791596\n",
      "Iteration 7956, loss = 0.02791064\n",
      "Iteration 7957, loss = 0.02790533\n",
      "Iteration 7958, loss = 0.02790002\n",
      "Iteration 7959, loss = 0.02789471\n",
      "Iteration 7960, loss = 0.02788941\n",
      "Iteration 7961, loss = 0.02788411\n",
      "Iteration 7962, loss = 0.02787881\n",
      "Iteration 7963, loss = 0.02787352\n",
      "Iteration 7964, loss = 0.02786823\n",
      "Iteration 7965, loss = 0.02786294\n",
      "Iteration 7966, loss = 0.02785765\n",
      "Iteration 7967, loss = 0.02785237\n",
      "Iteration 7968, loss = 0.02784709\n",
      "Iteration 7969, loss = 0.02784181\n",
      "Iteration 7970, loss = 0.02783654\n",
      "Iteration 7971, loss = 0.02783127\n",
      "Iteration 7972, loss = 0.02782600\n",
      "Iteration 7973, loss = 0.02782074\n",
      "Iteration 7974, loss = 0.02781548\n",
      "Iteration 7975, loss = 0.02781022\n",
      "Iteration 7976, loss = 0.02780496\n",
      "Iteration 7977, loss = 0.02779971\n",
      "Iteration 7978, loss = 0.02779446\n",
      "Iteration 7979, loss = 0.02778922\n",
      "Iteration 7980, loss = 0.02778397\n",
      "Iteration 7981, loss = 0.02777873\n",
      "Iteration 7982, loss = 0.02777350\n",
      "Iteration 7983, loss = 0.02776826\n",
      "Iteration 7984, loss = 0.02776303\n",
      "Iteration 7985, loss = 0.02775780\n",
      "Iteration 7986, loss = 0.02775258\n",
      "Iteration 7987, loss = 0.02774736\n",
      "Iteration 7988, loss = 0.02774214\n",
      "Iteration 7989, loss = 0.02773692\n",
      "Iteration 7990, loss = 0.02773171\n",
      "Iteration 7991, loss = 0.02772650\n",
      "Iteration 7992, loss = 0.02772129\n",
      "Iteration 7993, loss = 0.02771609\n",
      "Iteration 7994, loss = 0.02771089\n",
      "Iteration 7995, loss = 0.02770569\n",
      "Iteration 7996, loss = 0.02770049\n",
      "Iteration 7997, loss = 0.02769530\n",
      "Iteration 7998, loss = 0.02769011\n",
      "Iteration 7999, loss = 0.02768492\n",
      "Iteration 8000, loss = 0.02767974\n",
      "Iteration 8001, loss = 0.02767456\n",
      "Iteration 8002, loss = 0.02766938\n",
      "Iteration 8003, loss = 0.02766421\n",
      "Iteration 8004, loss = 0.02765903\n",
      "Iteration 8005, loss = 0.02765387\n",
      "Iteration 8006, loss = 0.02764870\n",
      "Iteration 8007, loss = 0.02764354\n",
      "Iteration 8008, loss = 0.02763838\n",
      "Iteration 8009, loss = 0.02763322\n",
      "Iteration 8010, loss = 0.02762806\n",
      "Iteration 8011, loss = 0.02762291\n",
      "Iteration 8012, loss = 0.02761776\n",
      "Iteration 8013, loss = 0.02761262\n",
      "Iteration 8014, loss = 0.02760747\n",
      "Iteration 8015, loss = 0.02760233\n",
      "Iteration 8016, loss = 0.02759720\n",
      "Iteration 8017, loss = 0.02759206\n",
      "Iteration 8018, loss = 0.02758693\n",
      "Iteration 8019, loss = 0.02758180\n",
      "Iteration 8020, loss = 0.02757668\n",
      "Iteration 8021, loss = 0.02757155\n",
      "Iteration 8022, loss = 0.02756643\n",
      "Iteration 8023, loss = 0.02756132\n",
      "Iteration 8024, loss = 0.02755620\n",
      "Iteration 8025, loss = 0.02755109\n",
      "Iteration 8026, loss = 0.02754598\n",
      "Iteration 8027, loss = 0.02754088\n",
      "Iteration 8028, loss = 0.02753577\n",
      "Iteration 8029, loss = 0.02753067\n",
      "Iteration 8030, loss = 0.02752558\n",
      "Iteration 8031, loss = 0.02752048\n",
      "Iteration 8032, loss = 0.02751539\n",
      "Iteration 8033, loss = 0.02751030\n",
      "Iteration 8034, loss = 0.02750522\n",
      "Iteration 8035, loss = 0.02750013\n",
      "Iteration 8036, loss = 0.02749505\n",
      "Iteration 8037, loss = 0.02748997\n",
      "Iteration 8038, loss = 0.02748490\n",
      "Iteration 8039, loss = 0.02747983\n",
      "Iteration 8040, loss = 0.02747476\n",
      "Iteration 8041, loss = 0.02746969\n",
      "Iteration 8042, loss = 0.02746463\n",
      "Iteration 8043, loss = 0.02745957\n",
      "Iteration 8044, loss = 0.02745451\n",
      "Iteration 8045, loss = 0.02744945\n",
      "Iteration 8046, loss = 0.02744440\n",
      "Iteration 8047, loss = 0.02743935\n",
      "Iteration 8048, loss = 0.02743431\n",
      "Iteration 8049, loss = 0.02742926\n",
      "Iteration 8050, loss = 0.02742422\n",
      "Iteration 8051, loss = 0.02741918\n",
      "Iteration 8052, loss = 0.02741414\n",
      "Iteration 8053, loss = 0.02740911\n",
      "Iteration 8054, loss = 0.02740408\n",
      "Iteration 8055, loss = 0.02739905\n",
      "Iteration 8056, loss = 0.02739403\n",
      "Iteration 8057, loss = 0.02738901\n",
      "Iteration 8058, loss = 0.02738399\n",
      "Iteration 8059, loss = 0.02737897\n",
      "Iteration 8060, loss = 0.02737395\n",
      "Iteration 8061, loss = 0.02736894\n",
      "Iteration 8062, loss = 0.02736393\n",
      "Iteration 8063, loss = 0.02735893\n",
      "Iteration 8064, loss = 0.02735393\n",
      "Iteration 8065, loss = 0.02734892\n",
      "Iteration 8066, loss = 0.02734393\n",
      "Iteration 8067, loss = 0.02733893\n",
      "Iteration 8068, loss = 0.02733394\n",
      "Iteration 8069, loss = 0.02732895\n",
      "Iteration 8070, loss = 0.02732396\n",
      "Iteration 8071, loss = 0.02731898\n",
      "Iteration 8072, loss = 0.02731399\n",
      "Iteration 8073, loss = 0.02730902\n",
      "Iteration 8074, loss = 0.02730404\n",
      "Iteration 8075, loss = 0.02729906\n",
      "Iteration 8076, loss = 0.02729409\n",
      "Iteration 8077, loss = 0.02728912\n",
      "Iteration 8078, loss = 0.02728416\n",
      "Iteration 8079, loss = 0.02727919\n",
      "Iteration 8080, loss = 0.02727423\n",
      "Iteration 8081, loss = 0.02726928\n",
      "Iteration 8082, loss = 0.02726432\n",
      "Iteration 8083, loss = 0.02725937\n",
      "Iteration 8084, loss = 0.02725442\n",
      "Iteration 8085, loss = 0.02724947\n",
      "Iteration 8086, loss = 0.02724452\n",
      "Iteration 8087, loss = 0.02723958\n",
      "Iteration 8088, loss = 0.02723464\n",
      "Iteration 8089, loss = 0.02722970\n",
      "Iteration 8090, loss = 0.02722477\n",
      "Iteration 8091, loss = 0.02721984\n",
      "Iteration 8092, loss = 0.02721491\n",
      "Iteration 8093, loss = 0.02720998\n",
      "Iteration 8094, loss = 0.02720506\n",
      "Iteration 8095, loss = 0.02720013\n",
      "Iteration 8096, loss = 0.02719522\n",
      "Iteration 8097, loss = 0.02719030\n",
      "Iteration 8098, loss = 0.02718538\n",
      "Iteration 8099, loss = 0.02718047\n",
      "Iteration 8100, loss = 0.02717556\n",
      "Iteration 8101, loss = 0.02717066\n",
      "Iteration 8102, loss = 0.02716575\n",
      "Iteration 8103, loss = 0.02716085\n",
      "Iteration 8104, loss = 0.02715595\n",
      "Iteration 8105, loss = 0.02715106\n",
      "Iteration 8106, loss = 0.02714616\n",
      "Iteration 8107, loss = 0.02714127\n",
      "Iteration 8108, loss = 0.02713638\n",
      "Iteration 8109, loss = 0.02713150\n",
      "Iteration 8110, loss = 0.02712661\n",
      "Iteration 8111, loss = 0.02712173\n",
      "Iteration 8112, loss = 0.02711685\n",
      "Iteration 8113, loss = 0.02711197\n",
      "Iteration 8114, loss = 0.02710710\n",
      "Iteration 8115, loss = 0.02710223\n",
      "Iteration 8116, loss = 0.02709736\n",
      "Iteration 8117, loss = 0.02709249\n",
      "Iteration 8118, loss = 0.02708763\n",
      "Iteration 8119, loss = 0.02708277\n",
      "Iteration 8120, loss = 0.02707791\n",
      "Iteration 8121, loss = 0.02707305\n",
      "Iteration 8122, loss = 0.02706820\n",
      "Iteration 8123, loss = 0.02706335\n",
      "Iteration 8124, loss = 0.02705850\n",
      "Iteration 8125, loss = 0.02705365\n",
      "Iteration 8126, loss = 0.02704881\n",
      "Iteration 8127, loss = 0.02704396\n",
      "Iteration 8128, loss = 0.02703912\n",
      "Iteration 8129, loss = 0.02703429\n",
      "Iteration 8130, loss = 0.02702945\n",
      "Iteration 8131, loss = 0.02702462\n",
      "Iteration 8132, loss = 0.02701979\n",
      "Iteration 8133, loss = 0.02701496\n",
      "Iteration 8134, loss = 0.02701014\n",
      "Iteration 8135, loss = 0.02700531\n",
      "Iteration 8136, loss = 0.02700049\n",
      "Iteration 8137, loss = 0.02699567\n",
      "Iteration 8138, loss = 0.02699086\n",
      "Iteration 8139, loss = 0.02698604\n",
      "Iteration 8140, loss = 0.02698123\n",
      "Iteration 8141, loss = 0.02697642\n",
      "Iteration 8142, loss = 0.02697162\n",
      "Iteration 8143, loss = 0.02696681\n",
      "Iteration 8144, loss = 0.02696201\n",
      "Iteration 8145, loss = 0.02695721\n",
      "Iteration 8146, loss = 0.02695242\n",
      "Iteration 8147, loss = 0.02694762\n",
      "Iteration 8148, loss = 0.02694283\n",
      "Iteration 8149, loss = 0.02693804\n",
      "Iteration 8150, loss = 0.02693325\n",
      "Iteration 8151, loss = 0.02692847\n",
      "Iteration 8152, loss = 0.02692368\n",
      "Iteration 8153, loss = 0.02691890\n",
      "Iteration 8154, loss = 0.02691412\n",
      "Iteration 8155, loss = 0.02690935\n",
      "Iteration 8156, loss = 0.02690457\n",
      "Iteration 8157, loss = 0.02689980\n",
      "Iteration 8158, loss = 0.02689503\n",
      "Iteration 8159, loss = 0.02689026\n",
      "Iteration 8160, loss = 0.02688550\n",
      "Iteration 8161, loss = 0.02688074\n",
      "Iteration 8162, loss = 0.02687598\n",
      "Iteration 8163, loss = 0.02687122\n",
      "Iteration 8164, loss = 0.02686646\n",
      "Iteration 8165, loss = 0.02686171\n",
      "Iteration 8166, loss = 0.02685696\n",
      "Iteration 8167, loss = 0.02685221\n",
      "Iteration 8168, loss = 0.02684746\n",
      "Iteration 8169, loss = 0.02684272\n",
      "Iteration 8170, loss = 0.02683797\n",
      "Iteration 8171, loss = 0.02683323\n",
      "Iteration 8172, loss = 0.02682850\n",
      "Iteration 8173, loss = 0.02682376\n",
      "Iteration 8174, loss = 0.02681903\n",
      "Iteration 8175, loss = 0.02681429\n",
      "Iteration 8176, loss = 0.02680956\n",
      "Iteration 8177, loss = 0.02680484\n",
      "Iteration 8178, loss = 0.02680011\n",
      "Iteration 8179, loss = 0.02679539\n",
      "Iteration 8180, loss = 0.02679067\n",
      "Iteration 8181, loss = 0.02678595\n",
      "Iteration 8182, loss = 0.02678123\n",
      "Iteration 8183, loss = 0.02677652\n",
      "Iteration 8184, loss = 0.02677181\n",
      "Iteration 8185, loss = 0.02676710\n",
      "Iteration 8186, loss = 0.02676239\n",
      "Iteration 8187, loss = 0.02675768\n",
      "Iteration 8188, loss = 0.02675298\n",
      "Iteration 8189, loss = 0.02674828\n",
      "Iteration 8190, loss = 0.02674358\n",
      "Iteration 8191, loss = 0.02673888\n",
      "Iteration 8192, loss = 0.02673418\n",
      "Iteration 8193, loss = 0.02672949\n",
      "Iteration 8194, loss = 0.02672480\n",
      "Iteration 8195, loss = 0.02672011\n",
      "Iteration 8196, loss = 0.02671542\n",
      "Iteration 8197, loss = 0.02671074\n",
      "Iteration 8198, loss = 0.02670605\n",
      "Iteration 8199, loss = 0.02670137\n",
      "Iteration 8200, loss = 0.02669669\n",
      "Iteration 8201, loss = 0.02669202\n",
      "Iteration 8202, loss = 0.02668734\n",
      "Iteration 8203, loss = 0.02668267\n",
      "Iteration 8204, loss = 0.02667800\n",
      "Iteration 8205, loss = 0.02667333\n",
      "Iteration 8206, loss = 0.02666866\n",
      "Iteration 8207, loss = 0.02666400\n",
      "Iteration 8208, loss = 0.02665934\n",
      "Iteration 8209, loss = 0.02665467\n",
      "Iteration 8210, loss = 0.02665002\n",
      "Iteration 8211, loss = 0.02664536\n",
      "Iteration 8212, loss = 0.02664070\n",
      "Iteration 8213, loss = 0.02663605\n",
      "Iteration 8214, loss = 0.02663140\n",
      "Iteration 8215, loss = 0.02662675\n",
      "Iteration 8216, loss = 0.02662210\n",
      "Iteration 8217, loss = 0.02661746\n",
      "Iteration 8218, loss = 0.02661282\n",
      "Iteration 8219, loss = 0.02660817\n",
      "Iteration 8220, loss = 0.02660354\n",
      "Iteration 8221, loss = 0.02659890\n",
      "Iteration 8222, loss = 0.02659426\n",
      "Iteration 8223, loss = 0.02658963\n",
      "Iteration 8224, loss = 0.02658500\n",
      "Iteration 8225, loss = 0.02658037\n",
      "Iteration 8226, loss = 0.02657574\n",
      "Iteration 8227, loss = 0.02657111\n",
      "Iteration 8228, loss = 0.02656649\n",
      "Iteration 8229, loss = 0.02656187\n",
      "Iteration 8230, loss = 0.02655725\n",
      "Iteration 8231, loss = 0.02655263\n",
      "Iteration 8232, loss = 0.02654801\n",
      "Iteration 8233, loss = 0.02654340\n",
      "Iteration 8234, loss = 0.02653878\n",
      "Iteration 8235, loss = 0.02653417\n",
      "Iteration 8236, loss = 0.02652956\n",
      "Iteration 8237, loss = 0.02652496\n",
      "Iteration 8238, loss = 0.02652035\n",
      "Iteration 8239, loss = 0.02651575\n",
      "Iteration 8240, loss = 0.02651114\n",
      "Iteration 8241, loss = 0.02650654\n",
      "Iteration 8242, loss = 0.02650195\n",
      "Iteration 8243, loss = 0.02649735\n",
      "Iteration 8244, loss = 0.02649275\n",
      "Iteration 8245, loss = 0.02648816\n",
      "Iteration 8246, loss = 0.02648357\n",
      "Iteration 8247, loss = 0.02647898\n",
      "Iteration 8248, loss = 0.02647439\n",
      "Iteration 8249, loss = 0.02646981\n",
      "Iteration 8250, loss = 0.02646522\n",
      "Iteration 8251, loss = 0.02646064\n",
      "Iteration 8252, loss = 0.02645606\n",
      "Iteration 8253, loss = 0.02645148\n",
      "Iteration 8254, loss = 0.02644690\n",
      "Iteration 8255, loss = 0.02644232\n",
      "Iteration 8256, loss = 0.02643775\n",
      "Iteration 8257, loss = 0.02643318\n",
      "Iteration 8258, loss = 0.02642861\n",
      "Iteration 8259, loss = 0.02642404\n",
      "Iteration 8260, loss = 0.02641947\n",
      "Iteration 8261, loss = 0.02641490\n",
      "Iteration 8262, loss = 0.02641034\n",
      "Iteration 8263, loss = 0.02640578\n",
      "Iteration 8264, loss = 0.02640122\n",
      "Iteration 8265, loss = 0.02639666\n",
      "Iteration 8266, loss = 0.02639210\n",
      "Iteration 8267, loss = 0.02638754\n",
      "Iteration 8268, loss = 0.02638299\n",
      "Iteration 8269, loss = 0.02637844\n",
      "Iteration 8270, loss = 0.02637389\n",
      "Iteration 8271, loss = 0.02636934\n",
      "Iteration 8272, loss = 0.02636479\n",
      "Iteration 8273, loss = 0.02636024\n",
      "Iteration 8274, loss = 0.02635570\n",
      "Iteration 8275, loss = 0.02635115\n",
      "Iteration 8276, loss = 0.02634661\n",
      "Iteration 8277, loss = 0.02634207\n",
      "Iteration 8278, loss = 0.02633753\n",
      "Iteration 8279, loss = 0.02633300\n",
      "Iteration 8280, loss = 0.02632846\n",
      "Iteration 8281, loss = 0.02632393\n",
      "Iteration 8282, loss = 0.02631940\n",
      "Iteration 8283, loss = 0.02631487\n",
      "Iteration 8284, loss = 0.02631034\n",
      "Iteration 8285, loss = 0.02630581\n",
      "Iteration 8286, loss = 0.02630128\n",
      "Iteration 8287, loss = 0.02629676\n",
      "Iteration 8288, loss = 0.02629223\n",
      "Iteration 8289, loss = 0.02628771\n",
      "Iteration 8290, loss = 0.02628319\n",
      "Iteration 8291, loss = 0.02627867\n",
      "Iteration 8292, loss = 0.02627415\n",
      "Iteration 8293, loss = 0.02626964\n",
      "Iteration 8294, loss = 0.02626512\n",
      "Iteration 8295, loss = 0.02626061\n",
      "Iteration 8296, loss = 0.02625610\n",
      "Iteration 8297, loss = 0.02625159\n",
      "Iteration 8298, loss = 0.02624708\n",
      "Iteration 8299, loss = 0.02624257\n",
      "Iteration 8300, loss = 0.02623806\n",
      "Iteration 8301, loss = 0.02623356\n",
      "Iteration 8302, loss = 0.02622906\n",
      "Iteration 8303, loss = 0.02622455\n",
      "Iteration 8304, loss = 0.02622005\n",
      "Iteration 8305, loss = 0.02621555\n",
      "Iteration 8306, loss = 0.02621106\n",
      "Iteration 8307, loss = 0.02620656\n",
      "Iteration 8308, loss = 0.02620206\n",
      "Iteration 8309, loss = 0.02619757\n",
      "Iteration 8310, loss = 0.02619308\n",
      "Iteration 8311, loss = 0.02618858\n",
      "Iteration 8312, loss = 0.02618409\n",
      "Iteration 8313, loss = 0.02617961\n",
      "Iteration 8314, loss = 0.02617512\n",
      "Iteration 8315, loss = 0.02617063\n",
      "Iteration 8316, loss = 0.02616615\n",
      "Iteration 8317, loss = 0.02616166\n",
      "Iteration 8318, loss = 0.02615718\n",
      "Iteration 8319, loss = 0.02615270\n",
      "Iteration 8320, loss = 0.02614822\n",
      "Iteration 8321, loss = 0.02614374\n",
      "Iteration 8322, loss = 0.02613926\n",
      "Iteration 8323, loss = 0.02613478\n",
      "Iteration 8324, loss = 0.02613031\n",
      "Iteration 8325, loss = 0.02612584\n",
      "Iteration 8326, loss = 0.02612136\n",
      "Iteration 8327, loss = 0.02611689\n",
      "Iteration 8328, loss = 0.02611242\n",
      "Iteration 8329, loss = 0.02610795\n",
      "Iteration 8330, loss = 0.02610348\n",
      "Iteration 8331, loss = 0.02609902\n",
      "Iteration 8332, loss = 0.02609455\n",
      "Iteration 8333, loss = 0.02609008\n",
      "Iteration 8334, loss = 0.02608562\n",
      "Iteration 8335, loss = 0.02608116\n",
      "Iteration 8336, loss = 0.02607670\n",
      "Iteration 8337, loss = 0.02607224\n",
      "Iteration 8338, loss = 0.02606778\n",
      "Iteration 8339, loss = 0.02606332\n",
      "Iteration 8340, loss = 0.02605886\n",
      "Iteration 8341, loss = 0.02605441\n",
      "Iteration 8342, loss = 0.02604995\n",
      "Iteration 8343, loss = 0.02604550\n",
      "Iteration 8344, loss = 0.02604104\n",
      "Iteration 8345, loss = 0.02603659\n",
      "Iteration 8346, loss = 0.02603214\n",
      "Iteration 8347, loss = 0.02602769\n",
      "Iteration 8348, loss = 0.02602324\n",
      "Iteration 8349, loss = 0.02601879\n",
      "Iteration 8350, loss = 0.02601435\n",
      "Iteration 8351, loss = 0.02600990\n",
      "Iteration 8352, loss = 0.02600546\n",
      "Iteration 8353, loss = 0.02600101\n",
      "Iteration 8354, loss = 0.02599657\n",
      "Iteration 8355, loss = 0.02599213\n",
      "Iteration 8356, loss = 0.02598769\n",
      "Iteration 8357, loss = 0.02598325\n",
      "Iteration 8358, loss = 0.02597881\n",
      "Iteration 8359, loss = 0.02597437\n",
      "Iteration 8360, loss = 0.02596993\n",
      "Iteration 8361, loss = 0.02596549\n",
      "Iteration 8362, loss = 0.02596106\n",
      "Iteration 8363, loss = 0.02595662\n",
      "Iteration 8364, loss = 0.02595219\n",
      "Iteration 8365, loss = 0.02594776\n",
      "Iteration 8366, loss = 0.02594332\n",
      "Iteration 8367, loss = 0.02593889\n",
      "Iteration 8368, loss = 0.02593446\n",
      "Iteration 8369, loss = 0.02593003\n",
      "Iteration 8370, loss = 0.02592560\n",
      "Iteration 8371, loss = 0.02592117\n",
      "Iteration 8372, loss = 0.02591675\n",
      "Iteration 8373, loss = 0.02591232\n",
      "Iteration 8374, loss = 0.02590789\n",
      "Iteration 8375, loss = 0.02590347\n",
      "Iteration 8376, loss = 0.02589905\n",
      "Iteration 8377, loss = 0.02589462\n",
      "Iteration 8378, loss = 0.02589020\n",
      "Iteration 8379, loss = 0.02588578\n",
      "Iteration 8380, loss = 0.02588136\n",
      "Iteration 8381, loss = 0.02587694\n",
      "Iteration 8382, loss = 0.02587252\n",
      "Iteration 8383, loss = 0.02586810\n",
      "Iteration 8384, loss = 0.02586368\n",
      "Iteration 8385, loss = 0.02585926\n",
      "Iteration 8386, loss = 0.02585484\n",
      "Iteration 8387, loss = 0.02585043\n",
      "Iteration 8388, loss = 0.02584601\n",
      "Iteration 8389, loss = 0.02584159\n",
      "Iteration 8390, loss = 0.02583718\n",
      "Iteration 8391, loss = 0.02583277\n",
      "Iteration 8392, loss = 0.02582835\n",
      "Iteration 8393, loss = 0.02582394\n",
      "Iteration 8394, loss = 0.02581953\n",
      "Iteration 8395, loss = 0.02581512\n",
      "Iteration 8396, loss = 0.02581070\n",
      "Iteration 8397, loss = 0.02580629\n",
      "Iteration 8398, loss = 0.02580188\n",
      "Iteration 8399, loss = 0.02579747\n",
      "Iteration 8400, loss = 0.02579307\n",
      "Iteration 8401, loss = 0.02578866\n",
      "Iteration 8402, loss = 0.02578425\n",
      "Iteration 8403, loss = 0.02577984\n",
      "Iteration 8404, loss = 0.02577544\n",
      "Iteration 8405, loss = 0.02577103\n",
      "Iteration 8406, loss = 0.02576662\n",
      "Iteration 8407, loss = 0.02576222\n",
      "Iteration 8408, loss = 0.02575781\n",
      "Iteration 8409, loss = 0.02575341\n",
      "Iteration 8410, loss = 0.02574900\n",
      "Iteration 8411, loss = 0.02574460\n",
      "Iteration 8412, loss = 0.02574020\n",
      "Iteration 8413, loss = 0.02573579\n",
      "Iteration 8414, loss = 0.02573139\n",
      "Iteration 8415, loss = 0.02572699\n",
      "Iteration 8416, loss = 0.02572259\n",
      "Iteration 8417, loss = 0.02571819\n",
      "Iteration 8418, loss = 0.02571379\n",
      "Iteration 8419, loss = 0.02570939\n",
      "Iteration 8420, loss = 0.02570499\n",
      "Iteration 8421, loss = 0.02570059\n",
      "Iteration 8422, loss = 0.02569619\n",
      "Iteration 8423, loss = 0.02569179\n",
      "Iteration 8424, loss = 0.02568739\n",
      "Iteration 8425, loss = 0.02568299\n",
      "Iteration 8426, loss = 0.02567859\n",
      "Iteration 8427, loss = 0.02567419\n",
      "Iteration 8428, loss = 0.02566979\n",
      "Iteration 8429, loss = 0.02566540\n",
      "Iteration 8430, loss = 0.02566100\n",
      "Iteration 8431, loss = 0.02565660\n",
      "Iteration 8432, loss = 0.02565221\n",
      "Iteration 8433, loss = 0.02564781\n",
      "Iteration 8434, loss = 0.02564341\n",
      "Iteration 8435, loss = 0.02563902\n",
      "Iteration 8436, loss = 0.02563462\n",
      "Iteration 8437, loss = 0.02563022\n",
      "Iteration 8438, loss = 0.02562583\n",
      "Iteration 8439, loss = 0.02562143\n",
      "Iteration 8440, loss = 0.02561704\n",
      "Iteration 8441, loss = 0.02561264\n",
      "Iteration 8442, loss = 0.02560825\n",
      "Iteration 8443, loss = 0.02560385\n",
      "Iteration 8444, loss = 0.02559946\n",
      "Iteration 8445, loss = 0.02559506\n",
      "Iteration 8446, loss = 0.02559067\n",
      "Iteration 8447, loss = 0.02558627\n",
      "Iteration 8448, loss = 0.02558188\n",
      "Iteration 8449, loss = 0.02557748\n",
      "Iteration 8450, loss = 0.02557309\n",
      "Iteration 8451, loss = 0.02556869\n",
      "Iteration 8452, loss = 0.02556430\n",
      "Iteration 8453, loss = 0.02555990\n",
      "Iteration 8454, loss = 0.02555551\n",
      "Iteration 8455, loss = 0.02555111\n",
      "Iteration 8456, loss = 0.02554672\n",
      "Iteration 8457, loss = 0.02554232\n",
      "Iteration 8458, loss = 0.02553793\n",
      "Iteration 8459, loss = 0.02553353\n",
      "Iteration 8460, loss = 0.02552914\n",
      "Iteration 8461, loss = 0.02552474\n",
      "Iteration 8462, loss = 0.02552035\n",
      "Iteration 8463, loss = 0.02551595\n",
      "Iteration 8464, loss = 0.02551156\n",
      "Iteration 8465, loss = 0.02550716\n",
      "Iteration 8466, loss = 0.02550276\n",
      "Iteration 8467, loss = 0.02549837\n",
      "Iteration 8468, loss = 0.02549397\n",
      "Iteration 8469, loss = 0.02548958\n",
      "Iteration 8470, loss = 0.02548518\n",
      "Iteration 8471, loss = 0.02548078\n",
      "Iteration 8472, loss = 0.02547639\n",
      "Iteration 8473, loss = 0.02547199\n",
      "Iteration 8474, loss = 0.02546759\n",
      "Iteration 8475, loss = 0.02546319\n",
      "Iteration 8476, loss = 0.02545880\n",
      "Iteration 8477, loss = 0.02545440\n",
      "Iteration 8478, loss = 0.02545000\n",
      "Iteration 8479, loss = 0.02544560\n",
      "Iteration 8480, loss = 0.02544120\n",
      "Iteration 8481, loss = 0.02543680\n",
      "Iteration 8482, loss = 0.02543240\n",
      "Iteration 8483, loss = 0.02542800\n",
      "Iteration 8484, loss = 0.02542360\n",
      "Iteration 8485, loss = 0.02541920\n",
      "Iteration 8486, loss = 0.02541480\n",
      "Iteration 8487, loss = 0.02541040\n",
      "Iteration 8488, loss = 0.02540600\n",
      "Iteration 8489, loss = 0.02540160\n",
      "Iteration 8490, loss = 0.02539719\n",
      "Iteration 8491, loss = 0.02539279\n",
      "Iteration 8492, loss = 0.02538839\n",
      "Iteration 8493, loss = 0.02538398\n",
      "Iteration 8494, loss = 0.02537958\n",
      "Iteration 8495, loss = 0.02537517\n",
      "Iteration 8496, loss = 0.02537077\n",
      "Iteration 8497, loss = 0.02536636\n",
      "Iteration 8498, loss = 0.02536196\n",
      "Iteration 8499, loss = 0.02535755\n",
      "Iteration 8500, loss = 0.02535314\n",
      "Iteration 8501, loss = 0.02534873\n",
      "Iteration 8502, loss = 0.02534433\n",
      "Iteration 8503, loss = 0.02533992\n",
      "Iteration 8504, loss = 0.02533551\n",
      "Iteration 8505, loss = 0.02533110\n",
      "Iteration 8506, loss = 0.02532669\n",
      "Iteration 8507, loss = 0.02532228\n",
      "Iteration 8508, loss = 0.02531786\n",
      "Iteration 8509, loss = 0.02531345\n",
      "Iteration 8510, loss = 0.02530904\n",
      "Iteration 8511, loss = 0.02530462\n",
      "Iteration 8512, loss = 0.02530021\n",
      "Iteration 8513, loss = 0.02529579\n",
      "Iteration 8514, loss = 0.02529138\n",
      "Iteration 8515, loss = 0.02528696\n",
      "Iteration 8516, loss = 0.02528255\n",
      "Iteration 8517, loss = 0.02527813\n",
      "Iteration 8518, loss = 0.02527371\n",
      "Iteration 8519, loss = 0.02526929\n",
      "Iteration 8520, loss = 0.02526487\n",
      "Iteration 8521, loss = 0.02526045\n",
      "Iteration 8522, loss = 0.02525603\n",
      "Iteration 8523, loss = 0.02525160\n",
      "Iteration 8524, loss = 0.02524718\n",
      "Iteration 8525, loss = 0.02524276\n",
      "Iteration 8526, loss = 0.02523833\n",
      "Iteration 8527, loss = 0.02523391\n",
      "Iteration 8528, loss = 0.02522948\n",
      "Iteration 8529, loss = 0.02522505\n",
      "Iteration 8530, loss = 0.02522062\n",
      "Iteration 8531, loss = 0.02521619\n",
      "Iteration 8532, loss = 0.02521176\n",
      "Iteration 8533, loss = 0.02520733\n",
      "Iteration 8534, loss = 0.02520290\n",
      "Iteration 8535, loss = 0.02519847\n",
      "Iteration 8536, loss = 0.02519404\n",
      "Iteration 8537, loss = 0.02518960\n",
      "Iteration 8538, loss = 0.02518516\n",
      "Iteration 8539, loss = 0.02518073\n",
      "Iteration 8540, loss = 0.02517629\n",
      "Iteration 8541, loss = 0.02517185\n",
      "Iteration 8542, loss = 0.02516741\n",
      "Iteration 8543, loss = 0.02516297\n",
      "Iteration 8544, loss = 0.02515853\n",
      "Iteration 8545, loss = 0.02515409\n",
      "Iteration 8546, loss = 0.02514964\n",
      "Iteration 8547, loss = 0.02514520\n",
      "Iteration 8548, loss = 0.02514075\n",
      "Iteration 8549, loss = 0.02513630\n",
      "Iteration 8550, loss = 0.02513186\n",
      "Iteration 8551, loss = 0.02512741\n",
      "Iteration 8552, loss = 0.02512296\n",
      "Iteration 8553, loss = 0.02511850\n",
      "Iteration 8554, loss = 0.02511405\n",
      "Iteration 8555, loss = 0.02510960\n",
      "Iteration 8556, loss = 0.02510514\n",
      "Iteration 8557, loss = 0.02510069\n",
      "Iteration 8558, loss = 0.02509623\n",
      "Iteration 8559, loss = 0.02509177\n",
      "Iteration 8560, loss = 0.02508731\n",
      "Iteration 8561, loss = 0.02508285\n",
      "Iteration 8562, loss = 0.02507839\n",
      "Iteration 8563, loss = 0.02507392\n",
      "Iteration 8564, loss = 0.02506946\n",
      "Iteration 8565, loss = 0.02506499\n",
      "Iteration 8566, loss = 0.02506052\n",
      "Iteration 8567, loss = 0.02505605\n",
      "Iteration 8568, loss = 0.02505158\n",
      "Iteration 8569, loss = 0.02504711\n",
      "Iteration 8570, loss = 0.02504264\n",
      "Iteration 8571, loss = 0.02503817\n",
      "Iteration 8572, loss = 0.02503369\n",
      "Iteration 8573, loss = 0.02502921\n",
      "Iteration 8574, loss = 0.02502473\n",
      "Iteration 8575, loss = 0.02502025\n",
      "Iteration 8576, loss = 0.02501577\n",
      "Iteration 8577, loss = 0.02501129\n",
      "Iteration 8578, loss = 0.02500681\n",
      "Iteration 8579, loss = 0.02500232\n",
      "Iteration 8580, loss = 0.02499783\n",
      "Iteration 8581, loss = 0.02499334\n",
      "Iteration 8582, loss = 0.02498885\n",
      "Iteration 8583, loss = 0.02498436\n",
      "Iteration 8584, loss = 0.02497987\n",
      "Iteration 8585, loss = 0.02497537\n",
      "Iteration 8586, loss = 0.02497088\n",
      "Iteration 8587, loss = 0.02496638\n",
      "Iteration 8588, loss = 0.02496188\n",
      "Iteration 8589, loss = 0.02495738\n",
      "Iteration 8590, loss = 0.02495288\n",
      "Iteration 8591, loss = 0.02494837\n",
      "Iteration 8592, loss = 0.02494387\n",
      "Iteration 8593, loss = 0.02493936\n",
      "Iteration 8594, loss = 0.02493485\n",
      "Iteration 8595, loss = 0.02493034\n",
      "Iteration 8596, loss = 0.02492583\n",
      "Iteration 8597, loss = 0.02492131\n",
      "Iteration 8598, loss = 0.02491680\n",
      "Iteration 8599, loss = 0.02491228\n",
      "Iteration 8600, loss = 0.02490776\n",
      "Iteration 8601, loss = 0.02490324\n",
      "Iteration 8602, loss = 0.02489872\n",
      "Iteration 8603, loss = 0.02489419\n",
      "Iteration 8604, loss = 0.02488967\n",
      "Iteration 8605, loss = 0.02488514\n",
      "Iteration 8606, loss = 0.02488061\n",
      "Iteration 8607, loss = 0.02487608\n",
      "Iteration 8608, loss = 0.02487154\n",
      "Iteration 8609, loss = 0.02486701\n",
      "Iteration 8610, loss = 0.02486247\n",
      "Iteration 8611, loss = 0.02485793\n",
      "Iteration 8612, loss = 0.02485339\n",
      "Iteration 8613, loss = 0.02484885\n",
      "Iteration 8614, loss = 0.02484430\n",
      "Iteration 8615, loss = 0.02483976\n",
      "Iteration 8616, loss = 0.02483521\n",
      "Iteration 8617, loss = 0.02483066\n",
      "Iteration 8618, loss = 0.02482611\n",
      "Iteration 8619, loss = 0.02482155\n",
      "Iteration 8620, loss = 0.02481700\n",
      "Iteration 8621, loss = 0.02481244\n",
      "Iteration 8622, loss = 0.02480788\n",
      "Iteration 8623, loss = 0.02480332\n",
      "Iteration 8624, loss = 0.02479875\n",
      "Iteration 8625, loss = 0.02479419\n",
      "Iteration 8626, loss = 0.02478962\n",
      "Iteration 8627, loss = 0.02478505\n",
      "Iteration 8628, loss = 0.02478047\n",
      "Iteration 8629, loss = 0.02477590\n",
      "Iteration 8630, loss = 0.02477132\n",
      "Iteration 8631, loss = 0.02476674\n",
      "Iteration 8632, loss = 0.02476216\n",
      "Iteration 8633, loss = 0.02475758\n",
      "Iteration 8634, loss = 0.02475300\n",
      "Iteration 8635, loss = 0.02474841\n",
      "Iteration 8636, loss = 0.02474382\n",
      "Iteration 8637, loss = 0.02473923\n",
      "Iteration 8638, loss = 0.02473463\n",
      "Iteration 8639, loss = 0.02473004\n",
      "Iteration 8640, loss = 0.02472544\n",
      "Iteration 8641, loss = 0.02472084\n",
      "Iteration 8642, loss = 0.02471624\n",
      "Iteration 8643, loss = 0.02471163\n",
      "Iteration 8644, loss = 0.02470702\n",
      "Iteration 8645, loss = 0.02470242\n",
      "Iteration 8646, loss = 0.02469780\n",
      "Iteration 8647, loss = 0.02469319\n",
      "Iteration 8648, loss = 0.02468857\n",
      "Iteration 8649, loss = 0.02468395\n",
      "Iteration 8650, loss = 0.02467933\n",
      "Iteration 8651, loss = 0.02467471\n",
      "Iteration 8652, loss = 0.02467008\n",
      "Iteration 8653, loss = 0.02466546\n",
      "Iteration 8654, loss = 0.02466083\n",
      "Iteration 8655, loss = 0.02465619\n",
      "Iteration 8656, loss = 0.02465156\n",
      "Iteration 8657, loss = 0.02464692\n",
      "Iteration 8658, loss = 0.02464228\n",
      "Iteration 8659, loss = 0.02463764\n",
      "Iteration 8660, loss = 0.02463299\n",
      "Iteration 8661, loss = 0.02462834\n",
      "Iteration 8662, loss = 0.02462369\n",
      "Iteration 8663, loss = 0.02461904\n",
      "Iteration 8664, loss = 0.02461439\n",
      "Iteration 8665, loss = 0.02460973\n",
      "Iteration 8666, loss = 0.02460507\n",
      "Iteration 8667, loss = 0.02460041\n",
      "Iteration 8668, loss = 0.02459574\n",
      "Iteration 8669, loss = 0.02459107\n",
      "Iteration 8670, loss = 0.02458640\n",
      "Iteration 8671, loss = 0.02458173\n",
      "Iteration 8672, loss = 0.02457705\n",
      "Iteration 8673, loss = 0.02457238\n",
      "Iteration 8674, loss = 0.02456770\n",
      "Iteration 8675, loss = 0.02456301\n",
      "Iteration 8676, loss = 0.02455833\n",
      "Iteration 8677, loss = 0.02455364\n",
      "Iteration 8678, loss = 0.02454895\n",
      "Iteration 8679, loss = 0.02454425\n",
      "Iteration 8680, loss = 0.02453955\n",
      "Iteration 8681, loss = 0.02453485\n",
      "Iteration 8682, loss = 0.02453015\n",
      "Iteration 8683, loss = 0.02452545\n",
      "Iteration 8684, loss = 0.02452074\n",
      "Iteration 8685, loss = 0.02451603\n",
      "Iteration 8686, loss = 0.02451132\n",
      "Iteration 8687, loss = 0.02450660\n",
      "Iteration 8688, loss = 0.02450188\n",
      "Iteration 8689, loss = 0.02449716\n",
      "Iteration 8690, loss = 0.02449243\n",
      "Iteration 8691, loss = 0.02448771\n",
      "Iteration 8692, loss = 0.02448298\n",
      "Iteration 8693, loss = 0.02447824\n",
      "Iteration 8694, loss = 0.02447351\n",
      "Iteration 8695, loss = 0.02446877\n",
      "Iteration 8696, loss = 0.02446403\n",
      "Iteration 8697, loss = 0.02445928\n",
      "Iteration 8698, loss = 0.02445453\n",
      "Iteration 8699, loss = 0.02444978\n",
      "Iteration 8700, loss = 0.02444503\n",
      "Iteration 8701, loss = 0.02444027\n",
      "Iteration 8702, loss = 0.02443551\n",
      "Iteration 8703, loss = 0.02443075\n",
      "Iteration 8704, loss = 0.02442599\n",
      "Iteration 8705, loss = 0.02442122\n",
      "Iteration 8706, loss = 0.02441645\n",
      "Iteration 8707, loss = 0.02441167\n",
      "Iteration 8708, loss = 0.02440689\n",
      "Iteration 8709, loss = 0.02440211\n",
      "Iteration 8710, loss = 0.02439733\n",
      "Iteration 8711, loss = 0.02439254\n",
      "Iteration 8712, loss = 0.02438775\n",
      "Iteration 8713, loss = 0.02438296\n",
      "Iteration 8714, loss = 0.02437817\n",
      "Iteration 8715, loss = 0.02437337\n",
      "Iteration 8716, loss = 0.02436856\n",
      "Iteration 8717, loss = 0.02436376\n",
      "Iteration 8718, loss = 0.02435895\n",
      "Iteration 8719, loss = 0.02435414\n",
      "Iteration 8720, loss = 0.02434932\n",
      "Iteration 8721, loss = 0.02434451\n",
      "Iteration 8722, loss = 0.02433968\n",
      "Iteration 8723, loss = 0.02433486\n",
      "Iteration 8724, loss = 0.02433003\n",
      "Iteration 8725, loss = 0.02432520\n",
      "Iteration 8726, loss = 0.02432037\n",
      "Iteration 8727, loss = 0.02431553\n",
      "Iteration 8728, loss = 0.02431069\n",
      "Iteration 8729, loss = 0.02430585\n",
      "Iteration 8730, loss = 0.02430100\n",
      "Iteration 8731, loss = 0.02429615\n",
      "Iteration 8732, loss = 0.02429129\n",
      "Iteration 8733, loss = 0.02428644\n",
      "Iteration 8734, loss = 0.02428158\n",
      "Iteration 8735, loss = 0.02427671\n",
      "Iteration 8736, loss = 0.02427185\n",
      "Iteration 8737, loss = 0.02426698\n",
      "Iteration 8738, loss = 0.02426210\n",
      "Iteration 8739, loss = 0.02425723\n",
      "Iteration 8740, loss = 0.02425234\n",
      "Iteration 8741, loss = 0.02424746\n",
      "Iteration 8742, loss = 0.02424257\n",
      "Iteration 8743, loss = 0.02423768\n",
      "Iteration 8744, loss = 0.02423279\n",
      "Iteration 8745, loss = 0.02422789\n",
      "Iteration 8746, loss = 0.02422299\n",
      "Iteration 8747, loss = 0.02421809\n",
      "Iteration 8748, loss = 0.02421318\n",
      "Iteration 8749, loss = 0.02420827\n",
      "Iteration 8750, loss = 0.02420335\n",
      "Iteration 8751, loss = 0.02419843\n",
      "Iteration 8752, loss = 0.02419351\n",
      "Iteration 8753, loss = 0.02418858\n",
      "Iteration 8754, loss = 0.02418365\n",
      "Iteration 8755, loss = 0.02417872\n",
      "Iteration 8756, loss = 0.02417379\n",
      "Iteration 8757, loss = 0.02416885\n",
      "Iteration 8758, loss = 0.02416390\n",
      "Iteration 8759, loss = 0.02415896\n",
      "Iteration 8760, loss = 0.02415401\n",
      "Iteration 8761, loss = 0.02414905\n",
      "Iteration 8762, loss = 0.02414409\n",
      "Iteration 8763, loss = 0.02413913\n",
      "Iteration 8764, loss = 0.02413417\n",
      "Iteration 8765, loss = 0.02412920\n",
      "Iteration 8766, loss = 0.02412423\n",
      "Iteration 8767, loss = 0.02411925\n",
      "Iteration 8768, loss = 0.02411427\n",
      "Iteration 8769, loss = 0.02410929\n",
      "Iteration 8770, loss = 0.02410430\n",
      "Iteration 8771, loss = 0.02409931\n",
      "Iteration 8772, loss = 0.02409431\n",
      "Iteration 8773, loss = 0.02408932\n",
      "Iteration 8774, loss = 0.02408431\n",
      "Iteration 8775, loss = 0.02407931\n",
      "Iteration 8776, loss = 0.02407430\n",
      "Iteration 8777, loss = 0.02406929\n",
      "Iteration 8778, loss = 0.02406427\n",
      "Iteration 8779, loss = 0.02405925\n",
      "Iteration 8780, loss = 0.02405422\n",
      "Iteration 8781, loss = 0.02404919\n",
      "Iteration 8782, loss = 0.02404416\n",
      "Iteration 8783, loss = 0.02403913\n",
      "Iteration 8784, loss = 0.02403409\n",
      "Iteration 8785, loss = 0.02402904\n",
      "Iteration 8786, loss = 0.02402400\n",
      "Iteration 8787, loss = 0.02401894\n",
      "Iteration 8788, loss = 0.02401389\n",
      "Iteration 8789, loss = 0.02400883\n",
      "Iteration 8790, loss = 0.02400377\n",
      "Iteration 8791, loss = 0.02399870\n",
      "Iteration 8792, loss = 0.02399363\n",
      "Iteration 8793, loss = 0.02398855\n",
      "Iteration 8794, loss = 0.02398348\n",
      "Iteration 8795, loss = 0.02397839\n",
      "Iteration 8796, loss = 0.02397331\n",
      "Iteration 8797, loss = 0.02396822\n",
      "Iteration 8798, loss = 0.02396312\n",
      "Iteration 8799, loss = 0.02395802\n",
      "Iteration 8800, loss = 0.02395292\n",
      "Iteration 8801, loss = 0.02394782\n",
      "Iteration 8802, loss = 0.02394271\n",
      "Iteration 8803, loss = 0.02393759\n",
      "Iteration 8804, loss = 0.02393247\n",
      "Iteration 8805, loss = 0.02392735\n",
      "Iteration 8806, loss = 0.02392222\n",
      "Iteration 8807, loss = 0.02391709\n",
      "Iteration 8808, loss = 0.02391196\n",
      "Iteration 8809, loss = 0.02390682\n",
      "Iteration 8810, loss = 0.02390168\n",
      "Iteration 8811, loss = 0.02389653\n",
      "Iteration 8812, loss = 0.02389138\n",
      "Iteration 8813, loss = 0.02388623\n",
      "Iteration 8814, loss = 0.02388107\n",
      "Iteration 8815, loss = 0.02387591\n",
      "Iteration 8816, loss = 0.02387074\n",
      "Iteration 8817, loss = 0.02386557\n",
      "Iteration 8818, loss = 0.02386039\n",
      "Iteration 8819, loss = 0.02385521\n",
      "Iteration 8820, loss = 0.02385003\n",
      "Iteration 8821, loss = 0.02384484\n",
      "Iteration 8822, loss = 0.02383965\n",
      "Iteration 8823, loss = 0.02383446\n",
      "Iteration 8824, loss = 0.02382926\n",
      "Iteration 8825, loss = 0.02382405\n",
      "Iteration 8826, loss = 0.02381884\n",
      "Iteration 8827, loss = 0.02381363\n",
      "Iteration 8828, loss = 0.02380842\n",
      "Iteration 8829, loss = 0.02380320\n",
      "Iteration 8830, loss = 0.02379797\n",
      "Iteration 8831, loss = 0.02379274\n",
      "Iteration 8832, loss = 0.02378751\n",
      "Iteration 8833, loss = 0.02378227\n",
      "Iteration 8834, loss = 0.02377703\n",
      "Iteration 8835, loss = 0.02377178\n",
      "Iteration 8836, loss = 0.02376653\n",
      "Iteration 8837, loss = 0.02376128\n",
      "Iteration 8838, loss = 0.02375602\n",
      "Iteration 8839, loss = 0.02375076\n",
      "Iteration 8840, loss = 0.02374549\n",
      "Iteration 8841, loss = 0.02374022\n",
      "Iteration 8842, loss = 0.02373494\n",
      "Iteration 8843, loss = 0.02372966\n",
      "Iteration 8844, loss = 0.02372438\n",
      "Iteration 8845, loss = 0.02371909\n",
      "Iteration 8846, loss = 0.02371379\n",
      "Iteration 8847, loss = 0.02370850\n",
      "Iteration 8848, loss = 0.02370319\n",
      "Iteration 8849, loss = 0.02369789\n",
      "Iteration 8850, loss = 0.02369258\n",
      "Iteration 8851, loss = 0.02368726\n",
      "Iteration 8852, loss = 0.02368194\n",
      "Iteration 8853, loss = 0.02367662\n",
      "Iteration 8854, loss = 0.02367129\n",
      "Iteration 8855, loss = 0.02366596\n",
      "Iteration 8856, loss = 0.02366062\n",
      "Iteration 8857, loss = 0.02365528\n",
      "Iteration 8858, loss = 0.02364994\n",
      "Iteration 8859, loss = 0.02364459\n",
      "Iteration 8860, loss = 0.02363923\n",
      "Iteration 8861, loss = 0.02363387\n",
      "Iteration 8862, loss = 0.02362851\n",
      "Iteration 8863, loss = 0.02362314\n",
      "Iteration 8864, loss = 0.02361777\n",
      "Iteration 8865, loss = 0.02361239\n",
      "Iteration 8866, loss = 0.02360701\n",
      "Iteration 8867, loss = 0.02360163\n",
      "Iteration 8868, loss = 0.02359624\n",
      "Iteration 8869, loss = 0.02359084\n",
      "Iteration 8870, loss = 0.02358544\n",
      "Iteration 8871, loss = 0.02358004\n",
      "Iteration 8872, loss = 0.02357463\n",
      "Iteration 8873, loss = 0.02356922\n",
      "Iteration 8874, loss = 0.02356380\n",
      "Iteration 8875, loss = 0.02355838\n",
      "Iteration 8876, loss = 0.02355295\n",
      "Iteration 8877, loss = 0.02354752\n",
      "Iteration 8878, loss = 0.02354209\n",
      "Iteration 8879, loss = 0.02353665\n",
      "Iteration 8880, loss = 0.02353121\n",
      "Iteration 8881, loss = 0.02352576\n",
      "Iteration 8882, loss = 0.02352030\n",
      "Iteration 8883, loss = 0.02351485\n",
      "Iteration 8884, loss = 0.02350938\n",
      "Iteration 8885, loss = 0.02350392\n",
      "Iteration 8886, loss = 0.02349844\n",
      "Iteration 8887, loss = 0.02349297\n",
      "Iteration 8888, loss = 0.02348749\n",
      "Iteration 8889, loss = 0.02348200\n",
      "Iteration 8890, loss = 0.02347651\n",
      "Iteration 8891, loss = 0.02347102\n",
      "Iteration 8892, loss = 0.02346552\n",
      "Iteration 8893, loss = 0.02346001\n",
      "Iteration 8894, loss = 0.02345451\n",
      "Iteration 8895, loss = 0.02344899\n",
      "Iteration 8896, loss = 0.02344347\n",
      "Iteration 8897, loss = 0.02343795\n",
      "Iteration 8898, loss = 0.02343243\n",
      "Iteration 8899, loss = 0.02342689\n",
      "Iteration 8900, loss = 0.02342136\n",
      "Iteration 8901, loss = 0.02341582\n",
      "Iteration 8902, loss = 0.02341027\n",
      "Iteration 8903, loss = 0.02340472\n",
      "Iteration 8904, loss = 0.02339916\n",
      "Iteration 8905, loss = 0.02339360\n",
      "Iteration 8906, loss = 0.02338804\n",
      "Iteration 8907, loss = 0.02338247\n",
      "Iteration 8908, loss = 0.02337689\n",
      "Iteration 8909, loss = 0.02337132\n",
      "Iteration 8910, loss = 0.02336573\n",
      "Iteration 8911, loss = 0.02336014\n",
      "Iteration 8912, loss = 0.02335455\n",
      "Iteration 8913, loss = 0.02334895\n",
      "Iteration 8914, loss = 0.02334335\n",
      "Iteration 8915, loss = 0.02333774\n",
      "Iteration 8916, loss = 0.02333213\n",
      "Iteration 8917, loss = 0.02332651\n",
      "Iteration 8918, loss = 0.02332089\n",
      "Iteration 8919, loss = 0.02331526\n",
      "Iteration 8920, loss = 0.02330963\n",
      "Iteration 8921, loss = 0.02330399\n",
      "Iteration 8922, loss = 0.02329835\n",
      "Iteration 8923, loss = 0.02329271\n",
      "Iteration 8924, loss = 0.02328705\n",
      "Iteration 8925, loss = 0.02328140\n",
      "Iteration 8926, loss = 0.02327574\n",
      "Iteration 8927, loss = 0.02327007\n",
      "Iteration 8928, loss = 0.02326440\n",
      "Iteration 8929, loss = 0.02325872\n",
      "Iteration 8930, loss = 0.02325304\n",
      "Iteration 8931, loss = 0.02324736\n",
      "Iteration 8932, loss = 0.02324167\n",
      "Iteration 8933, loss = 0.02323597\n",
      "Iteration 8934, loss = 0.02323027\n",
      "Iteration 8935, loss = 0.02322457\n",
      "Iteration 8936, loss = 0.02321886\n",
      "Iteration 8937, loss = 0.02321314\n",
      "Iteration 8938, loss = 0.02320742\n",
      "Iteration 8939, loss = 0.02320170\n",
      "Iteration 8940, loss = 0.02319597\n",
      "Iteration 8941, loss = 0.02319023\n",
      "Iteration 8942, loss = 0.02318449\n",
      "Iteration 8943, loss = 0.02317875\n",
      "Iteration 8944, loss = 0.02317300\n",
      "Iteration 8945, loss = 0.02316724\n",
      "Iteration 8946, loss = 0.02316148\n",
      "Iteration 8947, loss = 0.02315572\n",
      "Iteration 8948, loss = 0.02314995\n",
      "Iteration 8949, loss = 0.02314417\n",
      "Iteration 8950, loss = 0.02313839\n",
      "Iteration 8951, loss = 0.02313260\n",
      "Iteration 8952, loss = 0.02312681\n",
      "Iteration 8953, loss = 0.02312102\n",
      "Iteration 8954, loss = 0.02311522\n",
      "Iteration 8955, loss = 0.02310941\n",
      "Iteration 8956, loss = 0.02310360\n",
      "Iteration 8957, loss = 0.02309778\n",
      "Iteration 8958, loss = 0.02309196\n",
      "Iteration 8959, loss = 0.02308614\n",
      "Iteration 8960, loss = 0.02308030\n",
      "Iteration 8961, loss = 0.02307447\n",
      "Iteration 8962, loss = 0.02306863\n",
      "Iteration 8963, loss = 0.02306278\n",
      "Iteration 8964, loss = 0.02305693\n",
      "Iteration 8965, loss = 0.02305107\n",
      "Iteration 8966, loss = 0.02304521\n",
      "Iteration 8967, loss = 0.02303934\n",
      "Iteration 8968, loss = 0.02303346\n",
      "Iteration 8969, loss = 0.02302759\n",
      "Iteration 8970, loss = 0.02302170\n",
      "Iteration 8971, loss = 0.02301581\n",
      "Iteration 8972, loss = 0.02300992\n",
      "Iteration 8973, loss = 0.02300402\n",
      "Iteration 8974, loss = 0.02299811\n",
      "Iteration 8975, loss = 0.02299220\n",
      "Iteration 8976, loss = 0.02298629\n",
      "Iteration 8977, loss = 0.02298037\n",
      "Iteration 8978, loss = 0.02297444\n",
      "Iteration 8979, loss = 0.02296851\n",
      "Iteration 8980, loss = 0.02296257\n",
      "Iteration 8981, loss = 0.02295663\n",
      "Iteration 8982, loss = 0.02295068\n",
      "Iteration 8983, loss = 0.02294473\n",
      "Iteration 8984, loss = 0.02293877\n",
      "Iteration 8985, loss = 0.02293281\n",
      "Iteration 8986, loss = 0.02292684\n",
      "Iteration 8987, loss = 0.02292086\n",
      "Iteration 8988, loss = 0.02291488\n",
      "Iteration 8989, loss = 0.02290890\n",
      "Iteration 8990, loss = 0.02290291\n",
      "Iteration 8991, loss = 0.02289691\n",
      "Iteration 8992, loss = 0.02289091\n",
      "Iteration 8993, loss = 0.02288490\n",
      "Iteration 8994, loss = 0.02287889\n",
      "Iteration 8995, loss = 0.02287287\n",
      "Iteration 8996, loss = 0.02286685\n",
      "Iteration 8997, loss = 0.02286082\n",
      "Iteration 8998, loss = 0.02285478\n",
      "Iteration 8999, loss = 0.02284874\n",
      "Iteration 9000, loss = 0.02284269\n",
      "Iteration 9001, loss = 0.02283664\n",
      "Iteration 9002, loss = 0.02283058\n",
      "Iteration 9003, loss = 0.02282452\n",
      "Iteration 9004, loss = 0.02281845\n",
      "Iteration 9005, loss = 0.02281238\n",
      "Iteration 9006, loss = 0.02280630\n",
      "Iteration 9007, loss = 0.02280021\n",
      "Iteration 9008, loss = 0.02279412\n",
      "Iteration 9009, loss = 0.02278802\n",
      "Iteration 9010, loss = 0.02278192\n",
      "Iteration 9011, loss = 0.02277581\n",
      "Iteration 9012, loss = 0.02276970\n",
      "Iteration 9013, loss = 0.02276358\n",
      "Iteration 9014, loss = 0.02275745\n",
      "Iteration 9015, loss = 0.02275132\n",
      "Iteration 9016, loss = 0.02274519\n",
      "Iteration 9017, loss = 0.02273904\n",
      "Iteration 9018, loss = 0.02273289\n",
      "Iteration 9019, loss = 0.02272674\n",
      "Iteration 9020, loss = 0.02272058\n",
      "Iteration 9021, loss = 0.02271441\n",
      "Iteration 9022, loss = 0.02270824\n",
      "Iteration 9023, loss = 0.02270206\n",
      "Iteration 9024, loss = 0.02269588\n",
      "Iteration 9025, loss = 0.02268969\n",
      "Iteration 9026, loss = 0.02268349\n",
      "Iteration 9027, loss = 0.02267729\n",
      "Iteration 9028, loss = 0.02267108\n",
      "Iteration 9029, loss = 0.02266487\n",
      "Iteration 9030, loss = 0.02265865\n",
      "Iteration 9031, loss = 0.02265243\n",
      "Iteration 9032, loss = 0.02264619\n",
      "Iteration 9033, loss = 0.02263996\n",
      "Iteration 9034, loss = 0.02263371\n",
      "Iteration 9035, loss = 0.02262746\n",
      "Iteration 9036, loss = 0.02262121\n",
      "Iteration 9037, loss = 0.02261495\n",
      "Iteration 9038, loss = 0.02260868\n",
      "Iteration 9039, loss = 0.02260240\n",
      "Iteration 9040, loss = 0.02259612\n",
      "Iteration 9041, loss = 0.02258984\n",
      "Iteration 9042, loss = 0.02258355\n",
      "Iteration 9043, loss = 0.02257725\n",
      "Iteration 9044, loss = 0.02257094\n",
      "Iteration 9045, loss = 0.02256463\n",
      "Iteration 9046, loss = 0.02255831\n",
      "Iteration 9047, loss = 0.02255199\n",
      "Iteration 9048, loss = 0.02254566\n",
      "Iteration 9049, loss = 0.02253933\n",
      "Iteration 9050, loss = 0.02253298\n",
      "Iteration 9051, loss = 0.02252664\n",
      "Iteration 9052, loss = 0.02252028\n",
      "Iteration 9053, loss = 0.02251392\n",
      "Iteration 9054, loss = 0.02250755\n",
      "Iteration 9055, loss = 0.02250118\n",
      "Iteration 9056, loss = 0.02249480\n",
      "Iteration 9057, loss = 0.02248841\n",
      "Iteration 9058, loss = 0.02248202\n",
      "Iteration 9059, loss = 0.02247562\n",
      "Iteration 9060, loss = 0.02246921\n",
      "Iteration 9061, loss = 0.02246280\n",
      "Iteration 9062, loss = 0.02245638\n",
      "Iteration 9063, loss = 0.02244995\n",
      "Iteration 9064, loss = 0.02244352\n",
      "Iteration 9065, loss = 0.02243708\n",
      "Iteration 9066, loss = 0.02243064\n",
      "Iteration 9067, loss = 0.02242419\n",
      "Iteration 9068, loss = 0.02241773\n",
      "Iteration 9069, loss = 0.02241126\n",
      "Iteration 9070, loss = 0.02240479\n",
      "Iteration 9071, loss = 0.02239831\n",
      "Iteration 9072, loss = 0.02239183\n",
      "Iteration 9073, loss = 0.02238533\n",
      "Iteration 9074, loss = 0.02237884\n",
      "Iteration 9075, loss = 0.02237233\n",
      "Iteration 9076, loss = 0.02236582\n",
      "Iteration 9077, loss = 0.02235930\n",
      "Iteration 9078, loss = 0.02235277\n",
      "Iteration 9079, loss = 0.02234624\n",
      "Iteration 9080, loss = 0.02233970\n",
      "Iteration 9081, loss = 0.02233315\n",
      "Iteration 9082, loss = 0.02232660\n",
      "Iteration 9083, loss = 0.02232004\n",
      "Iteration 9084, loss = 0.02231347\n",
      "Iteration 9085, loss = 0.02230690\n",
      "Iteration 9086, loss = 0.02230032\n",
      "Iteration 9087, loss = 0.02229373\n",
      "Iteration 9088, loss = 0.02228713\n",
      "Iteration 9089, loss = 0.02228053\n",
      "Iteration 9090, loss = 0.02227392\n",
      "Iteration 9091, loss = 0.02226730\n",
      "Iteration 9092, loss = 0.02226068\n",
      "Iteration 9093, loss = 0.02225405\n",
      "Iteration 9094, loss = 0.02224741\n",
      "Iteration 9095, loss = 0.02224076\n",
      "Iteration 9096, loss = 0.02223411\n",
      "Iteration 9097, loss = 0.02222745\n",
      "Iteration 9098, loss = 0.02222078\n",
      "Iteration 9099, loss = 0.02221411\n",
      "Iteration 9100, loss = 0.02220742\n",
      "Iteration 9101, loss = 0.02220073\n",
      "Iteration 9102, loss = 0.02219404\n",
      "Iteration 9103, loss = 0.02218733\n",
      "Iteration 9104, loss = 0.02218062\n",
      "Iteration 9105, loss = 0.02217390\n",
      "Iteration 9106, loss = 0.02216717\n",
      "Iteration 9107, loss = 0.02216044\n",
      "Iteration 9108, loss = 0.02215370\n",
      "Iteration 9109, loss = 0.02214695\n",
      "Iteration 9110, loss = 0.02214019\n",
      "Iteration 9111, loss = 0.02213342\n",
      "Iteration 9112, loss = 0.02212665\n",
      "Iteration 9113, loss = 0.02211987\n",
      "Iteration 9114, loss = 0.02211308\n",
      "Iteration 9115, loss = 0.02210629\n",
      "Iteration 9116, loss = 0.02209948\n",
      "Iteration 9117, loss = 0.02209267\n",
      "Iteration 9118, loss = 0.02208585\n",
      "Iteration 9119, loss = 0.02207902\n",
      "Iteration 9120, loss = 0.02207219\n",
      "Iteration 9121, loss = 0.02206535\n",
      "Iteration 9122, loss = 0.02205849\n",
      "Iteration 9123, loss = 0.02205163\n",
      "Iteration 9124, loss = 0.02204477\n",
      "Iteration 9125, loss = 0.02203789\n",
      "Iteration 9126, loss = 0.02203101\n",
      "Iteration 9127, loss = 0.02202412\n",
      "Iteration 9128, loss = 0.02201722\n",
      "Iteration 9129, loss = 0.02201031\n",
      "Iteration 9130, loss = 0.02200339\n",
      "Iteration 9131, loss = 0.02199647\n",
      "Iteration 9132, loss = 0.02198953\n",
      "Iteration 9133, loss = 0.02198259\n",
      "Iteration 9134, loss = 0.02197564\n",
      "Iteration 9135, loss = 0.02196868\n",
      "Iteration 9136, loss = 0.02196172\n",
      "Iteration 9137, loss = 0.02195474\n",
      "Iteration 9138, loss = 0.02194776\n",
      "Iteration 9139, loss = 0.02194077\n",
      "Iteration 9140, loss = 0.02193377\n",
      "Iteration 9141, loss = 0.02192676\n",
      "Iteration 9142, loss = 0.02191974\n",
      "Iteration 9143, loss = 0.02191271\n",
      "Iteration 9144, loss = 0.02190568\n",
      "Iteration 9145, loss = 0.02189863\n",
      "Iteration 9146, loss = 0.02189158\n",
      "Iteration 9147, loss = 0.02188452\n",
      "Iteration 9148, loss = 0.02187745\n",
      "Iteration 9149, loss = 0.02187037\n",
      "Iteration 9150, loss = 0.02186328\n",
      "Iteration 9151, loss = 0.02185618\n",
      "Iteration 9152, loss = 0.02184908\n",
      "Iteration 9153, loss = 0.02184196\n",
      "Iteration 9154, loss = 0.02183484\n",
      "Iteration 9155, loss = 0.02182770\n",
      "Iteration 9156, loss = 0.02182056\n",
      "Iteration 9157, loss = 0.02181341\n",
      "Iteration 9158, loss = 0.02180625\n",
      "Iteration 9159, loss = 0.02179908\n",
      "Iteration 9160, loss = 0.02179190\n",
      "Iteration 9161, loss = 0.02178471\n",
      "Iteration 9162, loss = 0.02177751\n",
      "Iteration 9163, loss = 0.02177030\n",
      "Iteration 9164, loss = 0.02176308\n",
      "Iteration 9165, loss = 0.02175586\n",
      "Iteration 9166, loss = 0.02174862\n",
      "Iteration 9167, loss = 0.02174137\n",
      "Iteration 9168, loss = 0.02173412\n",
      "Iteration 9169, loss = 0.02172685\n",
      "Iteration 9170, loss = 0.02171958\n",
      "Iteration 9171, loss = 0.02171229\n",
      "Iteration 9172, loss = 0.02170500\n",
      "Iteration 9173, loss = 0.02169769\n",
      "Iteration 9174, loss = 0.02169038\n",
      "Iteration 9175, loss = 0.02168305\n",
      "Iteration 9176, loss = 0.02167572\n",
      "Iteration 9177, loss = 0.02166837\n",
      "Iteration 9178, loss = 0.02166102\n",
      "Iteration 9179, loss = 0.02165365\n",
      "Iteration 9180, loss = 0.02164628\n",
      "Iteration 9181, loss = 0.02163890\n",
      "Iteration 9182, loss = 0.02163150\n",
      "Iteration 9183, loss = 0.02162409\n",
      "Iteration 9184, loss = 0.02161668\n",
      "Iteration 9185, loss = 0.02160925\n",
      "Iteration 9186, loss = 0.02160182\n",
      "Iteration 9187, loss = 0.02159437\n",
      "Iteration 9188, loss = 0.02158691\n",
      "Iteration 9189, loss = 0.02157945\n",
      "Iteration 9190, loss = 0.02157197\n",
      "Iteration 9191, loss = 0.02156448\n",
      "Iteration 9192, loss = 0.02155698\n",
      "Iteration 9193, loss = 0.02154947\n",
      "Iteration 9194, loss = 0.02154195\n",
      "Iteration 9195, loss = 0.02153442\n",
      "Iteration 9196, loss = 0.02152687\n",
      "Iteration 9197, loss = 0.02151932\n",
      "Iteration 9198, loss = 0.02151176\n",
      "Iteration 9199, loss = 0.02150418\n",
      "Iteration 9200, loss = 0.02149660\n",
      "Iteration 9201, loss = 0.02148900\n",
      "Iteration 9202, loss = 0.02148139\n",
      "Iteration 9203, loss = 0.02147377\n",
      "Iteration 9204, loss = 0.02146614\n",
      "Iteration 9205, loss = 0.02145850\n",
      "Iteration 9206, loss = 0.02145085\n",
      "Iteration 9207, loss = 0.02144319\n",
      "Iteration 9208, loss = 0.02143551\n",
      "Iteration 9209, loss = 0.02142782\n",
      "Iteration 9210, loss = 0.02142013\n",
      "Iteration 9211, loss = 0.02141242\n",
      "Iteration 9212, loss = 0.02140470\n",
      "Iteration 9213, loss = 0.02139696\n",
      "Iteration 9214, loss = 0.02138922\n",
      "Iteration 9215, loss = 0.02138146\n",
      "Iteration 9216, loss = 0.02137370\n",
      "Iteration 9217, loss = 0.02136592\n",
      "Iteration 9218, loss = 0.02135813\n",
      "Iteration 9219, loss = 0.02135033\n",
      "Iteration 9220, loss = 0.02134251\n",
      "Iteration 9221, loss = 0.02133469\n",
      "Iteration 9222, loss = 0.02132685\n",
      "Iteration 9223, loss = 0.02131900\n",
      "Iteration 9224, loss = 0.02131114\n",
      "Iteration 9225, loss = 0.02130327\n",
      "Iteration 9226, loss = 0.02129538\n",
      "Iteration 9227, loss = 0.02128748\n",
      "Iteration 9228, loss = 0.02127957\n",
      "Iteration 9229, loss = 0.02127165\n",
      "Iteration 9230, loss = 0.02126372\n",
      "Iteration 9231, loss = 0.02125577\n",
      "Iteration 9232, loss = 0.02124782\n",
      "Iteration 9233, loss = 0.02123985\n",
      "Iteration 9234, loss = 0.02123186\n",
      "Iteration 9235, loss = 0.02122387\n",
      "Iteration 9236, loss = 0.02121586\n",
      "Iteration 9237, loss = 0.02120784\n",
      "Iteration 9238, loss = 0.02119981\n",
      "Iteration 9239, loss = 0.02119177\n",
      "Iteration 9240, loss = 0.02118371\n",
      "Iteration 9241, loss = 0.02117564\n",
      "Iteration 9242, loss = 0.02116756\n",
      "Iteration 9243, loss = 0.02115946\n",
      "Iteration 9244, loss = 0.02115136\n",
      "Iteration 9245, loss = 0.02114324\n",
      "Iteration 9246, loss = 0.02113510\n",
      "Iteration 9247, loss = 0.02112696\n",
      "Iteration 9248, loss = 0.02111880\n",
      "Iteration 9249, loss = 0.02111063\n",
      "Iteration 9250, loss = 0.02110245\n",
      "Iteration 9251, loss = 0.02109425\n",
      "Iteration 9252, loss = 0.02108604\n",
      "Iteration 9253, loss = 0.02107782\n",
      "Iteration 9254, loss = 0.02106959\n",
      "Iteration 9255, loss = 0.02106134\n",
      "Iteration 9256, loss = 0.02105308\n",
      "Iteration 9257, loss = 0.02104480\n",
      "Iteration 9258, loss = 0.02103652\n",
      "Iteration 9259, loss = 0.02102822\n",
      "Iteration 9260, loss = 0.02101991\n",
      "Iteration 9261, loss = 0.02101158\n",
      "Iteration 9262, loss = 0.02100324\n",
      "Iteration 9263, loss = 0.02099489\n",
      "Iteration 9264, loss = 0.02098653\n",
      "Iteration 9265, loss = 0.02097815\n",
      "Iteration 9266, loss = 0.02096976\n",
      "Iteration 9267, loss = 0.02096136\n",
      "Iteration 9268, loss = 0.02095294\n",
      "Iteration 9269, loss = 0.02094451\n",
      "Iteration 9270, loss = 0.02093607\n",
      "Iteration 9271, loss = 0.02092761\n",
      "Iteration 9272, loss = 0.02091914\n",
      "Iteration 9273, loss = 0.02091066\n",
      "Iteration 9274, loss = 0.02090216\n",
      "Iteration 9275, loss = 0.02089365\n",
      "Iteration 9276, loss = 0.02088513\n",
      "Iteration 9277, loss = 0.02087660\n",
      "Iteration 9278, loss = 0.02086805\n",
      "Iteration 9279, loss = 0.02085949\n",
      "Iteration 9280, loss = 0.02085091\n",
      "Iteration 9281, loss = 0.02084233\n",
      "Iteration 9282, loss = 0.02083372\n",
      "Iteration 9283, loss = 0.02082511\n",
      "Iteration 9284, loss = 0.02081648\n",
      "Iteration 9285, loss = 0.02080784\n",
      "Iteration 9286, loss = 0.02079919\n",
      "Iteration 9287, loss = 0.02079052\n",
      "Iteration 9288, loss = 0.02078184\n",
      "Iteration 9289, loss = 0.02077315\n",
      "Iteration 9290, loss = 0.02076444\n",
      "Iteration 9291, loss = 0.02075572\n",
      "Iteration 9292, loss = 0.02074699\n",
      "Iteration 9293, loss = 0.02073824\n",
      "Iteration 9294, loss = 0.02072948\n",
      "Iteration 9295, loss = 0.02072071\n",
      "Iteration 9296, loss = 0.02071193\n",
      "Iteration 9297, loss = 0.02070313\n",
      "Iteration 9298, loss = 0.02069432\n",
      "Iteration 9299, loss = 0.02068549\n",
      "Iteration 9300, loss = 0.02067665\n",
      "Iteration 9301, loss = 0.02066780\n",
      "Iteration 9302, loss = 0.02065894\n",
      "Iteration 9303, loss = 0.02065006\n",
      "Iteration 9304, loss = 0.02064117\n",
      "Iteration 9305, loss = 0.02063227\n",
      "Iteration 9306, loss = 0.02062335\n",
      "Iteration 9307, loss = 0.02061443\n",
      "Iteration 9308, loss = 0.02060548\n",
      "Iteration 9309, loss = 0.02059653\n",
      "Iteration 9310, loss = 0.02058756\n",
      "Iteration 9311, loss = 0.02057858\n",
      "Iteration 9312, loss = 0.02056959\n",
      "Iteration 9313, loss = 0.02056059\n",
      "Iteration 9314, loss = 0.02055157\n",
      "Iteration 9315, loss = 0.02054254\n",
      "Iteration 9316, loss = 0.02053350\n",
      "Iteration 9317, loss = 0.02052444\n",
      "Iteration 9318, loss = 0.02051537\n",
      "Iteration 9319, loss = 0.02050629\n",
      "Iteration 9320, loss = 0.02049720\n",
      "Iteration 9321, loss = 0.02048809\n",
      "Iteration 9322, loss = 0.02047897\n",
      "Iteration 9323, loss = 0.02046984\n",
      "Iteration 9324, loss = 0.02046070\n",
      "Iteration 9325, loss = 0.02045155\n",
      "Iteration 9326, loss = 0.02044238\n",
      "Iteration 9327, loss = 0.02043320\n",
      "Iteration 9328, loss = 0.02042401\n",
      "Iteration 9329, loss = 0.02041480\n",
      "Iteration 9330, loss = 0.02040559\n",
      "Iteration 9331, loss = 0.02039636\n",
      "Iteration 9332, loss = 0.02038712\n",
      "Iteration 9333, loss = 0.02037787\n",
      "Iteration 9334, loss = 0.02036860\n",
      "Iteration 9335, loss = 0.02035933\n",
      "Iteration 9336, loss = 0.02035004\n",
      "Iteration 9337, loss = 0.02034074\n",
      "Iteration 9338, loss = 0.02033143\n",
      "Iteration 9339, loss = 0.02032211\n",
      "Iteration 9340, loss = 0.02031278\n",
      "Iteration 9341, loss = 0.02030343\n",
      "Iteration 9342, loss = 0.02029408\n",
      "Iteration 9343, loss = 0.02028471\n",
      "Iteration 9344, loss = 0.02027533\n",
      "Iteration 9345, loss = 0.02026594\n",
      "Iteration 9346, loss = 0.02025654\n",
      "Iteration 9347, loss = 0.02024712\n",
      "Iteration 9348, loss = 0.02023770\n",
      "Iteration 9349, loss = 0.02022827\n",
      "Iteration 9350, loss = 0.02021882\n",
      "Iteration 9351, loss = 0.02020936\n",
      "Iteration 9352, loss = 0.02019990\n",
      "Iteration 9353, loss = 0.02019042\n",
      "Iteration 9354, loss = 0.02018093\n",
      "Iteration 9355, loss = 0.02017143\n",
      "Iteration 9356, loss = 0.02016192\n",
      "Iteration 9357, loss = 0.02015240\n",
      "Iteration 9358, loss = 0.02014287\n",
      "Iteration 9359, loss = 0.02013333\n",
      "Iteration 9360, loss = 0.02012377\n",
      "Iteration 9361, loss = 0.02011421\n",
      "Iteration 9362, loss = 0.02010464\n",
      "Iteration 9363, loss = 0.02009506\n",
      "Iteration 9364, loss = 0.02008547\n",
      "Iteration 9365, loss = 0.02007587\n",
      "Iteration 9366, loss = 0.02006625\n",
      "Iteration 9367, loss = 0.02005663\n",
      "Iteration 9368, loss = 0.02004700\n",
      "Iteration 9369, loss = 0.02003736\n",
      "Iteration 9370, loss = 0.02002771\n",
      "Iteration 9371, loss = 0.02001805\n",
      "Iteration 9372, loss = 0.02000838\n",
      "Iteration 9373, loss = 0.01999870\n",
      "Iteration 9374, loss = 0.01998902\n",
      "Iteration 9375, loss = 0.01997932\n",
      "Iteration 9376, loss = 0.01996961\n",
      "Iteration 9377, loss = 0.01995990\n",
      "Iteration 9378, loss = 0.01995017\n",
      "Iteration 9379, loss = 0.01994044\n",
      "Iteration 9380, loss = 0.01993070\n",
      "Iteration 9381, loss = 0.01992095\n",
      "Iteration 9382, loss = 0.01991119\n",
      "Iteration 9383, loss = 0.01990142\n",
      "Iteration 9384, loss = 0.01989165\n",
      "Iteration 9385, loss = 0.01988186\n",
      "Iteration 9386, loss = 0.01987207\n",
      "Iteration 9387, loss = 0.01986227\n",
      "Iteration 9388, loss = 0.01985246\n",
      "Iteration 9389, loss = 0.01984265\n",
      "Iteration 9390, loss = 0.01983282\n",
      "Iteration 9391, loss = 0.01982299\n",
      "Iteration 9392, loss = 0.01981315\n",
      "Iteration 9393, loss = 0.01980330\n",
      "Iteration 9394, loss = 0.01979344\n",
      "Iteration 9395, loss = 0.01978358\n",
      "Iteration 9396, loss = 0.01977371\n",
      "Iteration 9397, loss = 0.01976383\n",
      "Iteration 9398, loss = 0.01975395\n",
      "Iteration 9399, loss = 0.01974405\n",
      "Iteration 9400, loss = 0.01973415\n",
      "Iteration 9401, loss = 0.01972425\n",
      "Iteration 9402, loss = 0.01971433\n",
      "Iteration 9403, loss = 0.01970441\n",
      "Iteration 9404, loss = 0.01969449\n",
      "Iteration 9405, loss = 0.01968455\n",
      "Iteration 9406, loss = 0.01967461\n",
      "Iteration 9407, loss = 0.01966466\n",
      "Iteration 9408, loss = 0.01965471\n",
      "Iteration 9409, loss = 0.01964475\n",
      "Iteration 9410, loss = 0.01963478\n",
      "Iteration 9411, loss = 0.01962481\n",
      "Iteration 9412, loss = 0.01961483\n",
      "Iteration 9413, loss = 0.01960485\n",
      "Iteration 9414, loss = 0.01959485\n",
      "Iteration 9415, loss = 0.01958486\n",
      "Iteration 9416, loss = 0.01957485\n",
      "Iteration 9417, loss = 0.01956485\n",
      "Iteration 9418, loss = 0.01955483\n",
      "Iteration 9419, loss = 0.01954481\n",
      "Iteration 9420, loss = 0.01953479\n",
      "Iteration 9421, loss = 0.01952476\n",
      "Iteration 9422, loss = 0.01951472\n",
      "Iteration 9423, loss = 0.01950468\n",
      "Iteration 9424, loss = 0.01949463\n",
      "Iteration 9425, loss = 0.01948458\n",
      "Iteration 9426, loss = 0.01947453\n",
      "Iteration 9427, loss = 0.01946446\n",
      "Iteration 9428, loss = 0.01945440\n",
      "Iteration 9429, loss = 0.01944433\n",
      "Iteration 9430, loss = 0.01943425\n",
      "Iteration 9431, loss = 0.01942417\n",
      "Iteration 9432, loss = 0.01941409\n",
      "Iteration 9433, loss = 0.01940400\n",
      "Iteration 9434, loss = 0.01939391\n",
      "Iteration 9435, loss = 0.01938381\n",
      "Iteration 9436, loss = 0.01937371\n",
      "Iteration 9437, loss = 0.01936360\n",
      "Iteration 9438, loss = 0.01935349\n",
      "Iteration 9439, loss = 0.01934338\n",
      "Iteration 9440, loss = 0.01933326\n",
      "Iteration 9441, loss = 0.01932314\n",
      "Iteration 9442, loss = 0.01931301\n",
      "Iteration 9443, loss = 0.01930288\n",
      "Iteration 9444, loss = 0.01929275\n",
      "Iteration 9445, loss = 0.01928261\n",
      "Iteration 9446, loss = 0.01927248\n",
      "Iteration 9447, loss = 0.01926233\n",
      "Iteration 9448, loss = 0.01925219\n",
      "Iteration 9449, loss = 0.01924204\n",
      "Iteration 9450, loss = 0.01923189\n",
      "Iteration 9451, loss = 0.01922173\n",
      "Iteration 9452, loss = 0.01921157\n",
      "Iteration 9453, loss = 0.01920141\n",
      "Iteration 9454, loss = 0.01919125\n",
      "Iteration 9455, loss = 0.01918108\n",
      "Iteration 9456, loss = 0.01917091\n",
      "Iteration 9457, loss = 0.01916074\n",
      "Iteration 9458, loss = 0.01915056\n",
      "Iteration 9459, loss = 0.01914039\n",
      "Iteration 9460, loss = 0.01913021\n",
      "Iteration 9461, loss = 0.01912003\n",
      "Iteration 9462, loss = 0.01910984\n",
      "Iteration 9463, loss = 0.01909966\n",
      "Iteration 9464, loss = 0.01908947\n",
      "Iteration 9465, loss = 0.01907928\n",
      "Iteration 9466, loss = 0.01906909\n",
      "Iteration 9467, loss = 0.01905889\n",
      "Iteration 9468, loss = 0.01904870\n",
      "Iteration 9469, loss = 0.01903850\n",
      "Iteration 9470, loss = 0.01902830\n",
      "Iteration 9471, loss = 0.01901810\n",
      "Iteration 9472, loss = 0.01900790\n",
      "Iteration 9473, loss = 0.01899770\n",
      "Iteration 9474, loss = 0.01898749\n",
      "Iteration 9475, loss = 0.01897728\n",
      "Iteration 9476, loss = 0.01896708\n",
      "Iteration 9477, loss = 0.01895687\n",
      "Iteration 9478, loss = 0.01894666\n",
      "Iteration 9479, loss = 0.01893645\n",
      "Iteration 9480, loss = 0.01892623\n",
      "Iteration 9481, loss = 0.01891602\n",
      "Iteration 9482, loss = 0.01890581\n",
      "Iteration 9483, loss = 0.01889559\n",
      "Iteration 9484, loss = 0.01888538\n",
      "Iteration 9485, loss = 0.01887516\n",
      "Iteration 9486, loss = 0.01886494\n",
      "Iteration 9487, loss = 0.01885473\n",
      "Iteration 9488, loss = 0.01884451\n",
      "Iteration 9489, loss = 0.01883429\n",
      "Iteration 9490, loss = 0.01882407\n",
      "Iteration 9491, loss = 0.01881385\n",
      "Iteration 9492, loss = 0.01880363\n",
      "Iteration 9493, loss = 0.01879341\n",
      "Iteration 9494, loss = 0.01878319\n",
      "Iteration 9495, loss = 0.01877297\n",
      "Iteration 9496, loss = 0.01876275\n",
      "Iteration 9497, loss = 0.01875253\n",
      "Iteration 9498, loss = 0.01874231\n",
      "Iteration 9499, loss = 0.01873209\n",
      "Iteration 9500, loss = 0.01872187\n",
      "Iteration 9501, loss = 0.01871165\n",
      "Iteration 9502, loss = 0.01870144\n",
      "Iteration 9503, loss = 0.01869122\n",
      "Iteration 9504, loss = 0.01868100\n",
      "Iteration 9505, loss = 0.01867078\n",
      "Iteration 9506, loss = 0.01866056\n",
      "Iteration 9507, loss = 0.01865035\n",
      "Iteration 9508, loss = 0.01864013\n",
      "Iteration 9509, loss = 0.01862992\n",
      "Iteration 9510, loss = 0.01861970\n",
      "Iteration 9511, loss = 0.01860949\n",
      "Iteration 9512, loss = 0.01859927\n",
      "Iteration 9513, loss = 0.01858906\n",
      "Iteration 9514, loss = 0.01857885\n",
      "Iteration 9515, loss = 0.01856864\n",
      "Iteration 9516, loss = 0.01855843\n",
      "Iteration 9517, loss = 0.01854822\n",
      "Iteration 9518, loss = 0.01853802\n",
      "Iteration 9519, loss = 0.01852781\n",
      "Iteration 9520, loss = 0.01851761\n",
      "Iteration 9521, loss = 0.01850740\n",
      "Iteration 9522, loss = 0.01849720\n",
      "Iteration 9523, loss = 0.01848700\n",
      "Iteration 9524, loss = 0.01847680\n",
      "Iteration 9525, loss = 0.01846661\n",
      "Iteration 9526, loss = 0.01845641\n",
      "Iteration 9527, loss = 0.01844622\n",
      "Iteration 9528, loss = 0.01843602\n",
      "Iteration 9529, loss = 0.01842583\n",
      "Iteration 9530, loss = 0.01841564\n",
      "Iteration 9531, loss = 0.01840546\n",
      "Iteration 9532, loss = 0.01839527\n",
      "Iteration 9533, loss = 0.01838509\n",
      "Iteration 9534, loss = 0.01837490\n",
      "Iteration 9535, loss = 0.01836472\n",
      "Iteration 9536, loss = 0.01835455\n",
      "Iteration 9537, loss = 0.01834437\n",
      "Iteration 9538, loss = 0.01833420\n",
      "Iteration 9539, loss = 0.01832402\n",
      "Iteration 9540, loss = 0.01831385\n",
      "Iteration 9541, loss = 0.01830369\n",
      "Iteration 9542, loss = 0.01829352\n",
      "Iteration 9543, loss = 0.01828336\n",
      "Iteration 9544, loss = 0.01827320\n",
      "Iteration 9545, loss = 0.01826304\n",
      "Iteration 9546, loss = 0.01825288\n",
      "Iteration 9547, loss = 0.01824273\n",
      "Iteration 9548, loss = 0.01823258\n",
      "Iteration 9549, loss = 0.01822243\n",
      "Iteration 9550, loss = 0.01821229\n",
      "Iteration 9551, loss = 0.01820214\n",
      "Iteration 9552, loss = 0.01819200\n",
      "Iteration 9553, loss = 0.01818186\n",
      "Iteration 9554, loss = 0.01817173\n",
      "Iteration 9555, loss = 0.01816160\n",
      "Iteration 9556, loss = 0.01815147\n",
      "Iteration 9557, loss = 0.01814134\n",
      "Iteration 9558, loss = 0.01813122\n",
      "Iteration 9559, loss = 0.01812110\n",
      "Iteration 9560, loss = 0.01811098\n",
      "Iteration 9561, loss = 0.01810086\n",
      "Iteration 9562, loss = 0.01809075\n",
      "Iteration 9563, loss = 0.01808064\n",
      "Iteration 9564, loss = 0.01807054\n",
      "Iteration 9565, loss = 0.01806043\n",
      "Iteration 9566, loss = 0.01805033\n",
      "Iteration 9567, loss = 0.01804024\n",
      "Iteration 9568, loss = 0.01803014\n",
      "Iteration 9569, loss = 0.01802005\n",
      "Iteration 9570, loss = 0.01800997\n",
      "Iteration 9571, loss = 0.01799988\n",
      "Iteration 9572, loss = 0.01798980\n",
      "Iteration 9573, loss = 0.01797973\n",
      "Iteration 9574, loss = 0.01796965\n",
      "Iteration 9575, loss = 0.01795958\n",
      "Iteration 9576, loss = 0.01794952\n",
      "Iteration 9577, loss = 0.01793946\n",
      "Iteration 9578, loss = 0.01792940\n",
      "Iteration 9579, loss = 0.01791934\n",
      "Iteration 9580, loss = 0.01790929\n",
      "Iteration 9581, loss = 0.01789924\n",
      "Iteration 9582, loss = 0.01788920\n",
      "Iteration 9583, loss = 0.01787916\n",
      "Iteration 9584, loss = 0.01786912\n",
      "Iteration 9585, loss = 0.01785908\n",
      "Iteration 9586, loss = 0.01784906\n",
      "Iteration 9587, loss = 0.01783903\n",
      "Iteration 9588, loss = 0.01782901\n",
      "Iteration 9589, loss = 0.01781899\n",
      "Iteration 9590, loss = 0.01780898\n",
      "Iteration 9591, loss = 0.01779897\n",
      "Iteration 9592, loss = 0.01778896\n",
      "Iteration 9593, loss = 0.01777896\n",
      "Iteration 9594, loss = 0.01776896\n",
      "Iteration 9595, loss = 0.01775896\n",
      "Iteration 9596, loss = 0.01774897\n",
      "Iteration 9597, loss = 0.01773899\n",
      "Iteration 9598, loss = 0.01772901\n",
      "Iteration 9599, loss = 0.01771903\n",
      "Iteration 9600, loss = 0.01770906\n",
      "Iteration 9601, loss = 0.01769909\n",
      "Iteration 9602, loss = 0.01768912\n",
      "Iteration 9603, loss = 0.01767916\n",
      "Iteration 9604, loss = 0.01766921\n",
      "Iteration 9605, loss = 0.01765925\n",
      "Iteration 9606, loss = 0.01764931\n",
      "Iteration 9607, loss = 0.01763936\n",
      "Iteration 9608, loss = 0.01762942\n",
      "Iteration 9609, loss = 0.01761949\n",
      "Iteration 9610, loss = 0.01760956\n",
      "Iteration 9611, loss = 0.01759963\n",
      "Iteration 9612, loss = 0.01758971\n",
      "Iteration 9613, loss = 0.01757980\n",
      "Iteration 9614, loss = 0.01756988\n",
      "Iteration 9615, loss = 0.01755998\n",
      "Iteration 9616, loss = 0.01755007\n",
      "Iteration 9617, loss = 0.01754018\n",
      "Iteration 9618, loss = 0.01753028\n",
      "Iteration 9619, loss = 0.01752039\n",
      "Iteration 9620, loss = 0.01751051\n",
      "Iteration 9621, loss = 0.01750063\n",
      "Iteration 9622, loss = 0.01749075\n",
      "Iteration 9623, loss = 0.01748088\n",
      "Iteration 9624, loss = 0.01747102\n",
      "Iteration 9625, loss = 0.01746116\n",
      "Iteration 9626, loss = 0.01745130\n",
      "Iteration 9627, loss = 0.01744145\n",
      "Iteration 9628, loss = 0.01743161\n",
      "Iteration 9629, loss = 0.01742176\n",
      "Iteration 9630, loss = 0.01741193\n",
      "Iteration 9631, loss = 0.01740210\n",
      "Iteration 9632, loss = 0.01739227\n",
      "Iteration 9633, loss = 0.01738245\n",
      "Iteration 9634, loss = 0.01737263\n",
      "Iteration 9635, loss = 0.01736282\n",
      "Iteration 9636, loss = 0.01735301\n",
      "Iteration 9637, loss = 0.01734321\n",
      "Iteration 9638, loss = 0.01733342\n",
      "Iteration 9639, loss = 0.01732362\n",
      "Iteration 9640, loss = 0.01731384\n",
      "Iteration 9641, loss = 0.01730406\n",
      "Iteration 9642, loss = 0.01729428\n",
      "Iteration 9643, loss = 0.01728451\n",
      "Iteration 9644, loss = 0.01727474\n",
      "Iteration 9645, loss = 0.01726498\n",
      "Iteration 9646, loss = 0.01725523\n",
      "Iteration 9647, loss = 0.01724548\n",
      "Iteration 9648, loss = 0.01723573\n",
      "Iteration 9649, loss = 0.01722599\n",
      "Iteration 9650, loss = 0.01721626\n",
      "Iteration 9651, loss = 0.01720653\n",
      "Iteration 9652, loss = 0.01719680\n",
      "Iteration 9653, loss = 0.01718708\n",
      "Iteration 9654, loss = 0.01717737\n",
      "Iteration 9655, loss = 0.01716766\n",
      "Iteration 9656, loss = 0.01715796\n",
      "Iteration 9657, loss = 0.01714826\n",
      "Iteration 9658, loss = 0.01713857\n",
      "Iteration 9659, loss = 0.01712888\n",
      "Iteration 9660, loss = 0.01711920\n",
      "Iteration 9661, loss = 0.01710953\n",
      "Iteration 9662, loss = 0.01709986\n",
      "Iteration 9663, loss = 0.01709019\n",
      "Iteration 9664, loss = 0.01708053\n",
      "Iteration 9665, loss = 0.01707088\n",
      "Iteration 9666, loss = 0.01706123\n",
      "Iteration 9667, loss = 0.01705158\n",
      "Iteration 9668, loss = 0.01704195\n",
      "Iteration 9669, loss = 0.01703231\n",
      "Iteration 9670, loss = 0.01702269\n",
      "Iteration 9671, loss = 0.01701307\n",
      "Iteration 9672, loss = 0.01700345\n",
      "Iteration 9673, loss = 0.01699384\n",
      "Iteration 9674, loss = 0.01698424\n",
      "Iteration 9675, loss = 0.01697464\n",
      "Iteration 9676, loss = 0.01696504\n",
      "Iteration 9677, loss = 0.01695546\n",
      "Iteration 9678, loss = 0.01694588\n",
      "Iteration 9679, loss = 0.01693630\n",
      "Iteration 9680, loss = 0.01692673\n",
      "Iteration 9681, loss = 0.01691716\n",
      "Iteration 9682, loss = 0.01690760\n",
      "Iteration 9683, loss = 0.01689805\n",
      "Iteration 9684, loss = 0.01688850\n",
      "Iteration 9685, loss = 0.01687896\n",
      "Iteration 9686, loss = 0.01686942\n",
      "Iteration 9687, loss = 0.01685989\n",
      "Iteration 9688, loss = 0.01685037\n",
      "Iteration 9689, loss = 0.01684085\n",
      "Iteration 9690, loss = 0.01683133\n",
      "Iteration 9691, loss = 0.01682183\n",
      "Iteration 9692, loss = 0.01681232\n",
      "Iteration 9693, loss = 0.01680283\n",
      "Iteration 9694, loss = 0.01679334\n",
      "Iteration 9695, loss = 0.01678385\n",
      "Iteration 9696, loss = 0.01677437\n",
      "Iteration 9697, loss = 0.01676490\n",
      "Iteration 9698, loss = 0.01675543\n",
      "Iteration 9699, loss = 0.01674597\n",
      "Iteration 9700, loss = 0.01673651\n",
      "Iteration 9701, loss = 0.01672706\n",
      "Iteration 9702, loss = 0.01671762\n",
      "Iteration 9703, loss = 0.01670818\n",
      "Iteration 9704, loss = 0.01669875\n",
      "Iteration 9705, loss = 0.01668932\n",
      "Iteration 9706, loss = 0.01667990\n",
      "Iteration 9707, loss = 0.01667049\n",
      "Iteration 9708, loss = 0.01666108\n",
      "Iteration 9709, loss = 0.01665167\n",
      "Iteration 9710, loss = 0.01664228\n",
      "Iteration 9711, loss = 0.01663289\n",
      "Iteration 9712, loss = 0.01662350\n",
      "Iteration 9713, loss = 0.01661412\n",
      "Iteration 9714, loss = 0.01660475\n",
      "Iteration 9715, loss = 0.01659538\n",
      "Iteration 9716, loss = 0.01658602\n",
      "Iteration 9717, loss = 0.01657666\n",
      "Iteration 9718, loss = 0.01656731\n",
      "Iteration 9719, loss = 0.01655797\n",
      "Iteration 9720, loss = 0.01654863\n",
      "Iteration 9721, loss = 0.01653930\n",
      "Iteration 9722, loss = 0.01652997\n",
      "Iteration 9723, loss = 0.01652065\n",
      "Iteration 9724, loss = 0.01651134\n",
      "Iteration 9725, loss = 0.01650203\n",
      "Iteration 9726, loss = 0.01649273\n",
      "Iteration 9727, loss = 0.01648343\n",
      "Iteration 9728, loss = 0.01647414\n",
      "Iteration 9729, loss = 0.01646485\n",
      "Iteration 9730, loss = 0.01645558\n",
      "Iteration 9731, loss = 0.01644630\n",
      "Iteration 9732, loss = 0.01643704\n",
      "Iteration 9733, loss = 0.01642778\n",
      "Iteration 9734, loss = 0.01641852\n",
      "Iteration 9735, loss = 0.01640927\n",
      "Iteration 9736, loss = 0.01640003\n",
      "Iteration 9737, loss = 0.01639079\n",
      "Iteration 9738, loss = 0.01638156\n",
      "Iteration 9739, loss = 0.01637234\n",
      "Iteration 9740, loss = 0.01636312\n",
      "Iteration 9741, loss = 0.01635391\n",
      "Iteration 9742, loss = 0.01634470\n",
      "Iteration 9743, loss = 0.01633550\n",
      "Iteration 9744, loss = 0.01632631\n",
      "Iteration 9745, loss = 0.01631712\n",
      "Iteration 9746, loss = 0.01630794\n",
      "Iteration 9747, loss = 0.01629876\n",
      "Iteration 9748, loss = 0.01628959\n",
      "Iteration 9749, loss = 0.01628042\n",
      "Iteration 9750, loss = 0.01627127\n",
      "Iteration 9751, loss = 0.01626211\n",
      "Iteration 9752, loss = 0.01625297\n",
      "Iteration 9753, loss = 0.01624383\n",
      "Iteration 9754, loss = 0.01623469\n",
      "Iteration 9755, loss = 0.01622556\n",
      "Iteration 9756, loss = 0.01621644\n",
      "Iteration 9757, loss = 0.01620733\n",
      "Iteration 9758, loss = 0.01619822\n",
      "Iteration 9759, loss = 0.01618911\n",
      "Iteration 9760, loss = 0.01618001\n",
      "Iteration 9761, loss = 0.01617092\n",
      "Iteration 9762, loss = 0.01616184\n",
      "Iteration 9763, loss = 0.01615276\n",
      "Iteration 9764, loss = 0.01614368\n",
      "Iteration 9765, loss = 0.01613461\n",
      "Iteration 9766, loss = 0.01612555\n",
      "Iteration 9767, loss = 0.01611650\n",
      "Iteration 9768, loss = 0.01610745\n",
      "Iteration 9769, loss = 0.01609840\n",
      "Iteration 9770, loss = 0.01608937\n",
      "Iteration 9771, loss = 0.01608033\n",
      "Iteration 9772, loss = 0.01607131\n",
      "Iteration 9773, loss = 0.01606229\n",
      "Iteration 9774, loss = 0.01605327\n",
      "Iteration 9775, loss = 0.01604427\n",
      "Iteration 9776, loss = 0.01603527\n",
      "Iteration 9777, loss = 0.01602627\n",
      "Iteration 9778, loss = 0.01601728\n",
      "Iteration 9779, loss = 0.01600830\n",
      "Iteration 9780, loss = 0.01599932\n",
      "Iteration 9781, loss = 0.01599035\n",
      "Iteration 9782, loss = 0.01598138\n",
      "Iteration 9783, loss = 0.01597242\n",
      "Iteration 9784, loss = 0.01596347\n",
      "Iteration 9785, loss = 0.01595452\n",
      "Iteration 9786, loss = 0.01594558\n",
      "Iteration 9787, loss = 0.01593665\n",
      "Iteration 9788, loss = 0.01592772\n",
      "Iteration 9789, loss = 0.01591879\n",
      "Iteration 9790, loss = 0.01590988\n",
      "Iteration 9791, loss = 0.01590096\n",
      "Iteration 9792, loss = 0.01589206\n",
      "Iteration 9793, loss = 0.01588316\n",
      "Iteration 9794, loss = 0.01587427\n",
      "Iteration 9795, loss = 0.01586538\n",
      "Iteration 9796, loss = 0.01585650\n",
      "Iteration 9797, loss = 0.01584762\n",
      "Iteration 9798, loss = 0.01583875\n",
      "Iteration 9799, loss = 0.01582989\n",
      "Iteration 9800, loss = 0.01582103\n",
      "Iteration 9801, loss = 0.01581218\n",
      "Iteration 9802, loss = 0.01580334\n",
      "Iteration 9803, loss = 0.01579450\n",
      "Iteration 9804, loss = 0.01578566\n",
      "Iteration 9805, loss = 0.01577684\n",
      "Iteration 9806, loss = 0.01576802\n",
      "Iteration 9807, loss = 0.01575920\n",
      "Iteration 9808, loss = 0.01575039\n",
      "Iteration 9809, loss = 0.01574159\n",
      "Iteration 9810, loss = 0.01573279\n",
      "Iteration 9811, loss = 0.01572400\n",
      "Iteration 9812, loss = 0.01571521\n",
      "Iteration 9813, loss = 0.01570643\n",
      "Iteration 9814, loss = 0.01569766\n",
      "Iteration 9815, loss = 0.01568889\n",
      "Iteration 9816, loss = 0.01568013\n",
      "Iteration 9817, loss = 0.01567137\n",
      "Iteration 9818, loss = 0.01566262\n",
      "Iteration 9819, loss = 0.01565388\n",
      "Iteration 9820, loss = 0.01564514\n",
      "Iteration 9821, loss = 0.01563641\n",
      "Iteration 9822, loss = 0.01562768\n",
      "Iteration 9823, loss = 0.01561896\n",
      "Iteration 9824, loss = 0.01561025\n",
      "Iteration 9825, loss = 0.01560154\n",
      "Iteration 9826, loss = 0.01559284\n",
      "Iteration 9827, loss = 0.01558414\n",
      "Iteration 9828, loss = 0.01557545\n",
      "Iteration 9829, loss = 0.01556677\n",
      "Iteration 9830, loss = 0.01555809\n",
      "Iteration 9831, loss = 0.01554941\n",
      "Iteration 9832, loss = 0.01554075\n",
      "Iteration 9833, loss = 0.01553209\n",
      "Iteration 9834, loss = 0.01552343\n",
      "Iteration 9835, loss = 0.01551478\n",
      "Iteration 9836, loss = 0.01550614\n",
      "Iteration 9837, loss = 0.01549750\n",
      "Iteration 9838, loss = 0.01548887\n",
      "Iteration 9839, loss = 0.01548024\n",
      "Iteration 9840, loss = 0.01547162\n",
      "Iteration 9841, loss = 0.01546301\n",
      "Iteration 9842, loss = 0.01545440\n",
      "Iteration 9843, loss = 0.01544580\n",
      "Iteration 9844, loss = 0.01543720\n",
      "Iteration 9845, loss = 0.01542861\n",
      "Iteration 9846, loss = 0.01542002\n",
      "Iteration 9847, loss = 0.01541144\n",
      "Iteration 9848, loss = 0.01540287\n",
      "Iteration 9849, loss = 0.01539430\n",
      "Iteration 9850, loss = 0.01538574\n",
      "Iteration 9851, loss = 0.01537719\n",
      "Iteration 9852, loss = 0.01536864\n",
      "Iteration 9853, loss = 0.01536009\n",
      "Iteration 9854, loss = 0.01535155\n",
      "Iteration 9855, loss = 0.01534302\n",
      "Iteration 9856, loss = 0.01533449\n",
      "Iteration 9857, loss = 0.01532597\n",
      "Iteration 9858, loss = 0.01531746\n",
      "Iteration 9859, loss = 0.01530895\n",
      "Iteration 9860, loss = 0.01530045\n",
      "Iteration 9861, loss = 0.01529195\n",
      "Iteration 9862, loss = 0.01528346\n",
      "Iteration 9863, loss = 0.01527497\n",
      "Iteration 9864, loss = 0.01526649\n",
      "Iteration 9865, loss = 0.01525801\n",
      "Iteration 9866, loss = 0.01524955\n",
      "Iteration 9867, loss = 0.01524108\n",
      "Iteration 9868, loss = 0.01523262\n",
      "Iteration 9869, loss = 0.01522417\n",
      "Iteration 9870, loss = 0.01521573\n",
      "Iteration 9871, loss = 0.01520729\n",
      "Iteration 9872, loss = 0.01519885\n",
      "Iteration 9873, loss = 0.01519042\n",
      "Iteration 9874, loss = 0.01518200\n",
      "Iteration 9875, loss = 0.01517358\n",
      "Iteration 9876, loss = 0.01516517\n",
      "Iteration 9877, loss = 0.01515677\n",
      "Iteration 9878, loss = 0.01514836\n",
      "Iteration 9879, loss = 0.01513997\n",
      "Iteration 9880, loss = 0.01513158\n",
      "Iteration 9881, loss = 0.01512320\n",
      "Iteration 9882, loss = 0.01511482\n",
      "Iteration 9883, loss = 0.01510645\n",
      "Iteration 9884, loss = 0.01509808\n",
      "Iteration 9885, loss = 0.01508972\n",
      "Iteration 9886, loss = 0.01508137\n",
      "Iteration 9887, loss = 0.01507302\n",
      "Iteration 9888, loss = 0.01506468\n",
      "Iteration 9889, loss = 0.01505634\n",
      "Iteration 9890, loss = 0.01504801\n",
      "Iteration 9891, loss = 0.01503968\n",
      "Iteration 9892, loss = 0.01503136\n",
      "Iteration 9893, loss = 0.01502304\n",
      "Iteration 9894, loss = 0.01501473\n",
      "Iteration 9895, loss = 0.01500643\n",
      "Iteration 9896, loss = 0.01499813\n",
      "Iteration 9897, loss = 0.01498984\n",
      "Iteration 9898, loss = 0.01498155\n",
      "Iteration 9899, loss = 0.01497327\n",
      "Iteration 9900, loss = 0.01496499\n",
      "Iteration 9901, loss = 0.01495672\n",
      "Iteration 9902, loss = 0.01494846\n",
      "Iteration 9903, loss = 0.01494020\n",
      "Iteration 9904, loss = 0.01493195\n",
      "Iteration 9905, loss = 0.01492370\n",
      "Iteration 9906, loss = 0.01491546\n",
      "Iteration 9907, loss = 0.01490722\n",
      "Iteration 9908, loss = 0.01489899\n",
      "Iteration 9909, loss = 0.01489076\n",
      "Iteration 9910, loss = 0.01488254\n",
      "Iteration 9911, loss = 0.01487433\n",
      "Iteration 9912, loss = 0.01486612\n",
      "Iteration 9913, loss = 0.01485792\n",
      "Iteration 9914, loss = 0.01484972\n",
      "Iteration 9915, loss = 0.01484153\n",
      "Iteration 9916, loss = 0.01483334\n",
      "Iteration 9917, loss = 0.01482516\n",
      "Iteration 9918, loss = 0.01481698\n",
      "Iteration 9919, loss = 0.01480881\n",
      "Iteration 9920, loss = 0.01480065\n",
      "Iteration 9921, loss = 0.01479249\n",
      "Iteration 9922, loss = 0.01478433\n",
      "Iteration 9923, loss = 0.01477618\n",
      "Iteration 9924, loss = 0.01476804\n",
      "Iteration 9925, loss = 0.01475990\n",
      "Iteration 9926, loss = 0.01475177\n",
      "Iteration 9927, loss = 0.01474365\n",
      "Iteration 9928, loss = 0.01473552\n",
      "Iteration 9929, loss = 0.01472741\n",
      "Iteration 9930, loss = 0.01471930\n",
      "Iteration 9931, loss = 0.01471119\n",
      "Iteration 9932, loss = 0.01470310\n",
      "Iteration 9933, loss = 0.01469500\n",
      "Iteration 9934, loss = 0.01468691\n",
      "Iteration 9935, loss = 0.01467883\n",
      "Iteration 9936, loss = 0.01467075\n",
      "Iteration 9937, loss = 0.01466268\n",
      "Iteration 9938, loss = 0.01465462\n",
      "Iteration 9939, loss = 0.01464655\n",
      "Iteration 9940, loss = 0.01463850\n",
      "Iteration 9941, loss = 0.01463045\n",
      "Iteration 9942, loss = 0.01462240\n",
      "Iteration 9943, loss = 0.01461436\n",
      "Iteration 9944, loss = 0.01460633\n",
      "Iteration 9945, loss = 0.01459830\n",
      "Iteration 9946, loss = 0.01459028\n",
      "Iteration 9947, loss = 0.01458226\n",
      "Iteration 9948, loss = 0.01457425\n",
      "Iteration 9949, loss = 0.01456624\n",
      "Iteration 9950, loss = 0.01455824\n",
      "Iteration 9951, loss = 0.01455024\n",
      "Iteration 9952, loss = 0.01454225\n",
      "Iteration 9953, loss = 0.01453426\n",
      "Iteration 9954, loss = 0.01452628\n",
      "Iteration 9955, loss = 0.01451831\n",
      "Iteration 9956, loss = 0.01451034\n",
      "Iteration 9957, loss = 0.01450237\n",
      "Iteration 9958, loss = 0.01449441\n",
      "Iteration 9959, loss = 0.01448646\n",
      "Iteration 9960, loss = 0.01447851\n",
      "Iteration 9961, loss = 0.01447057\n",
      "Iteration 9962, loss = 0.01446263\n",
      "Iteration 9963, loss = 0.01445470\n",
      "Iteration 9964, loss = 0.01444677\n",
      "Iteration 9965, loss = 0.01443885\n",
      "Iteration 9966, loss = 0.01443093\n",
      "Iteration 9967, loss = 0.01442302\n",
      "Iteration 9968, loss = 0.01441511\n",
      "Iteration 9969, loss = 0.01440721\n",
      "Iteration 9970, loss = 0.01439932\n",
      "Iteration 9971, loss = 0.01439143\n",
      "Iteration 9972, loss = 0.01438354\n",
      "Iteration 9973, loss = 0.01437566\n",
      "Iteration 9974, loss = 0.01436779\n",
      "Iteration 9975, loss = 0.01435992\n",
      "Iteration 9976, loss = 0.01435205\n",
      "Iteration 9977, loss = 0.01434419\n",
      "Iteration 9978, loss = 0.01433634\n",
      "Iteration 9979, loss = 0.01432849\n",
      "Iteration 9980, loss = 0.01432065\n",
      "Iteration 9981, loss = 0.01431281\n",
      "Iteration 9982, loss = 0.01430497\n",
      "Iteration 9983, loss = 0.01429715\n",
      "Iteration 9984, loss = 0.01428932\n",
      "Iteration 9985, loss = 0.01428151\n",
      "Iteration 9986, loss = 0.01427369\n",
      "Iteration 9987, loss = 0.01426589\n",
      "Iteration 9988, loss = 0.01425808\n",
      "Iteration 9989, loss = 0.01425029\n",
      "Iteration 9990, loss = 0.01424250\n",
      "Iteration 9991, loss = 0.01423471\n",
      "Iteration 9992, loss = 0.01422693\n",
      "Iteration 9993, loss = 0.01421915\n",
      "Iteration 9994, loss = 0.01421138\n",
      "Iteration 9995, loss = 0.01420361\n",
      "Iteration 9996, loss = 0.01419585\n",
      "Iteration 9997, loss = 0.01418810\n",
      "Iteration 9998, loss = 0.01418035\n",
      "Iteration 9999, loss = 0.01417260\n",
      "Iteration 10000, loss = 0.01416486\n",
      "Iteration 10001, loss = 0.01415713\n",
      "Iteration 10002, loss = 0.01414940\n",
      "Iteration 10003, loss = 0.01414167\n",
      "Iteration 10004, loss = 0.01413395\n",
      "Iteration 10005, loss = 0.01412624\n",
      "Iteration 10006, loss = 0.01411853\n",
      "Iteration 10007, loss = 0.01411082\n",
      "Iteration 10008, loss = 0.01410312\n",
      "Iteration 10009, loss = 0.01409543\n",
      "Iteration 10010, loss = 0.01408774\n",
      "Iteration 10011, loss = 0.01408005\n",
      "Iteration 10012, loss = 0.01407237\n",
      "Iteration 10013, loss = 0.01406470\n",
      "Iteration 10014, loss = 0.01405703\n",
      "Iteration 10015, loss = 0.01404936\n",
      "Iteration 10016, loss = 0.01404170\n",
      "Iteration 10017, loss = 0.01403405\n",
      "Iteration 10018, loss = 0.01402640\n",
      "Iteration 10019, loss = 0.01401876\n",
      "Iteration 10020, loss = 0.01401112\n",
      "Iteration 10021, loss = 0.01400348\n",
      "Iteration 10022, loss = 0.01399585\n",
      "Iteration 10023, loss = 0.01398823\n",
      "Iteration 10024, loss = 0.01398061\n",
      "Iteration 10025, loss = 0.01397300\n",
      "Iteration 10026, loss = 0.01396539\n",
      "Iteration 10027, loss = 0.01395778\n",
      "Iteration 10028, loss = 0.01395018\n",
      "Iteration 10029, loss = 0.01394259\n",
      "Iteration 10030, loss = 0.01393500\n",
      "Iteration 10031, loss = 0.01392741\n",
      "Iteration 10032, loss = 0.01391983\n",
      "Iteration 10033, loss = 0.01391226\n",
      "Iteration 10034, loss = 0.01390469\n",
      "Iteration 10035, loss = 0.01389713\n",
      "Iteration 10036, loss = 0.01388957\n",
      "Iteration 10037, loss = 0.01388201\n",
      "Iteration 10038, loss = 0.01387446\n",
      "Iteration 10039, loss = 0.01386692\n",
      "Iteration 10040, loss = 0.01385938\n",
      "Iteration 10041, loss = 0.01385184\n",
      "Iteration 10042, loss = 0.01384431\n",
      "Iteration 10043, loss = 0.01383678\n",
      "Iteration 10044, loss = 0.01382926\n",
      "Iteration 10045, loss = 0.01382175\n",
      "Iteration 10046, loss = 0.01381424\n",
      "Iteration 10047, loss = 0.01380673\n",
      "Iteration 10048, loss = 0.01379923\n",
      "Iteration 10049, loss = 0.01379173\n",
      "Iteration 10050, loss = 0.01378424\n",
      "Iteration 10051, loss = 0.01377676\n",
      "Iteration 10052, loss = 0.01376927\n",
      "Iteration 10053, loss = 0.01376180\n",
      "Iteration 10054, loss = 0.01375433\n",
      "Iteration 10055, loss = 0.01374686\n",
      "Iteration 10056, loss = 0.01373940\n",
      "Iteration 10057, loss = 0.01373194\n",
      "Iteration 10058, loss = 0.01372449\n",
      "Iteration 10059, loss = 0.01371704\n",
      "Iteration 10060, loss = 0.01370960\n",
      "Iteration 10061, loss = 0.01370216\n",
      "Iteration 10062, loss = 0.01369472\n",
      "Iteration 10063, loss = 0.01368730\n",
      "Iteration 10064, loss = 0.01367987\n",
      "Iteration 10065, loss = 0.01367245\n",
      "Iteration 10066, loss = 0.01366504\n",
      "Iteration 10067, loss = 0.01365763\n",
      "Iteration 10068, loss = 0.01365023\n",
      "Iteration 10069, loss = 0.01364283\n",
      "Iteration 10070, loss = 0.01363543\n",
      "Iteration 10071, loss = 0.01362804\n",
      "Iteration 10072, loss = 0.01362065\n",
      "Iteration 10073, loss = 0.01361327\n",
      "Iteration 10074, loss = 0.01360590\n",
      "Iteration 10075, loss = 0.01359853\n",
      "Iteration 10076, loss = 0.01359116\n",
      "Iteration 10077, loss = 0.01358380\n",
      "Iteration 10078, loss = 0.01357644\n",
      "Iteration 10079, loss = 0.01356909\n",
      "Iteration 10080, loss = 0.01356174\n",
      "Iteration 10081, loss = 0.01355440\n",
      "Iteration 10082, loss = 0.01354706\n",
      "Iteration 10083, loss = 0.01353973\n",
      "Iteration 10084, loss = 0.01353240\n",
      "Iteration 10085, loss = 0.01352507\n",
      "Iteration 10086, loss = 0.01351775\n",
      "Iteration 10087, loss = 0.01351044\n",
      "Iteration 10088, loss = 0.01350313\n",
      "Iteration 10089, loss = 0.01349582\n",
      "Iteration 10090, loss = 0.01348852\n",
      "Iteration 10091, loss = 0.01348123\n",
      "Iteration 10092, loss = 0.01347394\n",
      "Iteration 10093, loss = 0.01346665\n",
      "Iteration 10094, loss = 0.01345937\n",
      "Iteration 10095, loss = 0.01345209\n",
      "Iteration 10096, loss = 0.01344482\n",
      "Iteration 10097, loss = 0.01343755\n",
      "Iteration 10098, loss = 0.01343028\n",
      "Iteration 10099, loss = 0.01342303\n",
      "Iteration 10100, loss = 0.01341577\n",
      "Iteration 10101, loss = 0.01340852\n",
      "Iteration 10102, loss = 0.01340128\n",
      "Iteration 10103, loss = 0.01339404\n",
      "Iteration 10104, loss = 0.01338680\n",
      "Iteration 10105, loss = 0.01337957\n",
      "Iteration 10106, loss = 0.01337234\n",
      "Iteration 10107, loss = 0.01336512\n",
      "Iteration 10108, loss = 0.01335790\n",
      "Iteration 10109, loss = 0.01335069\n",
      "Iteration 10110, loss = 0.01334348\n",
      "Iteration 10111, loss = 0.01333628\n",
      "Iteration 10112, loss = 0.01332908\n",
      "Iteration 10113, loss = 0.01332189\n",
      "Iteration 10114, loss = 0.01331470\n",
      "Iteration 10115, loss = 0.01330751\n",
      "Iteration 10116, loss = 0.01330033\n",
      "Iteration 10117, loss = 0.01329315\n",
      "Iteration 10118, loss = 0.01328598\n",
      "Iteration 10119, loss = 0.01327881\n",
      "Iteration 10120, loss = 0.01327165\n",
      "Iteration 10121, loss = 0.01326449\n",
      "Iteration 10122, loss = 0.01325734\n",
      "Iteration 10123, loss = 0.01325019\n",
      "Iteration 10124, loss = 0.01324305\n",
      "Iteration 10125, loss = 0.01323591\n",
      "Iteration 10126, loss = 0.01322877\n",
      "Iteration 10127, loss = 0.01322164\n",
      "Iteration 10128, loss = 0.01321451\n",
      "Iteration 10129, loss = 0.01320739\n",
      "Iteration 10130, loss = 0.01320028\n",
      "Iteration 10131, loss = 0.01319316\n",
      "Iteration 10132, loss = 0.01318605\n",
      "Iteration 10133, loss = 0.01317895\n",
      "Iteration 10134, loss = 0.01317185\n",
      "Iteration 10135, loss = 0.01316476\n",
      "Iteration 10136, loss = 0.01315766\n",
      "Iteration 10137, loss = 0.01315058\n",
      "Iteration 10138, loss = 0.01314350\n",
      "Iteration 10139, loss = 0.01313642\n",
      "Iteration 10140, loss = 0.01312935\n",
      "Iteration 10141, loss = 0.01312228\n",
      "Iteration 10142, loss = 0.01311521\n",
      "Iteration 10143, loss = 0.01310816\n",
      "Iteration 10144, loss = 0.01310110\n",
      "Iteration 10145, loss = 0.01309405\n",
      "Iteration 10146, loss = 0.01308700\n",
      "Iteration 10147, loss = 0.01307996\n",
      "Iteration 10148, loss = 0.01307292\n",
      "Iteration 10149, loss = 0.01306589\n",
      "Iteration 10150, loss = 0.01305886\n",
      "Iteration 10151, loss = 0.01305184\n",
      "Iteration 10152, loss = 0.01304482\n",
      "Iteration 10153, loss = 0.01303780\n",
      "Iteration 10154, loss = 0.01303079\n",
      "Iteration 10155, loss = 0.01302379\n",
      "Iteration 10156, loss = 0.01301678\n",
      "Iteration 10157, loss = 0.01300979\n",
      "Iteration 10158, loss = 0.01300279\n",
      "Iteration 10159, loss = 0.01299580\n",
      "Iteration 10160, loss = 0.01298882\n",
      "Iteration 10161, loss = 0.01298184\n",
      "Iteration 10162, loss = 0.01297486\n",
      "Iteration 10163, loss = 0.01296789\n",
      "Iteration 10164, loss = 0.01296092\n",
      "Iteration 10165, loss = 0.01295396\n",
      "Iteration 10166, loss = 0.01294700\n",
      "Iteration 10167, loss = 0.01294005\n",
      "Iteration 10168, loss = 0.01293310\n",
      "Iteration 10169, loss = 0.01292615\n",
      "Iteration 10170, loss = 0.01291921\n",
      "Iteration 10171, loss = 0.01291228\n",
      "Iteration 10172, loss = 0.01290534\n",
      "Iteration 10173, loss = 0.01289841\n",
      "Iteration 10174, loss = 0.01289149\n",
      "Iteration 10175, loss = 0.01288457\n",
      "Iteration 10176, loss = 0.01287766\n",
      "Iteration 10177, loss = 0.01287074\n",
      "Iteration 10178, loss = 0.01286384\n",
      "Iteration 10179, loss = 0.01285693\n",
      "Iteration 10180, loss = 0.01285004\n",
      "Iteration 10181, loss = 0.01284314\n",
      "Iteration 10182, loss = 0.01283625\n",
      "Iteration 10183, loss = 0.01282937\n",
      "Iteration 10184, loss = 0.01282249\n",
      "Iteration 10185, loss = 0.01281561\n",
      "Iteration 10186, loss = 0.01280874\n",
      "Iteration 10187, loss = 0.01280187\n",
      "Iteration 10188, loss = 0.01279500\n",
      "Iteration 10189, loss = 0.01278814\n",
      "Iteration 10190, loss = 0.01278129\n",
      "Iteration 10191, loss = 0.01277444\n",
      "Iteration 10192, loss = 0.01276759\n",
      "Iteration 10193, loss = 0.01276075\n",
      "Iteration 10194, loss = 0.01275391\n",
      "Iteration 10195, loss = 0.01274707\n",
      "Iteration 10196, loss = 0.01274024\n",
      "Iteration 10197, loss = 0.01273342\n",
      "Iteration 10198, loss = 0.01272660\n",
      "Iteration 10199, loss = 0.01271978\n",
      "Iteration 10200, loss = 0.01271296\n",
      "Iteration 10201, loss = 0.01270616\n",
      "Iteration 10202, loss = 0.01269935\n",
      "Iteration 10203, loss = 0.01269255\n",
      "Iteration 10204, loss = 0.01268575\n",
      "Iteration 10205, loss = 0.01267896\n",
      "Iteration 10206, loss = 0.01267217\n",
      "Iteration 10207, loss = 0.01266539\n",
      "Iteration 10208, loss = 0.01265861\n",
      "Iteration 10209, loss = 0.01265183\n",
      "Iteration 10210, loss = 0.01264506\n",
      "Iteration 10211, loss = 0.01263829\n",
      "Iteration 10212, loss = 0.01263153\n",
      "Iteration 10213, loss = 0.01262477\n",
      "Iteration 10214, loss = 0.01261802\n",
      "Iteration 10215, loss = 0.01261126\n",
      "Iteration 10216, loss = 0.01260452\n",
      "Iteration 10217, loss = 0.01259777\n",
      "Iteration 10218, loss = 0.01259104\n",
      "Iteration 10219, loss = 0.01258430\n",
      "Iteration 10220, loss = 0.01257757\n",
      "Iteration 10221, loss = 0.01257085\n",
      "Iteration 10222, loss = 0.01256412\n",
      "Iteration 10223, loss = 0.01255741\n",
      "Iteration 10224, loss = 0.01255069\n",
      "Iteration 10225, loss = 0.01254398\n",
      "Iteration 10226, loss = 0.01253728\n",
      "Iteration 10227, loss = 0.01253057\n",
      "Iteration 10228, loss = 0.01252388\n",
      "Iteration 10229, loss = 0.01251718\n",
      "Iteration 10230, loss = 0.01251049\n",
      "Iteration 10231, loss = 0.01250381\n",
      "Iteration 10232, loss = 0.01249713\n",
      "Iteration 10233, loss = 0.01249045\n",
      "Iteration 10234, loss = 0.01248378\n",
      "Iteration 10235, loss = 0.01247711\n",
      "Iteration 10236, loss = 0.01247044\n",
      "Iteration 10237, loss = 0.01246378\n",
      "Iteration 10238, loss = 0.01245713\n",
      "Iteration 10239, loss = 0.01245047\n",
      "Iteration 10240, loss = 0.01244383\n",
      "Iteration 10241, loss = 0.01243718\n",
      "Iteration 10242, loss = 0.01243054\n",
      "Iteration 10243, loss = 0.01242390\n",
      "Iteration 10244, loss = 0.01241727\n",
      "Iteration 10245, loss = 0.01241064\n",
      "Iteration 10246, loss = 0.01240402\n",
      "Iteration 10247, loss = 0.01239740\n",
      "Iteration 10248, loss = 0.01239078\n",
      "Iteration 10249, loss = 0.01238417\n",
      "Iteration 10250, loss = 0.01237756\n",
      "Iteration 10251, loss = 0.01237096\n",
      "Iteration 10252, loss = 0.01236436\n",
      "Iteration 10253, loss = 0.01235776\n",
      "Iteration 10254, loss = 0.01235117\n",
      "Iteration 10255, loss = 0.01234458\n",
      "Iteration 10256, loss = 0.01233799\n",
      "Iteration 10257, loss = 0.01233141\n",
      "Iteration 10258, loss = 0.01232484\n",
      "Iteration 10259, loss = 0.01231826\n",
      "Iteration 10260, loss = 0.01231170\n",
      "Iteration 10261, loss = 0.01230513\n",
      "Iteration 10262, loss = 0.01229857\n",
      "Iteration 10263, loss = 0.01229201\n",
      "Iteration 10264, loss = 0.01228546\n",
      "Iteration 10265, loss = 0.01227891\n",
      "Iteration 10266, loss = 0.01227237\n",
      "Iteration 10267, loss = 0.01226583\n",
      "Iteration 10268, loss = 0.01225929\n",
      "Iteration 10269, loss = 0.01225276\n",
      "Iteration 10270, loss = 0.01224623\n",
      "Iteration 10271, loss = 0.01223970\n",
      "Iteration 10272, loss = 0.01223318\n",
      "Iteration 10273, loss = 0.01222666\n",
      "Iteration 10274, loss = 0.01222015\n",
      "Iteration 10275, loss = 0.01221364\n",
      "Iteration 10276, loss = 0.01220714\n",
      "Iteration 10277, loss = 0.01220063\n",
      "Iteration 10278, loss = 0.01219414\n",
      "Iteration 10279, loss = 0.01218764\n",
      "Iteration 10280, loss = 0.01218115\n",
      "Iteration 10281, loss = 0.01217467\n",
      "Iteration 10282, loss = 0.01216819\n",
      "Iteration 10283, loss = 0.01216171\n",
      "Iteration 10284, loss = 0.01215523\n",
      "Iteration 10285, loss = 0.01214876\n",
      "Iteration 10286, loss = 0.01214230\n",
      "Iteration 10287, loss = 0.01213583\n",
      "Iteration 10288, loss = 0.01212937\n",
      "Iteration 10289, loss = 0.01212292\n",
      "Iteration 10290, loss = 0.01211647\n",
      "Iteration 10291, loss = 0.01211002\n",
      "Iteration 10292, loss = 0.01210358\n",
      "Iteration 10293, loss = 0.01209714\n",
      "Iteration 10294, loss = 0.01209070\n",
      "Iteration 10295, loss = 0.01208427\n",
      "Iteration 10296, loss = 0.01207784\n",
      "Iteration 10297, loss = 0.01207142\n",
      "Iteration 10298, loss = 0.01206500\n",
      "Iteration 10299, loss = 0.01205858\n",
      "Iteration 10300, loss = 0.01205217\n",
      "Iteration 10301, loss = 0.01204576\n",
      "Iteration 10302, loss = 0.01203936\n",
      "Iteration 10303, loss = 0.01203295\n",
      "Iteration 10304, loss = 0.01202656\n",
      "Iteration 10305, loss = 0.01202016\n",
      "Iteration 10306, loss = 0.01201377\n",
      "Iteration 10307, loss = 0.01200739\n",
      "Iteration 10308, loss = 0.01200101\n",
      "Iteration 10309, loss = 0.01199463\n",
      "Iteration 10310, loss = 0.01198825\n",
      "Iteration 10311, loss = 0.01198188\n",
      "Iteration 10312, loss = 0.01197552\n",
      "Iteration 10313, loss = 0.01196915\n",
      "Iteration 10314, loss = 0.01196279\n",
      "Iteration 10315, loss = 0.01195644\n",
      "Iteration 10316, loss = 0.01195009\n",
      "Iteration 10317, loss = 0.01194374\n",
      "Iteration 10318, loss = 0.01193739\n",
      "Iteration 10319, loss = 0.01193105\n",
      "Iteration 10320, loss = 0.01192472\n",
      "Iteration 10321, loss = 0.01191838\n",
      "Iteration 10322, loss = 0.01191205\n",
      "Iteration 10323, loss = 0.01190573\n",
      "Iteration 10324, loss = 0.01189941\n",
      "Iteration 10325, loss = 0.01189309\n",
      "Iteration 10326, loss = 0.01188677\n",
      "Iteration 10327, loss = 0.01188046\n",
      "Iteration 10328, loss = 0.01187416\n",
      "Iteration 10329, loss = 0.01186785\n",
      "Iteration 10330, loss = 0.01186155\n",
      "Iteration 10331, loss = 0.01185526\n",
      "Iteration 10332, loss = 0.01184897\n",
      "Iteration 10333, loss = 0.01184268\n",
      "Iteration 10334, loss = 0.01183639\n",
      "Iteration 10335, loss = 0.01183011\n",
      "Iteration 10336, loss = 0.01182384\n",
      "Iteration 10337, loss = 0.01181756\n",
      "Iteration 10338, loss = 0.01181129\n",
      "Iteration 10339, loss = 0.01180503\n",
      "Iteration 10340, loss = 0.01179877\n",
      "Iteration 10341, loss = 0.01179251\n",
      "Iteration 10342, loss = 0.01178625\n",
      "Iteration 10343, loss = 0.01178000\n",
      "Iteration 10344, loss = 0.01177375\n",
      "Iteration 10345, loss = 0.01176751\n",
      "Iteration 10346, loss = 0.01176127\n",
      "Iteration 10347, loss = 0.01175503\n",
      "Iteration 10348, loss = 0.01174880\n",
      "Iteration 10349, loss = 0.01174257\n",
      "Iteration 10350, loss = 0.01173635\n",
      "Iteration 10351, loss = 0.01173013\n",
      "Iteration 10352, loss = 0.01172391\n",
      "Iteration 10353, loss = 0.01171769\n",
      "Iteration 10354, loss = 0.01171148\n",
      "Iteration 10355, loss = 0.01170527\n",
      "Iteration 10356, loss = 0.01169907\n",
      "Iteration 10357, loss = 0.01169287\n",
      "Iteration 10358, loss = 0.01168668\n",
      "Iteration 10359, loss = 0.01168048\n",
      "Iteration 10360, loss = 0.01167429\n",
      "Iteration 10361, loss = 0.01166811\n",
      "Iteration 10362, loss = 0.01166193\n",
      "Iteration 10363, loss = 0.01165575\n",
      "Iteration 10364, loss = 0.01164957\n",
      "Iteration 10365, loss = 0.01164340\n",
      "Iteration 10366, loss = 0.01163724\n",
      "Iteration 10367, loss = 0.01163107\n",
      "Iteration 10368, loss = 0.01162491\n",
      "Iteration 10369, loss = 0.01161876\n",
      "Iteration 10370, loss = 0.01161260\n",
      "Iteration 10371, loss = 0.01160645\n",
      "Iteration 10372, loss = 0.01160031\n",
      "Iteration 10373, loss = 0.01159417\n",
      "Iteration 10374, loss = 0.01158803\n",
      "Iteration 10375, loss = 0.01158189\n",
      "Iteration 10376, loss = 0.01157576\n",
      "Iteration 10377, loss = 0.01156963\n",
      "Iteration 10378, loss = 0.01156351\n",
      "Iteration 10379, loss = 0.01155739\n",
      "Iteration 10380, loss = 0.01155127\n",
      "Iteration 10381, loss = 0.01154516\n",
      "Iteration 10382, loss = 0.01153905\n",
      "Iteration 10383, loss = 0.01153294\n",
      "Iteration 10384, loss = 0.01152684\n",
      "Iteration 10385, loss = 0.01152074\n",
      "Iteration 10386, loss = 0.01151465\n",
      "Iteration 10387, loss = 0.01150855\n",
      "Iteration 10388, loss = 0.01150247\n",
      "Iteration 10389, loss = 0.01149638\n",
      "Iteration 10390, loss = 0.01149030\n",
      "Iteration 10391, loss = 0.01148422\n",
      "Iteration 10392, loss = 0.01147815\n",
      "Iteration 10393, loss = 0.01147208\n",
      "Iteration 10394, loss = 0.01146601\n",
      "Iteration 10395, loss = 0.01145995\n",
      "Iteration 10396, loss = 0.01145389\n",
      "Iteration 10397, loss = 0.01144783\n",
      "Iteration 10398, loss = 0.01144178\n",
      "Iteration 10399, loss = 0.01143573\n",
      "Iteration 10400, loss = 0.01142968\n",
      "Iteration 10401, loss = 0.01142364\n",
      "Iteration 10402, loss = 0.01141760\n",
      "Iteration 10403, loss = 0.01141156\n",
      "Iteration 10404, loss = 0.01140553\n",
      "Iteration 10405, loss = 0.01139950\n",
      "Iteration 10406, loss = 0.01139348\n",
      "Iteration 10407, loss = 0.01138746\n",
      "Iteration 10408, loss = 0.01138144\n",
      "Iteration 10409, loss = 0.01137542\n",
      "Iteration 10410, loss = 0.01136941\n",
      "Iteration 10411, loss = 0.01136340\n",
      "Iteration 10412, loss = 0.01135740\n",
      "Iteration 10413, loss = 0.01135140\n",
      "Iteration 10414, loss = 0.01134540\n",
      "Iteration 10415, loss = 0.01133941\n",
      "Iteration 10416, loss = 0.01133342\n",
      "Iteration 10417, loss = 0.01132743\n",
      "Iteration 10418, loss = 0.01132145\n",
      "Iteration 10419, loss = 0.01131547\n",
      "Iteration 10420, loss = 0.01130949\n",
      "Iteration 10421, loss = 0.01130352\n",
      "Iteration 10422, loss = 0.01129755\n",
      "Iteration 10423, loss = 0.01129158\n",
      "Iteration 10424, loss = 0.01128562\n",
      "Iteration 10425, loss = 0.01127966\n",
      "Iteration 10426, loss = 0.01127371\n",
      "Iteration 10427, loss = 0.01126775\n",
      "Iteration 10428, loss = 0.01126180\n",
      "Iteration 10429, loss = 0.01125586\n",
      "Iteration 10430, loss = 0.01124992\n",
      "Iteration 10431, loss = 0.01124398\n",
      "Iteration 10432, loss = 0.01123804\n",
      "Iteration 10433, loss = 0.01123211\n",
      "Iteration 10434, loss = 0.01122618\n",
      "Iteration 10435, loss = 0.01122026\n",
      "Iteration 10436, loss = 0.01121434\n",
      "Iteration 10437, loss = 0.01120842\n",
      "Iteration 10438, loss = 0.01120250\n",
      "Iteration 10439, loss = 0.01119659\n",
      "Iteration 10440, loss = 0.01119068\n",
      "Iteration 10441, loss = 0.01118478\n",
      "Iteration 10442, loss = 0.01117888\n",
      "Iteration 10443, loss = 0.01117298\n",
      "Iteration 10444, loss = 0.01116709\n",
      "Iteration 10445, loss = 0.01116120\n",
      "Iteration 10446, loss = 0.01115531\n",
      "Iteration 10447, loss = 0.01114942\n",
      "Iteration 10448, loss = 0.01114354\n",
      "Iteration 10449, loss = 0.01113766\n",
      "Iteration 10450, loss = 0.01113179\n",
      "Iteration 10451, loss = 0.01112592\n",
      "Iteration 10452, loss = 0.01112005\n",
      "Iteration 10453, loss = 0.01111419\n",
      "Iteration 10454, loss = 0.01110833\n",
      "Iteration 10455, loss = 0.01110247\n",
      "Iteration 10456, loss = 0.01109662\n",
      "Iteration 10457, loss = 0.01109077\n",
      "Iteration 10458, loss = 0.01108492\n",
      "Iteration 10459, loss = 0.01107907\n",
      "Iteration 10460, loss = 0.01107323\n",
      "Iteration 10461, loss = 0.01106740\n",
      "Iteration 10462, loss = 0.01106156\n",
      "Iteration 10463, loss = 0.01105573\n",
      "Iteration 10464, loss = 0.01104991\n",
      "Iteration 10465, loss = 0.01104408\n",
      "Iteration 10466, loss = 0.01103826\n",
      "Iteration 10467, loss = 0.01103244\n",
      "Iteration 10468, loss = 0.01102663\n",
      "Iteration 10469, loss = 0.01102082\n",
      "Iteration 10470, loss = 0.01101501\n",
      "Iteration 10471, loss = 0.01100921\n",
      "Iteration 10472, loss = 0.01100341\n",
      "Iteration 10473, loss = 0.01099761\n",
      "Iteration 10474, loss = 0.01099182\n",
      "Iteration 10475, loss = 0.01098603\n",
      "Iteration 10476, loss = 0.01098024\n",
      "Iteration 10477, loss = 0.01097445\n",
      "Iteration 10478, loss = 0.01096867\n",
      "Iteration 10479, loss = 0.01096290\n",
      "Iteration 10480, loss = 0.01095712\n",
      "Iteration 10481, loss = 0.01095135\n",
      "Iteration 10482, loss = 0.01094558\n",
      "Iteration 10483, loss = 0.01093982\n",
      "Iteration 10484, loss = 0.01093406\n",
      "Iteration 10485, loss = 0.01092830\n",
      "Iteration 10486, loss = 0.01092254\n",
      "Iteration 10487, loss = 0.01091679\n",
      "Iteration 10488, loss = 0.01091105\n",
      "Iteration 10489, loss = 0.01090530\n",
      "Iteration 10490, loss = 0.01089956\n",
      "Iteration 10491, loss = 0.01089382\n",
      "Iteration 10492, loss = 0.01088809\n",
      "Iteration 10493, loss = 0.01088235\n",
      "Iteration 10494, loss = 0.01087663\n",
      "Iteration 10495, loss = 0.01087090\n",
      "Iteration 10496, loss = 0.01086518\n",
      "Iteration 10497, loss = 0.01085946\n",
      "Iteration 10498, loss = 0.01085375\n",
      "Iteration 10499, loss = 0.01084803\n",
      "Iteration 10500, loss = 0.01084232\n",
      "Iteration 10501, loss = 0.01083662\n",
      "Iteration 10502, loss = 0.01083092\n",
      "Iteration 10503, loss = 0.01082522\n",
      "Iteration 10504, loss = 0.01081952\n",
      "Iteration 10505, loss = 0.01081383\n",
      "Iteration 10506, loss = 0.01080814\n",
      "Iteration 10507, loss = 0.01080245\n",
      "Iteration 10508, loss = 0.01079677\n",
      "Iteration 10509, loss = 0.01079109\n",
      "Iteration 10510, loss = 0.01078541\n",
      "Iteration 10511, loss = 0.01077974\n",
      "Iteration 10512, loss = 0.01077407\n",
      "Iteration 10513, loss = 0.01076840\n",
      "Iteration 10514, loss = 0.01076274\n",
      "Iteration 10515, loss = 0.01075708\n",
      "Iteration 10516, loss = 0.01075142\n",
      "Iteration 10517, loss = 0.01074577\n",
      "Iteration 10518, loss = 0.01074011\n",
      "Iteration 10519, loss = 0.01073447\n",
      "Iteration 10520, loss = 0.01072882\n",
      "Iteration 10521, loss = 0.01072318\n",
      "Iteration 10522, loss = 0.01071754\n",
      "Iteration 10523, loss = 0.01071191\n",
      "Iteration 10524, loss = 0.01070628\n",
      "Iteration 10525, loss = 0.01070065\n",
      "Iteration 10526, loss = 0.01069502\n",
      "Iteration 10527, loss = 0.01068940\n",
      "Iteration 10528, loss = 0.01068378\n",
      "Iteration 10529, loss = 0.01067816\n",
      "Iteration 10530, loss = 0.01067255\n",
      "Iteration 10531, loss = 0.01066694\n",
      "Iteration 10532, loss = 0.01066133\n",
      "Iteration 10533, loss = 0.01065573\n",
      "Iteration 10534, loss = 0.01065013\n",
      "Iteration 10535, loss = 0.01064453\n",
      "Iteration 10536, loss = 0.01063894\n",
      "Iteration 10537, loss = 0.01063335\n",
      "Iteration 10538, loss = 0.01062776\n",
      "Iteration 10539, loss = 0.01062218\n",
      "Iteration 10540, loss = 0.01061660\n",
      "Iteration 10541, loss = 0.01061102\n",
      "Iteration 10542, loss = 0.01060544\n",
      "Iteration 10543, loss = 0.01059987\n",
      "Iteration 10544, loss = 0.01059430\n",
      "Iteration 10545, loss = 0.01058874\n",
      "Iteration 10546, loss = 0.01058317\n",
      "Iteration 10547, loss = 0.01057761\n",
      "Iteration 10548, loss = 0.01057206\n",
      "Iteration 10549, loss = 0.01056650\n",
      "Iteration 10550, loss = 0.01056095\n",
      "Iteration 10551, loss = 0.01055541\n",
      "Iteration 10552, loss = 0.01054986\n",
      "Iteration 10553, loss = 0.01054432\n",
      "Iteration 10554, loss = 0.01053879\n",
      "Iteration 10555, loss = 0.01053325\n",
      "Iteration 10556, loss = 0.01052772\n",
      "Iteration 10557, loss = 0.01052219\n",
      "Iteration 10558, loss = 0.01051667\n",
      "Iteration 10559, loss = 0.01051114\n",
      "Iteration 10560, loss = 0.01050562\n",
      "Iteration 10561, loss = 0.01050011\n",
      "Iteration 10562, loss = 0.01049460\n",
      "Iteration 10563, loss = 0.01048909\n",
      "Iteration 10564, loss = 0.01048358\n",
      "Iteration 10565, loss = 0.01047808\n",
      "Iteration 10566, loss = 0.01047258\n",
      "Iteration 10567, loss = 0.01046708\n",
      "Iteration 10568, loss = 0.01046158\n",
      "Iteration 10569, loss = 0.01045609\n",
      "Iteration 10570, loss = 0.01045060\n",
      "Iteration 10571, loss = 0.01044512\n",
      "Iteration 10572, loss = 0.01043964\n",
      "Iteration 10573, loss = 0.01043416\n",
      "Iteration 10574, loss = 0.01042868\n",
      "Iteration 10575, loss = 0.01042321\n",
      "Iteration 10576, loss = 0.01041774\n",
      "Iteration 10577, loss = 0.01041227\n",
      "Iteration 10578, loss = 0.01040681\n",
      "Iteration 10579, loss = 0.01040135\n",
      "Iteration 10580, loss = 0.01039589\n",
      "Iteration 10581, loss = 0.01039044\n",
      "Iteration 10582, loss = 0.01038499\n",
      "Iteration 10583, loss = 0.01037954\n",
      "Iteration 10584, loss = 0.01037409\n",
      "Iteration 10585, loss = 0.01036865\n",
      "Iteration 10586, loss = 0.01036321\n",
      "Iteration 10587, loss = 0.01035777\n",
      "Iteration 10588, loss = 0.01035234\n",
      "Iteration 10589, loss = 0.01034691\n",
      "Iteration 10590, loss = 0.01034148\n",
      "Iteration 10591, loss = 0.01033606\n",
      "Iteration 10592, loss = 0.01033064\n",
      "Iteration 10593, loss = 0.01032522\n",
      "Iteration 10594, loss = 0.01031981\n",
      "Iteration 10595, loss = 0.01031439\n",
      "Iteration 10596, loss = 0.01030898\n",
      "Iteration 10597, loss = 0.01030358\n",
      "Iteration 10598, loss = 0.01029818\n",
      "Iteration 10599, loss = 0.01029278\n",
      "Iteration 10600, loss = 0.01028738\n",
      "Iteration 10601, loss = 0.01028198\n",
      "Iteration 10602, loss = 0.01027659\n",
      "Iteration 10603, loss = 0.01027121\n",
      "Iteration 10604, loss = 0.01026582\n",
      "Iteration 10605, loss = 0.01026044\n",
      "Iteration 10606, loss = 0.01025506\n",
      "Iteration 10607, loss = 0.01024968\n",
      "Iteration 10608, loss = 0.01024431\n",
      "Iteration 10609, loss = 0.01023894\n",
      "Iteration 10610, loss = 0.01023357\n",
      "Iteration 10611, loss = 0.01022821\n",
      "Iteration 10612, loss = 0.01022285\n",
      "Iteration 10613, loss = 0.01021749\n",
      "Iteration 10614, loss = 0.01021214\n",
      "Iteration 10615, loss = 0.01020678\n",
      "Iteration 10616, loss = 0.01020143\n",
      "Iteration 10617, loss = 0.01019609\n",
      "Iteration 10618, loss = 0.01019074\n",
      "Iteration 10619, loss = 0.01018540\n",
      "Iteration 10620, loss = 0.01018007\n",
      "Iteration 10621, loss = 0.01017473\n",
      "Iteration 10622, loss = 0.01016940\n",
      "Iteration 10623, loss = 0.01016407\n",
      "Iteration 10624, loss = 0.01015875\n",
      "Iteration 10625, loss = 0.01015342\n",
      "Iteration 10626, loss = 0.01014811\n",
      "Iteration 10627, loss = 0.01014279\n",
      "Iteration 10628, loss = 0.01013747\n",
      "Iteration 10629, loss = 0.01013216\n",
      "Iteration 10630, loss = 0.01012686\n",
      "Iteration 10631, loss = 0.01012155\n",
      "Iteration 10632, loss = 0.01011625\n",
      "Iteration 10633, loss = 0.01011095\n",
      "Iteration 10634, loss = 0.01010565\n",
      "Iteration 10635, loss = 0.01010036\n",
      "Iteration 10636, loss = 0.01009507\n",
      "Iteration 10637, loss = 0.01008978\n",
      "Iteration 10638, loss = 0.01008450\n",
      "Iteration 10639, loss = 0.01007922\n",
      "Iteration 10640, loss = 0.01007394\n",
      "Iteration 10641, loss = 0.01006866\n",
      "Iteration 10642, loss = 0.01006339\n",
      "Iteration 10643, loss = 0.01005812\n",
      "Iteration 10644, loss = 0.01005285\n",
      "Iteration 10645, loss = 0.01004759\n",
      "Iteration 10646, loss = 0.01004233\n",
      "Iteration 10647, loss = 0.01003707\n",
      "Iteration 10648, loss = 0.01003182\n",
      "Iteration 10649, loss = 0.01002656\n",
      "Iteration 10650, loss = 0.01002132\n",
      "Iteration 10651, loss = 0.01001607\n",
      "Iteration 10652, loss = 0.01001082\n",
      "Iteration 10653, loss = 0.01000558\n",
      "Iteration 10654, loss = 0.01000035\n",
      "Iteration 10655, loss = 0.00999511\n",
      "Iteration 10656, loss = 0.00998988\n",
      "Iteration 10657, loss = 0.00998465\n",
      "Iteration 10658, loss = 0.00997942\n",
      "Iteration 10659, loss = 0.00997420\n",
      "Iteration 10660, loss = 0.00996898\n",
      "Iteration 10661, loss = 0.00996376\n",
      "Iteration 10662, loss = 0.00995855\n",
      "Iteration 10663, loss = 0.00995334\n",
      "Iteration 10664, loss = 0.00994813\n",
      "Iteration 10665, loss = 0.00994292\n",
      "Iteration 10666, loss = 0.00993772\n",
      "Iteration 10667, loss = 0.00993252\n",
      "Iteration 10668, loss = 0.00992732\n",
      "Iteration 10669, loss = 0.00992213\n",
      "Iteration 10670, loss = 0.00991693\n",
      "Iteration 10671, loss = 0.00991174\n",
      "Iteration 10672, loss = 0.00990656\n",
      "Iteration 10673, loss = 0.00990138\n",
      "Iteration 10674, loss = 0.00989619\n",
      "Iteration 10675, loss = 0.00989102\n",
      "Iteration 10676, loss = 0.00988584\n",
      "Iteration 10677, loss = 0.00988067\n",
      "Iteration 10678, loss = 0.00987550\n",
      "Iteration 10679, loss = 0.00987034\n",
      "Iteration 10680, loss = 0.00986517\n",
      "Iteration 10681, loss = 0.00986001\n",
      "Iteration 10682, loss = 0.00985485\n",
      "Iteration 10683, loss = 0.00984970\n",
      "Iteration 10684, loss = 0.00984455\n",
      "Iteration 10685, loss = 0.00983940\n",
      "Iteration 10686, loss = 0.00983425\n",
      "Iteration 10687, loss = 0.00982911\n",
      "Iteration 10688, loss = 0.00982397\n",
      "Iteration 10689, loss = 0.00981883\n",
      "Iteration 10690, loss = 0.00981370\n",
      "Iteration 10691, loss = 0.00980856\n",
      "Iteration 10692, loss = 0.00980343\n",
      "Iteration 10693, loss = 0.00979831\n",
      "Iteration 10694, loss = 0.00979318\n",
      "Iteration 10695, loss = 0.00978806\n",
      "Iteration 10696, loss = 0.00978294\n",
      "Iteration 10697, loss = 0.00977783\n",
      "Iteration 10698, loss = 0.00977272\n",
      "Iteration 10699, loss = 0.00976761\n",
      "Iteration 10700, loss = 0.00976250\n",
      "Iteration 10701, loss = 0.00975740\n",
      "Iteration 10702, loss = 0.00975230\n",
      "Iteration 10703, loss = 0.00974720\n",
      "Iteration 10704, loss = 0.00974210\n",
      "Iteration 10705, loss = 0.00973701\n",
      "Iteration 10706, loss = 0.00973192\n",
      "Iteration 10707, loss = 0.00972683\n",
      "Iteration 10708, loss = 0.00972175\n",
      "Iteration 10709, loss = 0.00971666\n",
      "Iteration 10710, loss = 0.00971159\n",
      "Iteration 10711, loss = 0.00970651\n",
      "Iteration 10712, loss = 0.00970144\n",
      "Iteration 10713, loss = 0.00969637\n",
      "Iteration 10714, loss = 0.00969130\n",
      "Iteration 10715, loss = 0.00968623\n",
      "Iteration 10716, loss = 0.00968117\n",
      "Iteration 10717, loss = 0.00967611\n",
      "Iteration 10718, loss = 0.00967105\n",
      "Iteration 10719, loss = 0.00966600\n",
      "Iteration 10720, loss = 0.00966095\n",
      "Iteration 10721, loss = 0.00965590\n",
      "Iteration 10722, loss = 0.00965085\n",
      "Iteration 10723, loss = 0.00964581\n",
      "Iteration 10724, loss = 0.00964077\n",
      "Iteration 10725, loss = 0.00963573\n",
      "Iteration 10726, loss = 0.00963070\n",
      "Iteration 10727, loss = 0.00962567\n",
      "Iteration 10728, loss = 0.00962064\n",
      "Iteration 10729, loss = 0.00961561\n",
      "Iteration 10730, loss = 0.00961059\n",
      "Iteration 10731, loss = 0.00960557\n",
      "Iteration 10732, loss = 0.00960055\n",
      "Iteration 10733, loss = 0.00959553\n",
      "Iteration 10734, loss = 0.00959052\n",
      "Iteration 10735, loss = 0.00958551\n",
      "Iteration 10736, loss = 0.00958050\n",
      "Iteration 10737, loss = 0.00957550\n",
      "Iteration 10738, loss = 0.00957050\n",
      "Iteration 10739, loss = 0.00956550\n",
      "Iteration 10740, loss = 0.00956050\n",
      "Iteration 10741, loss = 0.00955551\n",
      "Iteration 10742, loss = 0.00955052\n",
      "Iteration 10743, loss = 0.00954553\n",
      "Iteration 10744, loss = 0.00954054\n",
      "Iteration 10745, loss = 0.00953556\n",
      "Iteration 10746, loss = 0.00953058\n",
      "Iteration 10747, loss = 0.00952560\n",
      "Iteration 10748, loss = 0.00952063\n",
      "Iteration 10749, loss = 0.00951566\n",
      "Iteration 10750, loss = 0.00951069\n",
      "Iteration 10751, loss = 0.00950572\n",
      "Iteration 10752, loss = 0.00950076\n",
      "Iteration 10753, loss = 0.00949580\n",
      "Iteration 10754, loss = 0.00949084\n",
      "Iteration 10755, loss = 0.00948588\n",
      "Iteration 10756, loss = 0.00948093\n",
      "Iteration 10757, loss = 0.00947598\n",
      "Iteration 10758, loss = 0.00947103\n",
      "Iteration 10759, loss = 0.00946609\n",
      "Iteration 10760, loss = 0.00946115\n",
      "Iteration 10761, loss = 0.00945621\n",
      "Iteration 10762, loss = 0.00945127\n",
      "Iteration 10763, loss = 0.00944634\n",
      "Iteration 10764, loss = 0.00944140\n",
      "Iteration 10765, loss = 0.00943648\n",
      "Iteration 10766, loss = 0.00943155\n",
      "Iteration 10767, loss = 0.00942663\n",
      "Iteration 10768, loss = 0.00942171\n",
      "Iteration 10769, loss = 0.00941679\n",
      "Iteration 10770, loss = 0.00941187\n",
      "Iteration 10771, loss = 0.00940696\n",
      "Iteration 10772, loss = 0.00940205\n",
      "Iteration 10773, loss = 0.00939714\n",
      "Iteration 10774, loss = 0.00939224\n",
      "Iteration 10775, loss = 0.00938734\n",
      "Iteration 10776, loss = 0.00938244\n",
      "Iteration 10777, loss = 0.00937754\n",
      "Iteration 10778, loss = 0.00937265\n",
      "Iteration 10779, loss = 0.00936776\n",
      "Iteration 10780, loss = 0.00936287\n",
      "Iteration 10781, loss = 0.00935798\n",
      "Iteration 10782, loss = 0.00935310\n",
      "Iteration 10783, loss = 0.00934822\n",
      "Iteration 10784, loss = 0.00934334\n",
      "Iteration 10785, loss = 0.00933846\n",
      "Iteration 10786, loss = 0.00933359\n",
      "Iteration 10787, loss = 0.00932872\n",
      "Iteration 10788, loss = 0.00932385\n",
      "Iteration 10789, loss = 0.00931899\n",
      "Iteration 10790, loss = 0.00931413\n",
      "Iteration 10791, loss = 0.00930927\n",
      "Iteration 10792, loss = 0.00930441\n",
      "Iteration 10793, loss = 0.00929956\n",
      "Iteration 10794, loss = 0.00929470\n",
      "Iteration 10795, loss = 0.00928986\n",
      "Iteration 10796, loss = 0.00928501\n",
      "Iteration 10797, loss = 0.00928017\n",
      "Iteration 10798, loss = 0.00927532\n",
      "Iteration 10799, loss = 0.00927049\n",
      "Iteration 10800, loss = 0.00926565\n",
      "Iteration 10801, loss = 0.00926082\n",
      "Iteration 10802, loss = 0.00925599\n",
      "Iteration 10803, loss = 0.00925116\n",
      "Iteration 10804, loss = 0.00924633\n",
      "Iteration 10805, loss = 0.00924151\n",
      "Iteration 10806, loss = 0.00923669\n",
      "Iteration 10807, loss = 0.00923187\n",
      "Iteration 10808, loss = 0.00922706\n",
      "Iteration 10809, loss = 0.00922225\n",
      "Iteration 10810, loss = 0.00921744\n",
      "Iteration 10811, loss = 0.00921263\n",
      "Iteration 10812, loss = 0.00920782\n",
      "Iteration 10813, loss = 0.00920302\n",
      "Iteration 10814, loss = 0.00919822\n",
      "Iteration 10815, loss = 0.00919343\n",
      "Iteration 10816, loss = 0.00918863\n",
      "Iteration 10817, loss = 0.00918384\n",
      "Iteration 10818, loss = 0.00917905\n",
      "Iteration 10819, loss = 0.00917427\n",
      "Iteration 10820, loss = 0.00916948\n",
      "Iteration 10821, loss = 0.00916470\n",
      "Iteration 10822, loss = 0.00915992\n",
      "Iteration 10823, loss = 0.00915515\n",
      "Iteration 10824, loss = 0.00915037\n",
      "Iteration 10825, loss = 0.00914560\n",
      "Iteration 10826, loss = 0.00914083\n",
      "Iteration 10827, loss = 0.00913607\n",
      "Iteration 10828, loss = 0.00913131\n",
      "Iteration 10829, loss = 0.00912654\n",
      "Iteration 10830, loss = 0.00912179\n",
      "Iteration 10831, loss = 0.00911703\n",
      "Iteration 10832, loss = 0.00911228\n",
      "Iteration 10833, loss = 0.00910753\n",
      "Iteration 10834, loss = 0.00910278\n",
      "Iteration 10835, loss = 0.00909804\n",
      "Iteration 10836, loss = 0.00909329\n",
      "Iteration 10837, loss = 0.00908855\n",
      "Iteration 10838, loss = 0.00908382\n",
      "Iteration 10839, loss = 0.00907908\n",
      "Iteration 10840, loss = 0.00907435\n",
      "Iteration 10841, loss = 0.00906962\n",
      "Iteration 10842, loss = 0.00906489\n",
      "Iteration 10843, loss = 0.00906017\n",
      "Iteration 10844, loss = 0.00905545\n",
      "Iteration 10845, loss = 0.00905073\n",
      "Iteration 10846, loss = 0.00904601\n",
      "Iteration 10847, loss = 0.00904129\n",
      "Iteration 10848, loss = 0.00903658\n",
      "Iteration 10849, loss = 0.00903187\n",
      "Iteration 10850, loss = 0.00902717\n",
      "Iteration 10851, loss = 0.00902246\n",
      "Iteration 10852, loss = 0.00901776\n",
      "Iteration 10853, loss = 0.00901306\n",
      "Iteration 10854, loss = 0.00900837\n",
      "Iteration 10855, loss = 0.00900367\n",
      "Iteration 10856, loss = 0.00899898\n",
      "Iteration 10857, loss = 0.00899429\n",
      "Iteration 10858, loss = 0.00898960\n",
      "Iteration 10859, loss = 0.00898492\n",
      "Iteration 10860, loss = 0.00898024\n",
      "Iteration 10861, loss = 0.00897556\n",
      "Iteration 10862, loss = 0.00897088\n",
      "Iteration 10863, loss = 0.00896621\n",
      "Iteration 10864, loss = 0.00896154\n",
      "Iteration 10865, loss = 0.00895687\n",
      "Iteration 10866, loss = 0.00895220\n",
      "Iteration 10867, loss = 0.00894754\n",
      "Iteration 10868, loss = 0.00894288\n",
      "Iteration 10869, loss = 0.00893822\n",
      "Iteration 10870, loss = 0.00893356\n",
      "Iteration 10871, loss = 0.00892891\n",
      "Iteration 10872, loss = 0.00892426\n",
      "Iteration 10873, loss = 0.00891961\n",
      "Iteration 10874, loss = 0.00891496\n",
      "Iteration 10875, loss = 0.00891032\n",
      "Iteration 10876, loss = 0.00890568\n",
      "Iteration 10877, loss = 0.00890104\n",
      "Iteration 10878, loss = 0.00889640\n",
      "Iteration 10879, loss = 0.00889177\n",
      "Iteration 10880, loss = 0.00888714\n",
      "Iteration 10881, loss = 0.00888251\n",
      "Iteration 10882, loss = 0.00887788\n",
      "Iteration 10883, loss = 0.00887326\n",
      "Iteration 10884, loss = 0.00886864\n",
      "Iteration 10885, loss = 0.00886402\n",
      "Iteration 10886, loss = 0.00885940\n",
      "Iteration 10887, loss = 0.00885479\n",
      "Iteration 10888, loss = 0.00885018\n",
      "Iteration 10889, loss = 0.00884557\n",
      "Iteration 10890, loss = 0.00884096\n",
      "Iteration 10891, loss = 0.00883636\n",
      "Iteration 10892, loss = 0.00883176\n",
      "Iteration 10893, loss = 0.00882716\n",
      "Iteration 10894, loss = 0.00882256\n",
      "Iteration 10895, loss = 0.00881797\n",
      "Iteration 10896, loss = 0.00881338\n",
      "Iteration 10897, loss = 0.00880879\n",
      "Iteration 10898, loss = 0.00880420\n",
      "Iteration 10899, loss = 0.00879962\n",
      "Iteration 10900, loss = 0.00879504\n",
      "Iteration 10901, loss = 0.00879046\n",
      "Iteration 10902, loss = 0.00878588\n",
      "Iteration 10903, loss = 0.00878131\n",
      "Iteration 10904, loss = 0.00877673\n",
      "Iteration 10905, loss = 0.00877216\n",
      "Iteration 10906, loss = 0.00876760\n",
      "Iteration 10907, loss = 0.00876303\n",
      "Iteration 10908, loss = 0.00875847\n",
      "Iteration 10909, loss = 0.00875391\n",
      "Iteration 10910, loss = 0.00874936\n",
      "Iteration 10911, loss = 0.00874480\n",
      "Iteration 10912, loss = 0.00874025\n",
      "Iteration 10913, loss = 0.00873570\n",
      "Iteration 10914, loss = 0.00873115\n",
      "Iteration 10915, loss = 0.00872661\n",
      "Iteration 10916, loss = 0.00872206\n",
      "Iteration 10917, loss = 0.00871752\n",
      "Iteration 10918, loss = 0.00871299\n",
      "Iteration 10919, loss = 0.00870845\n",
      "Iteration 10920, loss = 0.00870392\n",
      "Iteration 10921, loss = 0.00869939\n",
      "Iteration 10922, loss = 0.00869486\n",
      "Iteration 10923, loss = 0.00869034\n",
      "Iteration 10924, loss = 0.00868581\n",
      "Iteration 10925, loss = 0.00868129\n",
      "Iteration 10926, loss = 0.00867677\n",
      "Iteration 10927, loss = 0.00867226\n",
      "Iteration 10928, loss = 0.00866775\n",
      "Iteration 10929, loss = 0.00866323\n",
      "Iteration 10930, loss = 0.00865873\n",
      "Iteration 10931, loss = 0.00865422\n",
      "Iteration 10932, loss = 0.00864972\n",
      "Iteration 10933, loss = 0.00864522\n",
      "Iteration 10934, loss = 0.00864072\n",
      "Iteration 10935, loss = 0.00863622\n",
      "Iteration 10936, loss = 0.00863173\n",
      "Iteration 10937, loss = 0.00862724\n",
      "Iteration 10938, loss = 0.00862275\n",
      "Iteration 10939, loss = 0.00861826\n",
      "Iteration 10940, loss = 0.00861378\n",
      "Iteration 10941, loss = 0.00860929\n",
      "Iteration 10942, loss = 0.00860481\n",
      "Iteration 10943, loss = 0.00860034\n",
      "Iteration 10944, loss = 0.00859586\n",
      "Iteration 10945, loss = 0.00859139\n",
      "Iteration 10946, loss = 0.00858692\n",
      "Iteration 10947, loss = 0.00858245\n",
      "Iteration 10948, loss = 0.00857799\n",
      "Iteration 10949, loss = 0.00857353\n",
      "Iteration 10950, loss = 0.00856907\n",
      "Iteration 10951, loss = 0.00856461\n",
      "Iteration 10952, loss = 0.00856015\n",
      "Iteration 10953, loss = 0.00855570\n",
      "Iteration 10954, loss = 0.00855125\n",
      "Iteration 10955, loss = 0.00854680\n",
      "Iteration 10956, loss = 0.00854235\n",
      "Iteration 10957, loss = 0.00853791\n",
      "Iteration 10958, loss = 0.00853347\n",
      "Iteration 10959, loss = 0.00852903\n",
      "Iteration 10960, loss = 0.00852459\n",
      "Iteration 10961, loss = 0.00852016\n",
      "Iteration 10962, loss = 0.00851573\n",
      "Iteration 10963, loss = 0.00851130\n",
      "Iteration 10964, loss = 0.00850687\n",
      "Iteration 10965, loss = 0.00850245\n",
      "Iteration 10966, loss = 0.00849803\n",
      "Iteration 10967, loss = 0.00849361\n",
      "Iteration 10968, loss = 0.00848919\n",
      "Iteration 10969, loss = 0.00848477\n",
      "Iteration 10970, loss = 0.00848036\n",
      "Iteration 10971, loss = 0.00847595\n",
      "Iteration 10972, loss = 0.00847154\n",
      "Iteration 10973, loss = 0.00846714\n",
      "Iteration 10974, loss = 0.00846273\n",
      "Iteration 10975, loss = 0.00845833\n",
      "Iteration 10976, loss = 0.00845393\n",
      "Iteration 10977, loss = 0.00844954\n",
      "Iteration 10978, loss = 0.00844514\n",
      "Iteration 10979, loss = 0.00844075\n",
      "Iteration 10980, loss = 0.00843636\n",
      "Iteration 10981, loss = 0.00843198\n",
      "Iteration 10982, loss = 0.00842759\n",
      "Iteration 10983, loss = 0.00842321\n",
      "Iteration 10984, loss = 0.00841883\n",
      "Iteration 10985, loss = 0.00841445\n",
      "Iteration 10986, loss = 0.00841008\n",
      "Iteration 10987, loss = 0.00840571\n",
      "Iteration 10988, loss = 0.00840133\n",
      "Iteration 10989, loss = 0.00839697\n",
      "Iteration 10990, loss = 0.00839260\n",
      "Iteration 10991, loss = 0.00838824\n",
      "Iteration 10992, loss = 0.00838388\n",
      "Iteration 10993, loss = 0.00837952\n",
      "Iteration 10994, loss = 0.00837516\n",
      "Iteration 10995, loss = 0.00837081\n",
      "Iteration 10996, loss = 0.00836646\n",
      "Iteration 10997, loss = 0.00836211\n",
      "Iteration 10998, loss = 0.00835776\n",
      "Iteration 10999, loss = 0.00835341\n",
      "Iteration 11000, loss = 0.00834907\n",
      "Iteration 11001, loss = 0.00834473\n",
      "Iteration 11002, loss = 0.00834039\n",
      "Iteration 11003, loss = 0.00833606\n",
      "Iteration 11004, loss = 0.00833173\n",
      "Iteration 11005, loss = 0.00832740\n",
      "Iteration 11006, loss = 0.00832307\n",
      "Iteration 11007, loss = 0.00831874\n",
      "Iteration 11008, loss = 0.00831442\n",
      "Iteration 11009, loss = 0.00831010\n",
      "Iteration 11010, loss = 0.00830578\n",
      "Iteration 11011, loss = 0.00830146\n",
      "Iteration 11012, loss = 0.00829714\n",
      "Iteration 11013, loss = 0.00829283\n",
      "Iteration 11014, loss = 0.00828852\n",
      "Iteration 11015, loss = 0.00828421\n",
      "Iteration 11016, loss = 0.00827991\n",
      "Iteration 11017, loss = 0.00827561\n",
      "Iteration 11018, loss = 0.00827130\n",
      "Iteration 11019, loss = 0.00826701\n",
      "Iteration 11020, loss = 0.00826271\n",
      "Iteration 11021, loss = 0.00825842\n",
      "Iteration 11022, loss = 0.00825412\n",
      "Iteration 11023, loss = 0.00824984\n",
      "Iteration 11024, loss = 0.00824555\n",
      "Iteration 11025, loss = 0.00824126\n",
      "Iteration 11026, loss = 0.00823698\n",
      "Iteration 11027, loss = 0.00823270\n",
      "Iteration 11028, loss = 0.00822842\n",
      "Iteration 11029, loss = 0.00822415\n",
      "Iteration 11030, loss = 0.00821987\n",
      "Iteration 11031, loss = 0.00821560\n",
      "Iteration 11032, loss = 0.00821133\n",
      "Iteration 11033, loss = 0.00820707\n",
      "Iteration 11034, loss = 0.00820280\n",
      "Iteration 11035, loss = 0.00819854\n",
      "Iteration 11036, loss = 0.00819428\n",
      "Iteration 11037, loss = 0.00819002\n",
      "Iteration 11038, loss = 0.00818577\n",
      "Iteration 11039, loss = 0.00818152\n",
      "Iteration 11040, loss = 0.00817727\n",
      "Iteration 11041, loss = 0.00817302\n",
      "Iteration 11042, loss = 0.00816877\n",
      "Iteration 11043, loss = 0.00816453\n",
      "Iteration 11044, loss = 0.00816029\n",
      "Iteration 11045, loss = 0.00815605\n",
      "Iteration 11046, loss = 0.00815181\n",
      "Iteration 11047, loss = 0.00814758\n",
      "Iteration 11048, loss = 0.00814334\n",
      "Iteration 11049, loss = 0.00813911\n",
      "Iteration 11050, loss = 0.00813489\n",
      "Iteration 11051, loss = 0.00813066\n",
      "Iteration 11052, loss = 0.00812644\n",
      "Iteration 11053, loss = 0.00812221\n",
      "Iteration 11054, loss = 0.00811800\n",
      "Iteration 11055, loss = 0.00811378\n",
      "Iteration 11056, loss = 0.00810956\n",
      "Iteration 11057, loss = 0.00810535\n",
      "Iteration 11058, loss = 0.00810114\n",
      "Iteration 11059, loss = 0.00809694\n",
      "Iteration 11060, loss = 0.00809273\n",
      "Iteration 11061, loss = 0.00808853\n",
      "Iteration 11062, loss = 0.00808433\n",
      "Iteration 11063, loss = 0.00808013\n",
      "Iteration 11064, loss = 0.00807593\n",
      "Iteration 11065, loss = 0.00807174\n",
      "Iteration 11066, loss = 0.00806754\n",
      "Iteration 11067, loss = 0.00806335\n",
      "Iteration 11068, loss = 0.00805917\n",
      "Iteration 11069, loss = 0.00805498\n",
      "Iteration 11070, loss = 0.00805080\n",
      "Iteration 11071, loss = 0.00804662\n",
      "Iteration 11072, loss = 0.00804244\n",
      "Iteration 11073, loss = 0.00803826\n",
      "Iteration 11074, loss = 0.00803409\n",
      "Iteration 11075, loss = 0.00802992\n",
      "Iteration 11076, loss = 0.00802575\n",
      "Iteration 11077, loss = 0.00802158\n",
      "Iteration 11078, loss = 0.00801741\n",
      "Iteration 11079, loss = 0.00801325\n",
      "Iteration 11080, loss = 0.00800909\n",
      "Iteration 11081, loss = 0.00800493\n",
      "Iteration 11082, loss = 0.00800078\n",
      "Iteration 11083, loss = 0.00799662\n",
      "Iteration 11084, loss = 0.00799247\n",
      "Iteration 11085, loss = 0.00798832\n",
      "Iteration 11086, loss = 0.00798417\n",
      "Iteration 11087, loss = 0.00798003\n",
      "Iteration 11088, loss = 0.00797588\n",
      "Iteration 11089, loss = 0.00797174\n",
      "Iteration 11090, loss = 0.00796761\n",
      "Iteration 11091, loss = 0.00796347\n",
      "Iteration 11092, loss = 0.00795933\n",
      "Iteration 11093, loss = 0.00795520\n",
      "Iteration 11094, loss = 0.00795107\n",
      "Iteration 11095, loss = 0.00794695\n",
      "Iteration 11096, loss = 0.00794282\n",
      "Iteration 11097, loss = 0.00793870\n",
      "Iteration 11098, loss = 0.00793458\n",
      "Iteration 11099, loss = 0.00793046\n",
      "Iteration 11100, loss = 0.00792634\n",
      "Iteration 11101, loss = 0.00792223\n",
      "Iteration 11102, loss = 0.00791811\n",
      "Iteration 11103, loss = 0.00791400\n",
      "Iteration 11104, loss = 0.00790990\n",
      "Iteration 11105, loss = 0.00790579\n",
      "Iteration 11106, loss = 0.00790169\n",
      "Iteration 11107, loss = 0.00789759\n",
      "Iteration 11108, loss = 0.00789349\n",
      "Iteration 11109, loss = 0.00788939\n",
      "Iteration 11110, loss = 0.00788529\n",
      "Iteration 11111, loss = 0.00788120\n",
      "Iteration 11112, loss = 0.00787711\n",
      "Iteration 11113, loss = 0.00787302\n",
      "Iteration 11114, loss = 0.00786894\n",
      "Iteration 11115, loss = 0.00786485\n",
      "Iteration 11116, loss = 0.00786077\n",
      "Iteration 11117, loss = 0.00785669\n",
      "Iteration 11118, loss = 0.00785262\n",
      "Iteration 11119, loss = 0.00784854\n",
      "Iteration 11120, loss = 0.00784447\n",
      "Iteration 11121, loss = 0.00784040\n",
      "Iteration 11122, loss = 0.00783633\n",
      "Iteration 11123, loss = 0.00783226\n",
      "Iteration 11124, loss = 0.00782820\n",
      "Iteration 11125, loss = 0.00782413\n",
      "Iteration 11126, loss = 0.00782007\n",
      "Iteration 11127, loss = 0.00781602\n",
      "Iteration 11128, loss = 0.00781196\n",
      "Iteration 11129, loss = 0.00780791\n",
      "Iteration 11130, loss = 0.00780386\n",
      "Iteration 11131, loss = 0.00779981\n",
      "Iteration 11132, loss = 0.00779576\n",
      "Iteration 11133, loss = 0.00779172\n",
      "Iteration 11134, loss = 0.00778767\n",
      "Iteration 11135, loss = 0.00778363\n",
      "Iteration 11136, loss = 0.00777959\n",
      "Iteration 11137, loss = 0.00777556\n",
      "Iteration 11138, loss = 0.00777152\n",
      "Iteration 11139, loss = 0.00776749\n",
      "Iteration 11140, loss = 0.00776346\n",
      "Iteration 11141, loss = 0.00775943\n",
      "Iteration 11142, loss = 0.00775541\n",
      "Iteration 11143, loss = 0.00775139\n",
      "Iteration 11144, loss = 0.00774736\n",
      "Iteration 11145, loss = 0.00774335\n",
      "Iteration 11146, loss = 0.00773933\n",
      "Iteration 11147, loss = 0.00773531\n",
      "Iteration 11148, loss = 0.00773130\n",
      "Iteration 11149, loss = 0.00772729\n",
      "Iteration 11150, loss = 0.00772328\n",
      "Iteration 11151, loss = 0.00771928\n",
      "Iteration 11152, loss = 0.00771527\n",
      "Iteration 11153, loss = 0.00771127\n",
      "Iteration 11154, loss = 0.00770727\n",
      "Iteration 11155, loss = 0.00770327\n",
      "Iteration 11156, loss = 0.00769928\n",
      "Iteration 11157, loss = 0.00769528\n",
      "Iteration 11158, loss = 0.00769129\n",
      "Iteration 11159, loss = 0.00768730\n",
      "Iteration 11160, loss = 0.00768332\n",
      "Iteration 11161, loss = 0.00767933\n",
      "Iteration 11162, loss = 0.00767535\n",
      "Iteration 11163, loss = 0.00767137\n",
      "Iteration 11164, loss = 0.00766739\n",
      "Iteration 11165, loss = 0.00766341\n",
      "Iteration 11166, loss = 0.00765944\n",
      "Iteration 11167, loss = 0.00765547\n",
      "Iteration 11168, loss = 0.00765150\n",
      "Iteration 11169, loss = 0.00764753\n",
      "Iteration 11170, loss = 0.00764356\n",
      "Iteration 11171, loss = 0.00763960\n",
      "Iteration 11172, loss = 0.00763564\n",
      "Iteration 11173, loss = 0.00763168\n",
      "Iteration 11174, loss = 0.00762772\n",
      "Iteration 11175, loss = 0.00762376\n",
      "Iteration 11176, loss = 0.00761981\n",
      "Iteration 11177, loss = 0.00761586\n",
      "Iteration 11178, loss = 0.00761191\n",
      "Iteration 11179, loss = 0.00760796\n",
      "Iteration 11180, loss = 0.00760402\n",
      "Iteration 11181, loss = 0.00760008\n",
      "Iteration 11182, loss = 0.00759614\n",
      "Iteration 11183, loss = 0.00759220\n",
      "Iteration 11184, loss = 0.00758826\n",
      "Iteration 11185, loss = 0.00758433\n",
      "Iteration 11186, loss = 0.00758039\n",
      "Iteration 11187, loss = 0.00757646\n",
      "Iteration 11188, loss = 0.00757254\n",
      "Iteration 11189, loss = 0.00756861\n",
      "Iteration 11190, loss = 0.00756469\n",
      "Iteration 11191, loss = 0.00756076\n",
      "Iteration 11192, loss = 0.00755684\n",
      "Iteration 11193, loss = 0.00755293\n",
      "Iteration 11194, loss = 0.00754901\n",
      "Iteration 11195, loss = 0.00754510\n",
      "Iteration 11196, loss = 0.00754119\n",
      "Iteration 11197, loss = 0.00753728\n",
      "Iteration 11198, loss = 0.00753337\n",
      "Iteration 11199, loss = 0.00752947\n",
      "Iteration 11200, loss = 0.00752556\n",
      "Iteration 11201, loss = 0.00752166\n",
      "Iteration 11202, loss = 0.00751776\n",
      "Iteration 11203, loss = 0.00751387\n",
      "Iteration 11204, loss = 0.00750997\n",
      "Iteration 11205, loss = 0.00750608\n",
      "Iteration 11206, loss = 0.00750219\n",
      "Iteration 11207, loss = 0.00749830\n",
      "Iteration 11208, loss = 0.00749441\n",
      "Iteration 11209, loss = 0.00749053\n",
      "Iteration 11210, loss = 0.00748665\n",
      "Iteration 11211, loss = 0.00748277\n",
      "Iteration 11212, loss = 0.00747889\n",
      "Iteration 11213, loss = 0.00747501\n",
      "Iteration 11214, loss = 0.00747114\n",
      "Iteration 11215, loss = 0.00746727\n",
      "Iteration 11216, loss = 0.00746340\n",
      "Iteration 11217, loss = 0.00745953\n",
      "Iteration 11218, loss = 0.00745566\n",
      "Iteration 11219, loss = 0.00745180\n",
      "Iteration 11220, loss = 0.00744794\n",
      "Iteration 11221, loss = 0.00744408\n",
      "Iteration 11222, loss = 0.00744022\n",
      "Iteration 11223, loss = 0.00743637\n",
      "Iteration 11224, loss = 0.00743251\n",
      "Iteration 11225, loss = 0.00742866\n",
      "Iteration 11226, loss = 0.00742481\n",
      "Iteration 11227, loss = 0.00742096\n",
      "Iteration 11228, loss = 0.00741712\n",
      "Iteration 11229, loss = 0.00741328\n",
      "Iteration 11230, loss = 0.00740943\n",
      "Iteration 11231, loss = 0.00740560\n",
      "Iteration 11232, loss = 0.00740176\n",
      "Iteration 11233, loss = 0.00739792\n",
      "Iteration 11234, loss = 0.00739409\n",
      "Iteration 11235, loss = 0.00739026\n",
      "Iteration 11236, loss = 0.00738643\n",
      "Iteration 11237, loss = 0.00738260\n",
      "Iteration 11238, loss = 0.00737878\n",
      "Iteration 11239, loss = 0.00737496\n",
      "Iteration 11240, loss = 0.00737114\n",
      "Iteration 11241, loss = 0.00736732\n",
      "Iteration 11242, loss = 0.00736350\n",
      "Iteration 11243, loss = 0.00735969\n",
      "Iteration 11244, loss = 0.00735587\n",
      "Iteration 11245, loss = 0.00735206\n",
      "Iteration 11246, loss = 0.00734825\n",
      "Iteration 11247, loss = 0.00734445\n",
      "Iteration 11248, loss = 0.00734064\n",
      "Iteration 11249, loss = 0.00733684\n",
      "Iteration 11250, loss = 0.00733304\n",
      "Iteration 11251, loss = 0.00732924\n",
      "Iteration 11252, loss = 0.00732545\n",
      "Iteration 11253, loss = 0.00732165\n",
      "Iteration 11254, loss = 0.00731786\n",
      "Iteration 11255, loss = 0.00731407\n",
      "Iteration 11256, loss = 0.00731028\n",
      "Iteration 11257, loss = 0.00730649\n",
      "Iteration 11258, loss = 0.00730271\n",
      "Iteration 11259, loss = 0.00729893\n",
      "Iteration 11260, loss = 0.00729515\n",
      "Iteration 11261, loss = 0.00729137\n",
      "Iteration 11262, loss = 0.00728759\n",
      "Iteration 11263, loss = 0.00728382\n",
      "Iteration 11264, loss = 0.00728005\n",
      "Iteration 11265, loss = 0.00727628\n",
      "Iteration 11266, loss = 0.00727251\n",
      "Iteration 11267, loss = 0.00726874\n",
      "Iteration 11268, loss = 0.00726498\n",
      "Iteration 11269, loss = 0.00726122\n",
      "Iteration 11270, loss = 0.00725746\n",
      "Iteration 11271, loss = 0.00725370\n",
      "Iteration 11272, loss = 0.00724994\n",
      "Iteration 11273, loss = 0.00724619\n",
      "Iteration 11274, loss = 0.00724244\n",
      "Iteration 11275, loss = 0.00723869\n",
      "Iteration 11276, loss = 0.00723494\n",
      "Iteration 11277, loss = 0.00723119\n",
      "Iteration 11278, loss = 0.00722745\n",
      "Iteration 11279, loss = 0.00722371\n",
      "Iteration 11280, loss = 0.00721997\n",
      "Iteration 11281, loss = 0.00721623\n",
      "Iteration 11282, loss = 0.00721249\n",
      "Iteration 11283, loss = 0.00720876\n",
      "Iteration 11284, loss = 0.00720503\n",
      "Iteration 11285, loss = 0.00720130\n",
      "Iteration 11286, loss = 0.00719757\n",
      "Iteration 11287, loss = 0.00719384\n",
      "Iteration 11288, loss = 0.00719012\n",
      "Iteration 11289, loss = 0.00718639\n",
      "Iteration 11290, loss = 0.00718267\n",
      "Iteration 11291, loss = 0.00717896\n",
      "Iteration 11292, loss = 0.00717524\n",
      "Iteration 11293, loss = 0.00717153\n",
      "Iteration 11294, loss = 0.00716781\n",
      "Iteration 11295, loss = 0.00716410\n",
      "Iteration 11296, loss = 0.00716039\n",
      "Iteration 11297, loss = 0.00715669\n",
      "Iteration 11298, loss = 0.00715298\n",
      "Iteration 11299, loss = 0.00714928\n",
      "Iteration 11300, loss = 0.00714558\n",
      "Iteration 11301, loss = 0.00714188\n",
      "Iteration 11302, loss = 0.00713819\n",
      "Iteration 11303, loss = 0.00713449\n",
      "Iteration 11304, loss = 0.00713080\n",
      "Iteration 11305, loss = 0.00712711\n",
      "Iteration 11306, loss = 0.00712342\n",
      "Iteration 11307, loss = 0.00711973\n",
      "Iteration 11308, loss = 0.00711605\n",
      "Iteration 11309, loss = 0.00711237\n",
      "Iteration 11310, loss = 0.00710868\n",
      "Iteration 11311, loss = 0.00710501\n",
      "Iteration 11312, loss = 0.00710133\n",
      "Iteration 11313, loss = 0.00709765\n",
      "Iteration 11314, loss = 0.00709398\n",
      "Iteration 11315, loss = 0.00709031\n",
      "Iteration 11316, loss = 0.00708664\n",
      "Iteration 11317, loss = 0.00708297\n",
      "Iteration 11318, loss = 0.00707931\n",
      "Iteration 11319, loss = 0.00707565\n",
      "Iteration 11320, loss = 0.00707198\n",
      "Iteration 11321, loss = 0.00706833\n",
      "Iteration 11322, loss = 0.00706467\n",
      "Iteration 11323, loss = 0.00706101\n",
      "Iteration 11324, loss = 0.00705736\n",
      "Iteration 11325, loss = 0.00705371\n",
      "Iteration 11326, loss = 0.00705006\n",
      "Iteration 11327, loss = 0.00704641\n",
      "Iteration 11328, loss = 0.00704277\n",
      "Iteration 11329, loss = 0.00703912\n",
      "Iteration 11330, loss = 0.00703548\n",
      "Iteration 11331, loss = 0.00703184\n",
      "Iteration 11332, loss = 0.00702820\n",
      "Iteration 11333, loss = 0.00702457\n",
      "Iteration 11334, loss = 0.00702093\n",
      "Iteration 11335, loss = 0.00701730\n",
      "Iteration 11336, loss = 0.00701367\n",
      "Iteration 11337, loss = 0.00701004\n",
      "Iteration 11338, loss = 0.00700642\n",
      "Iteration 11339, loss = 0.00700279\n",
      "Iteration 11340, loss = 0.00699917\n",
      "Iteration 11341, loss = 0.00699555\n",
      "Iteration 11342, loss = 0.00699193\n",
      "Iteration 11343, loss = 0.00698831\n",
      "Iteration 11344, loss = 0.00698470\n",
      "Iteration 11345, loss = 0.00698109\n",
      "Iteration 11346, loss = 0.00697748\n",
      "Iteration 11347, loss = 0.00697387\n",
      "Iteration 11348, loss = 0.00697026\n",
      "Iteration 11349, loss = 0.00696666\n",
      "Iteration 11350, loss = 0.00696305\n",
      "Iteration 11351, loss = 0.00695945\n",
      "Iteration 11352, loss = 0.00695585\n",
      "Iteration 11353, loss = 0.00695226\n",
      "Iteration 11354, loss = 0.00694866\n",
      "Iteration 11355, loss = 0.00694507\n",
      "Iteration 11356, loss = 0.00694148\n",
      "Iteration 11357, loss = 0.00693789\n",
      "Iteration 11358, loss = 0.00693430\n",
      "Iteration 11359, loss = 0.00693071\n",
      "Iteration 11360, loss = 0.00692713\n",
      "Iteration 11361, loss = 0.00692355\n",
      "Iteration 11362, loss = 0.00691997\n",
      "Iteration 11363, loss = 0.00691639\n",
      "Iteration 11364, loss = 0.00691281\n",
      "Iteration 11365, loss = 0.00690924\n",
      "Iteration 11366, loss = 0.00690567\n",
      "Iteration 11367, loss = 0.00690210\n",
      "Iteration 11368, loss = 0.00689853\n",
      "Iteration 11369, loss = 0.00689496\n",
      "Iteration 11370, loss = 0.00689140\n",
      "Iteration 11371, loss = 0.00688783\n",
      "Iteration 11372, loss = 0.00688427\n",
      "Iteration 11373, loss = 0.00688071\n",
      "Iteration 11374, loss = 0.00687716\n",
      "Iteration 11375, loss = 0.00687360\n",
      "Iteration 11376, loss = 0.00687005\n",
      "Iteration 11377, loss = 0.00686650\n",
      "Iteration 11378, loss = 0.00686295\n",
      "Iteration 11379, loss = 0.00685940\n",
      "Iteration 11380, loss = 0.00685585\n",
      "Iteration 11381, loss = 0.00685231\n",
      "Iteration 11382, loss = 0.00684877\n",
      "Iteration 11383, loss = 0.00684523\n",
      "Iteration 11384, loss = 0.00684169\n",
      "Iteration 11385, loss = 0.00683815\n",
      "Iteration 11386, loss = 0.00683462\n",
      "Iteration 11387, loss = 0.00683109\n",
      "Iteration 11388, loss = 0.00682755\n",
      "Iteration 11389, loss = 0.00682403\n",
      "Iteration 11390, loss = 0.00682050\n",
      "Iteration 11391, loss = 0.00681697\n",
      "Iteration 11392, loss = 0.00681345\n",
      "Iteration 11393, loss = 0.00680993\n",
      "Iteration 11394, loss = 0.00680641\n",
      "Iteration 11395, loss = 0.00680289\n",
      "Iteration 11396, loss = 0.00679938\n",
      "Iteration 11397, loss = 0.00679586\n",
      "Iteration 11398, loss = 0.00679235\n",
      "Iteration 11399, loss = 0.00678884\n",
      "Iteration 11400, loss = 0.00678533\n",
      "Iteration 11401, loss = 0.00678183\n",
      "Iteration 11402, loss = 0.00677832\n",
      "Iteration 11403, loss = 0.00677482\n",
      "Iteration 11404, loss = 0.00677132\n",
      "Iteration 11405, loss = 0.00676782\n",
      "Iteration 11406, loss = 0.00676432\n",
      "Iteration 11407, loss = 0.00676083\n",
      "Iteration 11408, loss = 0.00675734\n",
      "Iteration 11409, loss = 0.00675385\n",
      "Iteration 11410, loss = 0.00675036\n",
      "Iteration 11411, loss = 0.00674687\n",
      "Iteration 11412, loss = 0.00674338\n",
      "Iteration 11413, loss = 0.00673990\n",
      "Iteration 11414, loss = 0.00673642\n",
      "Iteration 11415, loss = 0.00673294\n",
      "Iteration 11416, loss = 0.00672946\n",
      "Iteration 11417, loss = 0.00672598\n",
      "Iteration 11418, loss = 0.00672251\n",
      "Iteration 11419, loss = 0.00671904\n",
      "Iteration 11420, loss = 0.00671557\n",
      "Iteration 11421, loss = 0.00671210\n",
      "Iteration 11422, loss = 0.00670863\n",
      "Iteration 11423, loss = 0.00670516\n",
      "Iteration 11424, loss = 0.00670170\n",
      "Iteration 11425, loss = 0.00669824\n",
      "Iteration 11426, loss = 0.00669478\n",
      "Iteration 11427, loss = 0.00669132\n",
      "Iteration 11428, loss = 0.00668787\n",
      "Iteration 11429, loss = 0.00668441\n",
      "Iteration 11430, loss = 0.00668096\n",
      "Iteration 11431, loss = 0.00667751\n",
      "Iteration 11432, loss = 0.00667406\n",
      "Iteration 11433, loss = 0.00667061\n",
      "Iteration 11434, loss = 0.00666717\n",
      "Iteration 11435, loss = 0.00666373\n",
      "Iteration 11436, loss = 0.00666029\n",
      "Iteration 11437, loss = 0.00665685\n",
      "Iteration 11438, loss = 0.00665341\n",
      "Iteration 11439, loss = 0.00664997\n",
      "Iteration 11440, loss = 0.00664654\n",
      "Iteration 11441, loss = 0.00664311\n",
      "Iteration 11442, loss = 0.00663968\n",
      "Iteration 11443, loss = 0.00663625\n",
      "Iteration 11444, loss = 0.00663282\n",
      "Iteration 11445, loss = 0.00662940\n",
      "Iteration 11446, loss = 0.00662598\n",
      "Iteration 11447, loss = 0.00662255\n",
      "Iteration 11448, loss = 0.00661914\n",
      "Iteration 11449, loss = 0.00661572\n",
      "Iteration 11450, loss = 0.00661230\n",
      "Iteration 11451, loss = 0.00660889\n",
      "Iteration 11452, loss = 0.00660548\n",
      "Iteration 11453, loss = 0.00660207\n",
      "Iteration 11454, loss = 0.00659866\n",
      "Iteration 11455, loss = 0.00659525\n",
      "Iteration 11456, loss = 0.00659185\n",
      "Iteration 11457, loss = 0.00658845\n",
      "Iteration 11458, loss = 0.00658504\n",
      "Iteration 11459, loss = 0.00658165\n",
      "Iteration 11460, loss = 0.00657825\n",
      "Iteration 11461, loss = 0.00657485\n",
      "Iteration 11462, loss = 0.00657146\n",
      "Iteration 11463, loss = 0.00656807\n",
      "Iteration 11464, loss = 0.00656468\n",
      "Iteration 11465, loss = 0.00656129\n",
      "Iteration 11466, loss = 0.00655790\n",
      "Iteration 11467, loss = 0.00655452\n",
      "Iteration 11468, loss = 0.00655114\n",
      "Iteration 11469, loss = 0.00654776\n",
      "Iteration 11470, loss = 0.00654438\n",
      "Iteration 11471, loss = 0.00654100\n",
      "Iteration 11472, loss = 0.00653762\n",
      "Iteration 11473, loss = 0.00653425\n",
      "Iteration 11474, loss = 0.00653088\n",
      "Iteration 11475, loss = 0.00652751\n",
      "Iteration 11476, loss = 0.00652414\n",
      "Iteration 11477, loss = 0.00652077\n",
      "Iteration 11478, loss = 0.00651741\n",
      "Iteration 11479, loss = 0.00651405\n",
      "Iteration 11480, loss = 0.00651069\n",
      "Iteration 11481, loss = 0.00650733\n",
      "Iteration 11482, loss = 0.00650397\n",
      "Iteration 11483, loss = 0.00650061\n",
      "Iteration 11484, loss = 0.00649726\n",
      "Iteration 11485, loss = 0.00649391\n",
      "Iteration 11486, loss = 0.00649056\n",
      "Iteration 11487, loss = 0.00648721\n",
      "Iteration 11488, loss = 0.00648386\n",
      "Iteration 11489, loss = 0.00648052\n",
      "Iteration 11490, loss = 0.00647718\n",
      "Iteration 11491, loss = 0.00647383\n",
      "Iteration 11492, loss = 0.00647049\n",
      "Iteration 11493, loss = 0.00646716\n",
      "Iteration 11494, loss = 0.00646382\n",
      "Iteration 11495, loss = 0.00646049\n",
      "Iteration 11496, loss = 0.00645715\n",
      "Iteration 11497, loss = 0.00645382\n",
      "Iteration 11498, loss = 0.00645050\n",
      "Iteration 11499, loss = 0.00644717\n",
      "Iteration 11500, loss = 0.00644384\n",
      "Iteration 11501, loss = 0.00644052\n",
      "Iteration 11502, loss = 0.00643720\n",
      "Iteration 11503, loss = 0.00643388\n",
      "Iteration 11504, loss = 0.00643056\n",
      "Iteration 11505, loss = 0.00642725\n",
      "Iteration 11506, loss = 0.00642393\n",
      "Iteration 11507, loss = 0.00642062\n",
      "Iteration 11508, loss = 0.00641731\n",
      "Iteration 11509, loss = 0.00641400\n",
      "Iteration 11510, loss = 0.00641069\n",
      "Iteration 11511, loss = 0.00640739\n",
      "Iteration 11512, loss = 0.00640408\n",
      "Iteration 11513, loss = 0.00640078\n",
      "Iteration 11514, loss = 0.00639748\n",
      "Iteration 11515, loss = 0.00639418\n",
      "Iteration 11516, loss = 0.00639088\n",
      "Iteration 11517, loss = 0.00638759\n",
      "Iteration 11518, loss = 0.00638430\n",
      "Iteration 11519, loss = 0.00638101\n",
      "Iteration 11520, loss = 0.00637772\n",
      "Iteration 11521, loss = 0.00637443\n",
      "Iteration 11522, loss = 0.00637114\n",
      "Iteration 11523, loss = 0.00636786\n",
      "Iteration 11524, loss = 0.00636457\n",
      "Iteration 11525, loss = 0.00636129\n",
      "Iteration 11526, loss = 0.00635802\n",
      "Iteration 11527, loss = 0.00635474\n",
      "Iteration 11528, loss = 0.00635146\n",
      "Iteration 11529, loss = 0.00634819\n",
      "Iteration 11530, loss = 0.00634492\n",
      "Iteration 11531, loss = 0.00634165\n",
      "Iteration 11532, loss = 0.00633838\n",
      "Iteration 11533, loss = 0.00633511\n",
      "Iteration 11534, loss = 0.00633185\n",
      "Iteration 11535, loss = 0.00632858\n",
      "Iteration 11536, loss = 0.00632532\n",
      "Iteration 11537, loss = 0.00632206\n",
      "Iteration 11538, loss = 0.00631880\n",
      "Iteration 11539, loss = 0.00631555\n",
      "Iteration 11540, loss = 0.00631229\n",
      "Iteration 11541, loss = 0.00630904\n",
      "Iteration 11542, loss = 0.00630579\n",
      "Iteration 11543, loss = 0.00630254\n",
      "Iteration 11544, loss = 0.00629929\n",
      "Iteration 11545, loss = 0.00629605\n",
      "Iteration 11546, loss = 0.00629280\n",
      "Iteration 11547, loss = 0.00628956\n",
      "Iteration 11548, loss = 0.00628632\n",
      "Iteration 11549, loss = 0.00628308\n",
      "Iteration 11550, loss = 0.00627985\n",
      "Iteration 11551, loss = 0.00627661\n",
      "Iteration 11552, loss = 0.00627338\n",
      "Iteration 11553, loss = 0.00627015\n",
      "Iteration 11554, loss = 0.00626692\n",
      "Iteration 11555, loss = 0.00626369\n",
      "Iteration 11556, loss = 0.00626046\n",
      "Iteration 11557, loss = 0.00625724\n",
      "Iteration 11558, loss = 0.00625401\n",
      "Iteration 11559, loss = 0.00625079\n",
      "Iteration 11560, loss = 0.00624757\n",
      "Iteration 11561, loss = 0.00624435\n",
      "Iteration 11562, loss = 0.00624114\n",
      "Iteration 11563, loss = 0.00623792\n",
      "Iteration 11564, loss = 0.00623471\n",
      "Iteration 11565, loss = 0.00623150\n",
      "Iteration 11566, loss = 0.00622829\n",
      "Iteration 11567, loss = 0.00622508\n",
      "Iteration 11568, loss = 0.00622188\n",
      "Iteration 11569, loss = 0.00621867\n",
      "Iteration 11570, loss = 0.00621547\n",
      "Iteration 11571, loss = 0.00621227\n",
      "Iteration 11572, loss = 0.00620907\n",
      "Iteration 11573, loss = 0.00620587\n",
      "Iteration 11574, loss = 0.00620268\n",
      "Iteration 11575, loss = 0.00619949\n",
      "Iteration 11576, loss = 0.00619629\n",
      "Iteration 11577, loss = 0.00619310\n",
      "Iteration 11578, loss = 0.00618992\n",
      "Iteration 11579, loss = 0.00618673\n",
      "Iteration 11580, loss = 0.00618354\n",
      "Iteration 11581, loss = 0.00618036\n",
      "Iteration 11582, loss = 0.00617718\n",
      "Iteration 11583, loss = 0.00617400\n",
      "Iteration 11584, loss = 0.00617082\n",
      "Iteration 11585, loss = 0.00616764\n",
      "Iteration 11586, loss = 0.00616447\n",
      "Iteration 11587, loss = 0.00616130\n",
      "Iteration 11588, loss = 0.00615812\n",
      "Iteration 11589, loss = 0.00615495\n",
      "Iteration 11590, loss = 0.00615179\n",
      "Iteration 11591, loss = 0.00614862\n",
      "Iteration 11592, loss = 0.00614546\n",
      "Iteration 11593, loss = 0.00614229\n",
      "Iteration 11594, loss = 0.00613913\n",
      "Iteration 11595, loss = 0.00613597\n",
      "Iteration 11596, loss = 0.00613281\n",
      "Iteration 11597, loss = 0.00612966\n",
      "Iteration 11598, loss = 0.00612650\n",
      "Iteration 11599, loss = 0.00612335\n",
      "Iteration 11600, loss = 0.00612020\n",
      "Iteration 11601, loss = 0.00611705\n",
      "Iteration 11602, loss = 0.00611390\n",
      "Iteration 11603, loss = 0.00611076\n",
      "Iteration 11604, loss = 0.00610761\n",
      "Iteration 11605, loss = 0.00610447\n",
      "Iteration 11606, loss = 0.00610133\n",
      "Iteration 11607, loss = 0.00609819\n",
      "Iteration 11608, loss = 0.00609505\n",
      "Iteration 11609, loss = 0.00609192\n",
      "Iteration 11610, loss = 0.00608878\n",
      "Iteration 11611, loss = 0.00608565\n",
      "Iteration 11612, loss = 0.00608252\n",
      "Iteration 11613, loss = 0.00607939\n",
      "Iteration 11614, loss = 0.00607627\n",
      "Iteration 11615, loss = 0.00607314\n",
      "Iteration 11616, loss = 0.00607002\n",
      "Iteration 11617, loss = 0.00606689\n",
      "Iteration 11618, loss = 0.00606377\n",
      "Iteration 11619, loss = 0.00606065\n",
      "Iteration 11620, loss = 0.00605754\n",
      "Iteration 11621, loss = 0.00605442\n",
      "Iteration 11622, loss = 0.00605131\n",
      "Iteration 11623, loss = 0.00604820\n",
      "Iteration 11624, loss = 0.00604509\n",
      "Iteration 11625, loss = 0.00604198\n",
      "Iteration 11626, loss = 0.00603887\n",
      "Iteration 11627, loss = 0.00603576\n",
      "Iteration 11628, loss = 0.00603266\n",
      "Iteration 11629, loss = 0.00602956\n",
      "Iteration 11630, loss = 0.00602646\n",
      "Iteration 11631, loss = 0.00602336\n",
      "Iteration 11632, loss = 0.00602026\n",
      "Iteration 11633, loss = 0.00601717\n",
      "Iteration 11634, loss = 0.00601407\n",
      "Iteration 11635, loss = 0.00601098\n",
      "Iteration 11636, loss = 0.00600789\n",
      "Iteration 11637, loss = 0.00600480\n",
      "Iteration 11638, loss = 0.00600172\n",
      "Iteration 11639, loss = 0.00599863\n",
      "Iteration 11640, loss = 0.00599555\n",
      "Iteration 11641, loss = 0.00599247\n",
      "Iteration 11642, loss = 0.00598939\n",
      "Iteration 11643, loss = 0.00598631\n",
      "Iteration 11644, loss = 0.00598323\n",
      "Iteration 11645, loss = 0.00598015\n",
      "Iteration 11646, loss = 0.00597708\n",
      "Iteration 11647, loss = 0.00597401\n",
      "Iteration 11648, loss = 0.00597094\n",
      "Iteration 11649, loss = 0.00596787\n",
      "Iteration 11650, loss = 0.00596480\n",
      "Iteration 11651, loss = 0.00596174\n",
      "Iteration 11652, loss = 0.00595867\n",
      "Iteration 11653, loss = 0.00595561\n",
      "Iteration 11654, loss = 0.00595255\n",
      "Iteration 11655, loss = 0.00594949\n",
      "Iteration 11656, loss = 0.00594644\n",
      "Iteration 11657, loss = 0.00594338\n",
      "Iteration 11658, loss = 0.00594033\n",
      "Iteration 11659, loss = 0.00593727\n",
      "Iteration 11660, loss = 0.00593422\n",
      "Iteration 11661, loss = 0.00593118\n",
      "Iteration 11662, loss = 0.00592813\n",
      "Iteration 11663, loss = 0.00592508\n",
      "Iteration 11664, loss = 0.00592204\n",
      "Iteration 11665, loss = 0.00591900\n",
      "Iteration 11666, loss = 0.00591596\n",
      "Iteration 11667, loss = 0.00591292\n",
      "Iteration 11668, loss = 0.00590988\n",
      "Iteration 11669, loss = 0.00590684\n",
      "Iteration 11670, loss = 0.00590381\n",
      "Iteration 11671, loss = 0.00590078\n",
      "Iteration 11672, loss = 0.00589775\n",
      "Iteration 11673, loss = 0.00589472\n",
      "Iteration 11674, loss = 0.00589169\n",
      "Iteration 11675, loss = 0.00588866\n",
      "Iteration 11676, loss = 0.00588564\n",
      "Iteration 11677, loss = 0.00588262\n",
      "Iteration 11678, loss = 0.00587960\n",
      "Iteration 11679, loss = 0.00587658\n",
      "Iteration 11680, loss = 0.00587356\n",
      "Iteration 11681, loss = 0.00587054\n",
      "Iteration 11682, loss = 0.00586753\n",
      "Iteration 11683, loss = 0.00586452\n",
      "Iteration 11684, loss = 0.00586151\n",
      "Iteration 11685, loss = 0.00585850\n",
      "Iteration 11686, loss = 0.00585549\n",
      "Iteration 11687, loss = 0.00585248\n",
      "Iteration 11688, loss = 0.00584948\n",
      "Iteration 11689, loss = 0.00584648\n",
      "Iteration 11690, loss = 0.00584347\n",
      "Iteration 11691, loss = 0.00584047\n",
      "Iteration 11692, loss = 0.00583748\n",
      "Iteration 11693, loss = 0.00583448\n",
      "Iteration 11694, loss = 0.00583149\n",
      "Iteration 11695, loss = 0.00582849\n",
      "Iteration 11696, loss = 0.00582550\n",
      "Iteration 11697, loss = 0.00582251\n",
      "Iteration 11698, loss = 0.00581952\n",
      "Iteration 11699, loss = 0.00581654\n",
      "Iteration 11700, loss = 0.00581355\n",
      "Iteration 11701, loss = 0.00581057\n",
      "Iteration 11702, loss = 0.00580759\n",
      "Iteration 11703, loss = 0.00580461\n",
      "Iteration 11704, loss = 0.00580163\n",
      "Iteration 11705, loss = 0.00579865\n",
      "Iteration 11706, loss = 0.00579567\n",
      "Iteration 11707, loss = 0.00579270\n",
      "Iteration 11708, loss = 0.00578973\n",
      "Iteration 11709, loss = 0.00578676\n",
      "Iteration 11710, loss = 0.00578379\n",
      "Iteration 11711, loss = 0.00578082\n",
      "Iteration 11712, loss = 0.00577786\n",
      "Iteration 11713, loss = 0.00577489\n",
      "Iteration 11714, loss = 0.00577193\n",
      "Iteration 11715, loss = 0.00576897\n",
      "Iteration 11716, loss = 0.00576601\n",
      "Iteration 11717, loss = 0.00576305\n",
      "Iteration 11718, loss = 0.00576010\n",
      "Iteration 11719, loss = 0.00575714\n",
      "Iteration 11720, loss = 0.00575419\n",
      "Iteration 11721, loss = 0.00575124\n",
      "Iteration 11722, loss = 0.00574829\n",
      "Iteration 11723, loss = 0.00574534\n",
      "Iteration 11724, loss = 0.00574239\n",
      "Iteration 11725, loss = 0.00573945\n",
      "Iteration 11726, loss = 0.00573650\n",
      "Iteration 11727, loss = 0.00573356\n",
      "Iteration 11728, loss = 0.00573062\n",
      "Iteration 11729, loss = 0.00572768\n",
      "Iteration 11730, loss = 0.00572475\n",
      "Iteration 11731, loss = 0.00572181\n",
      "Iteration 11732, loss = 0.00571888\n",
      "Iteration 11733, loss = 0.00571595\n",
      "Iteration 11734, loss = 0.00571302\n",
      "Iteration 11735, loss = 0.00571009\n",
      "Iteration 11736, loss = 0.00570716\n",
      "Iteration 11737, loss = 0.00570423\n",
      "Iteration 11738, loss = 0.00570131\n",
      "Iteration 11739, loss = 0.00569839\n",
      "Iteration 11740, loss = 0.00569547\n",
      "Iteration 11741, loss = 0.00569255\n",
      "Iteration 11742, loss = 0.00568963\n",
      "Iteration 11743, loss = 0.00568671\n",
      "Iteration 11744, loss = 0.00568380\n",
      "Iteration 11745, loss = 0.00568089\n",
      "Iteration 11746, loss = 0.00567797\n",
      "Iteration 11747, loss = 0.00567506\n",
      "Iteration 11748, loss = 0.00567216\n",
      "Iteration 11749, loss = 0.00566925\n",
      "Iteration 11750, loss = 0.00566634\n",
      "Iteration 11751, loss = 0.00566344\n",
      "Iteration 11752, loss = 0.00566054\n",
      "Iteration 11753, loss = 0.00565764\n",
      "Iteration 11754, loss = 0.00565474\n",
      "Iteration 11755, loss = 0.00565184\n",
      "Iteration 11756, loss = 0.00564895\n",
      "Iteration 11757, loss = 0.00564605\n",
      "Iteration 11758, loss = 0.00564316\n",
      "Iteration 11759, loss = 0.00564027\n",
      "Iteration 11760, loss = 0.00563738\n",
      "Iteration 11761, loss = 0.00563449\n",
      "Iteration 11762, loss = 0.00563161\n",
      "Iteration 11763, loss = 0.00562872\n",
      "Iteration 11764, loss = 0.00562584\n",
      "Iteration 11765, loss = 0.00562296\n",
      "Iteration 11766, loss = 0.00562008\n",
      "Iteration 11767, loss = 0.00561720\n",
      "Iteration 11768, loss = 0.00561432\n",
      "Iteration 11769, loss = 0.00561145\n",
      "Iteration 11770, loss = 0.00560857\n",
      "Iteration 11771, loss = 0.00560570\n",
      "Iteration 11772, loss = 0.00560283\n",
      "Iteration 11773, loss = 0.00559996\n",
      "Iteration 11774, loss = 0.00559709\n",
      "Iteration 11775, loss = 0.00559423\n",
      "Iteration 11776, loss = 0.00559136\n",
      "Iteration 11777, loss = 0.00558850\n",
      "Iteration 11778, loss = 0.00558564\n",
      "Iteration 11779, loss = 0.00558278\n",
      "Iteration 11780, loss = 0.00557992\n",
      "Iteration 11781, loss = 0.00557707\n",
      "Iteration 11782, loss = 0.00557421\n",
      "Iteration 11783, loss = 0.00557136\n",
      "Iteration 11784, loss = 0.00556851\n",
      "Iteration 11785, loss = 0.00556566\n",
      "Iteration 11786, loss = 0.00556281\n",
      "Iteration 11787, loss = 0.00555996\n",
      "Iteration 11788, loss = 0.00555712\n",
      "Iteration 11789, loss = 0.00555427\n",
      "Iteration 11790, loss = 0.00555143\n",
      "Iteration 11791, loss = 0.00554859\n",
      "Iteration 11792, loss = 0.00554575\n",
      "Iteration 11793, loss = 0.00554291\n",
      "Iteration 11794, loss = 0.00554007\n",
      "Iteration 11795, loss = 0.00553724\n",
      "Iteration 11796, loss = 0.00553441\n",
      "Iteration 11797, loss = 0.00553157\n",
      "Iteration 11798, loss = 0.00552874\n",
      "Iteration 11799, loss = 0.00552592\n",
      "Iteration 11800, loss = 0.00552309\n",
      "Iteration 11801, loss = 0.00552026\n",
      "Iteration 11802, loss = 0.00551744\n",
      "Iteration 11803, loss = 0.00551462\n",
      "Iteration 11804, loss = 0.00551180\n",
      "Iteration 11805, loss = 0.00550898\n",
      "Iteration 11806, loss = 0.00550616\n",
      "Iteration 11807, loss = 0.00550334\n",
      "Iteration 11808, loss = 0.00550053\n",
      "Iteration 11809, loss = 0.00549771\n",
      "Iteration 11810, loss = 0.00549490\n",
      "Iteration 11811, loss = 0.00549209\n",
      "Iteration 11812, loss = 0.00548928\n",
      "Iteration 11813, loss = 0.00548648\n",
      "Iteration 11814, loss = 0.00548367\n",
      "Iteration 11815, loss = 0.00548087\n",
      "Iteration 11816, loss = 0.00547807\n",
      "Iteration 11817, loss = 0.00547526\n",
      "Iteration 11818, loss = 0.00547247\n",
      "Iteration 11819, loss = 0.00546967\n",
      "Iteration 11820, loss = 0.00546687\n",
      "Iteration 11821, loss = 0.00546408\n",
      "Iteration 11822, loss = 0.00546128\n",
      "Iteration 11823, loss = 0.00545849\n",
      "Iteration 11824, loss = 0.00545570\n",
      "Iteration 11825, loss = 0.00545291\n",
      "Iteration 11826, loss = 0.00545013\n",
      "Iteration 11827, loss = 0.00544734\n",
      "Iteration 11828, loss = 0.00544456\n",
      "Iteration 11829, loss = 0.00544177\n",
      "Iteration 11830, loss = 0.00543899\n",
      "Iteration 11831, loss = 0.00543621\n",
      "Iteration 11832, loss = 0.00543343\n",
      "Iteration 11833, loss = 0.00543066\n",
      "Iteration 11834, loss = 0.00542788\n",
      "Iteration 11835, loss = 0.00542511\n",
      "Iteration 11836, loss = 0.00542234\n",
      "Iteration 11837, loss = 0.00541957\n",
      "Iteration 11838, loss = 0.00541680\n",
      "Iteration 11839, loss = 0.00541403\n",
      "Iteration 11840, loss = 0.00541126\n",
      "Iteration 11841, loss = 0.00540850\n",
      "Iteration 11842, loss = 0.00540574\n",
      "Iteration 11843, loss = 0.00540298\n",
      "Iteration 11844, loss = 0.00540022\n",
      "Iteration 11845, loss = 0.00539746\n",
      "Iteration 11846, loss = 0.00539470\n",
      "Iteration 11847, loss = 0.00539195\n",
      "Iteration 11848, loss = 0.00538919\n",
      "Iteration 11849, loss = 0.00538644\n",
      "Iteration 11850, loss = 0.00538369\n",
      "Iteration 11851, loss = 0.00538094\n",
      "Iteration 11852, loss = 0.00537819\n",
      "Iteration 11853, loss = 0.00537544\n",
      "Iteration 11854, loss = 0.00537270\n",
      "Iteration 11855, loss = 0.00536996\n",
      "Iteration 11856, loss = 0.00536721\n",
      "Iteration 11857, loss = 0.00536447\n",
      "Iteration 11858, loss = 0.00536174\n",
      "Iteration 11859, loss = 0.00535900\n",
      "Iteration 11860, loss = 0.00535626\n",
      "Iteration 11861, loss = 0.00535353\n",
      "Iteration 11862, loss = 0.00535080\n",
      "Iteration 11863, loss = 0.00534806\n",
      "Iteration 11864, loss = 0.00534533\n",
      "Iteration 11865, loss = 0.00534261\n",
      "Iteration 11866, loss = 0.00533988\n",
      "Iteration 11867, loss = 0.00533715\n",
      "Iteration 11868, loss = 0.00533443\n",
      "Iteration 11869, loss = 0.00533171\n",
      "Iteration 11870, loss = 0.00532899\n",
      "Iteration 11871, loss = 0.00532627\n",
      "Iteration 11872, loss = 0.00532355\n",
      "Iteration 11873, loss = 0.00532083\n",
      "Iteration 11874, loss = 0.00531812\n",
      "Iteration 11875, loss = 0.00531540\n",
      "Iteration 11876, loss = 0.00531269\n",
      "Iteration 11877, loss = 0.00530998\n",
      "Iteration 11878, loss = 0.00530727\n",
      "Iteration 11879, loss = 0.00530457\n",
      "Iteration 11880, loss = 0.00530186\n",
      "Iteration 11881, loss = 0.00529916\n",
      "Iteration 11882, loss = 0.00529645\n",
      "Iteration 11883, loss = 0.00529375\n",
      "Iteration 11884, loss = 0.00529105\n",
      "Iteration 11885, loss = 0.00528835\n",
      "Iteration 11886, loss = 0.00528566\n",
      "Iteration 11887, loss = 0.00528296\n",
      "Iteration 11888, loss = 0.00528027\n",
      "Iteration 11889, loss = 0.00527757\n",
      "Iteration 11890, loss = 0.00527488\n",
      "Iteration 11891, loss = 0.00527219\n",
      "Iteration 11892, loss = 0.00526950\n",
      "Iteration 11893, loss = 0.00526682\n",
      "Iteration 11894, loss = 0.00526413\n",
      "Iteration 11895, loss = 0.00526145\n",
      "Iteration 11896, loss = 0.00525877\n",
      "Iteration 11897, loss = 0.00525608\n",
      "Iteration 11898, loss = 0.00525341\n",
      "Iteration 11899, loss = 0.00525073\n",
      "Iteration 11900, loss = 0.00524805\n",
      "Iteration 11901, loss = 0.00524538\n",
      "Iteration 11902, loss = 0.00524270\n",
      "Iteration 11903, loss = 0.00524003\n",
      "Iteration 11904, loss = 0.00523736\n",
      "Iteration 11905, loss = 0.00523469\n",
      "Iteration 11906, loss = 0.00523202\n",
      "Iteration 11907, loss = 0.00522936\n",
      "Iteration 11908, loss = 0.00522669\n",
      "Iteration 11909, loss = 0.00522403\n",
      "Iteration 11910, loss = 0.00522137\n",
      "Iteration 11911, loss = 0.00521871\n",
      "Iteration 11912, loss = 0.00521605\n",
      "Iteration 11913, loss = 0.00521339\n",
      "Iteration 11914, loss = 0.00521073\n",
      "Iteration 11915, loss = 0.00520808\n",
      "Iteration 11916, loss = 0.00520543\n",
      "Iteration 11917, loss = 0.00520278\n",
      "Iteration 11918, loss = 0.00520013\n",
      "Iteration 11919, loss = 0.00519748\n",
      "Iteration 11920, loss = 0.00519483\n",
      "Iteration 11921, loss = 0.00519218\n",
      "Iteration 11922, loss = 0.00518954\n",
      "Iteration 11923, loss = 0.00518690\n",
      "Iteration 11924, loss = 0.00518426\n",
      "Iteration 11925, loss = 0.00518162\n",
      "Iteration 11926, loss = 0.00517898\n",
      "Iteration 11927, loss = 0.00517634\n",
      "Iteration 11928, loss = 0.00517370\n",
      "Iteration 11929, loss = 0.00517107\n",
      "Iteration 11930, loss = 0.00516844\n",
      "Iteration 11931, loss = 0.00516581\n",
      "Iteration 11932, loss = 0.00516318\n",
      "Iteration 11933, loss = 0.00516055\n",
      "Iteration 11934, loss = 0.00515792\n",
      "Iteration 11935, loss = 0.00515530\n",
      "Iteration 11936, loss = 0.00515267\n",
      "Iteration 11937, loss = 0.00515005\n",
      "Iteration 11938, loss = 0.00514743\n",
      "Iteration 11939, loss = 0.00514481\n",
      "Iteration 11940, loss = 0.00514219\n",
      "Iteration 11941, loss = 0.00513957\n",
      "Iteration 11942, loss = 0.00513696\n",
      "Iteration 11943, loss = 0.00513434\n",
      "Iteration 11944, loss = 0.00513173\n",
      "Iteration 11945, loss = 0.00512912\n",
      "Iteration 11946, loss = 0.00512651\n",
      "Iteration 11947, loss = 0.00512390\n",
      "Iteration 11948, loss = 0.00512130\n",
      "Iteration 11949, loss = 0.00511869\n",
      "Iteration 11950, loss = 0.00511609\n",
      "Iteration 11951, loss = 0.00511348\n",
      "Iteration 11952, loss = 0.00511088\n",
      "Iteration 11953, loss = 0.00510828\n",
      "Iteration 11954, loss = 0.00510569\n",
      "Iteration 11955, loss = 0.00510309\n",
      "Iteration 11956, loss = 0.00510049\n",
      "Iteration 11957, loss = 0.00509790\n",
      "Iteration 11958, loss = 0.00509531\n",
      "Iteration 11959, loss = 0.00509272\n",
      "Iteration 11960, loss = 0.00509013\n",
      "Iteration 11961, loss = 0.00508754\n",
      "Iteration 11962, loss = 0.00508495\n",
      "Iteration 11963, loss = 0.00508237\n",
      "Iteration 11964, loss = 0.00507978\n",
      "Iteration 11965, loss = 0.00507720\n",
      "Iteration 11966, loss = 0.00507462\n",
      "Iteration 11967, loss = 0.00507204\n",
      "Iteration 11968, loss = 0.00506946\n",
      "Iteration 11969, loss = 0.00506689\n",
      "Iteration 11970, loss = 0.00506431\n",
      "Iteration 11971, loss = 0.00506174\n",
      "Iteration 11972, loss = 0.00505916\n",
      "Iteration 11973, loss = 0.00505659\n",
      "Iteration 11974, loss = 0.00505402\n",
      "Iteration 11975, loss = 0.00505145\n",
      "Iteration 11976, loss = 0.00504889\n",
      "Iteration 11977, loss = 0.00504632\n",
      "Iteration 11978, loss = 0.00504376\n",
      "Iteration 11979, loss = 0.00504120\n",
      "Iteration 11980, loss = 0.00503863\n",
      "Iteration 11981, loss = 0.00503608\n",
      "Iteration 11982, loss = 0.00503352\n",
      "Iteration 11983, loss = 0.00503096\n",
      "Iteration 11984, loss = 0.00502840\n",
      "Iteration 11985, loss = 0.00502585\n",
      "Iteration 11986, loss = 0.00502330\n",
      "Iteration 11987, loss = 0.00502075\n",
      "Iteration 11988, loss = 0.00501820\n",
      "Iteration 11989, loss = 0.00501565\n",
      "Iteration 11990, loss = 0.00501310\n",
      "Iteration 11991, loss = 0.00501056\n",
      "Iteration 11992, loss = 0.00500801\n",
      "Iteration 11993, loss = 0.00500547\n",
      "Iteration 11994, loss = 0.00500293\n",
      "Iteration 11995, loss = 0.00500039\n",
      "Iteration 11996, loss = 0.00499785\n",
      "Iteration 11997, loss = 0.00499531\n",
      "Iteration 11998, loss = 0.00499278\n",
      "Iteration 11999, loss = 0.00499024\n",
      "Iteration 12000, loss = 0.00498771\n",
      "Iteration 12001, loss = 0.00498518\n",
      "Iteration 12002, loss = 0.00498265\n",
      "Iteration 12003, loss = 0.00498012\n",
      "Iteration 12004, loss = 0.00497759\n",
      "Iteration 12005, loss = 0.00497506\n",
      "Iteration 12006, loss = 0.00497254\n",
      "Iteration 12007, loss = 0.00497002\n",
      "Iteration 12008, loss = 0.00496749\n",
      "Iteration 12009, loss = 0.00496497\n",
      "Iteration 12010, loss = 0.00496246\n",
      "Iteration 12011, loss = 0.00495994\n",
      "Iteration 12012, loss = 0.00495742\n",
      "Iteration 12013, loss = 0.00495491\n",
      "Iteration 12014, loss = 0.00495239\n",
      "Iteration 12015, loss = 0.00494988\n",
      "Iteration 12016, loss = 0.00494737\n",
      "Iteration 12017, loss = 0.00494486\n",
      "Iteration 12018, loss = 0.00494235\n",
      "Iteration 12019, loss = 0.00493985\n",
      "Iteration 12020, loss = 0.00493734\n",
      "Iteration 12021, loss = 0.00493484\n",
      "Iteration 12022, loss = 0.00493234\n",
      "Iteration 12023, loss = 0.00492983\n",
      "Iteration 12024, loss = 0.00492734\n",
      "Iteration 12025, loss = 0.00492484\n",
      "Iteration 12026, loss = 0.00492234\n",
      "Iteration 12027, loss = 0.00491984\n",
      "Iteration 12028, loss = 0.00491735\n",
      "Iteration 12029, loss = 0.00491486\n",
      "Iteration 12030, loss = 0.00491237\n",
      "Iteration 12031, loss = 0.00490988\n",
      "Iteration 12032, loss = 0.00490739\n",
      "Iteration 12033, loss = 0.00490490\n",
      "Iteration 12034, loss = 0.00490242\n",
      "Iteration 12035, loss = 0.00489993\n",
      "Iteration 12036, loss = 0.00489745\n",
      "Iteration 12037, loss = 0.00489497\n",
      "Iteration 12038, loss = 0.00489249\n",
      "Iteration 12039, loss = 0.00489001\n",
      "Iteration 12040, loss = 0.00488753\n",
      "Iteration 12041, loss = 0.00488505\n",
      "Iteration 12042, loss = 0.00488258\n",
      "Iteration 12043, loss = 0.00488011\n",
      "Iteration 12044, loss = 0.00487763\n",
      "Iteration 12045, loss = 0.00487516\n",
      "Iteration 12046, loss = 0.00487269\n",
      "Iteration 12047, loss = 0.00487023\n",
      "Iteration 12048, loss = 0.00486776\n",
      "Iteration 12049, loss = 0.00486529\n",
      "Iteration 12050, loss = 0.00486283\n",
      "Iteration 12051, loss = 0.00486037\n",
      "Iteration 12052, loss = 0.00485791\n",
      "Iteration 12053, loss = 0.00485545\n",
      "Iteration 12054, loss = 0.00485299\n",
      "Iteration 12055, loss = 0.00485053\n",
      "Iteration 12056, loss = 0.00484808\n",
      "Iteration 12057, loss = 0.00484562\n",
      "Iteration 12058, loss = 0.00484317\n",
      "Iteration 12059, loss = 0.00484072\n",
      "Iteration 12060, loss = 0.00483827\n",
      "Iteration 12061, loss = 0.00483582\n",
      "Iteration 12062, loss = 0.00483337\n",
      "Iteration 12063, loss = 0.00483092\n",
      "Iteration 12064, loss = 0.00482848\n",
      "Iteration 12065, loss = 0.00482604\n",
      "Iteration 12066, loss = 0.00482359\n",
      "Iteration 12067, loss = 0.00482115\n",
      "Iteration 12068, loss = 0.00481871\n",
      "Iteration 12069, loss = 0.00481628\n",
      "Iteration 12070, loss = 0.00481384\n",
      "Iteration 12071, loss = 0.00481140\n",
      "Iteration 12072, loss = 0.00480897\n",
      "Iteration 12073, loss = 0.00480654\n",
      "Iteration 12074, loss = 0.00480411\n",
      "Iteration 12075, loss = 0.00480168\n",
      "Iteration 12076, loss = 0.00479925\n",
      "Iteration 12077, loss = 0.00479682\n",
      "Iteration 12078, loss = 0.00479440\n",
      "Iteration 12079, loss = 0.00479197\n",
      "Iteration 12080, loss = 0.00478955\n",
      "Iteration 12081, loss = 0.00478713\n",
      "Iteration 12082, loss = 0.00478471\n",
      "Iteration 12083, loss = 0.00478229\n",
      "Iteration 12084, loss = 0.00477987\n",
      "Iteration 12085, loss = 0.00477745\n",
      "Iteration 12086, loss = 0.00477504\n",
      "Iteration 12087, loss = 0.00477262\n",
      "Iteration 12088, loss = 0.00477021\n",
      "Iteration 12089, loss = 0.00476780\n",
      "Iteration 12090, loss = 0.00476539\n",
      "Iteration 12091, loss = 0.00476298\n",
      "Iteration 12092, loss = 0.00476058\n",
      "Iteration 12093, loss = 0.00475817\n",
      "Iteration 12094, loss = 0.00475577\n",
      "Iteration 12095, loss = 0.00475336\n",
      "Iteration 12096, loss = 0.00475096\n",
      "Iteration 12097, loss = 0.00474856\n",
      "Iteration 12098, loss = 0.00474616\n",
      "Iteration 12099, loss = 0.00474376\n",
      "Iteration 12100, loss = 0.00474137\n",
      "Iteration 12101, loss = 0.00473897\n",
      "Iteration 12102, loss = 0.00473658\n",
      "Iteration 12103, loss = 0.00473419\n",
      "Iteration 12104, loss = 0.00473180\n",
      "Iteration 12105, loss = 0.00472941\n",
      "Iteration 12106, loss = 0.00472702\n",
      "Iteration 12107, loss = 0.00472463\n",
      "Iteration 12108, loss = 0.00472225\n",
      "Iteration 12109, loss = 0.00471986\n",
      "Iteration 12110, loss = 0.00471748\n",
      "Iteration 12111, loss = 0.00471510\n",
      "Iteration 12112, loss = 0.00471272\n",
      "Iteration 12113, loss = 0.00471034\n",
      "Iteration 12114, loss = 0.00470796\n",
      "Iteration 12115, loss = 0.00470558\n",
      "Iteration 12116, loss = 0.00470321\n",
      "Iteration 12117, loss = 0.00470083\n",
      "Iteration 12118, loss = 0.00469846\n",
      "Iteration 12119, loss = 0.00469609\n",
      "Iteration 12120, loss = 0.00469372\n",
      "Iteration 12121, loss = 0.00469135\n",
      "Iteration 12122, loss = 0.00468899\n",
      "Iteration 12123, loss = 0.00468662\n",
      "Iteration 12124, loss = 0.00468426\n",
      "Iteration 12125, loss = 0.00468189\n",
      "Iteration 12126, loss = 0.00467953\n",
      "Iteration 12127, loss = 0.00467717\n",
      "Iteration 12128, loss = 0.00467481\n",
      "Iteration 12129, loss = 0.00467245\n",
      "Iteration 12130, loss = 0.00467010\n",
      "Iteration 12131, loss = 0.00466774\n",
      "Iteration 12132, loss = 0.00466539\n",
      "Iteration 12133, loss = 0.00466303\n",
      "Iteration 12134, loss = 0.00466068\n",
      "Iteration 12135, loss = 0.00465833\n",
      "Iteration 12136, loss = 0.00465598\n",
      "Iteration 12137, loss = 0.00465364\n",
      "Iteration 12138, loss = 0.00465129\n",
      "Iteration 12139, loss = 0.00464895\n",
      "Iteration 12140, loss = 0.00464660\n",
      "Iteration 12141, loss = 0.00464426\n",
      "Iteration 12142, loss = 0.00464192\n",
      "Iteration 12143, loss = 0.00463958\n",
      "Iteration 12144, loss = 0.00463724\n",
      "Iteration 12145, loss = 0.00463491\n",
      "Iteration 12146, loss = 0.00463257\n",
      "Iteration 12147, loss = 0.00463024\n",
      "Iteration 12148, loss = 0.00462790\n",
      "Iteration 12149, loss = 0.00462557\n",
      "Iteration 12150, loss = 0.00462324\n",
      "Iteration 12151, loss = 0.00462091\n",
      "Iteration 12152, loss = 0.00461858\n",
      "Iteration 12153, loss = 0.00461626\n",
      "Iteration 12154, loss = 0.00461393\n",
      "Iteration 12155, loss = 0.00461161\n",
      "Iteration 12156, loss = 0.00460929\n",
      "Iteration 12157, loss = 0.00460696\n",
      "Iteration 12158, loss = 0.00460464\n",
      "Iteration 12159, loss = 0.00460233\n",
      "Iteration 12160, loss = 0.00460001\n",
      "Iteration 12161, loss = 0.00459769\n",
      "Iteration 12162, loss = 0.00459538\n",
      "Iteration 12163, loss = 0.00459306\n",
      "Iteration 12164, loss = 0.00459075\n",
      "Iteration 12165, loss = 0.00458844\n",
      "Iteration 12166, loss = 0.00458613\n",
      "Iteration 12167, loss = 0.00458382\n",
      "Iteration 12168, loss = 0.00458152\n",
      "Iteration 12169, loss = 0.00457921\n",
      "Iteration 12170, loss = 0.00457691\n",
      "Iteration 12171, loss = 0.00457460\n",
      "Iteration 12172, loss = 0.00457230\n",
      "Iteration 12173, loss = 0.00457000\n",
      "Iteration 12174, loss = 0.00456770\n",
      "Iteration 12175, loss = 0.00456540\n",
      "Iteration 12176, loss = 0.00456311\n",
      "Iteration 12177, loss = 0.00456081\n",
      "Iteration 12178, loss = 0.00455852\n",
      "Iteration 12179, loss = 0.00455622\n",
      "Iteration 12180, loss = 0.00455393\n",
      "Iteration 12181, loss = 0.00455164\n",
      "Iteration 12182, loss = 0.00454935\n",
      "Iteration 12183, loss = 0.00454706\n",
      "Iteration 12184, loss = 0.00454478\n",
      "Iteration 12185, loss = 0.00454249\n",
      "Iteration 12186, loss = 0.00454021\n",
      "Iteration 12187, loss = 0.00453793\n",
      "Iteration 12188, loss = 0.00453565\n",
      "Iteration 12189, loss = 0.00453336\n",
      "Iteration 12190, loss = 0.00453109\n",
      "Iteration 12191, loss = 0.00452881\n",
      "Iteration 12192, loss = 0.00452653\n",
      "Iteration 12193, loss = 0.00452426\n",
      "Iteration 12194, loss = 0.00452198\n",
      "Iteration 12195, loss = 0.00451971\n",
      "Iteration 12196, loss = 0.00451744\n",
      "Iteration 12197, loss = 0.00451517\n",
      "Iteration 12198, loss = 0.00451290\n",
      "Iteration 12199, loss = 0.00451063\n",
      "Iteration 12200, loss = 0.00450837\n",
      "Iteration 12201, loss = 0.00450610\n",
      "Iteration 12202, loss = 0.00450384\n",
      "Iteration 12203, loss = 0.00450158\n",
      "Iteration 12204, loss = 0.00449932\n",
      "Iteration 12205, loss = 0.00449706\n",
      "Iteration 12206, loss = 0.00449480\n",
      "Iteration 12207, loss = 0.00449254\n",
      "Iteration 12208, loss = 0.00449029\n",
      "Iteration 12209, loss = 0.00448803\n",
      "Iteration 12210, loss = 0.00448578\n",
      "Iteration 12211, loss = 0.00448353\n",
      "Iteration 12212, loss = 0.00448128\n",
      "Iteration 12213, loss = 0.00447903\n",
      "Iteration 12214, loss = 0.00447678\n",
      "Iteration 12215, loss = 0.00447453\n",
      "Iteration 12216, loss = 0.00447228\n",
      "Iteration 12217, loss = 0.00447004\n",
      "Iteration 12218, loss = 0.00446780\n",
      "Iteration 12219, loss = 0.00446555\n",
      "Iteration 12220, loss = 0.00446331\n",
      "Iteration 12221, loss = 0.00446107\n",
      "Iteration 12222, loss = 0.00445884\n",
      "Iteration 12223, loss = 0.00445660\n",
      "Iteration 12224, loss = 0.00445436\n",
      "Iteration 12225, loss = 0.00445213\n",
      "Iteration 12226, loss = 0.00444990\n",
      "Iteration 12227, loss = 0.00444766\n",
      "Iteration 12228, loss = 0.00444543\n",
      "Iteration 12229, loss = 0.00444320\n",
      "Iteration 12230, loss = 0.00444098\n",
      "Iteration 12231, loss = 0.00443875\n",
      "Iteration 12232, loss = 0.00443652\n",
      "Iteration 12233, loss = 0.00443430\n",
      "Iteration 12234, loss = 0.00443208\n",
      "Iteration 12235, loss = 0.00442985\n",
      "Iteration 12236, loss = 0.00442763\n",
      "Iteration 12237, loss = 0.00442541\n",
      "Iteration 12238, loss = 0.00442320\n",
      "Iteration 12239, loss = 0.00442098\n",
      "Iteration 12240, loss = 0.00441876\n",
      "Iteration 12241, loss = 0.00441655\n",
      "Iteration 12242, loss = 0.00441434\n",
      "Iteration 12243, loss = 0.00441212\n",
      "Iteration 12244, loss = 0.00440991\n",
      "Iteration 12245, loss = 0.00440770\n",
      "Iteration 12246, loss = 0.00440550\n",
      "Iteration 12247, loss = 0.00440329\n",
      "Iteration 12248, loss = 0.00440108\n",
      "Iteration 12249, loss = 0.00439888\n",
      "Iteration 12250, loss = 0.00439668\n",
      "Iteration 12251, loss = 0.00439447\n",
      "Iteration 12252, loss = 0.00439227\n",
      "Iteration 12253, loss = 0.00439007\n",
      "Iteration 12254, loss = 0.00438788\n",
      "Iteration 12255, loss = 0.00438568\n",
      "Iteration 12256, loss = 0.00438348\n",
      "Iteration 12257, loss = 0.00438129\n",
      "Iteration 12258, loss = 0.00437909\n",
      "Iteration 12259, loss = 0.00437690\n",
      "Iteration 12260, loss = 0.00437471\n",
      "Iteration 12261, loss = 0.00437252\n",
      "Iteration 12262, loss = 0.00437033\n",
      "Iteration 12263, loss = 0.00436815\n",
      "Iteration 12264, loss = 0.00436596\n",
      "Iteration 12265, loss = 0.00436378\n",
      "Iteration 12266, loss = 0.00436159\n",
      "Iteration 12267, loss = 0.00435941\n",
      "Iteration 12268, loss = 0.00435723\n",
      "Iteration 12269, loss = 0.00435505\n",
      "Iteration 12270, loss = 0.00435287\n",
      "Iteration 12271, loss = 0.00435069\n",
      "Iteration 12272, loss = 0.00434852\n",
      "Iteration 12273, loss = 0.00434634\n",
      "Iteration 12274, loss = 0.00434417\n",
      "Iteration 12275, loss = 0.00434200\n",
      "Iteration 12276, loss = 0.00433983\n",
      "Iteration 12277, loss = 0.00433766\n",
      "Iteration 12278, loss = 0.00433549\n",
      "Iteration 12279, loss = 0.00433332\n",
      "Iteration 12280, loss = 0.00433115\n",
      "Iteration 12281, loss = 0.00432899\n",
      "Iteration 12282, loss = 0.00432683\n",
      "Iteration 12283, loss = 0.00432466\n",
      "Iteration 12284, loss = 0.00432250\n",
      "Iteration 12285, loss = 0.00432034\n",
      "Iteration 12286, loss = 0.00431818\n",
      "Iteration 12287, loss = 0.00431603\n",
      "Iteration 12288, loss = 0.00431387\n",
      "Iteration 12289, loss = 0.00431171\n",
      "Iteration 12290, loss = 0.00430956\n",
      "Iteration 12291, loss = 0.00430741\n",
      "Iteration 12292, loss = 0.00430526\n",
      "Iteration 12293, loss = 0.00430311\n",
      "Iteration 12294, loss = 0.00430096\n",
      "Iteration 12295, loss = 0.00429881\n",
      "Iteration 12296, loss = 0.00429666\n",
      "Iteration 12297, loss = 0.00429452\n",
      "Iteration 12298, loss = 0.00429237\n",
      "Iteration 12299, loss = 0.00429023\n",
      "Iteration 12300, loss = 0.00428809\n",
      "Iteration 12301, loss = 0.00428595\n",
      "Iteration 12302, loss = 0.00428381\n",
      "Iteration 12303, loss = 0.00428167\n",
      "Iteration 12304, loss = 0.00427953\n",
      "Iteration 12305, loss = 0.00427740\n",
      "Iteration 12306, loss = 0.00427526\n",
      "Iteration 12307, loss = 0.00427313\n",
      "Iteration 12308, loss = 0.00427100\n",
      "Iteration 12309, loss = 0.00426887\n",
      "Iteration 12310, loss = 0.00426674\n",
      "Iteration 12311, loss = 0.00426461\n",
      "Iteration 12312, loss = 0.00426248\n",
      "Iteration 12313, loss = 0.00426035\n",
      "Iteration 12314, loss = 0.00425823\n",
      "Iteration 12315, loss = 0.00425611\n",
      "Iteration 12316, loss = 0.00425398\n",
      "Iteration 12317, loss = 0.00425186\n",
      "Iteration 12318, loss = 0.00424974\n",
      "Iteration 12319, loss = 0.00424762\n",
      "Iteration 12320, loss = 0.00424550\n",
      "Iteration 12321, loss = 0.00424339\n",
      "Iteration 12322, loss = 0.00424127\n",
      "Iteration 12323, loss = 0.00423916\n",
      "Iteration 12324, loss = 0.00423705\n",
      "Iteration 12325, loss = 0.00423493\n",
      "Iteration 12326, loss = 0.00423282\n",
      "Iteration 12327, loss = 0.00423071\n",
      "Iteration 12328, loss = 0.00422861\n",
      "Iteration 12329, loss = 0.00422650\n",
      "Iteration 12330, loss = 0.00422439\n",
      "Iteration 12331, loss = 0.00422229\n",
      "Iteration 12332, loss = 0.00422019\n",
      "Iteration 12333, loss = 0.00421808\n",
      "Iteration 12334, loss = 0.00421598\n",
      "Iteration 12335, loss = 0.00421388\n",
      "Iteration 12336, loss = 0.00421179\n",
      "Iteration 12337, loss = 0.00420969\n",
      "Iteration 12338, loss = 0.00420759\n",
      "Iteration 12339, loss = 0.00420550\n",
      "Iteration 12340, loss = 0.00420340\n",
      "Iteration 12341, loss = 0.00420131\n",
      "Iteration 12342, loss = 0.00419922\n",
      "Iteration 12343, loss = 0.00419713\n",
      "Iteration 12344, loss = 0.00419504\n",
      "Iteration 12345, loss = 0.00419295\n",
      "Iteration 12346, loss = 0.00419087\n",
      "Iteration 12347, loss = 0.00418878\n",
      "Iteration 12348, loss = 0.00418670\n",
      "Iteration 12349, loss = 0.00418461\n",
      "Iteration 12350, loss = 0.00418253\n",
      "Iteration 12351, loss = 0.00418045\n",
      "Iteration 12352, loss = 0.00417837\n",
      "Iteration 12353, loss = 0.00417629\n",
      "Iteration 12354, loss = 0.00417422\n",
      "Iteration 12355, loss = 0.00417214\n",
      "Iteration 12356, loss = 0.00417006\n",
      "Iteration 12357, loss = 0.00416799\n",
      "Iteration 12358, loss = 0.00416592\n",
      "Iteration 12359, loss = 0.00416385\n",
      "Iteration 12360, loss = 0.00416178\n",
      "Iteration 12361, loss = 0.00415971\n",
      "Iteration 12362, loss = 0.00415764\n",
      "Iteration 12363, loss = 0.00415557\n",
      "Iteration 12364, loss = 0.00415351\n",
      "Iteration 12365, loss = 0.00415144\n",
      "Iteration 12366, loss = 0.00414938\n",
      "Iteration 12367, loss = 0.00414732\n",
      "Iteration 12368, loss = 0.00414526\n",
      "Iteration 12369, loss = 0.00414320\n",
      "Iteration 12370, loss = 0.00414114\n",
      "Iteration 12371, loss = 0.00413908\n",
      "Iteration 12372, loss = 0.00413703\n",
      "Iteration 12373, loss = 0.00413497\n",
      "Iteration 12374, loss = 0.00413292\n",
      "Iteration 12375, loss = 0.00413087\n",
      "Iteration 12376, loss = 0.00412882\n",
      "Iteration 12377, loss = 0.00412677\n",
      "Iteration 12378, loss = 0.00412472\n",
      "Iteration 12379, loss = 0.00412267\n",
      "Iteration 12380, loss = 0.00412062\n",
      "Iteration 12381, loss = 0.00411858\n",
      "Iteration 12382, loss = 0.00411653\n",
      "Iteration 12383, loss = 0.00411449\n",
      "Iteration 12384, loss = 0.00411245\n",
      "Iteration 12385, loss = 0.00411041\n",
      "Iteration 12386, loss = 0.00410837\n",
      "Iteration 12387, loss = 0.00410633\n",
      "Iteration 12388, loss = 0.00410429\n",
      "Iteration 12389, loss = 0.00410226\n",
      "Iteration 12390, loss = 0.00410022\n",
      "Iteration 12391, loss = 0.00409819\n",
      "Iteration 12392, loss = 0.00409616\n",
      "Iteration 12393, loss = 0.00409412\n",
      "Iteration 12394, loss = 0.00409209\n",
      "Iteration 12395, loss = 0.00409006\n",
      "Iteration 12396, loss = 0.00408804\n",
      "Iteration 12397, loss = 0.00408601\n",
      "Iteration 12398, loss = 0.00408398\n",
      "Iteration 12399, loss = 0.00408196\n",
      "Iteration 12400, loss = 0.00407994\n",
      "Iteration 12401, loss = 0.00407791\n",
      "Iteration 12402, loss = 0.00407589\n",
      "Iteration 12403, loss = 0.00407387\n",
      "Iteration 12404, loss = 0.00407186\n",
      "Iteration 12405, loss = 0.00406984\n",
      "Iteration 12406, loss = 0.00406782\n",
      "Iteration 12407, loss = 0.00406581\n",
      "Iteration 12408, loss = 0.00406379\n",
      "Iteration 12409, loss = 0.00406178\n",
      "Iteration 12410, loss = 0.00405977\n",
      "Iteration 12411, loss = 0.00405776\n",
      "Iteration 12412, loss = 0.00405575\n",
      "Iteration 12413, loss = 0.00405374\n",
      "Iteration 12414, loss = 0.00405173\n",
      "Iteration 12415, loss = 0.00404973\n",
      "Iteration 12416, loss = 0.00404772\n",
      "Iteration 12417, loss = 0.00404572\n",
      "Iteration 12418, loss = 0.00404372\n",
      "Iteration 12419, loss = 0.00404171\n",
      "Iteration 12420, loss = 0.00403971\n",
      "Iteration 12421, loss = 0.00403771\n",
      "Iteration 12422, loss = 0.00403572\n",
      "Iteration 12423, loss = 0.00403372\n",
      "Iteration 12424, loss = 0.00403172\n",
      "Iteration 12425, loss = 0.00402973\n",
      "Iteration 12426, loss = 0.00402774\n",
      "Iteration 12427, loss = 0.00402574\n",
      "Iteration 12428, loss = 0.00402375\n",
      "Iteration 12429, loss = 0.00402176\n",
      "Iteration 12430, loss = 0.00401977\n",
      "Iteration 12431, loss = 0.00401779\n",
      "Iteration 12432, loss = 0.00401580\n",
      "Iteration 12433, loss = 0.00401381\n",
      "Iteration 12434, loss = 0.00401183\n",
      "Iteration 12435, loss = 0.00400985\n",
      "Iteration 12436, loss = 0.00400787\n",
      "Iteration 12437, loss = 0.00400588\n",
      "Iteration 12438, loss = 0.00400390\n",
      "Iteration 12439, loss = 0.00400193\n",
      "Iteration 12440, loss = 0.00399995\n",
      "Iteration 12441, loss = 0.00399797\n",
      "Iteration 12442, loss = 0.00399600\n",
      "Iteration 12443, loss = 0.00399402\n",
      "Iteration 12444, loss = 0.00399205\n",
      "Iteration 12445, loss = 0.00399008\n",
      "Iteration 12446, loss = 0.00398811\n",
      "Iteration 12447, loss = 0.00398614\n",
      "Iteration 12448, loss = 0.00398417\n",
      "Iteration 12449, loss = 0.00398220\n",
      "Iteration 12450, loss = 0.00398024\n",
      "Iteration 12451, loss = 0.00397827\n",
      "Iteration 12452, loss = 0.00397631\n",
      "Iteration 12453, loss = 0.00397435\n",
      "Iteration 12454, loss = 0.00397238\n",
      "Iteration 12455, loss = 0.00397042\n",
      "Iteration 12456, loss = 0.00396846\n",
      "Iteration 12457, loss = 0.00396651\n",
      "Iteration 12458, loss = 0.00396455\n",
      "Iteration 12459, loss = 0.00396259\n",
      "Iteration 12460, loss = 0.00396064\n",
      "Iteration 12461, loss = 0.00395869\n",
      "Iteration 12462, loss = 0.00395673\n",
      "Iteration 12463, loss = 0.00395478\n",
      "Iteration 12464, loss = 0.00395283\n",
      "Iteration 12465, loss = 0.00395088\n",
      "Iteration 12466, loss = 0.00394893\n",
      "Iteration 12467, loss = 0.00394699\n",
      "Iteration 12468, loss = 0.00394504\n",
      "Iteration 12469, loss = 0.00394310\n",
      "Iteration 12470, loss = 0.00394115\n",
      "Iteration 12471, loss = 0.00393921\n",
      "Iteration 12472, loss = 0.00393727\n",
      "Iteration 12473, loss = 0.00393533\n",
      "Iteration 12474, loss = 0.00393339\n",
      "Iteration 12475, loss = 0.00393145\n",
      "Iteration 12476, loss = 0.00392952\n",
      "Iteration 12477, loss = 0.00392758\n",
      "Iteration 12478, loss = 0.00392565\n",
      "Iteration 12479, loss = 0.00392371\n",
      "Iteration 12480, loss = 0.00392178\n",
      "Iteration 12481, loss = 0.00391985\n",
      "Iteration 12482, loss = 0.00391792\n",
      "Iteration 12483, loss = 0.00391599\n",
      "Iteration 12484, loss = 0.00391406\n",
      "Iteration 12485, loss = 0.00391214\n",
      "Iteration 12486, loss = 0.00391021\n",
      "Iteration 12487, loss = 0.00390828\n",
      "Iteration 12488, loss = 0.00390636\n",
      "Iteration 12489, loss = 0.00390444\n",
      "Iteration 12490, loss = 0.00390252\n",
      "Iteration 12491, loss = 0.00390060\n",
      "Iteration 12492, loss = 0.00389868\n",
      "Iteration 12493, loss = 0.00389676\n",
      "Iteration 12494, loss = 0.00389484\n",
      "Iteration 12495, loss = 0.00389293\n",
      "Iteration 12496, loss = 0.00389101\n",
      "Iteration 12497, loss = 0.00388910\n",
      "Iteration 12498, loss = 0.00388719\n",
      "Iteration 12499, loss = 0.00388528\n",
      "Iteration 12500, loss = 0.00388337\n",
      "Iteration 12501, loss = 0.00388146\n",
      "Iteration 12502, loss = 0.00387955\n",
      "Iteration 12503, loss = 0.00387764\n",
      "Iteration 12504, loss = 0.00387574\n",
      "Iteration 12505, loss = 0.00387383\n",
      "Iteration 12506, loss = 0.00387193\n",
      "Iteration 12507, loss = 0.00387003\n",
      "Iteration 12508, loss = 0.00386813\n",
      "Iteration 12509, loss = 0.00386623\n",
      "Iteration 12510, loss = 0.00386433\n",
      "Iteration 12511, loss = 0.00386243\n",
      "Iteration 12512, loss = 0.00386053\n",
      "Iteration 12513, loss = 0.00385864\n",
      "Iteration 12514, loss = 0.00385674\n",
      "Iteration 12515, loss = 0.00385485\n",
      "Iteration 12516, loss = 0.00385296\n",
      "Iteration 12517, loss = 0.00385106\n",
      "Iteration 12518, loss = 0.00384917\n",
      "Iteration 12519, loss = 0.00384728\n",
      "Iteration 12520, loss = 0.00384540\n",
      "Iteration 12521, loss = 0.00384351\n",
      "Iteration 12522, loss = 0.00384162\n",
      "Iteration 12523, loss = 0.00383974\n",
      "Iteration 12524, loss = 0.00383786\n",
      "Iteration 12525, loss = 0.00383597\n",
      "Iteration 12526, loss = 0.00383409\n",
      "Iteration 12527, loss = 0.00383221\n",
      "Iteration 12528, loss = 0.00383033\n",
      "Iteration 12529, loss = 0.00382845\n",
      "Iteration 12530, loss = 0.00382658\n",
      "Iteration 12531, loss = 0.00382470\n",
      "Iteration 12532, loss = 0.00382283\n",
      "Iteration 12533, loss = 0.00382095\n",
      "Iteration 12534, loss = 0.00381908\n",
      "Iteration 12535, loss = 0.00381721\n",
      "Iteration 12536, loss = 0.00381534\n",
      "Iteration 12537, loss = 0.00381347\n",
      "Iteration 12538, loss = 0.00381160\n",
      "Iteration 12539, loss = 0.00380973\n",
      "Iteration 12540, loss = 0.00380786\n",
      "Iteration 12541, loss = 0.00380600\n",
      "Iteration 12542, loss = 0.00380414\n",
      "Iteration 12543, loss = 0.00380227\n",
      "Iteration 12544, loss = 0.00380041\n",
      "Iteration 12545, loss = 0.00379855\n",
      "Iteration 12546, loss = 0.00379669\n",
      "Iteration 12547, loss = 0.00379483\n",
      "Iteration 12548, loss = 0.00379297\n",
      "Iteration 12549, loss = 0.00379112\n",
      "Iteration 12550, loss = 0.00378926\n",
      "Iteration 12551, loss = 0.00378741\n",
      "Iteration 12552, loss = 0.00378555\n",
      "Iteration 12553, loss = 0.00378370\n",
      "Iteration 12554, loss = 0.00378185\n",
      "Iteration 12555, loss = 0.00378000\n",
      "Iteration 12556, loss = 0.00377815\n",
      "Iteration 12557, loss = 0.00377630\n",
      "Iteration 12558, loss = 0.00377446\n",
      "Iteration 12559, loss = 0.00377261\n",
      "Iteration 12560, loss = 0.00377077\n",
      "Iteration 12561, loss = 0.00376892\n",
      "Iteration 12562, loss = 0.00376708\n",
      "Iteration 12563, loss = 0.00376524\n",
      "Iteration 12564, loss = 0.00376340\n",
      "Iteration 12565, loss = 0.00376156\n",
      "Iteration 12566, loss = 0.00375972\n",
      "Iteration 12567, loss = 0.00375788\n",
      "Iteration 12568, loss = 0.00375605\n",
      "Iteration 12569, loss = 0.00375421\n",
      "Iteration 12570, loss = 0.00375238\n",
      "Iteration 12571, loss = 0.00375055\n",
      "Iteration 12572, loss = 0.00374871\n",
      "Iteration 12573, loss = 0.00374688\n",
      "Iteration 12574, loss = 0.00374505\n",
      "Iteration 12575, loss = 0.00374323\n",
      "Iteration 12576, loss = 0.00374140\n",
      "Iteration 12577, loss = 0.00373957\n",
      "Iteration 12578, loss = 0.00373775\n",
      "Iteration 12579, loss = 0.00373592\n",
      "Iteration 12580, loss = 0.00373410\n",
      "Iteration 12581, loss = 0.00373228\n",
      "Iteration 12582, loss = 0.00373046\n",
      "Iteration 12583, loss = 0.00372864\n",
      "Iteration 12584, loss = 0.00372682\n",
      "Iteration 12585, loss = 0.00372500\n",
      "Iteration 12586, loss = 0.00372318\n",
      "Iteration 12587, loss = 0.00372137\n",
      "Iteration 12588, loss = 0.00371955\n",
      "Iteration 12589, loss = 0.00371774\n",
      "Iteration 12590, loss = 0.00371593\n",
      "Iteration 12591, loss = 0.00371411\n",
      "Iteration 12592, loss = 0.00371230\n",
      "Iteration 12593, loss = 0.00371049\n",
      "Iteration 12594, loss = 0.00370869\n",
      "Iteration 12595, loss = 0.00370688\n",
      "Iteration 12596, loss = 0.00370507\n",
      "Iteration 12597, loss = 0.00370327\n",
      "Iteration 12598, loss = 0.00370146\n",
      "Iteration 12599, loss = 0.00369966\n",
      "Iteration 12600, loss = 0.00369786\n",
      "Iteration 12601, loss = 0.00369606\n",
      "Iteration 12602, loss = 0.00369426\n",
      "Iteration 12603, loss = 0.00369246\n",
      "Iteration 12604, loss = 0.00369066\n",
      "Iteration 12605, loss = 0.00368886\n",
      "Iteration 12606, loss = 0.00368707\n",
      "Iteration 12607, loss = 0.00368527\n",
      "Iteration 12608, loss = 0.00368348\n",
      "Iteration 12609, loss = 0.00368169\n",
      "Iteration 12610, loss = 0.00367990\n",
      "Iteration 12611, loss = 0.00367810\n",
      "Iteration 12612, loss = 0.00367632\n",
      "Iteration 12613, loss = 0.00367453\n",
      "Iteration 12614, loss = 0.00367274\n",
      "Iteration 12615, loss = 0.00367095\n",
      "Iteration 12616, loss = 0.00366917\n",
      "Iteration 12617, loss = 0.00366738\n",
      "Iteration 12618, loss = 0.00366560\n",
      "Iteration 12619, loss = 0.00366382\n",
      "Iteration 12620, loss = 0.00366204\n",
      "Iteration 12621, loss = 0.00366026\n",
      "Iteration 12622, loss = 0.00365848\n",
      "Iteration 12623, loss = 0.00365670\n",
      "Iteration 12624, loss = 0.00365492\n",
      "Iteration 12625, loss = 0.00365315\n",
      "Iteration 12626, loss = 0.00365137\n",
      "Iteration 12627, loss = 0.00364960\n",
      "Iteration 12628, loss = 0.00364783\n",
      "Iteration 12629, loss = 0.00364606\n",
      "Iteration 12630, loss = 0.00364429\n",
      "Iteration 12631, loss = 0.00364252\n",
      "Iteration 12632, loss = 0.00364075\n",
      "Iteration 12633, loss = 0.00363898\n",
      "Iteration 12634, loss = 0.00363721\n",
      "Iteration 12635, loss = 0.00363545\n",
      "Iteration 12636, loss = 0.00363368\n",
      "Iteration 12637, loss = 0.00363192\n",
      "Iteration 12638, loss = 0.00363016\n",
      "Iteration 12639, loss = 0.00362840\n",
      "Iteration 12640, loss = 0.00362664\n",
      "Iteration 12641, loss = 0.00362488\n",
      "Iteration 12642, loss = 0.00362312\n",
      "Iteration 12643, loss = 0.00362136\n",
      "Iteration 12644, loss = 0.00361961\n",
      "Iteration 12645, loss = 0.00361785\n",
      "Iteration 12646, loss = 0.00361610\n",
      "Iteration 12647, loss = 0.00361434\n",
      "Iteration 12648, loss = 0.00361259\n",
      "Iteration 12649, loss = 0.00361084\n",
      "Iteration 12650, loss = 0.00360909\n",
      "Iteration 12651, loss = 0.00360734\n",
      "Iteration 12652, loss = 0.00360560\n",
      "Iteration 12653, loss = 0.00360385\n",
      "Iteration 12654, loss = 0.00360210\n",
      "Iteration 12655, loss = 0.00360036\n",
      "Iteration 12656, loss = 0.00359861\n",
      "Iteration 12657, loss = 0.00359687\n",
      "Iteration 12658, loss = 0.00359513\n",
      "Iteration 12659, loss = 0.00359339\n",
      "Iteration 12660, loss = 0.00359165\n",
      "Iteration 12661, loss = 0.00358991\n",
      "Iteration 12662, loss = 0.00358817\n",
      "Iteration 12663, loss = 0.00358644\n",
      "Iteration 12664, loss = 0.00358470\n",
      "Iteration 12665, loss = 0.00358297\n",
      "Iteration 12666, loss = 0.00358123\n",
      "Iteration 12667, loss = 0.00357950\n",
      "Iteration 12668, loss = 0.00357777\n",
      "Iteration 12669, loss = 0.00357604\n",
      "Iteration 12670, loss = 0.00357431\n",
      "Iteration 12671, loss = 0.00357258\n",
      "Iteration 12672, loss = 0.00357086\n",
      "Iteration 12673, loss = 0.00356913\n",
      "Iteration 12674, loss = 0.00356740\n",
      "Iteration 12675, loss = 0.00356568\n",
      "Iteration 12676, loss = 0.00356396\n",
      "Iteration 12677, loss = 0.00356223\n",
      "Iteration 12678, loss = 0.00356051\n",
      "Iteration 12679, loss = 0.00355879\n",
      "Iteration 12680, loss = 0.00355707\n",
      "Iteration 12681, loss = 0.00355536\n",
      "Iteration 12682, loss = 0.00355364\n",
      "Iteration 12683, loss = 0.00355192\n",
      "Iteration 12684, loss = 0.00355021\n",
      "Iteration 12685, loss = 0.00354849\n",
      "Iteration 12686, loss = 0.00354678\n",
      "Iteration 12687, loss = 0.00354507\n",
      "Iteration 12688, loss = 0.00354336\n",
      "Iteration 12689, loss = 0.00354165\n",
      "Iteration 12690, loss = 0.00353994\n",
      "Iteration 12691, loss = 0.00353823\n",
      "Iteration 12692, loss = 0.00353652\n",
      "Iteration 12693, loss = 0.00353482\n",
      "Iteration 12694, loss = 0.00353311\n",
      "Iteration 12695, loss = 0.00353141\n",
      "Iteration 12696, loss = 0.00352971\n",
      "Iteration 12697, loss = 0.00352800\n",
      "Iteration 12698, loss = 0.00352630\n",
      "Iteration 12699, loss = 0.00352460\n",
      "Iteration 12700, loss = 0.00352290\n",
      "Iteration 12701, loss = 0.00352121\n",
      "Iteration 12702, loss = 0.00351951\n",
      "Iteration 12703, loss = 0.00351781\n",
      "Iteration 12704, loss = 0.00351612\n",
      "Iteration 12705, loss = 0.00351442\n",
      "Iteration 12706, loss = 0.00351273\n",
      "Iteration 12707, loss = 0.00351104\n",
      "Iteration 12708, loss = 0.00350935\n",
      "Iteration 12709, loss = 0.00350766\n",
      "Iteration 12710, loss = 0.00350597\n",
      "Iteration 12711, loss = 0.00350428\n",
      "Iteration 12712, loss = 0.00350260\n",
      "Iteration 12713, loss = 0.00350091\n",
      "Iteration 12714, loss = 0.00349922\n",
      "Iteration 12715, loss = 0.00349754\n",
      "Iteration 12716, loss = 0.00349586\n",
      "Iteration 12717, loss = 0.00349418\n",
      "Iteration 12718, loss = 0.00349249\n",
      "Iteration 12719, loss = 0.00349082\n",
      "Iteration 12720, loss = 0.00348914\n",
      "Iteration 12721, loss = 0.00348746\n",
      "Iteration 12722, loss = 0.00348578\n",
      "Iteration 12723, loss = 0.00348411\n",
      "Iteration 12724, loss = 0.00348243\n",
      "Iteration 12725, loss = 0.00348076\n",
      "Iteration 12726, loss = 0.00347908\n",
      "Iteration 12727, loss = 0.00347741\n",
      "Iteration 12728, loss = 0.00347574\n",
      "Iteration 12729, loss = 0.00347407\n",
      "Iteration 12730, loss = 0.00347240\n",
      "Iteration 12731, loss = 0.00347073\n",
      "Iteration 12732, loss = 0.00346907\n",
      "Iteration 12733, loss = 0.00346740\n",
      "Iteration 12734, loss = 0.00346574\n",
      "Iteration 12735, loss = 0.00346407\n",
      "Iteration 12736, loss = 0.00346241\n",
      "Iteration 12737, loss = 0.00346075\n",
      "Iteration 12738, loss = 0.00345909\n",
      "Iteration 12739, loss = 0.00345743\n",
      "Iteration 12740, loss = 0.00345577\n",
      "Iteration 12741, loss = 0.00345411\n",
      "Iteration 12742, loss = 0.00345245\n",
      "Iteration 12743, loss = 0.00345080\n",
      "Iteration 12744, loss = 0.00344914\n",
      "Iteration 12745, loss = 0.00344749\n",
      "Iteration 12746, loss = 0.00344583\n",
      "Iteration 12747, loss = 0.00344418\n",
      "Iteration 12748, loss = 0.00344253\n",
      "Iteration 12749, loss = 0.00344088\n",
      "Iteration 12750, loss = 0.00343923\n",
      "Iteration 12751, loss = 0.00343758\n",
      "Iteration 12752, loss = 0.00343593\n",
      "Iteration 12753, loss = 0.00343429\n",
      "Iteration 12754, loss = 0.00343264\n",
      "Iteration 12755, loss = 0.00343100\n",
      "Iteration 12756, loss = 0.00342936\n",
      "Iteration 12757, loss = 0.00342771\n",
      "Iteration 12758, loss = 0.00342607\n",
      "Iteration 12759, loss = 0.00342443\n",
      "Iteration 12760, loss = 0.00342279\n",
      "Iteration 12761, loss = 0.00342115\n",
      "Iteration 12762, loss = 0.00341952\n",
      "Iteration 12763, loss = 0.00341788\n",
      "Iteration 12764, loss = 0.00341624\n",
      "Iteration 12765, loss = 0.00341461\n",
      "Iteration 12766, loss = 0.00341298\n",
      "Iteration 12767, loss = 0.00341134\n",
      "Iteration 12768, loss = 0.00340971\n",
      "Iteration 12769, loss = 0.00340808\n",
      "Iteration 12770, loss = 0.00340645\n",
      "Iteration 12771, loss = 0.00340482\n",
      "Iteration 12772, loss = 0.00340319\n",
      "Iteration 12773, loss = 0.00340157\n",
      "Iteration 12774, loss = 0.00339994\n",
      "Iteration 12775, loss = 0.00339832\n",
      "Iteration 12776, loss = 0.00339669\n",
      "Iteration 12777, loss = 0.00339507\n",
      "Iteration 12778, loss = 0.00339345\n",
      "Iteration 12779, loss = 0.00339183\n",
      "Iteration 12780, loss = 0.00339021\n",
      "Iteration 12781, loss = 0.00338859\n",
      "Iteration 12782, loss = 0.00338697\n",
      "Iteration 12783, loss = 0.00338535\n",
      "Iteration 12784, loss = 0.00338374\n",
      "Iteration 12785, loss = 0.00338212\n",
      "Iteration 12786, loss = 0.00338051\n",
      "Iteration 12787, loss = 0.00337889\n",
      "Iteration 12788, loss = 0.00337728\n",
      "Iteration 12789, loss = 0.00337567\n",
      "Iteration 12790, loss = 0.00337406\n",
      "Iteration 12791, loss = 0.00337245\n",
      "Iteration 12792, loss = 0.00337084\n",
      "Iteration 12793, loss = 0.00336924\n",
      "Iteration 12794, loss = 0.00336763\n",
      "Iteration 12795, loss = 0.00336602\n",
      "Iteration 12796, loss = 0.00336442\n",
      "Iteration 12797, loss = 0.00336281\n",
      "Iteration 12798, loss = 0.00336121\n",
      "Iteration 12799, loss = 0.00335961\n",
      "Iteration 12800, loss = 0.00335801\n",
      "Iteration 12801, loss = 0.00335641\n",
      "Iteration 12802, loss = 0.00335481\n",
      "Iteration 12803, loss = 0.00335321\n",
      "Iteration 12804, loss = 0.00335162\n",
      "Iteration 12805, loss = 0.00335002\n",
      "Iteration 12806, loss = 0.00334843\n",
      "Iteration 12807, loss = 0.00334683\n",
      "Iteration 12808, loss = 0.00334524\n",
      "Iteration 12809, loss = 0.00334365\n",
      "Iteration 12810, loss = 0.00334205\n",
      "Iteration 12811, loss = 0.00334046\n",
      "Iteration 12812, loss = 0.00333887\n",
      "Iteration 12813, loss = 0.00333729\n",
      "Iteration 12814, loss = 0.00333570\n",
      "Iteration 12815, loss = 0.00333411\n",
      "Iteration 12816, loss = 0.00333253\n",
      "Iteration 12817, loss = 0.00333094\n",
      "Iteration 12818, loss = 0.00332936\n",
      "Iteration 12819, loss = 0.00332778\n",
      "Iteration 12820, loss = 0.00332619\n",
      "Iteration 12821, loss = 0.00332461\n",
      "Iteration 12822, loss = 0.00332303\n",
      "Iteration 12823, loss = 0.00332145\n",
      "Iteration 12824, loss = 0.00331988\n",
      "Iteration 12825, loss = 0.00331830\n",
      "Iteration 12826, loss = 0.00331672\n",
      "Iteration 12827, loss = 0.00331515\n",
      "Iteration 12828, loss = 0.00331357\n",
      "Iteration 12829, loss = 0.00331200\n",
      "Iteration 12830, loss = 0.00331043\n",
      "Iteration 12831, loss = 0.00330886\n",
      "Iteration 12832, loss = 0.00330729\n",
      "Iteration 12833, loss = 0.00330572\n",
      "Iteration 12834, loss = 0.00330415\n",
      "Iteration 12835, loss = 0.00330258\n",
      "Iteration 12836, loss = 0.00330101\n",
      "Iteration 12837, loss = 0.00329945\n",
      "Iteration 12838, loss = 0.00329788\n",
      "Iteration 12839, loss = 0.00329632\n",
      "Iteration 12840, loss = 0.00329476\n",
      "Iteration 12841, loss = 0.00329320\n",
      "Iteration 12842, loss = 0.00329163\n",
      "Iteration 12843, loss = 0.00329007\n",
      "Iteration 12844, loss = 0.00328852\n",
      "Iteration 12845, loss = 0.00328696\n",
      "Iteration 12846, loss = 0.00328540\n",
      "Iteration 12847, loss = 0.00328384\n",
      "Iteration 12848, loss = 0.00328229\n",
      "Iteration 12849, loss = 0.00328073\n",
      "Iteration 12850, loss = 0.00327918\n",
      "Iteration 12851, loss = 0.00327763\n",
      "Iteration 12852, loss = 0.00327608\n",
      "Iteration 12853, loss = 0.00327452\n",
      "Iteration 12854, loss = 0.00327297\n",
      "Iteration 12855, loss = 0.00327143\n",
      "Iteration 12856, loss = 0.00326988\n",
      "Iteration 12857, loss = 0.00326833\n",
      "Iteration 12858, loss = 0.00326678\n",
      "Iteration 12859, loss = 0.00326524\n",
      "Iteration 12860, loss = 0.00326370\n",
      "Iteration 12861, loss = 0.00326215\n",
      "Iteration 12862, loss = 0.00326061\n",
      "Iteration 12863, loss = 0.00325907\n",
      "Iteration 12864, loss = 0.00325753\n",
      "Iteration 12865, loss = 0.00325599\n",
      "Iteration 12866, loss = 0.00325445\n",
      "Iteration 12867, loss = 0.00325291\n",
      "Iteration 12868, loss = 0.00325137\n",
      "Iteration 12869, loss = 0.00324984\n",
      "Iteration 12870, loss = 0.00324830\n",
      "Iteration 12871, loss = 0.00324677\n",
      "Iteration 12872, loss = 0.00324524\n",
      "Iteration 12873, loss = 0.00324370\n",
      "Iteration 12874, loss = 0.00324217\n",
      "Iteration 12875, loss = 0.00324064\n",
      "Iteration 12876, loss = 0.00323911\n",
      "Iteration 12877, loss = 0.00323758\n",
      "Iteration 12878, loss = 0.00323606\n",
      "Iteration 12879, loss = 0.00323453\n",
      "Iteration 12880, loss = 0.00323300\n",
      "Iteration 12881, loss = 0.00323148\n",
      "Iteration 12882, loss = 0.00322996\n",
      "Iteration 12883, loss = 0.00322843\n",
      "Iteration 12884, loss = 0.00322691\n",
      "Iteration 12885, loss = 0.00322539\n",
      "Iteration 12886, loss = 0.00322387\n",
      "Iteration 12887, loss = 0.00322235\n",
      "Iteration 12888, loss = 0.00322083\n",
      "Iteration 12889, loss = 0.00321931\n",
      "Iteration 12890, loss = 0.00321780\n",
      "Iteration 12891, loss = 0.00321628\n",
      "Iteration 12892, loss = 0.00321477\n",
      "Iteration 12893, loss = 0.00321325\n",
      "Iteration 12894, loss = 0.00321174\n",
      "Iteration 12895, loss = 0.00321023\n",
      "Iteration 12896, loss = 0.00320872\n",
      "Iteration 12897, loss = 0.00320720\n",
      "Iteration 12898, loss = 0.00320570\n",
      "Iteration 12899, loss = 0.00320419\n",
      "Iteration 12900, loss = 0.00320268\n",
      "Iteration 12901, loss = 0.00320117\n",
      "Iteration 12902, loss = 0.00319967\n",
      "Iteration 12903, loss = 0.00319816\n",
      "Iteration 12904, loss = 0.00319666\n",
      "Iteration 12905, loss = 0.00319516\n",
      "Iteration 12906, loss = 0.00319365\n",
      "Iteration 12907, loss = 0.00319215\n",
      "Iteration 12908, loss = 0.00319065\n",
      "Iteration 12909, loss = 0.00318915\n",
      "Iteration 12910, loss = 0.00318765\n",
      "Iteration 12911, loss = 0.00318616\n",
      "Iteration 12912, loss = 0.00318466\n",
      "Iteration 12913, loss = 0.00318316\n",
      "Iteration 12914, loss = 0.00318167\n",
      "Iteration 12915, loss = 0.00318017\n",
      "Iteration 12916, loss = 0.00317868\n",
      "Iteration 12917, loss = 0.00317719\n",
      "Iteration 12918, loss = 0.00317570\n",
      "Iteration 12919, loss = 0.00317421\n",
      "Iteration 12920, loss = 0.00317272\n",
      "Iteration 12921, loss = 0.00317123\n",
      "Iteration 12922, loss = 0.00316974\n",
      "Iteration 12923, loss = 0.00316825\n",
      "Iteration 12924, loss = 0.00316677\n",
      "Iteration 12925, loss = 0.00316528\n",
      "Iteration 12926, loss = 0.00316380\n",
      "Iteration 12927, loss = 0.00316232\n",
      "Iteration 12928, loss = 0.00316083\n",
      "Iteration 12929, loss = 0.00315935\n",
      "Iteration 12930, loss = 0.00315787\n",
      "Iteration 12931, loss = 0.00315639\n",
      "Iteration 12932, loss = 0.00315491\n",
      "Iteration 12933, loss = 0.00315344\n",
      "Iteration 12934, loss = 0.00315196\n",
      "Iteration 12935, loss = 0.00315048\n",
      "Iteration 12936, loss = 0.00314901\n",
      "Iteration 12937, loss = 0.00314753\n",
      "Iteration 12938, loss = 0.00314606\n",
      "Iteration 12939, loss = 0.00314459\n",
      "Iteration 12940, loss = 0.00314312\n",
      "Iteration 12941, loss = 0.00314164\n",
      "Iteration 12942, loss = 0.00314017\n",
      "Iteration 12943, loss = 0.00313871\n",
      "Iteration 12944, loss = 0.00313724\n",
      "Iteration 12945, loss = 0.00313577\n",
      "Iteration 12946, loss = 0.00313430\n",
      "Iteration 12947, loss = 0.00313284\n",
      "Iteration 12948, loss = 0.00313137\n",
      "Iteration 12949, loss = 0.00312991\n",
      "Iteration 12950, loss = 0.00312845\n",
      "Iteration 12951, loss = 0.00312699\n",
      "Iteration 12952, loss = 0.00312552\n",
      "Iteration 12953, loss = 0.00312406\n",
      "Iteration 12954, loss = 0.00312261\n",
      "Iteration 12955, loss = 0.00312115\n",
      "Iteration 12956, loss = 0.00311969\n",
      "Iteration 12957, loss = 0.00311823\n",
      "Iteration 12958, loss = 0.00311678\n",
      "Iteration 12959, loss = 0.00311532\n",
      "Iteration 12960, loss = 0.00311387\n",
      "Iteration 12961, loss = 0.00311242\n",
      "Iteration 12962, loss = 0.00311096\n",
      "Iteration 12963, loss = 0.00310951\n",
      "Iteration 12964, loss = 0.00310806\n",
      "Iteration 12965, loss = 0.00310661\n",
      "Iteration 12966, loss = 0.00310516\n",
      "Iteration 12967, loss = 0.00310372\n",
      "Iteration 12968, loss = 0.00310227\n",
      "Iteration 12969, loss = 0.00310082\n",
      "Iteration 12970, loss = 0.00309938\n",
      "Iteration 12971, loss = 0.00309793\n",
      "Iteration 12972, loss = 0.00309649\n",
      "Iteration 12973, loss = 0.00309505\n",
      "Iteration 12974, loss = 0.00309360\n",
      "Iteration 12975, loss = 0.00309216\n",
      "Iteration 12976, loss = 0.00309072\n",
      "Iteration 12977, loss = 0.00308928\n",
      "Iteration 12978, loss = 0.00308785\n",
      "Iteration 12979, loss = 0.00308641\n",
      "Iteration 12980, loss = 0.00308497\n",
      "Iteration 12981, loss = 0.00308354\n",
      "Iteration 12982, loss = 0.00308210\n",
      "Iteration 12983, loss = 0.00308067\n",
      "Iteration 12984, loss = 0.00307924\n",
      "Iteration 12985, loss = 0.00307780\n",
      "Iteration 12986, loss = 0.00307637\n",
      "Iteration 12987, loss = 0.00307494\n",
      "Iteration 12988, loss = 0.00307351\n",
      "Iteration 12989, loss = 0.00307208\n",
      "Iteration 12990, loss = 0.00307066\n",
      "Iteration 12991, loss = 0.00306923\n",
      "Iteration 12992, loss = 0.00306780\n",
      "Iteration 12993, loss = 0.00306638\n",
      "Iteration 12994, loss = 0.00306495\n",
      "Iteration 12995, loss = 0.00306353\n",
      "Iteration 12996, loss = 0.00306211\n",
      "Iteration 12997, loss = 0.00306068\n",
      "Iteration 12998, loss = 0.00305926\n",
      "Iteration 12999, loss = 0.00305784\n",
      "Iteration 13000, loss = 0.00305642\n",
      "Iteration 13001, loss = 0.00305501\n",
      "Iteration 13002, loss = 0.00305359\n",
      "Iteration 13003, loss = 0.00305217\n",
      "Iteration 13004, loss = 0.00305076\n",
      "Iteration 13005, loss = 0.00304934\n",
      "Iteration 13006, loss = 0.00304793\n",
      "Iteration 13007, loss = 0.00304651\n",
      "Iteration 13008, loss = 0.00304510\n",
      "Iteration 13009, loss = 0.00304369\n",
      "Iteration 13010, loss = 0.00304228\n",
      "Iteration 13011, loss = 0.00304087\n",
      "Iteration 13012, loss = 0.00303946\n",
      "Iteration 13013, loss = 0.00303805\n",
      "Iteration 13014, loss = 0.00303665\n",
      "Iteration 13015, loss = 0.00303524\n",
      "Iteration 13016, loss = 0.00303383\n",
      "Iteration 13017, loss = 0.00303243\n",
      "Iteration 13018, loss = 0.00303103\n",
      "Iteration 13019, loss = 0.00302962\n",
      "Iteration 13020, loss = 0.00302822\n",
      "Iteration 13021, loss = 0.00302682\n",
      "Iteration 13022, loss = 0.00302542\n",
      "Iteration 13023, loss = 0.00302402\n",
      "Iteration 13024, loss = 0.00302262\n",
      "Iteration 13025, loss = 0.00302122\n",
      "Iteration 13026, loss = 0.00301983\n",
      "Iteration 13027, loss = 0.00301843\n",
      "Iteration 13028, loss = 0.00301703\n",
      "Iteration 13029, loss = 0.00301564\n",
      "Iteration 13030, loss = 0.00301425\n",
      "Iteration 13031, loss = 0.00301285\n",
      "Iteration 13032, loss = 0.00301146\n",
      "Iteration 13033, loss = 0.00301007\n",
      "Iteration 13034, loss = 0.00300868\n",
      "Iteration 13035, loss = 0.00300729\n",
      "Iteration 13036, loss = 0.00300590\n",
      "Iteration 13037, loss = 0.00300452\n",
      "Iteration 13038, loss = 0.00300313\n",
      "Iteration 13039, loss = 0.00300174\n",
      "Iteration 13040, loss = 0.00300036\n",
      "Iteration 13041, loss = 0.00299897\n",
      "Iteration 13042, loss = 0.00299759\n",
      "Iteration 13043, loss = 0.00299621\n",
      "Iteration 13044, loss = 0.00299482\n",
      "Iteration 13045, loss = 0.00299344\n",
      "Iteration 13046, loss = 0.00299206\n",
      "Iteration 13047, loss = 0.00299068\n",
      "Iteration 13048, loss = 0.00298931\n",
      "Iteration 13049, loss = 0.00298793\n",
      "Iteration 13050, loss = 0.00298655\n",
      "Iteration 13051, loss = 0.00298518\n",
      "Iteration 13052, loss = 0.00298380\n",
      "Iteration 13053, loss = 0.00298243\n",
      "Iteration 13054, loss = 0.00298105\n",
      "Iteration 13055, loss = 0.00297968\n",
      "Iteration 13056, loss = 0.00297831\n",
      "Iteration 13057, loss = 0.00297694\n",
      "Iteration 13058, loss = 0.00297557\n",
      "Iteration 13059, loss = 0.00297420\n",
      "Iteration 13060, loss = 0.00297283\n",
      "Iteration 13061, loss = 0.00297146\n",
      "Iteration 13062, loss = 0.00297010\n",
      "Iteration 13063, loss = 0.00296873\n",
      "Iteration 13064, loss = 0.00296736\n",
      "Iteration 13065, loss = 0.00296600\n",
      "Iteration 13066, loss = 0.00296464\n",
      "Iteration 13067, loss = 0.00296327\n",
      "Iteration 13068, loss = 0.00296191\n",
      "Iteration 13069, loss = 0.00296055\n",
      "Iteration 13070, loss = 0.00295919\n",
      "Iteration 13071, loss = 0.00295783\n",
      "Iteration 13072, loss = 0.00295647\n",
      "Iteration 13073, loss = 0.00295512\n",
      "Iteration 13074, loss = 0.00295376\n",
      "Iteration 13075, loss = 0.00295240\n",
      "Iteration 13076, loss = 0.00295105\n",
      "Iteration 13077, loss = 0.00294969\n",
      "Iteration 13078, loss = 0.00294834\n",
      "Iteration 13079, loss = 0.00294699\n",
      "Iteration 13080, loss = 0.00294564\n",
      "Iteration 13081, loss = 0.00294428\n",
      "Iteration 13082, loss = 0.00294293\n",
      "Iteration 13083, loss = 0.00294158\n",
      "Iteration 13084, loss = 0.00294024\n",
      "Iteration 13085, loss = 0.00293889\n",
      "Iteration 13086, loss = 0.00293754\n",
      "Iteration 13087, loss = 0.00293620\n",
      "Iteration 13088, loss = 0.00293485\n",
      "Iteration 13089, loss = 0.00293351\n",
      "Iteration 13090, loss = 0.00293216\n",
      "Iteration 13091, loss = 0.00293082\n",
      "Iteration 13092, loss = 0.00292948\n",
      "Iteration 13093, loss = 0.00292814\n",
      "Iteration 13094, loss = 0.00292680\n",
      "Iteration 13095, loss = 0.00292546\n",
      "Iteration 13096, loss = 0.00292412\n",
      "Iteration 13097, loss = 0.00292278\n",
      "Iteration 13098, loss = 0.00292144\n",
      "Iteration 13099, loss = 0.00292011\n",
      "Iteration 13100, loss = 0.00291877\n",
      "Iteration 13101, loss = 0.00291744\n",
      "Iteration 13102, loss = 0.00291610\n",
      "Iteration 13103, loss = 0.00291477\n",
      "Iteration 13104, loss = 0.00291344\n",
      "Iteration 13105, loss = 0.00291211\n",
      "Iteration 13106, loss = 0.00291077\n",
      "Iteration 13107, loss = 0.00290945\n",
      "Iteration 13108, loss = 0.00290812\n",
      "Iteration 13109, loss = 0.00290679\n",
      "Iteration 13110, loss = 0.00290546\n",
      "Iteration 13111, loss = 0.00290413\n",
      "Iteration 13112, loss = 0.00290281\n",
      "Iteration 13113, loss = 0.00290148\n",
      "Iteration 13114, loss = 0.00290016\n",
      "Iteration 13115, loss = 0.00289884\n",
      "Iteration 13116, loss = 0.00289751\n",
      "Iteration 13117, loss = 0.00289619\n",
      "Iteration 13118, loss = 0.00289487\n",
      "Iteration 13119, loss = 0.00289355\n",
      "Iteration 13120, loss = 0.00289223\n",
      "Iteration 13121, loss = 0.00289091\n",
      "Iteration 13122, loss = 0.00288960\n",
      "Iteration 13123, loss = 0.00288828\n",
      "Iteration 13124, loss = 0.00288696\n",
      "Iteration 13125, loss = 0.00288565\n",
      "Iteration 13126, loss = 0.00288433\n",
      "Iteration 13127, loss = 0.00288302\n",
      "Iteration 13128, loss = 0.00288171\n",
      "Iteration 13129, loss = 0.00288039\n",
      "Iteration 13130, loss = 0.00287908\n",
      "Iteration 13131, loss = 0.00287777\n",
      "Iteration 13132, loss = 0.00287646\n",
      "Iteration 13133, loss = 0.00287515\n",
      "Iteration 13134, loss = 0.00287385\n",
      "Iteration 13135, loss = 0.00287254\n",
      "Iteration 13136, loss = 0.00287123\n",
      "Iteration 13137, loss = 0.00286993\n",
      "Iteration 13138, loss = 0.00286862\n",
      "Iteration 13139, loss = 0.00286732\n",
      "Iteration 13140, loss = 0.00286601\n",
      "Iteration 13141, loss = 0.00286471\n",
      "Iteration 13142, loss = 0.00286341\n",
      "Iteration 13143, loss = 0.00286211\n",
      "Iteration 13144, loss = 0.00286081\n",
      "Iteration 13145, loss = 0.00285951\n",
      "Iteration 13146, loss = 0.00285821\n",
      "Iteration 13147, loss = 0.00285691\n",
      "Iteration 13148, loss = 0.00285562\n",
      "Iteration 13149, loss = 0.00285432\n",
      "Iteration 13150, loss = 0.00285303\n",
      "Iteration 13151, loss = 0.00285173\n",
      "Iteration 13152, loss = 0.00285044\n",
      "Iteration 13153, loss = 0.00284914\n",
      "Iteration 13154, loss = 0.00284785\n",
      "Iteration 13155, loss = 0.00284656\n",
      "Iteration 13156, loss = 0.00284527\n",
      "Iteration 13157, loss = 0.00284398\n",
      "Iteration 13158, loss = 0.00284269\n",
      "Iteration 13159, loss = 0.00284140\n",
      "Iteration 13160, loss = 0.00284012\n",
      "Iteration 13161, loss = 0.00283883\n",
      "Iteration 13162, loss = 0.00283754\n",
      "Iteration 13163, loss = 0.00283626\n",
      "Iteration 13164, loss = 0.00283497\n",
      "Iteration 13165, loss = 0.00283369\n",
      "Iteration 13166, loss = 0.00283241\n",
      "Iteration 13167, loss = 0.00283113\n",
      "Iteration 13168, loss = 0.00282984\n",
      "Iteration 13169, loss = 0.00282856\n",
      "Iteration 13170, loss = 0.00282728\n",
      "Iteration 13171, loss = 0.00282601\n",
      "Iteration 13172, loss = 0.00282473\n",
      "Iteration 13173, loss = 0.00282345\n",
      "Iteration 13174, loss = 0.00282217\n",
      "Iteration 13175, loss = 0.00282090\n",
      "Iteration 13176, loss = 0.00281962\n",
      "Iteration 13177, loss = 0.00281835\n",
      "Iteration 13178, loss = 0.00281708\n",
      "Iteration 13179, loss = 0.00281580\n",
      "Iteration 13180, loss = 0.00281453\n",
      "Iteration 13181, loss = 0.00281326\n",
      "Iteration 13182, loss = 0.00281199\n",
      "Iteration 13183, loss = 0.00281072\n",
      "Iteration 13184, loss = 0.00280945\n",
      "Iteration 13185, loss = 0.00280819\n",
      "Iteration 13186, loss = 0.00280692\n",
      "Iteration 13187, loss = 0.00280565\n",
      "Iteration 13188, loss = 0.00280439\n",
      "Iteration 13189, loss = 0.00280312\n",
      "Iteration 13190, loss = 0.00280186\n",
      "Iteration 13191, loss = 0.00280059\n",
      "Iteration 13192, loss = 0.00279933\n",
      "Iteration 13193, loss = 0.00279807\n",
      "Iteration 13194, loss = 0.00279681\n",
      "Iteration 13195, loss = 0.00279555\n",
      "Iteration 13196, loss = 0.00279429\n",
      "Iteration 13197, loss = 0.00279303\n",
      "Iteration 13198, loss = 0.00279177\n",
      "Iteration 13199, loss = 0.00279052\n",
      "Iteration 13200, loss = 0.00278926\n",
      "Iteration 13201, loss = 0.00278801\n",
      "Iteration 13202, loss = 0.00278675\n",
      "Iteration 13203, loss = 0.00278550\n",
      "Iteration 13204, loss = 0.00278424\n",
      "Iteration 13205, loss = 0.00278299\n",
      "Iteration 13206, loss = 0.00278174\n",
      "Iteration 13207, loss = 0.00278049\n",
      "Iteration 13208, loss = 0.00277924\n",
      "Iteration 13209, loss = 0.00277799\n",
      "Iteration 13210, loss = 0.00277674\n",
      "Iteration 13211, loss = 0.00277549\n",
      "Iteration 13212, loss = 0.00277425\n",
      "Iteration 13213, loss = 0.00277300\n",
      "Iteration 13214, loss = 0.00277175\n",
      "Iteration 13215, loss = 0.00277051\n",
      "Iteration 13216, loss = 0.00276927\n",
      "Iteration 13217, loss = 0.00276802\n",
      "Iteration 13218, loss = 0.00276678\n",
      "Iteration 13219, loss = 0.00276554\n",
      "Iteration 13220, loss = 0.00276430\n",
      "Iteration 13221, loss = 0.00276306\n",
      "Iteration 13222, loss = 0.00276182\n",
      "Iteration 13223, loss = 0.00276058\n",
      "Iteration 13224, loss = 0.00275934\n",
      "Iteration 13225, loss = 0.00275810\n",
      "Iteration 13226, loss = 0.00275687\n",
      "Iteration 13227, loss = 0.00275563\n",
      "Iteration 13228, loss = 0.00275440\n",
      "Iteration 13229, loss = 0.00275316\n",
      "Iteration 13230, loss = 0.00275193\n",
      "Iteration 13231, loss = 0.00275070\n",
      "Iteration 13232, loss = 0.00274947\n",
      "Iteration 13233, loss = 0.00274823\n",
      "Iteration 13234, loss = 0.00274700\n",
      "Iteration 13235, loss = 0.00274577\n",
      "Iteration 13236, loss = 0.00274455\n",
      "Iteration 13237, loss = 0.00274332\n",
      "Iteration 13238, loss = 0.00274209\n",
      "Iteration 13239, loss = 0.00274086\n",
      "Iteration 13240, loss = 0.00273964\n",
      "Iteration 13241, loss = 0.00273841\n",
      "Iteration 13242, loss = 0.00273719\n",
      "Iteration 13243, loss = 0.00273597\n",
      "Iteration 13244, loss = 0.00273474\n",
      "Iteration 13245, loss = 0.00273352\n",
      "Iteration 13246, loss = 0.00273230\n",
      "Iteration 13247, loss = 0.00273108\n",
      "Iteration 13248, loss = 0.00272986\n",
      "Iteration 13249, loss = 0.00272864\n",
      "Iteration 13250, loss = 0.00272742\n",
      "Iteration 13251, loss = 0.00272621\n",
      "Iteration 13252, loss = 0.00272499\n",
      "Iteration 13253, loss = 0.00272377\n",
      "Iteration 13254, loss = 0.00272256\n",
      "Iteration 13255, loss = 0.00272134\n",
      "Iteration 13256, loss = 0.00272013\n",
      "Iteration 13257, loss = 0.00271892\n",
      "Iteration 13258, loss = 0.00271770\n",
      "Iteration 13259, loss = 0.00271649\n",
      "Iteration 13260, loss = 0.00271528\n",
      "Iteration 13261, loss = 0.00271407\n",
      "Iteration 13262, loss = 0.00271286\n",
      "Iteration 13263, loss = 0.00271165\n",
      "Iteration 13264, loss = 0.00271045\n",
      "Iteration 13265, loss = 0.00270924\n",
      "Iteration 13266, loss = 0.00270803\n",
      "Iteration 13267, loss = 0.00270683\n",
      "Iteration 13268, loss = 0.00270562\n",
      "Iteration 13269, loss = 0.00270442\n",
      "Iteration 13270, loss = 0.00270321\n",
      "Iteration 13271, loss = 0.00270201\n",
      "Iteration 13272, loss = 0.00270081\n",
      "Iteration 13273, loss = 0.00269961\n",
      "Iteration 13274, loss = 0.00269841\n",
      "Iteration 13275, loss = 0.00269721\n",
      "Iteration 13276, loss = 0.00269601\n",
      "Iteration 13277, loss = 0.00269481\n",
      "Iteration 13278, loss = 0.00269362\n",
      "Iteration 13279, loss = 0.00269242\n",
      "Iteration 13280, loss = 0.00269122\n",
      "Iteration 13281, loss = 0.00269003\n",
      "Iteration 13282, loss = 0.00268883\n",
      "Iteration 13283, loss = 0.00268764\n",
      "Iteration 13284, loss = 0.00268645\n",
      "Iteration 13285, loss = 0.00268525\n",
      "Iteration 13286, loss = 0.00268406\n",
      "Iteration 13287, loss = 0.00268287\n",
      "Iteration 13288, loss = 0.00268168\n",
      "Iteration 13289, loss = 0.00268049\n",
      "Iteration 13290, loss = 0.00267930\n",
      "Iteration 13291, loss = 0.00267812\n",
      "Iteration 13292, loss = 0.00267693\n",
      "Iteration 13293, loss = 0.00267574\n",
      "Iteration 13294, loss = 0.00267456\n",
      "Iteration 13295, loss = 0.00267337\n",
      "Iteration 13296, loss = 0.00267219\n",
      "Iteration 13297, loss = 0.00267100\n",
      "Iteration 13298, loss = 0.00266982\n",
      "Iteration 13299, loss = 0.00266864\n",
      "Iteration 13300, loss = 0.00266746\n",
      "Iteration 13301, loss = 0.00266628\n",
      "Iteration 13302, loss = 0.00266510\n",
      "Iteration 13303, loss = 0.00266392\n",
      "Iteration 13304, loss = 0.00266274\n",
      "Iteration 13305, loss = 0.00266156\n",
      "Iteration 13306, loss = 0.00266039\n",
      "Iteration 13307, loss = 0.00265921\n",
      "Iteration 13308, loss = 0.00265803\n",
      "Iteration 13309, loss = 0.00265686\n",
      "Iteration 13310, loss = 0.00265569\n",
      "Iteration 13311, loss = 0.00265451\n",
      "Iteration 13312, loss = 0.00265334\n",
      "Iteration 13313, loss = 0.00265217\n",
      "Iteration 13314, loss = 0.00265100\n",
      "Iteration 13315, loss = 0.00264983\n",
      "Iteration 13316, loss = 0.00264866\n",
      "Iteration 13317, loss = 0.00264749\n",
      "Iteration 13318, loss = 0.00264632\n",
      "Iteration 13319, loss = 0.00264515\n",
      "Iteration 13320, loss = 0.00264399\n",
      "Iteration 13321, loss = 0.00264282\n",
      "Iteration 13322, loss = 0.00264165\n",
      "Iteration 13323, loss = 0.00264049\n",
      "Iteration 13324, loss = 0.00263933\n",
      "Iteration 13325, loss = 0.00263816\n",
      "Iteration 13326, loss = 0.00263700\n",
      "Iteration 13327, loss = 0.00263584\n",
      "Iteration 13328, loss = 0.00263468\n",
      "Iteration 13329, loss = 0.00263352\n",
      "Iteration 13330, loss = 0.00263236\n",
      "Iteration 13331, loss = 0.00263120\n",
      "Iteration 13332, loss = 0.00263004\n",
      "Iteration 13333, loss = 0.00262888\n",
      "Iteration 13334, loss = 0.00262773\n",
      "Iteration 13335, loss = 0.00262657\n",
      "Iteration 13336, loss = 0.00262542\n",
      "Iteration 13337, loss = 0.00262426\n",
      "Iteration 13338, loss = 0.00262311\n",
      "Iteration 13339, loss = 0.00262195\n",
      "Iteration 13340, loss = 0.00262080\n",
      "Iteration 13341, loss = 0.00261965\n",
      "Iteration 13342, loss = 0.00261850\n",
      "Iteration 13343, loss = 0.00261735\n",
      "Iteration 13344, loss = 0.00261620\n",
      "Iteration 13345, loss = 0.00261505\n",
      "Iteration 13346, loss = 0.00261390\n",
      "Iteration 13347, loss = 0.00261275\n",
      "Iteration 13348, loss = 0.00261161\n",
      "Iteration 13349, loss = 0.00261046\n",
      "Iteration 13350, loss = 0.00260932\n",
      "Iteration 13351, loss = 0.00260817\n",
      "Iteration 13352, loss = 0.00260703\n",
      "Iteration 13353, loss = 0.00260588\n",
      "Iteration 13354, loss = 0.00260474\n",
      "Iteration 13355, loss = 0.00260360\n",
      "Iteration 13356, loss = 0.00260246\n",
      "Iteration 13357, loss = 0.00260132\n",
      "Iteration 13358, loss = 0.00260018\n",
      "Iteration 13359, loss = 0.00259904\n",
      "Iteration 13360, loss = 0.00259790\n",
      "Iteration 13361, loss = 0.00259676\n",
      "Iteration 13362, loss = 0.00259563\n",
      "Iteration 13363, loss = 0.00259449\n",
      "Iteration 13364, loss = 0.00259336\n",
      "Iteration 13365, loss = 0.00259222\n",
      "Iteration 13366, loss = 0.00259109\n",
      "Iteration 13367, loss = 0.00258995\n",
      "Iteration 13368, loss = 0.00258882\n",
      "Iteration 13369, loss = 0.00258769\n",
      "Iteration 13370, loss = 0.00258656\n",
      "Iteration 13371, loss = 0.00258543\n",
      "Iteration 13372, loss = 0.00258430\n",
      "Iteration 13373, loss = 0.00258317\n",
      "Iteration 13374, loss = 0.00258204\n",
      "Iteration 13375, loss = 0.00258091\n",
      "Iteration 13376, loss = 0.00257979\n",
      "Iteration 13377, loss = 0.00257866\n",
      "Iteration 13378, loss = 0.00257753\n",
      "Iteration 13379, loss = 0.00257641\n",
      "Iteration 13380, loss = 0.00257529\n",
      "Iteration 13381, loss = 0.00257416\n",
      "Iteration 13382, loss = 0.00257304\n",
      "Iteration 13383, loss = 0.00257192\n",
      "Iteration 13384, loss = 0.00257080\n",
      "Iteration 13385, loss = 0.00256967\n",
      "Iteration 13386, loss = 0.00256855\n",
      "Iteration 13387, loss = 0.00256744\n",
      "Iteration 13388, loss = 0.00256632\n",
      "Iteration 13389, loss = 0.00256520\n",
      "Iteration 13390, loss = 0.00256408\n",
      "Iteration 13391, loss = 0.00256296\n",
      "Iteration 13392, loss = 0.00256185\n",
      "Iteration 13393, loss = 0.00256073\n",
      "Iteration 13394, loss = 0.00255962\n",
      "Iteration 13395, loss = 0.00255851\n",
      "Iteration 13396, loss = 0.00255739\n",
      "Iteration 13397, loss = 0.00255628\n",
      "Iteration 13398, loss = 0.00255517\n",
      "Iteration 13399, loss = 0.00255406\n",
      "Iteration 13400, loss = 0.00255295\n",
      "Iteration 13401, loss = 0.00255184\n",
      "Iteration 13402, loss = 0.00255073\n",
      "Iteration 13403, loss = 0.00254962\n",
      "Iteration 13404, loss = 0.00254851\n",
      "Iteration 13405, loss = 0.00254741\n",
      "Iteration 13406, loss = 0.00254630\n",
      "Iteration 13407, loss = 0.00254519\n",
      "Iteration 13408, loss = 0.00254409\n",
      "Iteration 13409, loss = 0.00254298\n",
      "Iteration 13410, loss = 0.00254188\n",
      "Iteration 13411, loss = 0.00254078\n",
      "Iteration 13412, loss = 0.00253968\n",
      "Iteration 13413, loss = 0.00253858\n",
      "Iteration 13414, loss = 0.00253747\n",
      "Iteration 13415, loss = 0.00253637\n",
      "Iteration 13416, loss = 0.00253527\n",
      "Iteration 13417, loss = 0.00253418\n",
      "Iteration 13418, loss = 0.00253308\n",
      "Iteration 13419, loss = 0.00253198\n",
      "Iteration 13420, loss = 0.00253088\n",
      "Iteration 13421, loss = 0.00252979\n",
      "Iteration 13422, loss = 0.00252869\n",
      "Iteration 13423, loss = 0.00252760\n",
      "Iteration 13424, loss = 0.00252650\n",
      "Iteration 13425, loss = 0.00252541\n",
      "Iteration 13426, loss = 0.00252432\n",
      "Iteration 13427, loss = 0.00252323\n",
      "Iteration 13428, loss = 0.00252214\n",
      "Iteration 13429, loss = 0.00252105\n",
      "Iteration 13430, loss = 0.00251996\n",
      "Iteration 13431, loss = 0.00251887\n",
      "Iteration 13432, loss = 0.00251778\n",
      "Iteration 13433, loss = 0.00251669\n",
      "Iteration 13434, loss = 0.00251560\n",
      "Iteration 13435, loss = 0.00251452\n",
      "Iteration 13436, loss = 0.00251343\n",
      "Iteration 13437, loss = 0.00251235\n",
      "Iteration 13438, loss = 0.00251126\n",
      "Iteration 13439, loss = 0.00251018\n",
      "Iteration 13440, loss = 0.00250909\n",
      "Iteration 13441, loss = 0.00250801\n",
      "Iteration 13442, loss = 0.00250693\n",
      "Iteration 13443, loss = 0.00250585\n",
      "Iteration 13444, loss = 0.00250477\n",
      "Iteration 13445, loss = 0.00250369\n",
      "Iteration 13446, loss = 0.00250261\n",
      "Iteration 13447, loss = 0.00250153\n",
      "Iteration 13448, loss = 0.00250045\n",
      "Iteration 13449, loss = 0.00249938\n",
      "Iteration 13450, loss = 0.00249830\n",
      "Iteration 13451, loss = 0.00249722\n",
      "Iteration 13452, loss = 0.00249615\n",
      "Iteration 13453, loss = 0.00249508\n",
      "Iteration 13454, loss = 0.00249400\n",
      "Iteration 13455, loss = 0.00249293\n",
      "Iteration 13456, loss = 0.00249186\n",
      "Iteration 13457, loss = 0.00249078\n",
      "Iteration 13458, loss = 0.00248971\n",
      "Iteration 13459, loss = 0.00248864\n",
      "Iteration 13460, loss = 0.00248757\n",
      "Iteration 13461, loss = 0.00248650\n",
      "Iteration 13462, loss = 0.00248544\n",
      "Iteration 13463, loss = 0.00248437\n",
      "Iteration 13464, loss = 0.00248330\n",
      "Iteration 13465, loss = 0.00248224\n",
      "Iteration 13466, loss = 0.00248117\n",
      "Iteration 13467, loss = 0.00248010\n",
      "Iteration 13468, loss = 0.00247904\n",
      "Iteration 13469, loss = 0.00247798\n",
      "Iteration 13470, loss = 0.00247691\n",
      "Iteration 13471, loss = 0.00247585\n",
      "Iteration 13472, loss = 0.00247479\n",
      "Iteration 13473, loss = 0.00247373\n",
      "Iteration 13474, loss = 0.00247267\n",
      "Iteration 13475, loss = 0.00247161\n",
      "Iteration 13476, loss = 0.00247055\n",
      "Iteration 13477, loss = 0.00246949\n",
      "Iteration 13478, loss = 0.00246843\n",
      "Iteration 13479, loss = 0.00246738\n",
      "Iteration 13480, loss = 0.00246632\n",
      "Iteration 13481, loss = 0.00246526\n",
      "Iteration 13482, loss = 0.00246421\n",
      "Iteration 13483, loss = 0.00246315\n",
      "Iteration 13484, loss = 0.00246210\n",
      "Iteration 13485, loss = 0.00246105\n",
      "Iteration 13486, loss = 0.00245999\n",
      "Iteration 13487, loss = 0.00245894\n",
      "Iteration 13488, loss = 0.00245789\n",
      "Iteration 13489, loss = 0.00245684\n",
      "Iteration 13490, loss = 0.00245579\n",
      "Iteration 13491, loss = 0.00245474\n",
      "Iteration 13492, loss = 0.00245369\n",
      "Iteration 13493, loss = 0.00245265\n",
      "Iteration 13494, loss = 0.00245160\n",
      "Iteration 13495, loss = 0.00245055\n",
      "Iteration 13496, loss = 0.00244951\n",
      "Iteration 13497, loss = 0.00244846\n",
      "Iteration 13498, loss = 0.00244742\n",
      "Iteration 13499, loss = 0.00244637\n",
      "Iteration 13500, loss = 0.00244533\n",
      "Iteration 13501, loss = 0.00244429\n",
      "Iteration 13502, loss = 0.00244324\n",
      "Iteration 13503, loss = 0.00244220\n",
      "Iteration 13504, loss = 0.00244116\n",
      "Iteration 13505, loss = 0.00244012\n",
      "Iteration 13506, loss = 0.00243908\n",
      "Iteration 13507, loss = 0.00243804\n",
      "Iteration 13508, loss = 0.00243701\n",
      "Iteration 13509, loss = 0.00243597\n",
      "Iteration 13510, loss = 0.00243493\n",
      "Iteration 13511, loss = 0.00243389\n",
      "Iteration 13512, loss = 0.00243286\n",
      "Iteration 13513, loss = 0.00243182\n",
      "Iteration 13514, loss = 0.00243079\n",
      "Iteration 13515, loss = 0.00242976\n",
      "Iteration 13516, loss = 0.00242872\n",
      "Iteration 13517, loss = 0.00242769\n",
      "Iteration 13518, loss = 0.00242666\n",
      "Iteration 13519, loss = 0.00242563\n",
      "Iteration 13520, loss = 0.00242460\n",
      "Iteration 13521, loss = 0.00242357\n",
      "Iteration 13522, loss = 0.00242254\n",
      "Iteration 13523, loss = 0.00242151\n",
      "Iteration 13524, loss = 0.00242048\n",
      "Iteration 13525, loss = 0.00241946\n",
      "Iteration 13526, loss = 0.00241843\n",
      "Iteration 13527, loss = 0.00241740\n",
      "Iteration 13528, loss = 0.00241638\n",
      "Iteration 13529, loss = 0.00241535\n",
      "Iteration 13530, loss = 0.00241433\n",
      "Iteration 13531, loss = 0.00241331\n",
      "Iteration 13532, loss = 0.00241228\n",
      "Iteration 13533, loss = 0.00241126\n",
      "Iteration 13534, loss = 0.00241024\n",
      "Iteration 13535, loss = 0.00240922\n",
      "Iteration 13536, loss = 0.00240820\n",
      "Iteration 13537, loss = 0.00240718\n",
      "Iteration 13538, loss = 0.00240616\n",
      "Iteration 13539, loss = 0.00240514\n",
      "Iteration 13540, loss = 0.00240413\n",
      "Iteration 13541, loss = 0.00240311\n",
      "Iteration 13542, loss = 0.00240209\n",
      "Iteration 13543, loss = 0.00240108\n",
      "Iteration 13544, loss = 0.00240006\n",
      "Iteration 13545, loss = 0.00239905\n",
      "Iteration 13546, loss = 0.00239803\n",
      "Iteration 13547, loss = 0.00239702\n",
      "Iteration 13548, loss = 0.00239601\n",
      "Iteration 13549, loss = 0.00239500\n",
      "Iteration 13550, loss = 0.00239398\n",
      "Iteration 13551, loss = 0.00239297\n",
      "Iteration 13552, loss = 0.00239196\n",
      "Iteration 13553, loss = 0.00239096\n",
      "Iteration 13554, loss = 0.00238995\n",
      "Iteration 13555, loss = 0.00238894\n",
      "Iteration 13556, loss = 0.00238793\n",
      "Iteration 13557, loss = 0.00238692\n",
      "Iteration 13558, loss = 0.00238592\n",
      "Iteration 13559, loss = 0.00238491\n",
      "Iteration 13560, loss = 0.00238391\n",
      "Iteration 13561, loss = 0.00238290\n",
      "Iteration 13562, loss = 0.00238190\n",
      "Iteration 13563, loss = 0.00238090\n",
      "Iteration 13564, loss = 0.00237989\n",
      "Iteration 13565, loss = 0.00237889\n",
      "Iteration 13566, loss = 0.00237789\n",
      "Iteration 13567, loss = 0.00237689\n",
      "Iteration 13568, loss = 0.00237589\n",
      "Iteration 13569, loss = 0.00237489\n",
      "Iteration 13570, loss = 0.00237389\n",
      "Iteration 13571, loss = 0.00237289\n",
      "Iteration 13572, loss = 0.00237190\n",
      "Iteration 13573, loss = 0.00237090\n",
      "Iteration 13574, loss = 0.00236990\n",
      "Iteration 13575, loss = 0.00236891\n",
      "Iteration 13576, loss = 0.00236791\n",
      "Iteration 13577, loss = 0.00236692\n",
      "Iteration 13578, loss = 0.00236593\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35542840\n",
      "Iteration 2, loss = 1.34795832\n",
      "Iteration 3, loss = 1.33750907\n",
      "Iteration 4, loss = 1.32448593\n",
      "Iteration 5, loss = 1.30899563\n",
      "Iteration 6, loss = 1.29112356\n",
      "Iteration 7, loss = 1.27261421\n",
      "Iteration 8, loss = 1.25365144\n",
      "Iteration 9, loss = 1.23436004\n",
      "Iteration 10, loss = 1.21584916\n",
      "Iteration 11, loss = 1.19787554\n",
      "Iteration 12, loss = 1.18493017\n",
      "Iteration 13, loss = 1.17279878\n",
      "Iteration 14, loss = 1.16112738\n",
      "Iteration 15, loss = 1.14902391\n",
      "Iteration 16, loss = 1.13229835\n",
      "Iteration 17, loss = 1.10879294\n",
      "Iteration 18, loss = 1.10005358\n",
      "Iteration 19, loss = 1.10810183\n",
      "Iteration 20, loss = 1.11618990\n",
      "Iteration 21, loss = 1.11397044\n",
      "Iteration 22, loss = 1.11099654\n",
      "Iteration 23, loss = 1.10818047\n",
      "Iteration 24, loss = 1.10578075\n",
      "Iteration 25, loss = 1.10381887\n",
      "Iteration 26, loss = 1.10225380\n",
      "Iteration 27, loss = 1.10103172\n",
      "Iteration 28, loss = 1.10009926\n",
      "Iteration 29, loss = 1.09940726\n",
      "Iteration 30, loss = 1.09891177\n",
      "Iteration 31, loss = 1.09857426\n",
      "Iteration 32, loss = 1.09836135\n",
      "Iteration 33, loss = 1.09824456\n",
      "Iteration 34, loss = 1.09819982\n",
      "Iteration 35, loss = 1.09820709\n",
      "Iteration 36, loss = 1.09824992\n",
      "Iteration 37, loss = 1.09831499\n",
      "Iteration 38, loss = 1.09839175\n",
      "Iteration 39, loss = 1.09847200\n",
      "Iteration 40, loss = 1.09854956\n",
      "Iteration 41, loss = 1.09861997\n",
      "Iteration 42, loss = 1.09868015\n",
      "Iteration 43, loss = 1.09872820\n",
      "Iteration 44, loss = 1.09876316\n",
      "Iteration 45, loss = 1.09878479\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.18606051\n",
      "Iteration 2, loss = 1.18575631\n",
      "Iteration 3, loss = 1.18532283\n",
      "Iteration 4, loss = 1.18477340\n",
      "Iteration 5, loss = 1.18412010\n",
      "Iteration 6, loss = 1.18337386\n",
      "Iteration 7, loss = 1.18254456\n",
      "Iteration 8, loss = 1.18164114\n",
      "Iteration 9, loss = 1.18067163\n",
      "Iteration 10, loss = 1.17964332\n",
      "Iteration 11, loss = 1.17856275\n",
      "Iteration 12, loss = 1.17743581\n",
      "Iteration 13, loss = 1.17626782\n",
      "Iteration 14, loss = 1.17506356\n",
      "Iteration 15, loss = 1.17382732\n",
      "Iteration 16, loss = 1.17256295\n",
      "Iteration 17, loss = 1.17127392\n",
      "Iteration 18, loss = 1.16996333\n",
      "Iteration 19, loss = 1.16863397\n",
      "Iteration 20, loss = 1.16728834\n",
      "Iteration 21, loss = 1.16592867\n",
      "Iteration 22, loss = 1.16455696\n",
      "Iteration 23, loss = 1.16317499\n",
      "Iteration 24, loss = 1.16178438\n",
      "Iteration 25, loss = 1.16038653\n",
      "Iteration 26, loss = 1.15898274\n",
      "Iteration 27, loss = 1.15757412\n",
      "Iteration 28, loss = 1.15616169\n",
      "Iteration 29, loss = 1.15474635\n",
      "Iteration 30, loss = 1.15332890\n",
      "Iteration 31, loss = 1.15191005\n",
      "Iteration 32, loss = 1.15049043\n",
      "Iteration 33, loss = 1.14907058\n",
      "Iteration 34, loss = 1.14765101\n",
      "Iteration 35, loss = 1.14623215\n",
      "Iteration 36, loss = 1.14481439\n",
      "Iteration 37, loss = 1.14339805\n",
      "Iteration 38, loss = 1.14198343\n",
      "Iteration 39, loss = 1.14057079\n",
      "Iteration 40, loss = 1.13916036\n",
      "Iteration 41, loss = 1.13775234\n",
      "Iteration 42, loss = 1.13634688\n",
      "Iteration 43, loss = 1.13494414\n",
      "Iteration 44, loss = 1.13354424\n",
      "Iteration 45, loss = 1.13214729\n",
      "Iteration 46, loss = 1.13075338\n",
      "Iteration 47, loss = 1.12936258\n",
      "Iteration 48, loss = 1.12797496\n",
      "Iteration 49, loss = 1.12659056\n",
      "Iteration 50, loss = 1.12520942\n",
      "Iteration 51, loss = 1.12383158\n",
      "Iteration 52, loss = 1.12245706\n",
      "Iteration 53, loss = 1.12108587\n",
      "Iteration 54, loss = 1.11971802\n",
      "Iteration 55, loss = 1.11835351\n",
      "Iteration 56, loss = 1.11699235\n",
      "Iteration 57, loss = 1.11563452\n",
      "Iteration 58, loss = 1.11428002\n",
      "Iteration 59, loss = 1.11292883\n",
      "Iteration 60, loss = 1.11158093\n",
      "Iteration 61, loss = 1.11023632\n",
      "Iteration 62, loss = 1.10889495\n",
      "Iteration 63, loss = 1.10755682\n",
      "Iteration 64, loss = 1.10622189\n",
      "Iteration 65, loss = 1.10489015\n",
      "Iteration 66, loss = 1.10356155\n",
      "Iteration 67, loss = 1.10223608\n",
      "Iteration 68, loss = 1.10091370\n",
      "Iteration 69, loss = 1.09959438\n",
      "Iteration 70, loss = 1.09827808\n",
      "Iteration 71, loss = 1.09696479\n",
      "Iteration 72, loss = 1.09565446\n",
      "Iteration 73, loss = 1.09434707\n",
      "Iteration 74, loss = 1.09304257\n",
      "Iteration 75, loss = 1.09174094\n",
      "Iteration 76, loss = 1.09044214\n",
      "Iteration 77, loss = 1.08914614\n",
      "Iteration 78, loss = 1.08785290\n",
      "Iteration 79, loss = 1.08656239\n",
      "Iteration 80, loss = 1.08527458\n",
      "Iteration 81, loss = 1.08398944\n",
      "Iteration 82, loss = 1.08270692\n",
      "Iteration 83, loss = 1.08142700\n",
      "Iteration 84, loss = 1.08014965\n",
      "Iteration 85, loss = 1.07887482\n",
      "Iteration 86, loss = 1.07760249\n",
      "Iteration 87, loss = 1.07633263\n",
      "Iteration 88, loss = 1.07506519\n",
      "Iteration 89, loss = 1.07380016\n",
      "Iteration 90, loss = 1.07253750\n",
      "Iteration 91, loss = 1.07127717\n",
      "Iteration 92, loss = 1.07001914\n",
      "Iteration 93, loss = 1.06876339\n",
      "Iteration 94, loss = 1.06750988\n",
      "Iteration 95, loss = 1.06625858\n",
      "Iteration 96, loss = 1.06500946\n",
      "Iteration 97, loss = 1.06376249\n",
      "Iteration 98, loss = 1.06251764\n",
      "Iteration 99, loss = 1.06127488\n",
      "Iteration 100, loss = 1.06003418\n",
      "Iteration 101, loss = 1.05879551\n",
      "Iteration 102, loss = 1.05755884\n",
      "Iteration 103, loss = 1.05632415\n",
      "Iteration 104, loss = 1.05509140\n",
      "Iteration 105, loss = 1.05386058\n",
      "Iteration 106, loss = 1.05263163\n",
      "Iteration 107, loss = 1.05140456\n",
      "Iteration 108, loss = 1.05017931\n",
      "Iteration 109, loss = 1.04895588\n",
      "Iteration 110, loss = 1.04773422\n",
      "Iteration 111, loss = 1.04651432\n",
      "Iteration 112, loss = 1.04529614\n",
      "Iteration 113, loss = 1.04407967\n",
      "Iteration 114, loss = 1.04286487\n",
      "Iteration 115, loss = 1.04165173\n",
      "Iteration 116, loss = 1.04044021\n",
      "Iteration 117, loss = 1.03923029\n",
      "Iteration 118, loss = 1.03802195\n",
      "Iteration 119, loss = 1.03681516\n",
      "Iteration 120, loss = 1.03560989\n",
      "Iteration 121, loss = 1.03440613\n",
      "Iteration 122, loss = 1.03320385\n",
      "Iteration 123, loss = 1.03200303\n",
      "Iteration 124, loss = 1.03080364\n",
      "Iteration 125, loss = 1.02960566\n",
      "Iteration 126, loss = 1.02840907\n",
      "Iteration 127, loss = 1.02721385\n",
      "Iteration 128, loss = 1.02601997\n",
      "Iteration 129, loss = 1.02482741\n",
      "Iteration 130, loss = 1.02363615\n",
      "Iteration 131, loss = 1.02244617\n",
      "Iteration 132, loss = 1.02125745\n",
      "Iteration 133, loss = 1.02006997\n",
      "Iteration 134, loss = 1.01888370\n",
      "Iteration 135, loss = 1.01769863\n",
      "Iteration 136, loss = 1.01651473\n",
      "Iteration 137, loss = 1.01533199\n",
      "Iteration 138, loss = 1.01415039\n",
      "Iteration 139, loss = 1.01296990\n",
      "Iteration 140, loss = 1.01179051\n",
      "Iteration 141, loss = 1.01061220\n",
      "Iteration 142, loss = 1.00943494\n",
      "Iteration 143, loss = 1.00825873\n",
      "Iteration 144, loss = 1.00708354\n",
      "Iteration 145, loss = 1.00590936\n",
      "Iteration 146, loss = 1.00473616\n",
      "Iteration 147, loss = 1.00356393\n",
      "Iteration 148, loss = 1.00239266\n",
      "Iteration 149, loss = 1.00122232\n",
      "Iteration 150, loss = 1.00005289\n",
      "Iteration 151, loss = 0.99888437\n",
      "Iteration 152, loss = 0.99771673\n",
      "Iteration 153, loss = 0.99654995\n",
      "Iteration 154, loss = 0.99538403\n",
      "Iteration 155, loss = 0.99421895\n",
      "Iteration 156, loss = 0.99305468\n",
      "Iteration 157, loss = 0.99189122\n",
      "Iteration 158, loss = 0.99072855\n",
      "Iteration 159, loss = 0.98956665\n",
      "Iteration 160, loss = 0.98840551\n",
      "Iteration 161, loss = 0.98724512\n",
      "Iteration 162, loss = 0.98608545\n",
      "Iteration 163, loss = 0.98492650\n",
      "Iteration 164, loss = 0.98376826\n",
      "Iteration 165, loss = 0.98261070\n",
      "Iteration 166, loss = 0.98145381\n",
      "Iteration 167, loss = 0.98029758\n",
      "Iteration 168, loss = 0.97914200\n",
      "Iteration 169, loss = 0.97798706\n",
      "Iteration 170, loss = 0.97683274\n",
      "Iteration 171, loss = 0.97567902\n",
      "Iteration 172, loss = 0.97452590\n",
      "Iteration 173, loss = 0.97337337\n",
      "Iteration 174, loss = 0.97222140\n",
      "Iteration 175, loss = 0.97107000\n",
      "Iteration 176, loss = 0.96991914\n",
      "Iteration 177, loss = 0.96876882\n",
      "Iteration 178, loss = 0.96761902\n",
      "Iteration 179, loss = 0.96646974\n",
      "Iteration 180, loss = 0.96532096\n",
      "Iteration 181, loss = 0.96417267\n",
      "Iteration 182, loss = 0.96302486\n",
      "Iteration 183, loss = 0.96187753\n",
      "Iteration 184, loss = 0.96073065\n",
      "Iteration 185, loss = 0.95958423\n",
      "Iteration 186, loss = 0.95843824\n",
      "Iteration 187, loss = 0.95729268\n",
      "Iteration 188, loss = 0.95614755\n",
      "Iteration 189, loss = 0.95500283\n",
      "Iteration 190, loss = 0.95385850\n",
      "Iteration 191, loss = 0.95271458\n",
      "Iteration 192, loss = 0.95157103\n",
      "Iteration 193, loss = 0.95042787\n",
      "Iteration 194, loss = 0.94928507\n",
      "Iteration 195, loss = 0.94814262\n",
      "Iteration 196, loss = 0.94700053\n",
      "Iteration 197, loss = 0.94585878\n",
      "Iteration 198, loss = 0.94471737\n",
      "Iteration 199, loss = 0.94357628\n",
      "Iteration 200, loss = 0.94243551\n",
      "Iteration 201, loss = 0.94129506\n",
      "Iteration 202, loss = 0.94015490\n",
      "Iteration 203, loss = 0.93901505\n",
      "Iteration 204, loss = 0.93787549\n",
      "Iteration 205, loss = 0.93673621\n",
      "Iteration 206, loss = 0.93559721\n",
      "Iteration 207, loss = 0.93445848\n",
      "Iteration 208, loss = 0.93332001\n",
      "Iteration 209, loss = 0.93218180\n",
      "Iteration 210, loss = 0.93104385\n",
      "Iteration 211, loss = 0.92990614\n",
      "Iteration 212, loss = 0.92876867\n",
      "Iteration 213, loss = 0.92763144\n",
      "Iteration 214, loss = 0.92649444\n",
      "Iteration 215, loss = 0.92535767\n",
      "Iteration 216, loss = 0.92422112\n",
      "Iteration 217, loss = 0.92308478\n",
      "Iteration 218, loss = 0.92194865\n",
      "Iteration 219, loss = 0.92081273\n",
      "Iteration 220, loss = 0.91967701\n",
      "Iteration 221, loss = 0.91854149\n",
      "Iteration 222, loss = 0.91740616\n",
      "Iteration 223, loss = 0.91627102\n",
      "Iteration 224, loss = 0.91513606\n",
      "Iteration 225, loss = 0.91400129\n",
      "Iteration 226, loss = 0.91286669\n",
      "Iteration 227, loss = 0.91173227\n",
      "Iteration 228, loss = 0.91059802\n",
      "Iteration 229, loss = 0.90946394\n",
      "Iteration 230, loss = 0.90833003\n",
      "Iteration 231, loss = 0.90719627\n",
      "Iteration 232, loss = 0.90606268\n",
      "Iteration 233, loss = 0.90492924\n",
      "Iteration 234, loss = 0.90379596\n",
      "Iteration 235, loss = 0.90266283\n",
      "Iteration 236, loss = 0.90152985\n",
      "Iteration 237, loss = 0.90039702\n",
      "Iteration 238, loss = 0.89926433\n",
      "Iteration 239, loss = 0.89813179\n",
      "Iteration 240, loss = 0.89699939\n",
      "Iteration 241, loss = 0.89586713\n",
      "Iteration 242, loss = 0.89473501\n",
      "Iteration 243, loss = 0.89360302\n",
      "Iteration 244, loss = 0.89247118\n",
      "Iteration 245, loss = 0.89133947\n",
      "Iteration 246, loss = 0.89020789\n",
      "Iteration 247, loss = 0.88907645\n",
      "Iteration 248, loss = 0.88794514\n",
      "Iteration 249, loss = 0.88681397\n",
      "Iteration 250, loss = 0.88568293\n",
      "Iteration 251, loss = 0.88455201\n",
      "Iteration 252, loss = 0.88342123\n",
      "Iteration 253, loss = 0.88229059\n",
      "Iteration 254, loss = 0.88116007\n",
      "Iteration 255, loss = 0.88002969\n",
      "Iteration 256, loss = 0.87889943\n",
      "Iteration 257, loss = 0.87776931\n",
      "Iteration 258, loss = 0.87663932\n",
      "Iteration 259, loss = 0.87550947\n",
      "Iteration 260, loss = 0.87437975\n",
      "Iteration 261, loss = 0.87325016\n",
      "Iteration 262, loss = 0.87212071\n",
      "Iteration 263, loss = 0.87099140\n",
      "Iteration 264, loss = 0.86986222\n",
      "Iteration 265, loss = 0.86873319\n",
      "Iteration 266, loss = 0.86760429\n",
      "Iteration 267, loss = 0.86647554\n",
      "Iteration 268, loss = 0.86534693\n",
      "Iteration 269, loss = 0.86421846\n",
      "Iteration 270, loss = 0.86309014\n",
      "Iteration 271, loss = 0.86196197\n",
      "Iteration 272, loss = 0.86083395\n",
      "Iteration 273, loss = 0.85970608\n",
      "Iteration 274, loss = 0.85857837\n",
      "Iteration 275, loss = 0.85745081\n",
      "Iteration 276, loss = 0.85632341\n",
      "Iteration 277, loss = 0.85519617\n",
      "Iteration 278, loss = 0.85406910\n",
      "Iteration 279, loss = 0.85294220\n",
      "Iteration 280, loss = 0.85181546\n",
      "Iteration 281, loss = 0.85068890\n",
      "Iteration 282, loss = 0.84956251\n",
      "Iteration 283, loss = 0.84843629\n",
      "Iteration 284, loss = 0.84731026\n",
      "Iteration 285, loss = 0.84618442\n",
      "Iteration 286, loss = 0.84505876\n",
      "Iteration 287, loss = 0.84393329\n",
      "Iteration 288, loss = 0.84280801\n",
      "Iteration 289, loss = 0.84168294\n",
      "Iteration 290, loss = 0.84055806\n",
      "Iteration 291, loss = 0.83943339\n",
      "Iteration 292, loss = 0.83830892\n",
      "Iteration 293, loss = 0.83718467\n",
      "Iteration 294, loss = 0.83606064\n",
      "Iteration 295, loss = 0.83493682\n",
      "Iteration 296, loss = 0.83381323\n",
      "Iteration 297, loss = 0.83268986\n",
      "Iteration 298, loss = 0.83156673\n",
      "Iteration 299, loss = 0.83044384\n",
      "Iteration 300, loss = 0.82932118\n",
      "Iteration 301, loss = 0.82819877\n",
      "Iteration 302, loss = 0.82707661\n",
      "Iteration 303, loss = 0.82595470\n",
      "Iteration 304, loss = 0.82483306\n",
      "Iteration 305, loss = 0.82371167\n",
      "Iteration 306, loss = 0.82259055\n",
      "Iteration 307, loss = 0.82146971\n",
      "Iteration 308, loss = 0.82034914\n",
      "Iteration 309, loss = 0.81922886\n",
      "Iteration 310, loss = 0.81810886\n",
      "Iteration 311, loss = 0.81698916\n",
      "Iteration 312, loss = 0.81586975\n",
      "Iteration 313, loss = 0.81475065\n",
      "Iteration 314, loss = 0.81363185\n",
      "Iteration 315, loss = 0.81251337\n",
      "Iteration 316, loss = 0.81139521\n",
      "Iteration 317, loss = 0.81027737\n",
      "Iteration 318, loss = 0.80915985\n",
      "Iteration 319, loss = 0.80804268\n",
      "Iteration 320, loss = 0.80692584\n",
      "Iteration 321, loss = 0.80580935\n",
      "Iteration 322, loss = 0.80469322\n",
      "Iteration 323, loss = 0.80357743\n",
      "Iteration 324, loss = 0.80246202\n",
      "Iteration 325, loss = 0.80134697\n",
      "Iteration 326, loss = 0.80023229\n",
      "Iteration 327, loss = 0.79911800\n",
      "Iteration 328, loss = 0.79800409\n",
      "Iteration 329, loss = 0.79689058\n",
      "Iteration 330, loss = 0.79577746\n",
      "Iteration 331, loss = 0.79466475\n",
      "Iteration 332, loss = 0.79355245\n",
      "Iteration 333, loss = 0.79244056\n",
      "Iteration 334, loss = 0.79132910\n",
      "Iteration 335, loss = 0.79021807\n",
      "Iteration 336, loss = 0.78910747\n",
      "Iteration 337, loss = 0.78799732\n",
      "Iteration 338, loss = 0.78688761\n",
      "Iteration 339, loss = 0.78577835\n",
      "Iteration 340, loss = 0.78466956\n",
      "Iteration 341, loss = 0.78356124\n",
      "Iteration 342, loss = 0.78245339\n",
      "Iteration 343, loss = 0.78134601\n",
      "Iteration 344, loss = 0.78023913\n",
      "Iteration 345, loss = 0.77913274\n",
      "Iteration 346, loss = 0.77802684\n",
      "Iteration 347, loss = 0.77692146\n",
      "Iteration 348, loss = 0.77581659\n",
      "Iteration 349, loss = 0.77471223\n",
      "Iteration 350, loss = 0.77360841\n",
      "Iteration 351, loss = 0.77250511\n",
      "Iteration 352, loss = 0.77140236\n",
      "Iteration 353, loss = 0.77030015\n",
      "Iteration 354, loss = 0.76919849\n",
      "Iteration 355, loss = 0.76809740\n",
      "Iteration 356, loss = 0.76699687\n",
      "Iteration 357, loss = 0.76589692\n",
      "Iteration 358, loss = 0.76479755\n",
      "Iteration 359, loss = 0.76369876\n",
      "Iteration 360, loss = 0.76260057\n",
      "Iteration 361, loss = 0.76150298\n",
      "Iteration 362, loss = 0.76040599\n",
      "Iteration 363, loss = 0.75930963\n",
      "Iteration 364, loss = 0.75821388\n",
      "Iteration 365, loss = 0.75711876\n",
      "Iteration 366, loss = 0.75602428\n",
      "Iteration 367, loss = 0.75493045\n",
      "Iteration 368, loss = 0.75383726\n",
      "Iteration 369, loss = 0.75274473\n",
      "Iteration 370, loss = 0.75165286\n",
      "Iteration 371, loss = 0.75056166\n",
      "Iteration 372, loss = 0.74947114\n",
      "Iteration 373, loss = 0.74838131\n",
      "Iteration 374, loss = 0.74729216\n",
      "Iteration 375, loss = 0.74620372\n",
      "Iteration 376, loss = 0.74511598\n",
      "Iteration 377, loss = 0.74402896\n",
      "Iteration 378, loss = 0.74294265\n",
      "Iteration 379, loss = 0.74185707\n",
      "Iteration 380, loss = 0.74077223\n",
      "Iteration 381, loss = 0.73968813\n",
      "Iteration 382, loss = 0.73860477\n",
      "Iteration 383, loss = 0.73752217\n",
      "Iteration 384, loss = 0.73644033\n",
      "Iteration 385, loss = 0.73535926\n",
      "Iteration 386, loss = 0.73427897\n",
      "Iteration 387, loss = 0.73319946\n",
      "Iteration 388, loss = 0.73212074\n",
      "Iteration 389, loss = 0.73104282\n",
      "Iteration 390, loss = 0.72996570\n",
      "Iteration 391, loss = 0.72888939\n",
      "Iteration 392, loss = 0.72781390\n",
      "Iteration 393, loss = 0.72673924\n",
      "Iteration 394, loss = 0.72566541\n",
      "Iteration 395, loss = 0.72459242\n",
      "Iteration 396, loss = 0.72352027\n",
      "Iteration 397, loss = 0.72244898\n",
      "Iteration 398, loss = 0.72137854\n",
      "Iteration 399, loss = 0.72030897\n",
      "Iteration 400, loss = 0.71924028\n",
      "Iteration 401, loss = 0.71817247\n",
      "Iteration 402, loss = 0.71710554\n",
      "Iteration 403, loss = 0.71603951\n",
      "Iteration 404, loss = 0.71497438\n",
      "Iteration 405, loss = 0.71391015\n",
      "Iteration 406, loss = 0.71284685\n",
      "Iteration 407, loss = 0.71178446\n",
      "Iteration 408, loss = 0.71072300\n",
      "Iteration 409, loss = 0.70966248\n",
      "Iteration 410, loss = 0.70860289\n",
      "Iteration 411, loss = 0.70754426\n",
      "Iteration 412, loss = 0.70648658\n",
      "Iteration 413, loss = 0.70542986\n",
      "Iteration 414, loss = 0.70437411\n",
      "Iteration 415, loss = 0.70331934\n",
      "Iteration 416, loss = 0.70226555\n",
      "Iteration 417, loss = 0.70121274\n",
      "Iteration 418, loss = 0.70016093\n",
      "Iteration 419, loss = 0.69911012\n",
      "Iteration 420, loss = 0.69806032\n",
      "Iteration 421, loss = 0.69701154\n",
      "Iteration 422, loss = 0.69596377\n",
      "Iteration 423, loss = 0.69491703\n",
      "Iteration 424, loss = 0.69387133\n",
      "Iteration 425, loss = 0.69282666\n",
      "Iteration 426, loss = 0.69178304\n",
      "Iteration 427, loss = 0.69074047\n",
      "Iteration 428, loss = 0.68969897\n",
      "Iteration 429, loss = 0.68865852\n",
      "Iteration 430, loss = 0.68761915\n",
      "Iteration 431, loss = 0.68658086\n",
      "Iteration 432, loss = 0.68554364\n",
      "Iteration 433, loss = 0.68450752\n",
      "Iteration 434, loss = 0.68347250\n",
      "Iteration 435, loss = 0.68243858\n",
      "Iteration 436, loss = 0.68140576\n",
      "Iteration 437, loss = 0.68037406\n",
      "Iteration 438, loss = 0.67934348\n",
      "Iteration 439, loss = 0.67831402\n",
      "Iteration 440, loss = 0.67728570\n",
      "Iteration 441, loss = 0.67625852\n",
      "Iteration 442, loss = 0.67523247\n",
      "Iteration 443, loss = 0.67420758\n",
      "Iteration 444, loss = 0.67318385\n",
      "Iteration 445, loss = 0.67216127\n",
      "Iteration 446, loss = 0.67113986\n",
      "Iteration 447, loss = 0.67011962\n",
      "Iteration 448, loss = 0.66910057\n",
      "Iteration 449, loss = 0.66808269\n",
      "Iteration 450, loss = 0.66706601\n",
      "Iteration 451, loss = 0.66605051\n",
      "Iteration 452, loss = 0.66503622\n",
      "Iteration 453, loss = 0.66402314\n",
      "Iteration 454, loss = 0.66301127\n",
      "Iteration 455, loss = 0.66200061\n",
      "Iteration 456, loss = 0.66099118\n",
      "Iteration 457, loss = 0.65998297\n",
      "Iteration 458, loss = 0.65897599\n",
      "Iteration 459, loss = 0.65797026\n",
      "Iteration 460, loss = 0.65696576\n",
      "Iteration 461, loss = 0.65596252\n",
      "Iteration 462, loss = 0.65496052\n",
      "Iteration 463, loss = 0.65395979\n",
      "Iteration 464, loss = 0.65296032\n",
      "Iteration 465, loss = 0.65196212\n",
      "Iteration 466, loss = 0.65096519\n",
      "Iteration 467, loss = 0.64996954\n",
      "Iteration 468, loss = 0.64897517\n",
      "Iteration 469, loss = 0.64798209\n",
      "Iteration 470, loss = 0.64699031\n",
      "Iteration 471, loss = 0.64599982\n",
      "Iteration 472, loss = 0.64501063\n",
      "Iteration 473, loss = 0.64402275\n",
      "Iteration 474, loss = 0.64303618\n",
      "Iteration 475, loss = 0.64205093\n",
      "Iteration 476, loss = 0.64106700\n",
      "Iteration 477, loss = 0.64008440\n",
      "Iteration 478, loss = 0.63910312\n",
      "Iteration 479, loss = 0.63812318\n",
      "Iteration 480, loss = 0.63714458\n",
      "Iteration 481, loss = 0.63616732\n",
      "Iteration 482, loss = 0.63519141\n",
      "Iteration 483, loss = 0.63421685\n",
      "Iteration 484, loss = 0.63324364\n",
      "Iteration 485, loss = 0.63227180\n",
      "Iteration 486, loss = 0.63130132\n",
      "Iteration 487, loss = 0.63033221\n",
      "Iteration 488, loss = 0.62936447\n",
      "Iteration 489, loss = 0.62839810\n",
      "Iteration 490, loss = 0.62743312\n",
      "Iteration 491, loss = 0.62646952\n",
      "Iteration 492, loss = 0.62550731\n",
      "Iteration 493, loss = 0.62454649\n",
      "Iteration 494, loss = 0.62358707\n",
      "Iteration 495, loss = 0.62262905\n",
      "Iteration 496, loss = 0.62167243\n",
      "Iteration 497, loss = 0.62071721\n",
      "Iteration 498, loss = 0.61976341\n",
      "Iteration 499, loss = 0.61881102\n",
      "Iteration 500, loss = 0.61786005\n",
      "Iteration 501, loss = 0.61691051\n",
      "Iteration 502, loss = 0.61596238\n",
      "Iteration 503, loss = 0.61501568\n",
      "Iteration 504, loss = 0.61407042\n",
      "Iteration 505, loss = 0.61312659\n",
      "Iteration 506, loss = 0.61218420\n",
      "Iteration 507, loss = 0.61124325\n",
      "Iteration 508, loss = 0.61030374\n",
      "Iteration 509, loss = 0.60936568\n",
      "Iteration 510, loss = 0.60842907\n",
      "Iteration 511, loss = 0.60749392\n",
      "Iteration 512, loss = 0.60656022\n",
      "Iteration 513, loss = 0.60562799\n",
      "Iteration 514, loss = 0.60469721\n",
      "Iteration 515, loss = 0.60376791\n",
      "Iteration 516, loss = 0.60284007\n",
      "Iteration 517, loss = 0.60191370\n",
      "Iteration 518, loss = 0.60098880\n",
      "Iteration 519, loss = 0.60006539\n",
      "Iteration 520, loss = 0.59914345\n",
      "Iteration 521, loss = 0.59822300\n",
      "Iteration 522, loss = 0.59730403\n",
      "Iteration 523, loss = 0.59638654\n",
      "Iteration 524, loss = 0.59547055\n",
      "Iteration 525, loss = 0.59455605\n",
      "Iteration 526, loss = 0.59364305\n",
      "Iteration 527, loss = 0.59273154\n",
      "Iteration 528, loss = 0.59182153\n",
      "Iteration 529, loss = 0.59091303\n",
      "Iteration 530, loss = 0.59000603\n",
      "Iteration 531, loss = 0.58910053\n",
      "Iteration 532, loss = 0.58819654\n",
      "Iteration 533, loss = 0.58729407\n",
      "Iteration 534, loss = 0.58639311\n",
      "Iteration 535, loss = 0.58549366\n",
      "Iteration 536, loss = 0.58459573\n",
      "Iteration 537, loss = 0.58369932\n",
      "Iteration 538, loss = 0.58280443\n",
      "Iteration 539, loss = 0.58191107\n",
      "Iteration 540, loss = 0.58101923\n",
      "Iteration 541, loss = 0.58012891\n",
      "Iteration 542, loss = 0.57924012\n",
      "Iteration 543, loss = 0.57835287\n",
      "Iteration 544, loss = 0.57746715\n",
      "Iteration 545, loss = 0.57658296\n",
      "Iteration 546, loss = 0.57570030\n",
      "Iteration 547, loss = 0.57481918\n",
      "Iteration 548, loss = 0.57393960\n",
      "Iteration 549, loss = 0.57306156\n",
      "Iteration 550, loss = 0.57218507\n",
      "Iteration 551, loss = 0.57131011\n",
      "Iteration 552, loss = 0.57043670\n",
      "Iteration 553, loss = 0.56956483\n",
      "Iteration 554, loss = 0.56869452\n",
      "Iteration 555, loss = 0.56782575\n",
      "Iteration 556, loss = 0.56695853\n",
      "Iteration 557, loss = 0.56609286\n",
      "Iteration 558, loss = 0.56522874\n",
      "Iteration 559, loss = 0.56436618\n",
      "Iteration 560, loss = 0.56350517\n",
      "Iteration 561, loss = 0.56264571\n",
      "Iteration 562, loss = 0.56178781\n",
      "Iteration 563, loss = 0.56093147\n",
      "Iteration 564, loss = 0.56007669\n",
      "Iteration 565, loss = 0.55922346\n",
      "Iteration 566, loss = 0.55837180\n",
      "Iteration 567, loss = 0.55752170\n",
      "Iteration 568, loss = 0.55667316\n",
      "Iteration 569, loss = 0.55582618\n",
      "Iteration 570, loss = 0.55498076\n",
      "Iteration 571, loss = 0.55413691\n",
      "Iteration 572, loss = 0.55329462\n",
      "Iteration 573, loss = 0.55245390\n",
      "Iteration 574, loss = 0.55161475\n",
      "Iteration 575, loss = 0.55077716\n",
      "Iteration 576, loss = 0.54994113\n",
      "Iteration 577, loss = 0.54910668\n",
      "Iteration 578, loss = 0.54827379\n",
      "Iteration 579, loss = 0.54744247\n",
      "Iteration 580, loss = 0.54661272\n",
      "Iteration 581, loss = 0.54578454\n",
      "Iteration 582, loss = 0.54495793\n",
      "Iteration 583, loss = 0.54413289\n",
      "Iteration 584, loss = 0.54330942\n",
      "Iteration 585, loss = 0.54248752\n",
      "Iteration 586, loss = 0.54166719\n",
      "Iteration 587, loss = 0.54084843\n",
      "Iteration 588, loss = 0.54003124\n",
      "Iteration 589, loss = 0.53921562\n",
      "Iteration 590, loss = 0.53840158\n",
      "Iteration 591, loss = 0.53758910\n",
      "Iteration 592, loss = 0.53677820\n",
      "Iteration 593, loss = 0.53596886\n",
      "Iteration 594, loss = 0.53516110\n",
      "Iteration 595, loss = 0.53435491\n",
      "Iteration 596, loss = 0.53355029\n",
      "Iteration 597, loss = 0.53274724\n",
      "Iteration 598, loss = 0.53194576\n",
      "Iteration 599, loss = 0.53114585\n",
      "Iteration 600, loss = 0.53034751\n",
      "Iteration 601, loss = 0.52955074\n",
      "Iteration 602, loss = 0.52875554\n",
      "Iteration 603, loss = 0.52796192\n",
      "Iteration 604, loss = 0.52716985\n",
      "Iteration 605, loss = 0.52637936\n",
      "Iteration 606, loss = 0.52559044\n",
      "Iteration 607, loss = 0.52480308\n",
      "Iteration 608, loss = 0.52401729\n",
      "Iteration 609, loss = 0.52323307\n",
      "Iteration 610, loss = 0.52245042\n",
      "Iteration 611, loss = 0.52166933\n",
      "Iteration 612, loss = 0.52088980\n",
      "Iteration 613, loss = 0.52011185\n",
      "Iteration 614, loss = 0.51933545\n",
      "Iteration 615, loss = 0.51856062\n",
      "Iteration 616, loss = 0.51778735\n",
      "Iteration 617, loss = 0.51701564\n",
      "Iteration 618, loss = 0.51624550\n",
      "Iteration 619, loss = 0.51547691\n",
      "Iteration 620, loss = 0.51470989\n",
      "Iteration 621, loss = 0.51394442\n",
      "Iteration 622, loss = 0.51318052\n",
      "Iteration 623, loss = 0.51241817\n",
      "Iteration 624, loss = 0.51165737\n",
      "Iteration 625, loss = 0.51089814\n",
      "Iteration 626, loss = 0.51014046\n",
      "Iteration 627, loss = 0.50938433\n",
      "Iteration 628, loss = 0.50862975\n",
      "Iteration 629, loss = 0.50787673\n",
      "Iteration 630, loss = 0.50712526\n",
      "Iteration 631, loss = 0.50637533\n",
      "Iteration 632, loss = 0.50562696\n",
      "Iteration 633, loss = 0.50488013\n",
      "Iteration 634, loss = 0.50413486\n",
      "Iteration 635, loss = 0.50339112\n",
      "Iteration 636, loss = 0.50264893\n",
      "Iteration 637, loss = 0.50190829\n",
      "Iteration 638, loss = 0.50116919\n",
      "Iteration 639, loss = 0.50043162\n",
      "Iteration 640, loss = 0.49969560\n",
      "Iteration 641, loss = 0.49896112\n",
      "Iteration 642, loss = 0.49822817\n",
      "Iteration 643, loss = 0.49749676\n",
      "Iteration 644, loss = 0.49676688\n",
      "Iteration 645, loss = 0.49603854\n",
      "Iteration 646, loss = 0.49531173\n",
      "Iteration 647, loss = 0.49458645\n",
      "Iteration 648, loss = 0.49386270\n",
      "Iteration 649, loss = 0.49314047\n",
      "Iteration 650, loss = 0.49241978\n",
      "Iteration 651, loss = 0.49170061\n",
      "Iteration 652, loss = 0.49098296\n",
      "Iteration 653, loss = 0.49026683\n",
      "Iteration 654, loss = 0.48955223\n",
      "Iteration 655, loss = 0.48883914\n",
      "Iteration 656, loss = 0.48812757\n",
      "Iteration 657, loss = 0.48741752\n",
      "Iteration 658, loss = 0.48670898\n",
      "Iteration 659, loss = 0.48600195\n",
      "Iteration 660, loss = 0.48529643\n",
      "Iteration 661, loss = 0.48459243\n",
      "Iteration 662, loss = 0.48388993\n",
      "Iteration 663, loss = 0.48318894\n",
      "Iteration 664, loss = 0.48248945\n",
      "Iteration 665, loss = 0.48179147\n",
      "Iteration 666, loss = 0.48109499\n",
      "Iteration 667, loss = 0.48040000\n",
      "Iteration 668, loss = 0.47970652\n",
      "Iteration 669, loss = 0.47901453\n",
      "Iteration 670, loss = 0.47832403\n",
      "Iteration 671, loss = 0.47763503\n",
      "Iteration 672, loss = 0.47694752\n",
      "Iteration 673, loss = 0.47626149\n",
      "Iteration 674, loss = 0.47557696\n",
      "Iteration 675, loss = 0.47489390\n",
      "Iteration 676, loss = 0.47421234\n",
      "Iteration 677, loss = 0.47353225\n",
      "Iteration 678, loss = 0.47285364\n",
      "Iteration 679, loss = 0.47217651\n",
      "Iteration 680, loss = 0.47150086\n",
      "Iteration 681, loss = 0.47082668\n",
      "Iteration 682, loss = 0.47015397\n",
      "Iteration 683, loss = 0.46948273\n",
      "Iteration 684, loss = 0.46881296\n",
      "Iteration 685, loss = 0.46814466\n",
      "Iteration 686, loss = 0.46747782\n",
      "Iteration 687, loss = 0.46681244\n",
      "Iteration 688, loss = 0.46614852\n",
      "Iteration 689, loss = 0.46548606\n",
      "Iteration 690, loss = 0.46482505\n",
      "Iteration 691, loss = 0.46416550\n",
      "Iteration 692, loss = 0.46350740\n",
      "Iteration 693, loss = 0.46285076\n",
      "Iteration 694, loss = 0.46219555\n",
      "Iteration 695, loss = 0.46154180\n",
      "Iteration 696, loss = 0.46088948\n",
      "Iteration 697, loss = 0.46023861\n",
      "Iteration 698, loss = 0.45958918\n",
      "Iteration 699, loss = 0.45894118\n",
      "Iteration 700, loss = 0.45829462\n",
      "Iteration 701, loss = 0.45764950\n",
      "Iteration 702, loss = 0.45700580\n",
      "Iteration 703, loss = 0.45636353\n",
      "Iteration 704, loss = 0.45572269\n",
      "Iteration 705, loss = 0.45508327\n",
      "Iteration 706, loss = 0.45444527\n",
      "Iteration 707, loss = 0.45380869\n",
      "Iteration 708, loss = 0.45317353\n",
      "Iteration 709, loss = 0.45253979\n",
      "Iteration 710, loss = 0.45190745\n",
      "Iteration 711, loss = 0.45127653\n",
      "Iteration 712, loss = 0.45064702\n",
      "Iteration 713, loss = 0.45001891\n",
      "Iteration 714, loss = 0.44939220\n",
      "Iteration 715, loss = 0.44876690\n",
      "Iteration 716, loss = 0.44814300\n",
      "Iteration 717, loss = 0.44752049\n",
      "Iteration 718, loss = 0.44689938\n",
      "Iteration 719, loss = 0.44627965\n",
      "Iteration 720, loss = 0.44566132\n",
      "Iteration 721, loss = 0.44504438\n",
      "Iteration 722, loss = 0.44442882\n",
      "Iteration 723, loss = 0.44381464\n",
      "Iteration 724, loss = 0.44320185\n",
      "Iteration 725, loss = 0.44259043\n",
      "Iteration 726, loss = 0.44198039\n",
      "Iteration 727, loss = 0.44137172\n",
      "Iteration 728, loss = 0.44076442\n",
      "Iteration 729, loss = 0.44015849\n",
      "Iteration 730, loss = 0.43955393\n",
      "Iteration 731, loss = 0.43895073\n",
      "Iteration 732, loss = 0.43834889\n",
      "Iteration 733, loss = 0.43774841\n",
      "Iteration 734, loss = 0.43714929\n",
      "Iteration 735, loss = 0.43655152\n",
      "Iteration 736, loss = 0.43595510\n",
      "Iteration 737, loss = 0.43536003\n",
      "Iteration 738, loss = 0.43476631\n",
      "Iteration 739, loss = 0.43417393\n",
      "Iteration 740, loss = 0.43358289\n",
      "Iteration 741, loss = 0.43299319\n",
      "Iteration 742, loss = 0.43240483\n",
      "Iteration 743, loss = 0.43181781\n",
      "Iteration 744, loss = 0.43123211\n",
      "Iteration 745, loss = 0.43064774\n",
      "Iteration 746, loss = 0.43006471\n",
      "Iteration 747, loss = 0.42948299\n",
      "Iteration 748, loss = 0.42890260\n",
      "Iteration 749, loss = 0.42832352\n",
      "Iteration 750, loss = 0.42774577\n",
      "Iteration 751, loss = 0.42716932\n",
      "Iteration 752, loss = 0.42659419\n",
      "Iteration 753, loss = 0.42602037\n",
      "Iteration 754, loss = 0.42544785\n",
      "Iteration 755, loss = 0.42487664\n",
      "Iteration 756, loss = 0.42430673\n",
      "Iteration 757, loss = 0.42373812\n",
      "Iteration 758, loss = 0.42317080\n",
      "Iteration 759, loss = 0.42260478\n",
      "Iteration 760, loss = 0.42204005\n",
      "Iteration 761, loss = 0.42147661\n",
      "Iteration 762, loss = 0.42091446\n",
      "Iteration 763, loss = 0.42035359\n",
      "Iteration 764, loss = 0.41979400\n",
      "Iteration 765, loss = 0.41923569\n",
      "Iteration 766, loss = 0.41867865\n",
      "Iteration 767, loss = 0.41812289\n",
      "Iteration 768, loss = 0.41756840\n",
      "Iteration 769, loss = 0.41701518\n",
      "Iteration 770, loss = 0.41646322\n",
      "Iteration 771, loss = 0.41591253\n",
      "Iteration 772, loss = 0.41536310\n",
      "Iteration 773, loss = 0.41481492\n",
      "Iteration 774, loss = 0.41426801\n",
      "Iteration 775, loss = 0.41372234\n",
      "Iteration 776, loss = 0.41317792\n",
      "Iteration 777, loss = 0.41263476\n",
      "Iteration 778, loss = 0.41209283\n",
      "Iteration 779, loss = 0.41155215\n",
      "Iteration 780, loss = 0.41101271\n",
      "Iteration 781, loss = 0.41047451\n",
      "Iteration 782, loss = 0.40993754\n",
      "Iteration 783, loss = 0.40940180\n",
      "Iteration 784, loss = 0.40886729\n",
      "Iteration 785, loss = 0.40833401\n",
      "Iteration 786, loss = 0.40780196\n",
      "Iteration 787, loss = 0.40727112\n",
      "Iteration 788, loss = 0.40674151\n",
      "Iteration 789, loss = 0.40621311\n",
      "Iteration 790, loss = 0.40568592\n",
      "Iteration 791, loss = 0.40515995\n",
      "Iteration 792, loss = 0.40463519\n",
      "Iteration 793, loss = 0.40411163\n",
      "Iteration 794, loss = 0.40358927\n",
      "Iteration 795, loss = 0.40306812\n",
      "Iteration 796, loss = 0.40254816\n",
      "Iteration 797, loss = 0.40202940\n",
      "Iteration 798, loss = 0.40151183\n",
      "Iteration 799, loss = 0.40099546\n",
      "Iteration 800, loss = 0.40048027\n",
      "Iteration 801, loss = 0.39996626\n",
      "Iteration 802, loss = 0.39945344\n",
      "Iteration 803, loss = 0.39894180\n",
      "Iteration 804, loss = 0.39843134\n",
      "Iteration 805, loss = 0.39792205\n",
      "Iteration 806, loss = 0.39741393\n",
      "Iteration 807, loss = 0.39690698\n",
      "Iteration 808, loss = 0.39640120\n",
      "Iteration 809, loss = 0.39589659\n",
      "Iteration 810, loss = 0.39539313\n",
      "Iteration 811, loss = 0.39489084\n",
      "Iteration 812, loss = 0.39438970\n",
      "Iteration 813, loss = 0.39388971\n",
      "Iteration 814, loss = 0.39339088\n",
      "Iteration 815, loss = 0.39289320\n",
      "Iteration 816, loss = 0.39239666\n",
      "Iteration 817, loss = 0.39190126\n",
      "Iteration 818, loss = 0.39140701\n",
      "Iteration 819, loss = 0.39091390\n",
      "Iteration 820, loss = 0.39042192\n",
      "Iteration 821, loss = 0.38993107\n",
      "Iteration 822, loss = 0.38944135\n",
      "Iteration 823, loss = 0.38895277\n",
      "Iteration 824, loss = 0.38846531\n",
      "Iteration 825, loss = 0.38797897\n",
      "Iteration 826, loss = 0.38749375\n",
      "Iteration 827, loss = 0.38700965\n",
      "Iteration 828, loss = 0.38652666\n",
      "Iteration 829, loss = 0.38604479\n",
      "Iteration 830, loss = 0.38556402\n",
      "Iteration 831, loss = 0.38508437\n",
      "Iteration 832, loss = 0.38460582\n",
      "Iteration 833, loss = 0.38412837\n",
      "Iteration 834, loss = 0.38365202\n",
      "Iteration 835, loss = 0.38317677\n",
      "Iteration 836, loss = 0.38270261\n",
      "Iteration 837, loss = 0.38222954\n",
      "Iteration 838, loss = 0.38175757\n",
      "Iteration 839, loss = 0.38128668\n",
      "Iteration 840, loss = 0.38081687\n",
      "Iteration 841, loss = 0.38034815\n",
      "Iteration 842, loss = 0.37988051\n",
      "Iteration 843, loss = 0.37941394\n",
      "Iteration 844, loss = 0.37894845\n",
      "Iteration 845, loss = 0.37848403\n",
      "Iteration 846, loss = 0.37802067\n",
      "Iteration 847, loss = 0.37755839\n",
      "Iteration 848, loss = 0.37709717\n",
      "Iteration 849, loss = 0.37663700\n",
      "Iteration 850, loss = 0.37617790\n",
      "Iteration 851, loss = 0.37571986\n",
      "Iteration 852, loss = 0.37526287\n",
      "Iteration 853, loss = 0.37480693\n",
      "Iteration 854, loss = 0.37435203\n",
      "Iteration 855, loss = 0.37389819\n",
      "Iteration 856, loss = 0.37344539\n",
      "Iteration 857, loss = 0.37299363\n",
      "Iteration 858, loss = 0.37254290\n",
      "Iteration 859, loss = 0.37209322\n",
      "Iteration 860, loss = 0.37164457\n",
      "Iteration 861, loss = 0.37119695\n",
      "Iteration 862, loss = 0.37075036\n",
      "Iteration 863, loss = 0.37030479\n",
      "Iteration 864, loss = 0.36986025\n",
      "Iteration 865, loss = 0.36941673\n",
      "Iteration 866, loss = 0.36897423\n",
      "Iteration 867, loss = 0.36853274\n",
      "Iteration 868, loss = 0.36809227\n",
      "Iteration 869, loss = 0.36765281\n",
      "Iteration 870, loss = 0.36721436\n",
      "Iteration 871, loss = 0.36677691\n",
      "Iteration 872, loss = 0.36634047\n",
      "Iteration 873, loss = 0.36590503\n",
      "Iteration 874, loss = 0.36547059\n",
      "Iteration 875, loss = 0.36503714\n",
      "Iteration 876, loss = 0.36460469\n",
      "Iteration 877, loss = 0.36417323\n",
      "Iteration 878, loss = 0.36374277\n",
      "Iteration 879, loss = 0.36331328\n",
      "Iteration 880, loss = 0.36288478\n",
      "Iteration 881, loss = 0.36245727\n",
      "Iteration 882, loss = 0.36203073\n",
      "Iteration 883, loss = 0.36160517\n",
      "Iteration 884, loss = 0.36118058\n",
      "Iteration 885, loss = 0.36075697\n",
      "Iteration 886, loss = 0.36033433\n",
      "Iteration 887, loss = 0.35991265\n",
      "Iteration 888, loss = 0.35949194\n",
      "Iteration 889, loss = 0.35907219\n",
      "Iteration 890, loss = 0.35865340\n",
      "Iteration 891, loss = 0.35823557\n",
      "Iteration 892, loss = 0.35781870\n",
      "Iteration 893, loss = 0.35740277\n",
      "Iteration 894, loss = 0.35698780\n",
      "Iteration 895, loss = 0.35657378\n",
      "Iteration 896, loss = 0.35616070\n",
      "Iteration 897, loss = 0.35574856\n",
      "Iteration 898, loss = 0.35533737\n",
      "Iteration 899, loss = 0.35492712\n",
      "Iteration 900, loss = 0.35451780\n",
      "Iteration 901, loss = 0.35410941\n",
      "Iteration 902, loss = 0.35370196\n",
      "Iteration 903, loss = 0.35329544\n",
      "Iteration 904, loss = 0.35288984\n",
      "Iteration 905, loss = 0.35248517\n",
      "Iteration 906, loss = 0.35208142\n",
      "Iteration 907, loss = 0.35167859\n",
      "Iteration 908, loss = 0.35127668\n",
      "Iteration 909, loss = 0.35087568\n",
      "Iteration 910, loss = 0.35047560\n",
      "Iteration 911, loss = 0.35007642\n",
      "Iteration 912, loss = 0.34967816\n",
      "Iteration 913, loss = 0.34928080\n",
      "Iteration 914, loss = 0.34888434\n",
      "Iteration 915, loss = 0.34848879\n",
      "Iteration 916, loss = 0.34809413\n",
      "Iteration 917, loss = 0.34770038\n",
      "Iteration 918, loss = 0.34730751\n",
      "Iteration 919, loss = 0.34691554\n",
      "Iteration 920, loss = 0.34652446\n",
      "Iteration 921, loss = 0.34613427\n",
      "Iteration 922, loss = 0.34574496\n",
      "Iteration 923, loss = 0.34535653\n",
      "Iteration 924, loss = 0.34496899\n",
      "Iteration 925, loss = 0.34458233\n",
      "Iteration 926, loss = 0.34419654\n",
      "Iteration 927, loss = 0.34381162\n",
      "Iteration 928, loss = 0.34342758\n",
      "Iteration 929, loss = 0.34304441\n",
      "Iteration 930, loss = 0.34266210\n",
      "Iteration 931, loss = 0.34228066\n",
      "Iteration 932, loss = 0.34190008\n",
      "Iteration 933, loss = 0.34152037\n",
      "Iteration 934, loss = 0.34114151\n",
      "Iteration 935, loss = 0.34076351\n",
      "Iteration 936, loss = 0.34038636\n",
      "Iteration 937, loss = 0.34001007\n",
      "Iteration 938, loss = 0.33963462\n",
      "Iteration 939, loss = 0.33926002\n",
      "Iteration 940, loss = 0.33888627\n",
      "Iteration 941, loss = 0.33851336\n",
      "Iteration 942, loss = 0.33814129\n",
      "Iteration 943, loss = 0.33777006\n",
      "Iteration 944, loss = 0.33739967\n",
      "Iteration 945, loss = 0.33703011\n",
      "Iteration 946, loss = 0.33666138\n",
      "Iteration 947, loss = 0.33629349\n",
      "Iteration 948, loss = 0.33592642\n",
      "Iteration 949, loss = 0.33556018\n",
      "Iteration 950, loss = 0.33519476\n",
      "Iteration 951, loss = 0.33483016\n",
      "Iteration 952, loss = 0.33446639\n",
      "Iteration 953, loss = 0.33410343\n",
      "Iteration 954, loss = 0.33374128\n",
      "Iteration 955, loss = 0.33337995\n",
      "Iteration 956, loss = 0.33301943\n",
      "Iteration 957, loss = 0.33265972\n",
      "Iteration 958, loss = 0.33230081\n",
      "Iteration 959, loss = 0.33194271\n",
      "Iteration 960, loss = 0.33158541\n",
      "Iteration 961, loss = 0.33122892\n",
      "Iteration 962, loss = 0.33087322\n",
      "Iteration 963, loss = 0.33051832\n",
      "Iteration 964, loss = 0.33016421\n",
      "Iteration 965, loss = 0.32981089\n",
      "Iteration 966, loss = 0.32945837\n",
      "Iteration 967, loss = 0.32910663\n",
      "Iteration 968, loss = 0.32875568\n",
      "Iteration 969, loss = 0.32840551\n",
      "Iteration 970, loss = 0.32805612\n",
      "Iteration 971, loss = 0.32770751\n",
      "Iteration 972, loss = 0.32735969\n",
      "Iteration 973, loss = 0.32701263\n",
      "Iteration 974, loss = 0.32666635\n",
      "Iteration 975, loss = 0.32632085\n",
      "Iteration 976, loss = 0.32597611\n",
      "Iteration 977, loss = 0.32563214\n",
      "Iteration 978, loss = 0.32528893\n",
      "Iteration 979, loss = 0.32494649\n",
      "Iteration 980, loss = 0.32460481\n",
      "Iteration 981, loss = 0.32426389\n",
      "Iteration 982, loss = 0.32392373\n",
      "Iteration 983, loss = 0.32358432\n",
      "Iteration 984, loss = 0.32324567\n",
      "Iteration 985, loss = 0.32290777\n",
      "Iteration 986, loss = 0.32257062\n",
      "Iteration 987, loss = 0.32223421\n",
      "Iteration 988, loss = 0.32189855\n",
      "Iteration 989, loss = 0.32156364\n",
      "Iteration 990, loss = 0.32122947\n",
      "Iteration 991, loss = 0.32089604\n",
      "Iteration 992, loss = 0.32056334\n",
      "Iteration 993, loss = 0.32023138\n",
      "Iteration 994, loss = 0.31990016\n",
      "Iteration 995, loss = 0.31956967\n",
      "Iteration 996, loss = 0.31923990\n",
      "Iteration 997, loss = 0.31891087\n",
      "Iteration 998, loss = 0.31858256\n",
      "Iteration 999, loss = 0.31825498\n",
      "Iteration 1000, loss = 0.31792812\n",
      "Iteration 1001, loss = 0.31760198\n",
      "Iteration 1002, loss = 0.31727656\n",
      "Iteration 1003, loss = 0.31695185\n",
      "Iteration 1004, loss = 0.31662786\n",
      "Iteration 1005, loss = 0.31630459\n",
      "Iteration 1006, loss = 0.31598202\n",
      "Iteration 1007, loss = 0.31566017\n",
      "Iteration 1008, loss = 0.31533902\n",
      "Iteration 1009, loss = 0.31501857\n",
      "Iteration 1010, loss = 0.31469883\n",
      "Iteration 1011, loss = 0.31437980\n",
      "Iteration 1012, loss = 0.31406146\n",
      "Iteration 1013, loss = 0.31374382\n",
      "Iteration 1014, loss = 0.31342688\n",
      "Iteration 1015, loss = 0.31311063\n",
      "Iteration 1016, loss = 0.31279507\n",
      "Iteration 1017, loss = 0.31248021\n",
      "Iteration 1018, loss = 0.31216603\n",
      "Iteration 1019, loss = 0.31185254\n",
      "Iteration 1020, loss = 0.31153974\n",
      "Iteration 1021, loss = 0.31122762\n",
      "Iteration 1022, loss = 0.31091618\n",
      "Iteration 1023, loss = 0.31060542\n",
      "Iteration 1024, loss = 0.31029534\n",
      "Iteration 1025, loss = 0.30998594\n",
      "Iteration 1026, loss = 0.30967721\n",
      "Iteration 1027, loss = 0.30936915\n",
      "Iteration 1028, loss = 0.30906177\n",
      "Iteration 1029, loss = 0.30875505\n",
      "Iteration 1030, loss = 0.30844900\n",
      "Iteration 1031, loss = 0.30814362\n",
      "Iteration 1032, loss = 0.30783890\n",
      "Iteration 1033, loss = 0.30753484\n",
      "Iteration 1034, loss = 0.30723144\n",
      "Iteration 1035, loss = 0.30692870\n",
      "Iteration 1036, loss = 0.30662662\n",
      "Iteration 1037, loss = 0.30632520\n",
      "Iteration 1038, loss = 0.30602442\n",
      "Iteration 1039, loss = 0.30572430\n",
      "Iteration 1040, loss = 0.30542483\n",
      "Iteration 1041, loss = 0.30512601\n",
      "Iteration 1042, loss = 0.30482783\n",
      "Iteration 1043, loss = 0.30453030\n",
      "Iteration 1044, loss = 0.30423341\n",
      "Iteration 1045, loss = 0.30393716\n",
      "Iteration 1046, loss = 0.30364155\n",
      "Iteration 1047, loss = 0.30334658\n",
      "Iteration 1048, loss = 0.30305225\n",
      "Iteration 1049, loss = 0.30275855\n",
      "Iteration 1050, loss = 0.30246549\n",
      "Iteration 1051, loss = 0.30217305\n",
      "Iteration 1052, loss = 0.30188125\n",
      "Iteration 1053, loss = 0.30159007\n",
      "Iteration 1054, loss = 0.30129952\n",
      "Iteration 1055, loss = 0.30100959\n",
      "Iteration 1056, loss = 0.30072029\n",
      "Iteration 1057, loss = 0.30043161\n",
      "Iteration 1058, loss = 0.30014354\n",
      "Iteration 1059, loss = 0.29985610\n",
      "Iteration 1060, loss = 0.29956927\n",
      "Iteration 1061, loss = 0.29928306\n",
      "Iteration 1062, loss = 0.29899746\n",
      "Iteration 1063, loss = 0.29871247\n",
      "Iteration 1064, loss = 0.29842809\n",
      "Iteration 1065, loss = 0.29814432\n",
      "Iteration 1066, loss = 0.29786116\n",
      "Iteration 1067, loss = 0.29757860\n",
      "Iteration 1068, loss = 0.29729665\n",
      "Iteration 1069, loss = 0.29701529\n",
      "Iteration 1070, loss = 0.29673454\n",
      "Iteration 1071, loss = 0.29645438\n",
      "Iteration 1072, loss = 0.29617483\n",
      "Iteration 1073, loss = 0.29589587\n",
      "Iteration 1074, loss = 0.29561750\n",
      "Iteration 1075, loss = 0.29533972\n",
      "Iteration 1076, loss = 0.29506254\n",
      "Iteration 1077, loss = 0.29478594\n",
      "Iteration 1078, loss = 0.29450993\n",
      "Iteration 1079, loss = 0.29423451\n",
      "Iteration 1080, loss = 0.29395968\n",
      "Iteration 1081, loss = 0.29368542\n",
      "Iteration 1082, loss = 0.29341175\n",
      "Iteration 1083, loss = 0.29313866\n",
      "Iteration 1084, loss = 0.29286614\n",
      "Iteration 1085, loss = 0.29259420\n",
      "Iteration 1086, loss = 0.29232284\n",
      "Iteration 1087, loss = 0.29205205\n",
      "Iteration 1088, loss = 0.29178184\n",
      "Iteration 1089, loss = 0.29151219\n",
      "Iteration 1090, loss = 0.29124312\n",
      "Iteration 1091, loss = 0.29097461\n",
      "Iteration 1092, loss = 0.29070667\n",
      "Iteration 1093, loss = 0.29043929\n",
      "Iteration 1094, loss = 0.29017248\n",
      "Iteration 1095, loss = 0.28990622\n",
      "Iteration 1096, loss = 0.28964053\n",
      "Iteration 1097, loss = 0.28937540\n",
      "Iteration 1098, loss = 0.28911083\n",
      "Iteration 1099, loss = 0.28884681\n",
      "Iteration 1100, loss = 0.28858334\n",
      "Iteration 1101, loss = 0.28832043\n",
      "Iteration 1102, loss = 0.28805807\n",
      "Iteration 1103, loss = 0.28779626\n",
      "Iteration 1104, loss = 0.28753500\n",
      "Iteration 1105, loss = 0.28727428\n",
      "Iteration 1106, loss = 0.28701411\n",
      "Iteration 1107, loss = 0.28675449\n",
      "Iteration 1108, loss = 0.28649541\n",
      "Iteration 1109, loss = 0.28623687\n",
      "Iteration 1110, loss = 0.28597887\n",
      "Iteration 1111, loss = 0.28572140\n",
      "Iteration 1112, loss = 0.28546448\n",
      "Iteration 1113, loss = 0.28520809\n",
      "Iteration 1114, loss = 0.28495223\n",
      "Iteration 1115, loss = 0.28469691\n",
      "Iteration 1116, loss = 0.28444212\n",
      "Iteration 1117, loss = 0.28418786\n",
      "Iteration 1118, loss = 0.28393413\n",
      "Iteration 1119, loss = 0.28368093\n",
      "Iteration 1120, loss = 0.28342825\n",
      "Iteration 1121, loss = 0.28317610\n",
      "Iteration 1122, loss = 0.28292446\n",
      "Iteration 1123, loss = 0.28267336\n",
      "Iteration 1124, loss = 0.28242277\n",
      "Iteration 1125, loss = 0.28217270\n",
      "Iteration 1126, loss = 0.28192315\n",
      "Iteration 1127, loss = 0.28167411\n",
      "Iteration 1128, loss = 0.28142559\n",
      "Iteration 1129, loss = 0.28117758\n",
      "Iteration 1130, loss = 0.28093009\n",
      "Iteration 1131, loss = 0.28068311\n",
      "Iteration 1132, loss = 0.28043663\n",
      "Iteration 1133, loss = 0.28019067\n",
      "Iteration 1134, loss = 0.27994521\n",
      "Iteration 1135, loss = 0.27970026\n",
      "Iteration 1136, loss = 0.27945581\n",
      "Iteration 1137, loss = 0.27921186\n",
      "Iteration 1138, loss = 0.27896842\n",
      "Iteration 1139, loss = 0.27872547\n",
      "Iteration 1140, loss = 0.27848303\n",
      "Iteration 1141, loss = 0.27824108\n",
      "Iteration 1142, loss = 0.27799963\n",
      "Iteration 1143, loss = 0.27775867\n",
      "Iteration 1144, loss = 0.27751821\n",
      "Iteration 1145, loss = 0.27727824\n",
      "Iteration 1146, loss = 0.27703876\n",
      "Iteration 1147, loss = 0.27679977\n",
      "Iteration 1148, loss = 0.27656127\n",
      "Iteration 1149, loss = 0.27632326\n",
      "Iteration 1150, loss = 0.27608574\n",
      "Iteration 1151, loss = 0.27584869\n",
      "Iteration 1152, loss = 0.27561214\n",
      "Iteration 1153, loss = 0.27537606\n",
      "Iteration 1154, loss = 0.27514047\n",
      "Iteration 1155, loss = 0.27490535\n",
      "Iteration 1156, loss = 0.27467071\n",
      "Iteration 1157, loss = 0.27443655\n",
      "Iteration 1158, loss = 0.27420287\n",
      "Iteration 1159, loss = 0.27396966\n",
      "Iteration 1160, loss = 0.27373693\n",
      "Iteration 1161, loss = 0.27350467\n",
      "Iteration 1162, loss = 0.27327287\n",
      "Iteration 1163, loss = 0.27304155\n",
      "Iteration 1164, loss = 0.27281070\n",
      "Iteration 1165, loss = 0.27258031\n",
      "Iteration 1166, loss = 0.27235039\n",
      "Iteration 1167, loss = 0.27212094\n",
      "Iteration 1168, loss = 0.27189195\n",
      "Iteration 1169, loss = 0.27166342\n",
      "Iteration 1170, loss = 0.27143535\n",
      "Iteration 1171, loss = 0.27120775\n",
      "Iteration 1172, loss = 0.27098060\n",
      "Iteration 1173, loss = 0.27075391\n",
      "Iteration 1174, loss = 0.27052767\n",
      "Iteration 1175, loss = 0.27030190\n",
      "Iteration 1176, loss = 0.27007657\n",
      "Iteration 1177, loss = 0.26985170\n",
      "Iteration 1178, loss = 0.26962728\n",
      "Iteration 1179, loss = 0.26940331\n",
      "Iteration 1180, loss = 0.26917980\n",
      "Iteration 1181, loss = 0.26895672\n",
      "Iteration 1182, loss = 0.26873410\n",
      "Iteration 1183, loss = 0.26851192\n",
      "Iteration 1184, loss = 0.26829019\n",
      "Iteration 1185, loss = 0.26806890\n",
      "Iteration 1186, loss = 0.26784806\n",
      "Iteration 1187, loss = 0.26762765\n",
      "Iteration 1188, loss = 0.26740769\n",
      "Iteration 1189, loss = 0.26718816\n",
      "Iteration 1190, loss = 0.26696907\n",
      "Iteration 1191, loss = 0.26675042\n",
      "Iteration 1192, loss = 0.26653221\n",
      "Iteration 1193, loss = 0.26631443\n",
      "Iteration 1194, loss = 0.26609708\n",
      "Iteration 1195, loss = 0.26588017\n",
      "Iteration 1196, loss = 0.26566368\n",
      "Iteration 1197, loss = 0.26544763\n",
      "Iteration 1198, loss = 0.26523201\n",
      "Iteration 1199, loss = 0.26501681\n",
      "Iteration 1200, loss = 0.26480204\n",
      "Iteration 1201, loss = 0.26458770\n",
      "Iteration 1202, loss = 0.26437378\n",
      "Iteration 1203, loss = 0.26416028\n",
      "Iteration 1204, loss = 0.26394721\n",
      "Iteration 1205, loss = 0.26373456\n",
      "Iteration 1206, loss = 0.26352233\n",
      "Iteration 1207, loss = 0.26331051\n",
      "Iteration 1208, loss = 0.26309912\n",
      "Iteration 1209, loss = 0.26288814\n",
      "Iteration 1210, loss = 0.26267758\n",
      "Iteration 1211, loss = 0.26246743\n",
      "Iteration 1212, loss = 0.26225770\n",
      "Iteration 1213, loss = 0.26204838\n",
      "Iteration 1214, loss = 0.26183947\n",
      "Iteration 1215, loss = 0.26163098\n",
      "Iteration 1216, loss = 0.26142289\n",
      "Iteration 1217, loss = 0.26121521\n",
      "Iteration 1218, loss = 0.26100794\n",
      "Iteration 1219, loss = 0.26080107\n",
      "Iteration 1220, loss = 0.26059461\n",
      "Iteration 1221, loss = 0.26038856\n",
      "Iteration 1222, loss = 0.26018290\n",
      "Iteration 1223, loss = 0.25997765\n",
      "Iteration 1224, loss = 0.25977281\n",
      "Iteration 1225, loss = 0.25956836\n",
      "Iteration 1226, loss = 0.25936431\n",
      "Iteration 1227, loss = 0.25916066\n",
      "Iteration 1228, loss = 0.25895740\n",
      "Iteration 1229, loss = 0.25875455\n",
      "Iteration 1230, loss = 0.25855208\n",
      "Iteration 1231, loss = 0.25835001\n",
      "Iteration 1232, loss = 0.25814834\n",
      "Iteration 1233, loss = 0.25794706\n",
      "Iteration 1234, loss = 0.25774616\n",
      "Iteration 1235, loss = 0.25754566\n",
      "Iteration 1236, loss = 0.25734555\n",
      "Iteration 1237, loss = 0.25714582\n",
      "Iteration 1238, loss = 0.25694649\n",
      "Iteration 1239, loss = 0.25674754\n",
      "Iteration 1240, loss = 0.25654897\n",
      "Iteration 1241, loss = 0.25635079\n",
      "Iteration 1242, loss = 0.25615299\n",
      "Iteration 1243, loss = 0.25595557\n",
      "Iteration 1244, loss = 0.25575854\n",
      "Iteration 1245, loss = 0.25556188\n",
      "Iteration 1246, loss = 0.25536561\n",
      "Iteration 1247, loss = 0.25516971\n",
      "Iteration 1248, loss = 0.25497419\n",
      "Iteration 1249, loss = 0.25477905\n",
      "Iteration 1250, loss = 0.25458428\n",
      "Iteration 1251, loss = 0.25438989\n",
      "Iteration 1252, loss = 0.25419587\n",
      "Iteration 1253, loss = 0.25400222\n",
      "Iteration 1254, loss = 0.25380895\n",
      "Iteration 1255, loss = 0.25361604\n",
      "Iteration 1256, loss = 0.25342351\n",
      "Iteration 1257, loss = 0.25323135\n",
      "Iteration 1258, loss = 0.25303955\n",
      "Iteration 1259, loss = 0.25284812\n",
      "Iteration 1260, loss = 0.25265705\n",
      "Iteration 1261, loss = 0.25246636\n",
      "Iteration 1262, loss = 0.25227602\n",
      "Iteration 1263, loss = 0.25208605\n",
      "Iteration 1264, loss = 0.25189644\n",
      "Iteration 1265, loss = 0.25170719\n",
      "Iteration 1266, loss = 0.25151831\n",
      "Iteration 1267, loss = 0.25132978\n",
      "Iteration 1268, loss = 0.25114161\n",
      "Iteration 1269, loss = 0.25095380\n",
      "Iteration 1270, loss = 0.25076635\n",
      "Iteration 1271, loss = 0.25057925\n",
      "Iteration 1272, loss = 0.25039251\n",
      "Iteration 1273, loss = 0.25020612\n",
      "Iteration 1274, loss = 0.25002009\n",
      "Iteration 1275, loss = 0.24983441\n",
      "Iteration 1276, loss = 0.24964908\n",
      "Iteration 1277, loss = 0.24946410\n",
      "Iteration 1278, loss = 0.24927947\n",
      "Iteration 1279, loss = 0.24909519\n",
      "Iteration 1280, loss = 0.24891126\n",
      "Iteration 1281, loss = 0.24872768\n",
      "Iteration 1282, loss = 0.24854444\n",
      "Iteration 1283, loss = 0.24836155\n",
      "Iteration 1284, loss = 0.24817900\n",
      "Iteration 1285, loss = 0.24799680\n",
      "Iteration 1286, loss = 0.24781493\n",
      "Iteration 1287, loss = 0.24763342\n",
      "Iteration 1288, loss = 0.24745224\n",
      "Iteration 1289, loss = 0.24727140\n",
      "Iteration 1290, loss = 0.24709090\n",
      "Iteration 1291, loss = 0.24691075\n",
      "Iteration 1292, loss = 0.24673092\n",
      "Iteration 1293, loss = 0.24655144\n",
      "Iteration 1294, loss = 0.24637229\n",
      "Iteration 1295, loss = 0.24619348\n",
      "Iteration 1296, loss = 0.24601500\n",
      "Iteration 1297, loss = 0.24583686\n",
      "Iteration 1298, loss = 0.24565905\n",
      "Iteration 1299, loss = 0.24548157\n",
      "Iteration 1300, loss = 0.24530442\n",
      "Iteration 1301, loss = 0.24512760\n",
      "Iteration 1302, loss = 0.24495111\n",
      "Iteration 1303, loss = 0.24477495\n",
      "Iteration 1304, loss = 0.24459912\n",
      "Iteration 1305, loss = 0.24442362\n",
      "Iteration 1306, loss = 0.24424844\n",
      "Iteration 1307, loss = 0.24407359\n",
      "Iteration 1308, loss = 0.24389906\n",
      "Iteration 1309, loss = 0.24372486\n",
      "Iteration 1310, loss = 0.24355098\n",
      "Iteration 1311, loss = 0.24337742\n",
      "Iteration 1312, loss = 0.24320418\n",
      "Iteration 1313, loss = 0.24303126\n",
      "Iteration 1314, loss = 0.24285867\n",
      "Iteration 1315, loss = 0.24268639\n",
      "Iteration 1316, loss = 0.24251443\n",
      "Iteration 1317, loss = 0.24234279\n",
      "Iteration 1318, loss = 0.24217146\n",
      "Iteration 1319, loss = 0.24200046\n",
      "Iteration 1320, loss = 0.24182976\n",
      "Iteration 1321, loss = 0.24165938\n",
      "Iteration 1322, loss = 0.24148932\n",
      "Iteration 1323, loss = 0.24131957\n",
      "Iteration 1324, loss = 0.24115012\n",
      "Iteration 1325, loss = 0.24098100\n",
      "Iteration 1326, loss = 0.24081218\n",
      "Iteration 1327, loss = 0.24064367\n",
      "Iteration 1328, loss = 0.24047547\n",
      "Iteration 1329, loss = 0.24030758\n",
      "Iteration 1330, loss = 0.24013999\n",
      "Iteration 1331, loss = 0.23997272\n",
      "Iteration 1332, loss = 0.23980575\n",
      "Iteration 1333, loss = 0.23963908\n",
      "Iteration 1334, loss = 0.23947272\n",
      "Iteration 1335, loss = 0.23930666\n",
      "Iteration 1336, loss = 0.23914091\n",
      "Iteration 1337, loss = 0.23897545\n",
      "Iteration 1338, loss = 0.23881030\n",
      "Iteration 1339, loss = 0.23864545\n",
      "Iteration 1340, loss = 0.23848090\n",
      "Iteration 1341, loss = 0.23831665\n",
      "Iteration 1342, loss = 0.23815270\n",
      "Iteration 1343, loss = 0.23798905\n",
      "Iteration 1344, loss = 0.23782569\n",
      "Iteration 1345, loss = 0.23766263\n",
      "Iteration 1346, loss = 0.23749987\n",
      "Iteration 1347, loss = 0.23733740\n",
      "Iteration 1348, loss = 0.23717522\n",
      "Iteration 1349, loss = 0.23701334\n",
      "Iteration 1350, loss = 0.23685175\n",
      "Iteration 1351, loss = 0.23669046\n",
      "Iteration 1352, loss = 0.23652945\n",
      "Iteration 1353, loss = 0.23636874\n",
      "Iteration 1354, loss = 0.23620831\n",
      "Iteration 1355, loss = 0.23604818\n",
      "Iteration 1356, loss = 0.23588833\n",
      "Iteration 1357, loss = 0.23572878\n",
      "Iteration 1358, loss = 0.23556950\n",
      "Iteration 1359, loss = 0.23541052\n",
      "Iteration 1360, loss = 0.23525182\n",
      "Iteration 1361, loss = 0.23509341\n",
      "Iteration 1362, loss = 0.23493528\n",
      "Iteration 1363, loss = 0.23477743\n",
      "Iteration 1364, loss = 0.23461987\n",
      "Iteration 1365, loss = 0.23446259\n",
      "Iteration 1366, loss = 0.23430559\n",
      "Iteration 1367, loss = 0.23414888\n",
      "Iteration 1368, loss = 0.23399244\n",
      "Iteration 1369, loss = 0.23383628\n",
      "Iteration 1370, loss = 0.23368041\n",
      "Iteration 1371, loss = 0.23352481\n",
      "Iteration 1372, loss = 0.23336949\n",
      "Iteration 1373, loss = 0.23321444\n",
      "Iteration 1374, loss = 0.23305967\n",
      "Iteration 1375, loss = 0.23290518\n",
      "Iteration 1376, loss = 0.23275096\n",
      "Iteration 1377, loss = 0.23259702\n",
      "Iteration 1378, loss = 0.23244335\n",
      "Iteration 1379, loss = 0.23228995\n",
      "Iteration 1380, loss = 0.23213683\n",
      "Iteration 1381, loss = 0.23198398\n",
      "Iteration 1382, loss = 0.23183139\n",
      "Iteration 1383, loss = 0.23167908\n",
      "Iteration 1384, loss = 0.23152704\n",
      "Iteration 1385, loss = 0.23137527\n",
      "Iteration 1386, loss = 0.23122377\n",
      "Iteration 1387, loss = 0.23107253\n",
      "Iteration 1388, loss = 0.23092156\n",
      "Iteration 1389, loss = 0.23077086\n",
      "Iteration 1390, loss = 0.23062042\n",
      "Iteration 1391, loss = 0.23047025\n",
      "Iteration 1392, loss = 0.23032035\n",
      "Iteration 1393, loss = 0.23017071\n",
      "Iteration 1394, loss = 0.23002133\n",
      "Iteration 1395, loss = 0.22987221\n",
      "Iteration 1396, loss = 0.22972336\n",
      "Iteration 1397, loss = 0.22957477\n",
      "Iteration 1398, loss = 0.22942644\n",
      "Iteration 1399, loss = 0.22927837\n",
      "Iteration 1400, loss = 0.22913056\n",
      "Iteration 1401, loss = 0.22898300\n",
      "Iteration 1402, loss = 0.22883571\n",
      "Iteration 1403, loss = 0.22868867\n",
      "Iteration 1404, loss = 0.22854190\n",
      "Iteration 1405, loss = 0.22839537\n",
      "Iteration 1406, loss = 0.22824911\n",
      "Iteration 1407, loss = 0.22810310\n",
      "Iteration 1408, loss = 0.22795734\n",
      "Iteration 1409, loss = 0.22781184\n",
      "Iteration 1410, loss = 0.22766659\n",
      "Iteration 1411, loss = 0.22752160\n",
      "Iteration 1412, loss = 0.22737686\n",
      "Iteration 1413, loss = 0.22723237\n",
      "Iteration 1414, loss = 0.22708813\n",
      "Iteration 1415, loss = 0.22694414\n",
      "Iteration 1416, loss = 0.22680040\n",
      "Iteration 1417, loss = 0.22665691\n",
      "Iteration 1418, loss = 0.22651367\n",
      "Iteration 1419, loss = 0.22637068\n",
      "Iteration 1420, loss = 0.22622794\n",
      "Iteration 1421, loss = 0.22608544\n",
      "Iteration 1422, loss = 0.22594319\n",
      "Iteration 1423, loss = 0.22580118\n",
      "Iteration 1424, loss = 0.22565942\n",
      "Iteration 1425, loss = 0.22551791\n",
      "Iteration 1426, loss = 0.22537664\n",
      "Iteration 1427, loss = 0.22523561\n",
      "Iteration 1428, loss = 0.22509483\n",
      "Iteration 1429, loss = 0.22495429\n",
      "Iteration 1430, loss = 0.22481399\n",
      "Iteration 1431, loss = 0.22467393\n",
      "Iteration 1432, loss = 0.22453411\n",
      "Iteration 1433, loss = 0.22439454\n",
      "Iteration 1434, loss = 0.22425520\n",
      "Iteration 1435, loss = 0.22411610\n",
      "Iteration 1436, loss = 0.22397724\n",
      "Iteration 1437, loss = 0.22383862\n",
      "Iteration 1438, loss = 0.22370024\n",
      "Iteration 1439, loss = 0.22356209\n",
      "Iteration 1440, loss = 0.22342418\n",
      "Iteration 1441, loss = 0.22328651\n",
      "Iteration 1442, loss = 0.22314907\n",
      "Iteration 1443, loss = 0.22301186\n",
      "Iteration 1444, loss = 0.22287489\n",
      "Iteration 1445, loss = 0.22273816\n",
      "Iteration 1446, loss = 0.22260165\n",
      "Iteration 1447, loss = 0.22246538\n",
      "Iteration 1448, loss = 0.22232934\n",
      "Iteration 1449, loss = 0.22219353\n",
      "Iteration 1450, loss = 0.22205796\n",
      "Iteration 1451, loss = 0.22192261\n",
      "Iteration 1452, loss = 0.22178749\n",
      "Iteration 1453, loss = 0.22165261\n",
      "Iteration 1454, loss = 0.22151795\n",
      "Iteration 1455, loss = 0.22138352\n",
      "Iteration 1456, loss = 0.22124931\n",
      "Iteration 1457, loss = 0.22111534\n",
      "Iteration 1458, loss = 0.22098159\n",
      "Iteration 1459, loss = 0.22084807\n",
      "Iteration 1460, loss = 0.22071477\n",
      "Iteration 1461, loss = 0.22058170\n",
      "Iteration 1462, loss = 0.22044885\n",
      "Iteration 1463, loss = 0.22031623\n",
      "Iteration 1464, loss = 0.22018383\n",
      "Iteration 1465, loss = 0.22005165\n",
      "Iteration 1466, loss = 0.21991970\n",
      "Iteration 1467, loss = 0.21978797\n",
      "Iteration 1468, loss = 0.21965646\n",
      "Iteration 1469, loss = 0.21952517\n",
      "Iteration 1470, loss = 0.21939410\n",
      "Iteration 1471, loss = 0.21926325\n",
      "Iteration 1472, loss = 0.21913262\n",
      "Iteration 1473, loss = 0.21900221\n",
      "Iteration 1474, loss = 0.21887202\n",
      "Iteration 1475, loss = 0.21874204\n",
      "Iteration 1476, loss = 0.21861229\n",
      "Iteration 1477, loss = 0.21848275\n",
      "Iteration 1478, loss = 0.21835343\n",
      "Iteration 1479, loss = 0.21822432\n",
      "Iteration 1480, loss = 0.21809543\n",
      "Iteration 1481, loss = 0.21796675\n",
      "Iteration 1482, loss = 0.21783829\n",
      "Iteration 1483, loss = 0.21771004\n",
      "Iteration 1484, loss = 0.21758201\n",
      "Iteration 1485, loss = 0.21745419\n",
      "Iteration 1486, loss = 0.21732658\n",
      "Iteration 1487, loss = 0.21719918\n",
      "Iteration 1488, loss = 0.21707200\n",
      "Iteration 1489, loss = 0.21694502\n",
      "Iteration 1490, loss = 0.21681826\n",
      "Iteration 1491, loss = 0.21669171\n",
      "Iteration 1492, loss = 0.21656537\n",
      "Iteration 1493, loss = 0.21643923\n",
      "Iteration 1494, loss = 0.21631331\n",
      "Iteration 1495, loss = 0.21618759\n",
      "Iteration 1496, loss = 0.21606208\n",
      "Iteration 1497, loss = 0.21593678\n",
      "Iteration 1498, loss = 0.21581168\n",
      "Iteration 1499, loss = 0.21568680\n",
      "Iteration 1500, loss = 0.21556211\n",
      "Iteration 1501, loss = 0.21543764\n",
      "Iteration 1502, loss = 0.21531336\n",
      "Iteration 1503, loss = 0.21518930\n",
      "Iteration 1504, loss = 0.21506543\n",
      "Iteration 1505, loss = 0.21494177\n",
      "Iteration 1506, loss = 0.21481832\n",
      "Iteration 1507, loss = 0.21469506\n",
      "Iteration 1508, loss = 0.21457201\n",
      "Iteration 1509, loss = 0.21444916\n",
      "Iteration 1510, loss = 0.21432651\n",
      "Iteration 1511, loss = 0.21420407\n",
      "Iteration 1512, loss = 0.21408182\n",
      "Iteration 1513, loss = 0.21395977\n",
      "Iteration 1514, loss = 0.21383792\n",
      "Iteration 1515, loss = 0.21371628\n",
      "Iteration 1516, loss = 0.21359483\n",
      "Iteration 1517, loss = 0.21347357\n",
      "Iteration 1518, loss = 0.21335252\n",
      "Iteration 1519, loss = 0.21323166\n",
      "Iteration 1520, loss = 0.21311100\n",
      "Iteration 1521, loss = 0.21299054\n",
      "Iteration 1522, loss = 0.21287027\n",
      "Iteration 1523, loss = 0.21275020\n",
      "Iteration 1524, loss = 0.21263032\n",
      "Iteration 1525, loss = 0.21251064\n",
      "Iteration 1526, loss = 0.21239116\n",
      "Iteration 1527, loss = 0.21227186\n",
      "Iteration 1528, loss = 0.21215276\n",
      "Iteration 1529, loss = 0.21203385\n",
      "Iteration 1530, loss = 0.21191514\n",
      "Iteration 1531, loss = 0.21179662\n",
      "Iteration 1532, loss = 0.21167828\n",
      "Iteration 1533, loss = 0.21156014\n",
      "Iteration 1534, loss = 0.21144219\n",
      "Iteration 1535, loss = 0.21132444\n",
      "Iteration 1536, loss = 0.21120687\n",
      "Iteration 1537, loss = 0.21108949\n",
      "Iteration 1538, loss = 0.21097230\n",
      "Iteration 1539, loss = 0.21085530\n",
      "Iteration 1540, loss = 0.21073848\n",
      "Iteration 1541, loss = 0.21062186\n",
      "Iteration 1542, loss = 0.21050542\n",
      "Iteration 1543, loss = 0.21038917\n",
      "Iteration 1544, loss = 0.21027311\n",
      "Iteration 1545, loss = 0.21015723\n",
      "Iteration 1546, loss = 0.21004154\n",
      "Iteration 1547, loss = 0.20992603\n",
      "Iteration 1548, loss = 0.20981071\n",
      "Iteration 1549, loss = 0.20969557\n",
      "Iteration 1550, loss = 0.20958062\n",
      "Iteration 1551, loss = 0.20946585\n",
      "Iteration 1552, loss = 0.20935126\n",
      "Iteration 1553, loss = 0.20923686\n",
      "Iteration 1554, loss = 0.20912264\n",
      "Iteration 1555, loss = 0.20900860\n",
      "Iteration 1556, loss = 0.20889475\n",
      "Iteration 1557, loss = 0.20878107\n",
      "Iteration 1558, loss = 0.20866758\n",
      "Iteration 1559, loss = 0.20855426\n",
      "Iteration 1560, loss = 0.20844113\n",
      "Iteration 1561, loss = 0.20832818\n",
      "Iteration 1562, loss = 0.20821540\n",
      "Iteration 1563, loss = 0.20810281\n",
      "Iteration 1564, loss = 0.20799039\n",
      "Iteration 1565, loss = 0.20787815\n",
      "Iteration 1566, loss = 0.20776609\n",
      "Iteration 1567, loss = 0.20765421\n",
      "Iteration 1568, loss = 0.20754250\n",
      "Iteration 1569, loss = 0.20743097\n",
      "Iteration 1570, loss = 0.20731962\n",
      "Iteration 1571, loss = 0.20720844\n",
      "Iteration 1572, loss = 0.20709744\n",
      "Iteration 1573, loss = 0.20698662\n",
      "Iteration 1574, loss = 0.20687596\n",
      "Iteration 1575, loss = 0.20676549\n",
      "Iteration 1576, loss = 0.20665518\n",
      "Iteration 1577, loss = 0.20654505\n",
      "Iteration 1578, loss = 0.20643510\n",
      "Iteration 1579, loss = 0.20632531\n",
      "Iteration 1580, loss = 0.20621570\n",
      "Iteration 1581, loss = 0.20610626\n",
      "Iteration 1582, loss = 0.20599700\n",
      "Iteration 1583, loss = 0.20588790\n",
      "Iteration 1584, loss = 0.20577897\n",
      "Iteration 1585, loss = 0.20567022\n",
      "Iteration 1586, loss = 0.20556164\n",
      "Iteration 1587, loss = 0.20545322\n",
      "Iteration 1588, loss = 0.20534498\n",
      "Iteration 1589, loss = 0.20523690\n",
      "Iteration 1590, loss = 0.20512899\n",
      "Iteration 1591, loss = 0.20502126\n",
      "Iteration 1592, loss = 0.20491369\n",
      "Iteration 1593, loss = 0.20480628\n",
      "Iteration 1594, loss = 0.20469905\n",
      "Iteration 1595, loss = 0.20459198\n",
      "Iteration 1596, loss = 0.20448508\n",
      "Iteration 1597, loss = 0.20437834\n",
      "Iteration 1598, loss = 0.20427177\n",
      "Iteration 1599, loss = 0.20416537\n",
      "Iteration 1600, loss = 0.20405913\n",
      "Iteration 1601, loss = 0.20395306\n",
      "Iteration 1602, loss = 0.20384715\n",
      "Iteration 1603, loss = 0.20374140\n",
      "Iteration 1604, loss = 0.20363582\n",
      "Iteration 1605, loss = 0.20353040\n",
      "Iteration 1606, loss = 0.20342515\n",
      "Iteration 1607, loss = 0.20332005\n",
      "Iteration 1608, loss = 0.20321512\n",
      "Iteration 1609, loss = 0.20311036\n",
      "Iteration 1610, loss = 0.20300575\n",
      "Iteration 1611, loss = 0.20290130\n",
      "Iteration 1612, loss = 0.20279702\n",
      "Iteration 1613, loss = 0.20269290\n",
      "Iteration 1614, loss = 0.20258893\n",
      "Iteration 1615, loss = 0.20248513\n",
      "Iteration 1616, loss = 0.20238149\n",
      "Iteration 1617, loss = 0.20227800\n",
      "Iteration 1618, loss = 0.20217468\n",
      "Iteration 1619, loss = 0.20207151\n",
      "Iteration 1620, loss = 0.20196850\n",
      "Iteration 1621, loss = 0.20186565\n",
      "Iteration 1622, loss = 0.20176296\n",
      "Iteration 1623, loss = 0.20166043\n",
      "Iteration 1624, loss = 0.20155805\n",
      "Iteration 1625, loss = 0.20145583\n",
      "Iteration 1626, loss = 0.20135376\n",
      "Iteration 1627, loss = 0.20125185\n",
      "Iteration 1628, loss = 0.20115010\n",
      "Iteration 1629, loss = 0.20104850\n",
      "Iteration 1630, loss = 0.20094705\n",
      "Iteration 1631, loss = 0.20084576\n",
      "Iteration 1632, loss = 0.20074463\n",
      "Iteration 1633, loss = 0.20064365\n",
      "Iteration 1634, loss = 0.20054282\n",
      "Iteration 1635, loss = 0.20044215\n",
      "Iteration 1636, loss = 0.20034163\n",
      "Iteration 1637, loss = 0.20024126\n",
      "Iteration 1638, loss = 0.20014104\n",
      "Iteration 1639, loss = 0.20004098\n",
      "Iteration 1640, loss = 0.19994107\n",
      "Iteration 1641, loss = 0.19984131\n",
      "Iteration 1642, loss = 0.19974170\n",
      "Iteration 1643, loss = 0.19964224\n",
      "Iteration 1644, loss = 0.19954293\n",
      "Iteration 1645, loss = 0.19944377\n",
      "Iteration 1646, loss = 0.19934476\n",
      "Iteration 1647, loss = 0.19924590\n",
      "Iteration 1648, loss = 0.19914719\n",
      "Iteration 1649, loss = 0.19904863\n",
      "Iteration 1650, loss = 0.19895022\n",
      "Iteration 1651, loss = 0.19885196\n",
      "Iteration 1652, loss = 0.19875384\n",
      "Iteration 1653, loss = 0.19865587\n",
      "Iteration 1654, loss = 0.19855805\n",
      "Iteration 1655, loss = 0.19846038\n",
      "Iteration 1656, loss = 0.19836285\n",
      "Iteration 1657, loss = 0.19826547\n",
      "Iteration 1658, loss = 0.19816823\n",
      "Iteration 1659, loss = 0.19807115\n",
      "Iteration 1660, loss = 0.19797420\n",
      "Iteration 1661, loss = 0.19787740\n",
      "Iteration 1662, loss = 0.19778075\n",
      "Iteration 1663, loss = 0.19768424\n",
      "Iteration 1664, loss = 0.19758788\n",
      "Iteration 1665, loss = 0.19749166\n",
      "Iteration 1666, loss = 0.19739558\n",
      "Iteration 1667, loss = 0.19729965\n",
      "Iteration 1668, loss = 0.19720386\n",
      "Iteration 1669, loss = 0.19710821\n",
      "Iteration 1670, loss = 0.19701270\n",
      "Iteration 1671, loss = 0.19691734\n",
      "Iteration 1672, loss = 0.19682212\n",
      "Iteration 1673, loss = 0.19672704\n",
      "Iteration 1674, loss = 0.19663210\n",
      "Iteration 1675, loss = 0.19653731\n",
      "Iteration 1676, loss = 0.19644265\n",
      "Iteration 1677, loss = 0.19634813\n",
      "Iteration 1678, loss = 0.19625376\n",
      "Iteration 1679, loss = 0.19615952\n",
      "Iteration 1680, loss = 0.19606543\n",
      "Iteration 1681, loss = 0.19597147\n",
      "Iteration 1682, loss = 0.19587765\n",
      "Iteration 1683, loss = 0.19578397\n",
      "Iteration 1684, loss = 0.19569043\n",
      "Iteration 1685, loss = 0.19559703\n",
      "Iteration 1686, loss = 0.19550377\n",
      "Iteration 1687, loss = 0.19541064\n",
      "Iteration 1688, loss = 0.19531765\n",
      "Iteration 1689, loss = 0.19522480\n",
      "Iteration 1690, loss = 0.19513208\n",
      "Iteration 1691, loss = 0.19503950\n",
      "Iteration 1692, loss = 0.19494706\n",
      "Iteration 1693, loss = 0.19485475\n",
      "Iteration 1694, loss = 0.19476258\n",
      "Iteration 1695, loss = 0.19467055\n",
      "Iteration 1696, loss = 0.19457864\n",
      "Iteration 1697, loss = 0.19448688\n",
      "Iteration 1698, loss = 0.19439525\n",
      "Iteration 1699, loss = 0.19430375\n",
      "Iteration 1700, loss = 0.19421239\n",
      "Iteration 1701, loss = 0.19412116\n",
      "Iteration 1702, loss = 0.19403006\n",
      "Iteration 1703, loss = 0.19393910\n",
      "Iteration 1704, loss = 0.19384827\n",
      "Iteration 1705, loss = 0.19375757\n",
      "Iteration 1706, loss = 0.19366700\n",
      "Iteration 1707, loss = 0.19357657\n",
      "Iteration 1708, loss = 0.19348627\n",
      "Iteration 1709, loss = 0.19339610\n",
      "Iteration 1710, loss = 0.19330606\n",
      "Iteration 1711, loss = 0.19321615\n",
      "Iteration 1712, loss = 0.19312638\n",
      "Iteration 1713, loss = 0.19303673\n",
      "Iteration 1714, loss = 0.19294722\n",
      "Iteration 1715, loss = 0.19285783\n",
      "Iteration 1716, loss = 0.19276857\n",
      "Iteration 1717, loss = 0.19267945\n",
      "Iteration 1718, loss = 0.19259045\n",
      "Iteration 1719, loss = 0.19250158\n",
      "Iteration 1720, loss = 0.19241284\n",
      "Iteration 1721, loss = 0.19232423\n",
      "Iteration 1722, loss = 0.19223575\n",
      "Iteration 1723, loss = 0.19214739\n",
      "Iteration 1724, loss = 0.19205916\n",
      "Iteration 1725, loss = 0.19197106\n",
      "Iteration 1726, loss = 0.19188309\n",
      "Iteration 1727, loss = 0.19179524\n",
      "Iteration 1728, loss = 0.19170752\n",
      "Iteration 1729, loss = 0.19161993\n",
      "Iteration 1730, loss = 0.19153246\n",
      "Iteration 1731, loss = 0.19144512\n",
      "Iteration 1732, loss = 0.19135790\n",
      "Iteration 1733, loss = 0.19127081\n",
      "Iteration 1734, loss = 0.19118385\n",
      "Iteration 1735, loss = 0.19109700\n",
      "Iteration 1736, loss = 0.19101029\n",
      "Iteration 1737, loss = 0.19092370\n",
      "Iteration 1738, loss = 0.19083723\n",
      "Iteration 1739, loss = 0.19075088\n",
      "Iteration 1740, loss = 0.19066466\n",
      "Iteration 1741, loss = 0.19057857\n",
      "Iteration 1742, loss = 0.19049259\n",
      "Iteration 1743, loss = 0.19040674\n",
      "Iteration 1744, loss = 0.19032101\n",
      "Iteration 1745, loss = 0.19023541\n",
      "Iteration 1746, loss = 0.19014992\n",
      "Iteration 1747, loss = 0.19006456\n",
      "Iteration 1748, loss = 0.18997932\n",
      "Iteration 1749, loss = 0.18989420\n",
      "Iteration 1750, loss = 0.18980920\n",
      "Iteration 1751, loss = 0.18972433\n",
      "Iteration 1752, loss = 0.18963957\n",
      "Iteration 1753, loss = 0.18955493\n",
      "Iteration 1754, loss = 0.18947042\n",
      "Iteration 1755, loss = 0.18938602\n",
      "Iteration 1756, loss = 0.18930175\n",
      "Iteration 1757, loss = 0.18921759\n",
      "Iteration 1758, loss = 0.18913355\n",
      "Iteration 1759, loss = 0.18904964\n",
      "Iteration 1760, loss = 0.18896584\n",
      "Iteration 1761, loss = 0.18888216\n",
      "Iteration 1762, loss = 0.18879859\n",
      "Iteration 1763, loss = 0.18871515\n",
      "Iteration 1764, loss = 0.18863182\n",
      "Iteration 1765, loss = 0.18854861\n",
      "Iteration 1766, loss = 0.18846552\n",
      "Iteration 1767, loss = 0.18838255\n",
      "Iteration 1768, loss = 0.18829969\n",
      "Iteration 1769, loss = 0.18821695\n",
      "Iteration 1770, loss = 0.18813432\n",
      "Iteration 1771, loss = 0.18805181\n",
      "Iteration 1772, loss = 0.18796942\n",
      "Iteration 1773, loss = 0.18788715\n",
      "Iteration 1774, loss = 0.18780499\n",
      "Iteration 1775, loss = 0.18772294\n",
      "Iteration 1776, loss = 0.18764101\n",
      "Iteration 1777, loss = 0.18755919\n",
      "Iteration 1778, loss = 0.18747749\n",
      "Iteration 1779, loss = 0.18739591\n",
      "Iteration 1780, loss = 0.18731443\n",
      "Iteration 1781, loss = 0.18723308\n",
      "Iteration 1782, loss = 0.18715183\n",
      "Iteration 1783, loss = 0.18707070\n",
      "Iteration 1784, loss = 0.18698968\n",
      "Iteration 1785, loss = 0.18690878\n",
      "Iteration 1786, loss = 0.18682799\n",
      "Iteration 1787, loss = 0.18674731\n",
      "Iteration 1788, loss = 0.18666674\n",
      "Iteration 1789, loss = 0.18658629\n",
      "Iteration 1790, loss = 0.18650594\n",
      "Iteration 1791, loss = 0.18642571\n",
      "Iteration 1792, loss = 0.18634559\n",
      "Iteration 1793, loss = 0.18626558\n",
      "Iteration 1794, loss = 0.18618569\n",
      "Iteration 1795, loss = 0.18610590\n",
      "Iteration 1796, loss = 0.18602623\n",
      "Iteration 1797, loss = 0.18594666\n",
      "Iteration 1798, loss = 0.18586721\n",
      "Iteration 1799, loss = 0.18578786\n",
      "Iteration 1800, loss = 0.18570863\n",
      "Iteration 1801, loss = 0.18562950\n",
      "Iteration 1802, loss = 0.18555049\n",
      "Iteration 1803, loss = 0.18547158\n",
      "Iteration 1804, loss = 0.18539279\n",
      "Iteration 1805, loss = 0.18531410\n",
      "Iteration 1806, loss = 0.18523552\n",
      "Iteration 1807, loss = 0.18515704\n",
      "Iteration 1808, loss = 0.18507868\n",
      "Iteration 1809, loss = 0.18500043\n",
      "Iteration 1810, loss = 0.18492228\n",
      "Iteration 1811, loss = 0.18484424\n",
      "Iteration 1812, loss = 0.18476631\n",
      "Iteration 1813, loss = 0.18468848\n",
      "Iteration 1814, loss = 0.18461076\n",
      "Iteration 1815, loss = 0.18453315\n",
      "Iteration 1816, loss = 0.18445564\n",
      "Iteration 1817, loss = 0.18437824\n",
      "Iteration 1818, loss = 0.18430095\n",
      "Iteration 1819, loss = 0.18422376\n",
      "Iteration 1820, loss = 0.18414668\n",
      "Iteration 1821, loss = 0.18406970\n",
      "Iteration 1822, loss = 0.18399283\n",
      "Iteration 1823, loss = 0.18391607\n",
      "Iteration 1824, loss = 0.18383941\n",
      "Iteration 1825, loss = 0.18376285\n",
      "Iteration 1826, loss = 0.18368640\n",
      "Iteration 1827, loss = 0.18361005\n",
      "Iteration 1828, loss = 0.18353381\n",
      "Iteration 1829, loss = 0.18345767\n",
      "Iteration 1830, loss = 0.18338163\n",
      "Iteration 1831, loss = 0.18330570\n",
      "Iteration 1832, loss = 0.18322987\n",
      "Iteration 1833, loss = 0.18315414\n",
      "Iteration 1834, loss = 0.18307852\n",
      "Iteration 1835, loss = 0.18300300\n",
      "Iteration 1836, loss = 0.18292758\n",
      "Iteration 1837, loss = 0.18285227\n",
      "Iteration 1838, loss = 0.18277705\n",
      "Iteration 1839, loss = 0.18270194\n",
      "Iteration 1840, loss = 0.18262693\n",
      "Iteration 1841, loss = 0.18255202\n",
      "Iteration 1842, loss = 0.18247722\n",
      "Iteration 1843, loss = 0.18240251\n",
      "Iteration 1844, loss = 0.18232791\n",
      "Iteration 1845, loss = 0.18225340\n",
      "Iteration 1846, loss = 0.18217900\n",
      "Iteration 1847, loss = 0.18210470\n",
      "Iteration 1848, loss = 0.18203049\n",
      "Iteration 1849, loss = 0.18195639\n",
      "Iteration 1850, loss = 0.18188239\n",
      "Iteration 1851, loss = 0.18180848\n",
      "Iteration 1852, loss = 0.18173468\n",
      "Iteration 1853, loss = 0.18166097\n",
      "Iteration 1854, loss = 0.18158737\n",
      "Iteration 1855, loss = 0.18151386\n",
      "Iteration 1856, loss = 0.18144045\n",
      "Iteration 1857, loss = 0.18136714\n",
      "Iteration 1858, loss = 0.18129393\n",
      "Iteration 1859, loss = 0.18122082\n",
      "Iteration 1860, loss = 0.18114780\n",
      "Iteration 1861, loss = 0.18107489\n",
      "Iteration 1862, loss = 0.18100207\n",
      "Iteration 1863, loss = 0.18092934\n",
      "Iteration 1864, loss = 0.18085672\n",
      "Iteration 1865, loss = 0.18078419\n",
      "Iteration 1866, loss = 0.18071176\n",
      "Iteration 1867, loss = 0.18063943\n",
      "Iteration 1868, loss = 0.18056719\n",
      "Iteration 1869, loss = 0.18049505\n",
      "Iteration 1870, loss = 0.18042300\n",
      "Iteration 1871, loss = 0.18035105\n",
      "Iteration 1872, loss = 0.18027920\n",
      "Iteration 1873, loss = 0.18020744\n",
      "Iteration 1874, loss = 0.18013577\n",
      "Iteration 1875, loss = 0.18006421\n",
      "Iteration 1876, loss = 0.17999273\n",
      "Iteration 1877, loss = 0.17992136\n",
      "Iteration 1878, loss = 0.17985007\n",
      "Iteration 1879, loss = 0.17977889\n",
      "Iteration 1880, loss = 0.17970779\n",
      "Iteration 1881, loss = 0.17963679\n",
      "Iteration 1882, loss = 0.17956589\n",
      "Iteration 1883, loss = 0.17949507\n",
      "Iteration 1884, loss = 0.17942436\n",
      "Iteration 1885, loss = 0.17935373\n",
      "Iteration 1886, loss = 0.17928320\n",
      "Iteration 1887, loss = 0.17921276\n",
      "Iteration 1888, loss = 0.17914242\n",
      "Iteration 1889, loss = 0.17907216\n",
      "Iteration 1890, loss = 0.17900200\n",
      "Iteration 1891, loss = 0.17893194\n",
      "Iteration 1892, loss = 0.17886196\n",
      "Iteration 1893, loss = 0.17879208\n",
      "Iteration 1894, loss = 0.17872229\n",
      "Iteration 1895, loss = 0.17865259\n",
      "Iteration 1896, loss = 0.17858298\n",
      "Iteration 1897, loss = 0.17851346\n",
      "Iteration 1898, loss = 0.17844404\n",
      "Iteration 1899, loss = 0.17837470\n",
      "Iteration 1900, loss = 0.17830546\n",
      "Iteration 1901, loss = 0.17823631\n",
      "Iteration 1902, loss = 0.17816725\n",
      "Iteration 1903, loss = 0.17809828\n",
      "Iteration 1904, loss = 0.17802939\n",
      "Iteration 1905, loss = 0.17796060\n",
      "Iteration 1906, loss = 0.17789190\n",
      "Iteration 1907, loss = 0.17782329\n",
      "Iteration 1908, loss = 0.17775477\n",
      "Iteration 1909, loss = 0.17768634\n",
      "Iteration 1910, loss = 0.17761799\n",
      "Iteration 1911, loss = 0.17754974\n",
      "Iteration 1912, loss = 0.17748158\n",
      "Iteration 1913, loss = 0.17741350\n",
      "Iteration 1914, loss = 0.17734551\n",
      "Iteration 1915, loss = 0.17727761\n",
      "Iteration 1916, loss = 0.17720980\n",
      "Iteration 1917, loss = 0.17714208\n",
      "Iteration 1918, loss = 0.17707445\n",
      "Iteration 1919, loss = 0.17700690\n",
      "Iteration 1920, loss = 0.17693944\n",
      "Iteration 1921, loss = 0.17687207\n",
      "Iteration 1922, loss = 0.17680479\n",
      "Iteration 1923, loss = 0.17673759\n",
      "Iteration 1924, loss = 0.17667048\n",
      "Iteration 1925, loss = 0.17660346\n",
      "Iteration 1926, loss = 0.17653652\n",
      "Iteration 1927, loss = 0.17646967\n",
      "Iteration 1928, loss = 0.17640291\n",
      "Iteration 1929, loss = 0.17633623\n",
      "Iteration 1930, loss = 0.17626964\n",
      "Iteration 1931, loss = 0.17620313\n",
      "Iteration 1932, loss = 0.17613671\n",
      "Iteration 1933, loss = 0.17607038\n",
      "Iteration 1934, loss = 0.17600413\n",
      "Iteration 1935, loss = 0.17593797\n",
      "Iteration 1936, loss = 0.17587189\n",
      "Iteration 1937, loss = 0.17580590\n",
      "Iteration 1938, loss = 0.17573999\n",
      "Iteration 1939, loss = 0.17567417\n",
      "Iteration 1940, loss = 0.17560843\n",
      "Iteration 1941, loss = 0.17554278\n",
      "Iteration 1942, loss = 0.17547720\n",
      "Iteration 1943, loss = 0.17541172\n",
      "Iteration 1944, loss = 0.17534632\n",
      "Iteration 1945, loss = 0.17528100\n",
      "Iteration 1946, loss = 0.17521576\n",
      "Iteration 1947, loss = 0.17515061\n",
      "Iteration 1948, loss = 0.17508555\n",
      "Iteration 1949, loss = 0.17502056\n",
      "Iteration 1950, loss = 0.17495566\n",
      "Iteration 1951, loss = 0.17489084\n",
      "Iteration 1952, loss = 0.17482610\n",
      "Iteration 1953, loss = 0.17476145\n",
      "Iteration 1954, loss = 0.17469688\n",
      "Iteration 1955, loss = 0.17463239\n",
      "Iteration 1956, loss = 0.17456798\n",
      "Iteration 1957, loss = 0.17450366\n",
      "Iteration 1958, loss = 0.17443942\n",
      "Iteration 1959, loss = 0.17437526\n",
      "Iteration 1960, loss = 0.17431118\n",
      "Iteration 1961, loss = 0.17424718\n",
      "Iteration 1962, loss = 0.17418326\n",
      "Iteration 1963, loss = 0.17411943\n",
      "Iteration 1964, loss = 0.17405567\n",
      "Iteration 1965, loss = 0.17399200\n",
      "Iteration 1966, loss = 0.17392841\n",
      "Iteration 1967, loss = 0.17386489\n",
      "Iteration 1968, loss = 0.17380146\n",
      "Iteration 1969, loss = 0.17373811\n",
      "Iteration 1970, loss = 0.17367484\n",
      "Iteration 1971, loss = 0.17361165\n",
      "Iteration 1972, loss = 0.17354853\n",
      "Iteration 1973, loss = 0.17348550\n",
      "Iteration 1974, loss = 0.17342255\n",
      "Iteration 1975, loss = 0.17335967\n",
      "Iteration 1976, loss = 0.17329688\n",
      "Iteration 1977, loss = 0.17323417\n",
      "Iteration 1978, loss = 0.17317153\n",
      "Iteration 1979, loss = 0.17310897\n",
      "Iteration 1980, loss = 0.17304649\n",
      "Iteration 1981, loss = 0.17298409\n",
      "Iteration 1982, loss = 0.17292177\n",
      "Iteration 1983, loss = 0.17285953\n",
      "Iteration 1984, loss = 0.17279737\n",
      "Iteration 1985, loss = 0.17273528\n",
      "Iteration 1986, loss = 0.17267327\n",
      "Iteration 1987, loss = 0.17261134\n",
      "Iteration 1988, loss = 0.17254948\n",
      "Iteration 1989, loss = 0.17248771\n",
      "Iteration 1990, loss = 0.17242601\n",
      "Iteration 1991, loss = 0.17236439\n",
      "Iteration 1992, loss = 0.17230284\n",
      "Iteration 1993, loss = 0.17224137\n",
      "Iteration 1994, loss = 0.17217998\n",
      "Iteration 1995, loss = 0.17211867\n",
      "Iteration 1996, loss = 0.17205743\n",
      "Iteration 1997, loss = 0.17199627\n",
      "Iteration 1998, loss = 0.17193519\n",
      "Iteration 1999, loss = 0.17187418\n",
      "Iteration 2000, loss = 0.17181324\n",
      "Iteration 2001, loss = 0.17175239\n",
      "Iteration 2002, loss = 0.17169160\n",
      "Iteration 2003, loss = 0.17163090\n",
      "Iteration 2004, loss = 0.17157027\n",
      "Iteration 2005, loss = 0.17150971\n",
      "Iteration 2006, loss = 0.17144923\n",
      "Iteration 2007, loss = 0.17138883\n",
      "Iteration 2008, loss = 0.17132850\n",
      "Iteration 2009, loss = 0.17126824\n",
      "Iteration 2010, loss = 0.17120806\n",
      "Iteration 2011, loss = 0.17114796\n",
      "Iteration 2012, loss = 0.17108793\n",
      "Iteration 2013, loss = 0.17102797\n",
      "Iteration 2014, loss = 0.17096809\n",
      "Iteration 2015, loss = 0.17090828\n",
      "Iteration 2016, loss = 0.17084854\n",
      "Iteration 2017, loss = 0.17078888\n",
      "Iteration 2018, loss = 0.17072929\n",
      "Iteration 2019, loss = 0.17066978\n",
      "Iteration 2020, loss = 0.17061034\n",
      "Iteration 2021, loss = 0.17055097\n",
      "Iteration 2022, loss = 0.17049167\n",
      "Iteration 2023, loss = 0.17043245\n",
      "Iteration 2024, loss = 0.17037330\n",
      "Iteration 2025, loss = 0.17031423\n",
      "Iteration 2026, loss = 0.17025523\n",
      "Iteration 2027, loss = 0.17019629\n",
      "Iteration 2028, loss = 0.17013744\n",
      "Iteration 2029, loss = 0.17007865\n",
      "Iteration 2030, loss = 0.17001994\n",
      "Iteration 2031, loss = 0.16996129\n",
      "Iteration 2032, loss = 0.16990272\n",
      "Iteration 2033, loss = 0.16984422\n",
      "Iteration 2034, loss = 0.16978580\n",
      "Iteration 2035, loss = 0.16972744\n",
      "Iteration 2036, loss = 0.16966916\n",
      "Iteration 2037, loss = 0.16961095\n",
      "Iteration 2038, loss = 0.16955280\n",
      "Iteration 2039, loss = 0.16949473\n",
      "Iteration 2040, loss = 0.16943673\n",
      "Iteration 2041, loss = 0.16937881\n",
      "Iteration 2042, loss = 0.16932095\n",
      "Iteration 2043, loss = 0.16926316\n",
      "Iteration 2044, loss = 0.16920544\n",
      "Iteration 2045, loss = 0.16914780\n",
      "Iteration 2046, loss = 0.16909022\n",
      "Iteration 2047, loss = 0.16903271\n",
      "Iteration 2048, loss = 0.16897527\n",
      "Iteration 2049, loss = 0.16891791\n",
      "Iteration 2050, loss = 0.16886061\n",
      "Iteration 2051, loss = 0.16880338\n",
      "Iteration 2052, loss = 0.16874622\n",
      "Iteration 2053, loss = 0.16868913\n",
      "Iteration 2054, loss = 0.16863211\n",
      "Iteration 2055, loss = 0.16857516\n",
      "Iteration 2056, loss = 0.16851828\n",
      "Iteration 2057, loss = 0.16846147\n",
      "Iteration 2058, loss = 0.16840472\n",
      "Iteration 2059, loss = 0.16834805\n",
      "Iteration 2060, loss = 0.16829144\n",
      "Iteration 2061, loss = 0.16823490\n",
      "Iteration 2062, loss = 0.16817843\n",
      "Iteration 2063, loss = 0.16812203\n",
      "Iteration 2064, loss = 0.16806570\n",
      "Iteration 2065, loss = 0.16800943\n",
      "Iteration 2066, loss = 0.16795323\n",
      "Iteration 2067, loss = 0.16789710\n",
      "Iteration 2068, loss = 0.16784104\n",
      "Iteration 2069, loss = 0.16778504\n",
      "Iteration 2070, loss = 0.16772911\n",
      "Iteration 2071, loss = 0.16767325\n",
      "Iteration 2072, loss = 0.16761746\n",
      "Iteration 2073, loss = 0.16756173\n",
      "Iteration 2074, loss = 0.16750607\n",
      "Iteration 2075, loss = 0.16745048\n",
      "Iteration 2076, loss = 0.16739495\n",
      "Iteration 2077, loss = 0.16733949\n",
      "Iteration 2078, loss = 0.16728410\n",
      "Iteration 2079, loss = 0.16722877\n",
      "Iteration 2080, loss = 0.16717351\n",
      "Iteration 2081, loss = 0.16711831\n",
      "Iteration 2082, loss = 0.16706318\n",
      "Iteration 2083, loss = 0.16700812\n",
      "Iteration 2084, loss = 0.16695312\n",
      "Iteration 2085, loss = 0.16689819\n",
      "Iteration 2086, loss = 0.16684332\n",
      "Iteration 2087, loss = 0.16678852\n",
      "Iteration 2088, loss = 0.16673379\n",
      "Iteration 2089, loss = 0.16667912\n",
      "Iteration 2090, loss = 0.16662451\n",
      "Iteration 2091, loss = 0.16656997\n",
      "Iteration 2092, loss = 0.16651550\n",
      "Iteration 2093, loss = 0.16646109\n",
      "Iteration 2094, loss = 0.16640674\n",
      "Iteration 2095, loss = 0.16635246\n",
      "Iteration 2096, loss = 0.16629824\n",
      "Iteration 2097, loss = 0.16624409\n",
      "Iteration 2098, loss = 0.16619000\n",
      "Iteration 2099, loss = 0.16613598\n",
      "Iteration 2100, loss = 0.16608202\n",
      "Iteration 2101, loss = 0.16602812\n",
      "Iteration 2102, loss = 0.16597429\n",
      "Iteration 2103, loss = 0.16592052\n",
      "Iteration 2104, loss = 0.16586682\n",
      "Iteration 2105, loss = 0.16581318\n",
      "Iteration 2106, loss = 0.16575960\n",
      "Iteration 2107, loss = 0.16570608\n",
      "Iteration 2108, loss = 0.16565263\n",
      "Iteration 2109, loss = 0.16559924\n",
      "Iteration 2110, loss = 0.16554592\n",
      "Iteration 2111, loss = 0.16549266\n",
      "Iteration 2112, loss = 0.16543946\n",
      "Iteration 2113, loss = 0.16538632\n",
      "Iteration 2114, loss = 0.16533325\n",
      "Iteration 2115, loss = 0.16528023\n",
      "Iteration 2116, loss = 0.16522728\n",
      "Iteration 2117, loss = 0.16517440\n",
      "Iteration 2118, loss = 0.16512157\n",
      "Iteration 2119, loss = 0.16506881\n",
      "Iteration 2120, loss = 0.16501611\n",
      "Iteration 2121, loss = 0.16496347\n",
      "Iteration 2122, loss = 0.16491089\n",
      "Iteration 2123, loss = 0.16485837\n",
      "Iteration 2124, loss = 0.16480592\n",
      "Iteration 2125, loss = 0.16475353\n",
      "Iteration 2126, loss = 0.16470119\n",
      "Iteration 2127, loss = 0.16464892\n",
      "Iteration 2128, loss = 0.16459671\n",
      "Iteration 2129, loss = 0.16454456\n",
      "Iteration 2130, loss = 0.16449248\n",
      "Iteration 2131, loss = 0.16444045\n",
      "Iteration 2132, loss = 0.16438848\n",
      "Iteration 2133, loss = 0.16433658\n",
      "Iteration 2134, loss = 0.16428473\n",
      "Iteration 2135, loss = 0.16423295\n",
      "Iteration 2136, loss = 0.16418123\n",
      "Iteration 2137, loss = 0.16412956\n",
      "Iteration 2138, loss = 0.16407796\n",
      "Iteration 2139, loss = 0.16402641\n",
      "Iteration 2140, loss = 0.16397493\n",
      "Iteration 2141, loss = 0.16392351\n",
      "Iteration 2142, loss = 0.16387214\n",
      "Iteration 2143, loss = 0.16382084\n",
      "Iteration 2144, loss = 0.16376959\n",
      "Iteration 2145, loss = 0.16371841\n",
      "Iteration 2146, loss = 0.16366728\n",
      "Iteration 2147, loss = 0.16361621\n",
      "Iteration 2148, loss = 0.16356521\n",
      "Iteration 2149, loss = 0.16351426\n",
      "Iteration 2150, loss = 0.16346337\n",
      "Iteration 2151, loss = 0.16341254\n",
      "Iteration 2152, loss = 0.16336176\n",
      "Iteration 2153, loss = 0.16331105\n",
      "Iteration 2154, loss = 0.16326040\n",
      "Iteration 2155, loss = 0.16320980\n",
      "Iteration 2156, loss = 0.16315926\n",
      "Iteration 2157, loss = 0.16310878\n",
      "Iteration 2158, loss = 0.16305836\n",
      "Iteration 2159, loss = 0.16300800\n",
      "Iteration 2160, loss = 0.16295769\n",
      "Iteration 2161, loss = 0.16290744\n",
      "Iteration 2162, loss = 0.16285725\n",
      "Iteration 2163, loss = 0.16280712\n",
      "Iteration 2164, loss = 0.16275705\n",
      "Iteration 2165, loss = 0.16270703\n",
      "Iteration 2166, loss = 0.16265707\n",
      "Iteration 2167, loss = 0.16260717\n",
      "Iteration 2168, loss = 0.16255732\n",
      "Iteration 2169, loss = 0.16250754\n",
      "Iteration 2170, loss = 0.16245781\n",
      "Iteration 2171, loss = 0.16240813\n",
      "Iteration 2172, loss = 0.16235852\n",
      "Iteration 2173, loss = 0.16230896\n",
      "Iteration 2174, loss = 0.16225945\n",
      "Iteration 2175, loss = 0.16221001\n",
      "Iteration 2176, loss = 0.16216062\n",
      "Iteration 2177, loss = 0.16211128\n",
      "Iteration 2178, loss = 0.16206201\n",
      "Iteration 2179, loss = 0.16201278\n",
      "Iteration 2180, loss = 0.16196362\n",
      "Iteration 2181, loss = 0.16191451\n",
      "Iteration 2182, loss = 0.16186546\n",
      "Iteration 2183, loss = 0.16181646\n",
      "Iteration 2184, loss = 0.16176752\n",
      "Iteration 2185, loss = 0.16171863\n",
      "Iteration 2186, loss = 0.16166981\n",
      "Iteration 2187, loss = 0.16162103\n",
      "Iteration 2188, loss = 0.16157231\n",
      "Iteration 2189, loss = 0.16152365\n",
      "Iteration 2190, loss = 0.16147504\n",
      "Iteration 2191, loss = 0.16142649\n",
      "Iteration 2192, loss = 0.16137799\n",
      "Iteration 2193, loss = 0.16132955\n",
      "Iteration 2194, loss = 0.16128116\n",
      "Iteration 2195, loss = 0.16123283\n",
      "Iteration 2196, loss = 0.16118455\n",
      "Iteration 2197, loss = 0.16113632\n",
      "Iteration 2198, loss = 0.16108815\n",
      "Iteration 2199, loss = 0.16104004\n",
      "Iteration 2200, loss = 0.16099198\n",
      "Iteration 2201, loss = 0.16094397\n",
      "Iteration 2202, loss = 0.16089602\n",
      "Iteration 2203, loss = 0.16084812\n",
      "Iteration 2204, loss = 0.16080028\n",
      "Iteration 2205, loss = 0.16075249\n",
      "Iteration 2206, loss = 0.16070475\n",
      "Iteration 2207, loss = 0.16065707\n",
      "Iteration 2208, loss = 0.16060944\n",
      "Iteration 2209, loss = 0.16056187\n",
      "Iteration 2210, loss = 0.16051435\n",
      "Iteration 2211, loss = 0.16046688\n",
      "Iteration 2212, loss = 0.16041947\n",
      "Iteration 2213, loss = 0.16037211\n",
      "Iteration 2214, loss = 0.16032480\n",
      "Iteration 2215, loss = 0.16027754\n",
      "Iteration 2216, loss = 0.16023034\n",
      "Iteration 2217, loss = 0.16018319\n",
      "Iteration 2218, loss = 0.16013610\n",
      "Iteration 2219, loss = 0.16008905\n",
      "Iteration 2220, loss = 0.16004206\n",
      "Iteration 2221, loss = 0.15999512\n",
      "Iteration 2222, loss = 0.15994824\n",
      "Iteration 2223, loss = 0.15990141\n",
      "Iteration 2224, loss = 0.15985463\n",
      "Iteration 2225, loss = 0.15980790\n",
      "Iteration 2226, loss = 0.15976122\n",
      "Iteration 2227, loss = 0.15971460\n",
      "Iteration 2228, loss = 0.15966803\n",
      "Iteration 2229, loss = 0.15962151\n",
      "Iteration 2230, loss = 0.15957504\n",
      "Iteration 2231, loss = 0.15952862\n",
      "Iteration 2232, loss = 0.15948226\n",
      "Iteration 2233, loss = 0.15943594\n",
      "Iteration 2234, loss = 0.15938968\n",
      "Iteration 2235, loss = 0.15934347\n",
      "Iteration 2236, loss = 0.15929731\n",
      "Iteration 2237, loss = 0.15925121\n",
      "Iteration 2238, loss = 0.15920515\n",
      "Iteration 2239, loss = 0.15915914\n",
      "Iteration 2240, loss = 0.15911319\n",
      "Iteration 2241, loss = 0.15906729\n",
      "Iteration 2242, loss = 0.15902144\n",
      "Iteration 2243, loss = 0.15897563\n",
      "Iteration 2244, loss = 0.15892988\n",
      "Iteration 2245, loss = 0.15888418\n",
      "Iteration 2246, loss = 0.15883854\n",
      "Iteration 2247, loss = 0.15879294\n",
      "Iteration 2248, loss = 0.15874739\n",
      "Iteration 2249, loss = 0.15870189\n",
      "Iteration 2250, loss = 0.15865644\n",
      "Iteration 2251, loss = 0.15861104\n",
      "Iteration 2252, loss = 0.15856570\n",
      "Iteration 2253, loss = 0.15852040\n",
      "Iteration 2254, loss = 0.15847515\n",
      "Iteration 2255, loss = 0.15842996\n",
      "Iteration 2256, loss = 0.15838481\n",
      "Iteration 2257, loss = 0.15833971\n",
      "Iteration 2258, loss = 0.15829466\n",
      "Iteration 2259, loss = 0.15824966\n",
      "Iteration 2260, loss = 0.15820471\n",
      "Iteration 2261, loss = 0.15815981\n",
      "Iteration 2262, loss = 0.15811496\n",
      "Iteration 2263, loss = 0.15807016\n",
      "Iteration 2264, loss = 0.15802541\n",
      "Iteration 2265, loss = 0.15798071\n",
      "Iteration 2266, loss = 0.15793605\n",
      "Iteration 2267, loss = 0.15789145\n",
      "Iteration 2268, loss = 0.15784689\n",
      "Iteration 2269, loss = 0.15780238\n",
      "Iteration 2270, loss = 0.15775792\n",
      "Iteration 2271, loss = 0.15771351\n",
      "Iteration 2272, loss = 0.15766915\n",
      "Iteration 2273, loss = 0.15762484\n",
      "Iteration 2274, loss = 0.15758058\n",
      "Iteration 2275, loss = 0.15753636\n",
      "Iteration 2276, loss = 0.15749219\n",
      "Iteration 2277, loss = 0.15744807\n",
      "Iteration 2278, loss = 0.15740400\n",
      "Iteration 2279, loss = 0.15735998\n",
      "Iteration 2280, loss = 0.15731600\n",
      "Iteration 2281, loss = 0.15727207\n",
      "Iteration 2282, loss = 0.15722819\n",
      "Iteration 2283, loss = 0.15718436\n",
      "Iteration 2284, loss = 0.15714058\n",
      "Iteration 2285, loss = 0.15709684\n",
      "Iteration 2286, loss = 0.15705315\n",
      "Iteration 2287, loss = 0.15700951\n",
      "Iteration 2288, loss = 0.15696592\n",
      "Iteration 2289, loss = 0.15692237\n",
      "Iteration 2290, loss = 0.15687887\n",
      "Iteration 2291, loss = 0.15683542\n",
      "Iteration 2292, loss = 0.15679201\n",
      "Iteration 2293, loss = 0.15674865\n",
      "Iteration 2294, loss = 0.15670534\n",
      "Iteration 2295, loss = 0.15666208\n",
      "Iteration 2296, loss = 0.15661886\n",
      "Iteration 2297, loss = 0.15657569\n",
      "Iteration 2298, loss = 0.15653256\n",
      "Iteration 2299, loss = 0.15648949\n",
      "Iteration 2300, loss = 0.15644646\n",
      "Iteration 2301, loss = 0.15640347\n",
      "Iteration 2302, loss = 0.15636053\n",
      "Iteration 2303, loss = 0.15631764\n",
      "Iteration 2304, loss = 0.15627480\n",
      "Iteration 2305, loss = 0.15623200\n",
      "Iteration 2306, loss = 0.15618924\n",
      "Iteration 2307, loss = 0.15614654\n",
      "Iteration 2308, loss = 0.15610387\n",
      "Iteration 2309, loss = 0.15606126\n",
      "Iteration 2310, loss = 0.15601869\n",
      "Iteration 2311, loss = 0.15597617\n",
      "Iteration 2312, loss = 0.15593369\n",
      "Iteration 2313, loss = 0.15589125\n",
      "Iteration 2314, loss = 0.15584887\n",
      "Iteration 2315, loss = 0.15580653\n",
      "Iteration 2316, loss = 0.15576423\n",
      "Iteration 2317, loss = 0.15572198\n",
      "Iteration 2318, loss = 0.15567977\n",
      "Iteration 2319, loss = 0.15563761\n",
      "Iteration 2320, loss = 0.15559550\n",
      "Iteration 2321, loss = 0.15555343\n",
      "Iteration 2322, loss = 0.15551140\n",
      "Iteration 2323, loss = 0.15546942\n",
      "Iteration 2324, loss = 0.15542749\n",
      "Iteration 2325, loss = 0.15538560\n",
      "Iteration 2326, loss = 0.15534375\n",
      "Iteration 2327, loss = 0.15530195\n",
      "Iteration 2328, loss = 0.15526019\n",
      "Iteration 2329, loss = 0.15521848\n",
      "Iteration 2330, loss = 0.15517682\n",
      "Iteration 2331, loss = 0.15513519\n",
      "Iteration 2332, loss = 0.15509361\n",
      "Iteration 2333, loss = 0.15505208\n",
      "Iteration 2334, loss = 0.15501059\n",
      "Iteration 2335, loss = 0.15496914\n",
      "Iteration 2336, loss = 0.15492774\n",
      "Iteration 2337, loss = 0.15488638\n",
      "Iteration 2338, loss = 0.15484507\n",
      "Iteration 2339, loss = 0.15480380\n",
      "Iteration 2340, loss = 0.15476257\n",
      "Iteration 2341, loss = 0.15472139\n",
      "Iteration 2342, loss = 0.15468025\n",
      "Iteration 2343, loss = 0.15463916\n",
      "Iteration 2344, loss = 0.15459810\n",
      "Iteration 2345, loss = 0.15455710\n",
      "Iteration 2346, loss = 0.15451613\n",
      "Iteration 2347, loss = 0.15447521\n",
      "Iteration 2348, loss = 0.15443433\n",
      "Iteration 2349, loss = 0.15439350\n",
      "Iteration 2350, loss = 0.15435270\n",
      "Iteration 2351, loss = 0.15431195\n",
      "Iteration 2352, loss = 0.15427125\n",
      "Iteration 2353, loss = 0.15423059\n",
      "Iteration 2354, loss = 0.15418997\n",
      "Iteration 2355, loss = 0.15414939\n",
      "Iteration 2356, loss = 0.15410885\n",
      "Iteration 2357, loss = 0.15406836\n",
      "Iteration 2358, loss = 0.15402791\n",
      "Iteration 2359, loss = 0.15398751\n",
      "Iteration 2360, loss = 0.15394714\n",
      "Iteration 2361, loss = 0.15390682\n",
      "Iteration 2362, loss = 0.15386654\n",
      "Iteration 2363, loss = 0.15382631\n",
      "Iteration 2364, loss = 0.15378611\n",
      "Iteration 2365, loss = 0.15374596\n",
      "Iteration 2366, loss = 0.15370585\n",
      "Iteration 2367, loss = 0.15366578\n",
      "Iteration 2368, loss = 0.15362576\n",
      "Iteration 2369, loss = 0.15358577\n",
      "Iteration 2370, loss = 0.15354583\n",
      "Iteration 2371, loss = 0.15350593\n",
      "Iteration 2372, loss = 0.15346607\n",
      "Iteration 2373, loss = 0.15342625\n",
      "Iteration 2374, loss = 0.15338648\n",
      "Iteration 2375, loss = 0.15334674\n",
      "Iteration 2376, loss = 0.15330705\n",
      "Iteration 2377, loss = 0.15326740\n",
      "Iteration 2378, loss = 0.15322779\n",
      "Iteration 2379, loss = 0.15318822\n",
      "Iteration 2380, loss = 0.15314870\n",
      "Iteration 2381, loss = 0.15310921\n",
      "Iteration 2382, loss = 0.15306977\n",
      "Iteration 2383, loss = 0.15303036\n",
      "Iteration 2384, loss = 0.15299100\n",
      "Iteration 2385, loss = 0.15295168\n",
      "Iteration 2386, loss = 0.15291240\n",
      "Iteration 2387, loss = 0.15287316\n",
      "Iteration 2388, loss = 0.15283396\n",
      "Iteration 2389, loss = 0.15279480\n",
      "Iteration 2390, loss = 0.15275569\n",
      "Iteration 2391, loss = 0.15271661\n",
      "Iteration 2392, loss = 0.15267757\n",
      "Iteration 2393, loss = 0.15263858\n",
      "Iteration 2394, loss = 0.15259962\n",
      "Iteration 2395, loss = 0.15256071\n",
      "Iteration 2396, loss = 0.15252183\n",
      "Iteration 2397, loss = 0.15248300\n",
      "Iteration 2398, loss = 0.15244421\n",
      "Iteration 2399, loss = 0.15240545\n",
      "Iteration 2400, loss = 0.15236674\n",
      "Iteration 2401, loss = 0.15232806\n",
      "Iteration 2402, loss = 0.15228943\n",
      "Iteration 2403, loss = 0.15225084\n",
      "Iteration 2404, loss = 0.15221228\n",
      "Iteration 2405, loss = 0.15217377\n",
      "Iteration 2406, loss = 0.15213529\n",
      "Iteration 2407, loss = 0.15209686\n",
      "Iteration 2408, loss = 0.15205846\n",
      "Iteration 2409, loss = 0.15202011\n",
      "Iteration 2410, loss = 0.15198179\n",
      "Iteration 2411, loss = 0.15194352\n",
      "Iteration 2412, loss = 0.15190528\n",
      "Iteration 2413, loss = 0.15186708\n",
      "Iteration 2414, loss = 0.15182892\n",
      "Iteration 2415, loss = 0.15179080\n",
      "Iteration 2416, loss = 0.15175272\n",
      "Iteration 2417, loss = 0.15171468\n",
      "Iteration 2418, loss = 0.15167668\n",
      "Iteration 2419, loss = 0.15163872\n",
      "Iteration 2420, loss = 0.15160079\n",
      "Iteration 2421, loss = 0.15156291\n",
      "Iteration 2422, loss = 0.15152506\n",
      "Iteration 2423, loss = 0.15148725\n",
      "Iteration 2424, loss = 0.15144948\n",
      "Iteration 2425, loss = 0.15141175\n",
      "Iteration 2426, loss = 0.15137406\n",
      "Iteration 2427, loss = 0.15133641\n",
      "Iteration 2428, loss = 0.15129879\n",
      "Iteration 2429, loss = 0.15126122\n",
      "Iteration 2430, loss = 0.15122368\n",
      "Iteration 2431, loss = 0.15118618\n",
      "Iteration 2432, loss = 0.15114872\n",
      "Iteration 2433, loss = 0.15111129\n",
      "Iteration 2434, loss = 0.15107391\n",
      "Iteration 2435, loss = 0.15103656\n",
      "Iteration 2436, loss = 0.15099925\n",
      "Iteration 2437, loss = 0.15096198\n",
      "Iteration 2438, loss = 0.15092475\n",
      "Iteration 2439, loss = 0.15088755\n",
      "Iteration 2440, loss = 0.15085040\n",
      "Iteration 2441, loss = 0.15081328\n",
      "Iteration 2442, loss = 0.15077619\n",
      "Iteration 2443, loss = 0.15073915\n",
      "Iteration 2444, loss = 0.15070214\n",
      "Iteration 2445, loss = 0.15066517\n",
      "Iteration 2446, loss = 0.15062824\n",
      "Iteration 2447, loss = 0.15059135\n",
      "Iteration 2448, loss = 0.15055449\n",
      "Iteration 2449, loss = 0.15051767\n",
      "Iteration 2450, loss = 0.15048089\n",
      "Iteration 2451, loss = 0.15044415\n",
      "Iteration 2452, loss = 0.15040744\n",
      "Iteration 2453, loss = 0.15037077\n",
      "Iteration 2454, loss = 0.15033413\n",
      "Iteration 2455, loss = 0.15029754\n",
      "Iteration 2456, loss = 0.15026098\n",
      "Iteration 2457, loss = 0.15022446\n",
      "Iteration 2458, loss = 0.15018797\n",
      "Iteration 2459, loss = 0.15015152\n",
      "Iteration 2460, loss = 0.15011511\n",
      "Iteration 2461, loss = 0.15007873\n",
      "Iteration 2462, loss = 0.15004240\n",
      "Iteration 2463, loss = 0.15000609\n",
      "Iteration 2464, loss = 0.14996983\n",
      "Iteration 2465, loss = 0.14993360\n",
      "Iteration 2466, loss = 0.14989741\n",
      "Iteration 2467, loss = 0.14986125\n",
      "Iteration 2468, loss = 0.14982513\n",
      "Iteration 2469, loss = 0.14978905\n",
      "Iteration 2470, loss = 0.14975300\n",
      "Iteration 2471, loss = 0.14971699\n",
      "Iteration 2472, loss = 0.14968101\n",
      "Iteration 2473, loss = 0.14964507\n",
      "Iteration 2474, loss = 0.14960917\n",
      "Iteration 2475, loss = 0.14957330\n",
      "Iteration 2476, loss = 0.14953747\n",
      "Iteration 2477, loss = 0.14950168\n",
      "Iteration 2478, loss = 0.14946592\n",
      "Iteration 2479, loss = 0.14943020\n",
      "Iteration 2480, loss = 0.14939451\n",
      "Iteration 2481, loss = 0.14935886\n",
      "Iteration 2482, loss = 0.14932324\n",
      "Iteration 2483, loss = 0.14928766\n",
      "Iteration 2484, loss = 0.14925211\n",
      "Iteration 2485, loss = 0.14921660\n",
      "Iteration 2486, loss = 0.14918113\n",
      "Iteration 2487, loss = 0.14914569\n",
      "Iteration 2488, loss = 0.14911029\n",
      "Iteration 2489, loss = 0.14907492\n",
      "Iteration 2490, loss = 0.14903958\n",
      "Iteration 2491, loss = 0.14900429\n",
      "Iteration 2492, loss = 0.14896902\n",
      "Iteration 2493, loss = 0.14893380\n",
      "Iteration 2494, loss = 0.14889860\n",
      "Iteration 2495, loss = 0.14886344\n",
      "Iteration 2496, loss = 0.14882832\n",
      "Iteration 2497, loss = 0.14879323\n",
      "Iteration 2498, loss = 0.14875818\n",
      "Iteration 2499, loss = 0.14872316\n",
      "Iteration 2500, loss = 0.14868818\n",
      "Iteration 2501, loss = 0.14865323\n",
      "Iteration 2502, loss = 0.14861832\n",
      "Iteration 2503, loss = 0.14858344\n",
      "Iteration 2504, loss = 0.14854859\n",
      "Iteration 2505, loss = 0.14851378\n",
      "Iteration 2506, loss = 0.14847900\n",
      "Iteration 2507, loss = 0.14844426\n",
      "Iteration 2508, loss = 0.14840955\n",
      "Iteration 2509, loss = 0.14837488\n",
      "Iteration 2510, loss = 0.14834024\n",
      "Iteration 2511, loss = 0.14830564\n",
      "Iteration 2512, loss = 0.14827107\n",
      "Iteration 2513, loss = 0.14823653\n",
      "Iteration 2514, loss = 0.14820203\n",
      "Iteration 2515, loss = 0.14816756\n",
      "Iteration 2516, loss = 0.14813313\n",
      "Iteration 2517, loss = 0.14809872\n",
      "Iteration 2518, loss = 0.14806436\n",
      "Iteration 2519, loss = 0.14803003\n",
      "Iteration 2520, loss = 0.14799573\n",
      "Iteration 2521, loss = 0.14796146\n",
      "Iteration 2522, loss = 0.14792723\n",
      "Iteration 2523, loss = 0.14789303\n",
      "Iteration 2524, loss = 0.14785887\n",
      "Iteration 2525, loss = 0.14782474\n",
      "Iteration 2526, loss = 0.14779064\n",
      "Iteration 2527, loss = 0.14775658\n",
      "Iteration 2528, loss = 0.14772255\n",
      "Iteration 2529, loss = 0.14768855\n",
      "Iteration 2530, loss = 0.14765458\n",
      "Iteration 2531, loss = 0.14762065\n",
      "Iteration 2532, loss = 0.14758676\n",
      "Iteration 2533, loss = 0.14755289\n",
      "Iteration 2534, loss = 0.14751906\n",
      "Iteration 2535, loss = 0.14748526\n",
      "Iteration 2536, loss = 0.14745150\n",
      "Iteration 2537, loss = 0.14741777\n",
      "Iteration 2538, loss = 0.14738407\n",
      "Iteration 2539, loss = 0.14735040\n",
      "Iteration 2540, loss = 0.14731677\n",
      "Iteration 2541, loss = 0.14728317\n",
      "Iteration 2542, loss = 0.14724960\n",
      "Iteration 2543, loss = 0.14721607\n",
      "Iteration 2544, loss = 0.14718256\n",
      "Iteration 2545, loss = 0.14714910\n",
      "Iteration 2546, loss = 0.14711566\n",
      "Iteration 2547, loss = 0.14708225\n",
      "Iteration 2548, loss = 0.14704888\n",
      "Iteration 2549, loss = 0.14701554\n",
      "Iteration 2550, loss = 0.14698224\n",
      "Iteration 2551, loss = 0.14694896\n",
      "Iteration 2552, loss = 0.14691572\n",
      "Iteration 2553, loss = 0.14688251\n",
      "Iteration 2554, loss = 0.14684933\n",
      "Iteration 2555, loss = 0.14681619\n",
      "Iteration 2556, loss = 0.14678307\n",
      "Iteration 2557, loss = 0.14674999\n",
      "Iteration 2558, loss = 0.14671694\n",
      "Iteration 2559, loss = 0.14668392\n",
      "Iteration 2560, loss = 0.14665094\n",
      "Iteration 2561, loss = 0.14661799\n",
      "Iteration 2562, loss = 0.14658506\n",
      "Iteration 2563, loss = 0.14655217\n",
      "Iteration 2564, loss = 0.14651932\n",
      "Iteration 2565, loss = 0.14648649\n",
      "Iteration 2566, loss = 0.14645370\n",
      "Iteration 2567, loss = 0.14642093\n",
      "Iteration 2568, loss = 0.14638820\n",
      "Iteration 2569, loss = 0.14635550\n",
      "Iteration 2570, loss = 0.14632283\n",
      "Iteration 2571, loss = 0.14629020\n",
      "Iteration 2572, loss = 0.14625759\n",
      "Iteration 2573, loss = 0.14622502\n",
      "Iteration 2574, loss = 0.14619248\n",
      "Iteration 2575, loss = 0.14615997\n",
      "Iteration 2576, loss = 0.14612749\n",
      "Iteration 2577, loss = 0.14609504\n",
      "Iteration 2578, loss = 0.14606262\n",
      "Iteration 2579, loss = 0.14603023\n",
      "Iteration 2580, loss = 0.14599788\n",
      "Iteration 2581, loss = 0.14596556\n",
      "Iteration 2582, loss = 0.14593326\n",
      "Iteration 2583, loss = 0.14590100\n",
      "Iteration 2584, loss = 0.14586877\n",
      "Iteration 2585, loss = 0.14583657\n",
      "Iteration 2586, loss = 0.14580440\n",
      "Iteration 2587, loss = 0.14577226\n",
      "Iteration 2588, loss = 0.14574015\n",
      "Iteration 2589, loss = 0.14570808\n",
      "Iteration 2590, loss = 0.14567603\n",
      "Iteration 2591, loss = 0.14564402\n",
      "Iteration 2592, loss = 0.14561203\n",
      "Iteration 2593, loss = 0.14558008\n",
      "Iteration 2594, loss = 0.14554815\n",
      "Iteration 2595, loss = 0.14551626\n",
      "Iteration 2596, loss = 0.14548440\n",
      "Iteration 2597, loss = 0.14545256\n",
      "Iteration 2598, loss = 0.14542076\n",
      "Iteration 2599, loss = 0.14538899\n",
      "Iteration 2600, loss = 0.14535725\n",
      "Iteration 2601, loss = 0.14532554\n",
      "Iteration 2602, loss = 0.14529386\n",
      "Iteration 2603, loss = 0.14526220\n",
      "Iteration 2604, loss = 0.14523058\n",
      "Iteration 2605, loss = 0.14519899\n",
      "Iteration 2606, loss = 0.14516743\n",
      "Iteration 2607, loss = 0.14513590\n",
      "Iteration 2608, loss = 0.14510440\n",
      "Iteration 2609, loss = 0.14507293\n",
      "Iteration 2610, loss = 0.14504149\n",
      "Iteration 2611, loss = 0.14501008\n",
      "Iteration 2612, loss = 0.14497870\n",
      "Iteration 2613, loss = 0.14494734\n",
      "Iteration 2614, loss = 0.14491602\n",
      "Iteration 2615, loss = 0.14488473\n",
      "Iteration 2616, loss = 0.14485347\n",
      "Iteration 2617, loss = 0.14482223\n",
      "Iteration 2618, loss = 0.14479103\n",
      "Iteration 2619, loss = 0.14475986\n",
      "Iteration 2620, loss = 0.14472871\n",
      "Iteration 2621, loss = 0.14469760\n",
      "Iteration 2622, loss = 0.14466651\n",
      "Iteration 2623, loss = 0.14463546\n",
      "Iteration 2624, loss = 0.14460443\n",
      "Iteration 2625, loss = 0.14457343\n",
      "Iteration 2626, loss = 0.14454246\n",
      "Iteration 2627, loss = 0.14451152\n",
      "Iteration 2628, loss = 0.14448061\n",
      "Iteration 2629, loss = 0.14444973\n",
      "Iteration 2630, loss = 0.14441888\n",
      "Iteration 2631, loss = 0.14438806\n",
      "Iteration 2632, loss = 0.14435726\n",
      "Iteration 2633, loss = 0.14432650\n",
      "Iteration 2634, loss = 0.14429576\n",
      "Iteration 2635, loss = 0.14426506\n",
      "Iteration 2636, loss = 0.14423438\n",
      "Iteration 2637, loss = 0.14420373\n",
      "Iteration 2638, loss = 0.14417311\n",
      "Iteration 2639, loss = 0.14414252\n",
      "Iteration 2640, loss = 0.14411195\n",
      "Iteration 2641, loss = 0.14408142\n",
      "Iteration 2642, loss = 0.14405091\n",
      "Iteration 2643, loss = 0.14402044\n",
      "Iteration 2644, loss = 0.14398999\n",
      "Iteration 2645, loss = 0.14395957\n",
      "Iteration 2646, loss = 0.14392918\n",
      "Iteration 2647, loss = 0.14389881\n",
      "Iteration 2648, loss = 0.14386848\n",
      "Iteration 2649, loss = 0.14383817\n",
      "Iteration 2650, loss = 0.14380789\n",
      "Iteration 2651, loss = 0.14377764\n",
      "Iteration 2652, loss = 0.14374742\n",
      "Iteration 2653, loss = 0.14371723\n",
      "Iteration 2654, loss = 0.14368706\n",
      "Iteration 2655, loss = 0.14365693\n",
      "Iteration 2656, loss = 0.14362682\n",
      "Iteration 2657, loss = 0.14359674\n",
      "Iteration 2658, loss = 0.14356668\n",
      "Iteration 2659, loss = 0.14353666\n",
      "Iteration 2660, loss = 0.14350666\n",
      "Iteration 2661, loss = 0.14347669\n",
      "Iteration 2662, loss = 0.14344675\n",
      "Iteration 2663, loss = 0.14341684\n",
      "Iteration 2664, loss = 0.14338695\n",
      "Iteration 2665, loss = 0.14335710\n",
      "Iteration 2666, loss = 0.14332727\n",
      "Iteration 2667, loss = 0.14329746\n",
      "Iteration 2668, loss = 0.14326769\n",
      "Iteration 2669, loss = 0.14323794\n",
      "Iteration 2670, loss = 0.14320822\n",
      "Iteration 2671, loss = 0.14317853\n",
      "Iteration 2672, loss = 0.14314887\n",
      "Iteration 2673, loss = 0.14311923\n",
      "Iteration 2674, loss = 0.14308962\n",
      "Iteration 2675, loss = 0.14306004\n",
      "Iteration 2676, loss = 0.14303048\n",
      "Iteration 2677, loss = 0.14300096\n",
      "Iteration 2678, loss = 0.14297146\n",
      "Iteration 2679, loss = 0.14294198\n",
      "Iteration 2680, loss = 0.14291254\n",
      "Iteration 2681, loss = 0.14288312\n",
      "Iteration 2682, loss = 0.14285373\n",
      "Iteration 2683, loss = 0.14282436\n",
      "Iteration 2684, loss = 0.14279503\n",
      "Iteration 2685, loss = 0.14276572\n",
      "Iteration 2686, loss = 0.14273644\n",
      "Iteration 2687, loss = 0.14270718\n",
      "Iteration 2688, loss = 0.14267795\n",
      "Iteration 2689, loss = 0.14264875\n",
      "Iteration 2690, loss = 0.14261957\n",
      "Iteration 2691, loss = 0.14259043\n",
      "Iteration 2692, loss = 0.14256130\n",
      "Iteration 2693, loss = 0.14253221\n",
      "Iteration 2694, loss = 0.14250314\n",
      "Iteration 2695, loss = 0.14247410\n",
      "Iteration 2696, loss = 0.14244509\n",
      "Iteration 2697, loss = 0.14241610\n",
      "Iteration 2698, loss = 0.14238714\n",
      "Iteration 2699, loss = 0.14235820\n",
      "Iteration 2700, loss = 0.14232929\n",
      "Iteration 2701, loss = 0.14230041\n",
      "Iteration 2702, loss = 0.14227156\n",
      "Iteration 2703, loss = 0.14224273\n",
      "Iteration 2704, loss = 0.14221393\n",
      "Iteration 2705, loss = 0.14218515\n",
      "Iteration 2706, loss = 0.14215640\n",
      "Iteration 2707, loss = 0.14212768\n",
      "Iteration 2708, loss = 0.14209898\n",
      "Iteration 2709, loss = 0.14207031\n",
      "Iteration 2710, loss = 0.14204166\n",
      "Iteration 2711, loss = 0.14201304\n",
      "Iteration 2712, loss = 0.14198445\n",
      "Iteration 2713, loss = 0.14195589\n",
      "Iteration 2714, loss = 0.14192735\n",
      "Iteration 2715, loss = 0.14189883\n",
      "Iteration 2716, loss = 0.14187034\n",
      "Iteration 2717, loss = 0.14184188\n",
      "Iteration 2718, loss = 0.14181344\n",
      "Iteration 2719, loss = 0.14178503\n",
      "Iteration 2720, loss = 0.14175665\n",
      "Iteration 2721, loss = 0.14172829\n",
      "Iteration 2722, loss = 0.14169996\n",
      "Iteration 2723, loss = 0.14167165\n",
      "Iteration 2724, loss = 0.14164337\n",
      "Iteration 2725, loss = 0.14161511\n",
      "Iteration 2726, loss = 0.14158688\n",
      "Iteration 2727, loss = 0.14155868\n",
      "Iteration 2728, loss = 0.14153050\n",
      "Iteration 2729, loss = 0.14150234\n",
      "Iteration 2730, loss = 0.14147422\n",
      "Iteration 2731, loss = 0.14144611\n",
      "Iteration 2732, loss = 0.14141804\n",
      "Iteration 2733, loss = 0.14138999\n",
      "Iteration 2734, loss = 0.14136196\n",
      "Iteration 2735, loss = 0.14133396\n",
      "Iteration 2736, loss = 0.14130598\n",
      "Iteration 2737, loss = 0.14127803\n",
      "Iteration 2738, loss = 0.14125011\n",
      "Iteration 2739, loss = 0.14122221\n",
      "Iteration 2740, loss = 0.14119433\n",
      "Iteration 2741, loss = 0.14116649\n",
      "Iteration 2742, loss = 0.14113866\n",
      "Iteration 2743, loss = 0.14111086\n",
      "Iteration 2744, loss = 0.14108309\n",
      "Iteration 2745, loss = 0.14105534\n",
      "Iteration 2746, loss = 0.14102762\n",
      "Iteration 2747, loss = 0.14099992\n",
      "Iteration 2748, loss = 0.14097224\n",
      "Iteration 2749, loss = 0.14094459\n",
      "Iteration 2750, loss = 0.14091697\n",
      "Iteration 2751, loss = 0.14088937\n",
      "Iteration 2752, loss = 0.14086180\n",
      "Iteration 2753, loss = 0.14083425\n",
      "Iteration 2754, loss = 0.14080672\n",
      "Iteration 2755, loss = 0.14077922\n",
      "Iteration 2756, loss = 0.14075174\n",
      "Iteration 2757, loss = 0.14072429\n",
      "Iteration 2758, loss = 0.14069687\n",
      "Iteration 2759, loss = 0.14066947\n",
      "Iteration 2760, loss = 0.14064209\n",
      "Iteration 2761, loss = 0.14061474\n",
      "Iteration 2762, loss = 0.14058741\n",
      "Iteration 2763, loss = 0.14056010\n",
      "Iteration 2764, loss = 0.14053283\n",
      "Iteration 2765, loss = 0.14050557\n",
      "Iteration 2766, loss = 0.14047834\n",
      "Iteration 2767, loss = 0.14045113\n",
      "Iteration 2768, loss = 0.14042395\n",
      "Iteration 2769, loss = 0.14039680\n",
      "Iteration 2770, loss = 0.14036966\n",
      "Iteration 2771, loss = 0.14034255\n",
      "Iteration 2772, loss = 0.14031547\n",
      "Iteration 2773, loss = 0.14028841\n",
      "Iteration 2774, loss = 0.14026137\n",
      "Iteration 2775, loss = 0.14023436\n",
      "Iteration 2776, loss = 0.14020737\n",
      "Iteration 2777, loss = 0.14018041\n",
      "Iteration 2778, loss = 0.14015347\n",
      "Iteration 2779, loss = 0.14012655\n",
      "Iteration 2780, loss = 0.14009966\n",
      "Iteration 2781, loss = 0.14007279\n",
      "Iteration 2782, loss = 0.14004595\n",
      "Iteration 2783, loss = 0.14001913\n",
      "Iteration 2784, loss = 0.13999233\n",
      "Iteration 2785, loss = 0.13996556\n",
      "Iteration 2786, loss = 0.13993881\n",
      "Iteration 2787, loss = 0.13991208\n",
      "Iteration 2788, loss = 0.13988538\n",
      "Iteration 2789, loss = 0.13985870\n",
      "Iteration 2790, loss = 0.13983205\n",
      "Iteration 2791, loss = 0.13980542\n",
      "Iteration 2792, loss = 0.13977881\n",
      "Iteration 2793, loss = 0.13975223\n",
      "Iteration 2794, loss = 0.13972567\n",
      "Iteration 2795, loss = 0.13969913\n",
      "Iteration 2796, loss = 0.13967262\n",
      "Iteration 2797, loss = 0.13964613\n",
      "Iteration 2798, loss = 0.13961966\n",
      "Iteration 2799, loss = 0.13959322\n",
      "Iteration 2800, loss = 0.13956680\n",
      "Iteration 2801, loss = 0.13954040\n",
      "Iteration 2802, loss = 0.13951403\n",
      "Iteration 2803, loss = 0.13948768\n",
      "Iteration 2804, loss = 0.13946136\n",
      "Iteration 2805, loss = 0.13943505\n",
      "Iteration 2806, loss = 0.13940877\n",
      "Iteration 2807, loss = 0.13938252\n",
      "Iteration 2808, loss = 0.13935628\n",
      "Iteration 2809, loss = 0.13933007\n",
      "Iteration 2810, loss = 0.13930389\n",
      "Iteration 2811, loss = 0.13927772\n",
      "Iteration 2812, loss = 0.13925158\n",
      "Iteration 2813, loss = 0.13922546\n",
      "Iteration 2814, loss = 0.13919937\n",
      "Iteration 2815, loss = 0.13917329\n",
      "Iteration 2816, loss = 0.13914725\n",
      "Iteration 2817, loss = 0.13912122\n",
      "Iteration 2818, loss = 0.13909522\n",
      "Iteration 2819, loss = 0.13906923\n",
      "Iteration 2820, loss = 0.13904328\n",
      "Iteration 2821, loss = 0.13901734\n",
      "Iteration 2822, loss = 0.13899143\n",
      "Iteration 2823, loss = 0.13896554\n",
      "Iteration 2824, loss = 0.13893967\n",
      "Iteration 2825, loss = 0.13891383\n",
      "Iteration 2826, loss = 0.13888800\n",
      "Iteration 2827, loss = 0.13886221\n",
      "Iteration 2828, loss = 0.13883643\n",
      "Iteration 2829, loss = 0.13881067\n",
      "Iteration 2830, loss = 0.13878494\n",
      "Iteration 2831, loss = 0.13875923\n",
      "Iteration 2832, loss = 0.13873355\n",
      "Iteration 2833, loss = 0.13870788\n",
      "Iteration 2834, loss = 0.13868224\n",
      "Iteration 2835, loss = 0.13865662\n",
      "Iteration 2836, loss = 0.13863102\n",
      "Iteration 2837, loss = 0.13860545\n",
      "Iteration 2838, loss = 0.13857990\n",
      "Iteration 2839, loss = 0.13855437\n",
      "Iteration 2840, loss = 0.13852886\n",
      "Iteration 2841, loss = 0.13850337\n",
      "Iteration 2842, loss = 0.13847791\n",
      "Iteration 2843, loss = 0.13845247\n",
      "Iteration 2844, loss = 0.13842705\n",
      "Iteration 2845, loss = 0.13840165\n",
      "Iteration 2846, loss = 0.13837627\n",
      "Iteration 2847, loss = 0.13835092\n",
      "Iteration 2848, loss = 0.13832559\n",
      "Iteration 2849, loss = 0.13830028\n",
      "Iteration 2850, loss = 0.13827499\n",
      "Iteration 2851, loss = 0.13824973\n",
      "Iteration 2852, loss = 0.13822448\n",
      "Iteration 2853, loss = 0.13819926\n",
      "Iteration 2854, loss = 0.13817406\n",
      "Iteration 2855, loss = 0.13814888\n",
      "Iteration 2856, loss = 0.13812373\n",
      "Iteration 2857, loss = 0.13809859\n",
      "Iteration 2858, loss = 0.13807348\n",
      "Iteration 2859, loss = 0.13804839\n",
      "Iteration 2860, loss = 0.13802332\n",
      "Iteration 2861, loss = 0.13799827\n",
      "Iteration 2862, loss = 0.13797325\n",
      "Iteration 2863, loss = 0.13794824\n",
      "Iteration 2864, loss = 0.13792326\n",
      "Iteration 2865, loss = 0.13789830\n",
      "Iteration 2866, loss = 0.13787336\n",
      "Iteration 2867, loss = 0.13784844\n",
      "Iteration 2868, loss = 0.13782355\n",
      "Iteration 2869, loss = 0.13779867\n",
      "Iteration 2870, loss = 0.13777382\n",
      "Iteration 2871, loss = 0.13774899\n",
      "Iteration 2872, loss = 0.13772417\n",
      "Iteration 2873, loss = 0.13769938\n",
      "Iteration 2874, loss = 0.13767462\n",
      "Iteration 2875, loss = 0.13764987\n",
      "Iteration 2876, loss = 0.13762514\n",
      "Iteration 2877, loss = 0.13760044\n",
      "Iteration 2878, loss = 0.13757576\n",
      "Iteration 2879, loss = 0.13755109\n",
      "Iteration 2880, loss = 0.13752645\n",
      "Iteration 2881, loss = 0.13750183\n",
      "Iteration 2882, loss = 0.13747724\n",
      "Iteration 2883, loss = 0.13745266\n",
      "Iteration 2884, loss = 0.13742810\n",
      "Iteration 2885, loss = 0.13740357\n",
      "Iteration 2886, loss = 0.13737905\n",
      "Iteration 2887, loss = 0.13735456\n",
      "Iteration 2888, loss = 0.13733009\n",
      "Iteration 2889, loss = 0.13730564\n",
      "Iteration 2890, loss = 0.13728121\n",
      "Iteration 2891, loss = 0.13725680\n",
      "Iteration 2892, loss = 0.13723241\n",
      "Iteration 2893, loss = 0.13720804\n",
      "Iteration 2894, loss = 0.13718369\n",
      "Iteration 2895, loss = 0.13715937\n",
      "Iteration 2896, loss = 0.13713506\n",
      "Iteration 2897, loss = 0.13711078\n",
      "Iteration 2898, loss = 0.13708651\n",
      "Iteration 2899, loss = 0.13706227\n",
      "Iteration 2900, loss = 0.13703805\n",
      "Iteration 2901, loss = 0.13701384\n",
      "Iteration 2902, loss = 0.13698966\n",
      "Iteration 2903, loss = 0.13696550\n",
      "Iteration 2904, loss = 0.13694136\n",
      "Iteration 2905, loss = 0.13691724\n",
      "Iteration 2906, loss = 0.13689314\n",
      "Iteration 2907, loss = 0.13686906\n",
      "Iteration 2908, loss = 0.13684501\n",
      "Iteration 2909, loss = 0.13682097\n",
      "Iteration 2910, loss = 0.13679695\n",
      "Iteration 2911, loss = 0.13677295\n",
      "Iteration 2912, loss = 0.13674898\n",
      "Iteration 2913, loss = 0.13672502\n",
      "Iteration 2914, loss = 0.13670108\n",
      "Iteration 2915, loss = 0.13667717\n",
      "Iteration 2916, loss = 0.13665327\n",
      "Iteration 2917, loss = 0.13662940\n",
      "Iteration 2918, loss = 0.13660554\n",
      "Iteration 2919, loss = 0.13658171\n",
      "Iteration 2920, loss = 0.13655789\n",
      "Iteration 2921, loss = 0.13653410\n",
      "Iteration 2922, loss = 0.13651033\n",
      "Iteration 2923, loss = 0.13648657\n",
      "Iteration 2924, loss = 0.13646284\n",
      "Iteration 2925, loss = 0.13643912\n",
      "Iteration 2926, loss = 0.13641543\n",
      "Iteration 2927, loss = 0.13639176\n",
      "Iteration 2928, loss = 0.13636810\n",
      "Iteration 2929, loss = 0.13634447\n",
      "Iteration 2930, loss = 0.13632085\n",
      "Iteration 2931, loss = 0.13629726\n",
      "Iteration 2932, loss = 0.13627369\n",
      "Iteration 2933, loss = 0.13625013\n",
      "Iteration 2934, loss = 0.13622660\n",
      "Iteration 2935, loss = 0.13620308\n",
      "Iteration 2936, loss = 0.13617959\n",
      "Iteration 2937, loss = 0.13615611\n",
      "Iteration 2938, loss = 0.13613266\n",
      "Iteration 2939, loss = 0.13610922\n",
      "Iteration 2940, loss = 0.13608581\n",
      "Iteration 2941, loss = 0.13606241\n",
      "Iteration 2942, loss = 0.13603903\n",
      "Iteration 2943, loss = 0.13601568\n",
      "Iteration 2944, loss = 0.13599234\n",
      "Iteration 2945, loss = 0.13596902\n",
      "Iteration 2946, loss = 0.13594572\n",
      "Iteration 2947, loss = 0.13592244\n",
      "Iteration 2948, loss = 0.13589919\n",
      "Iteration 2949, loss = 0.13587595\n",
      "Iteration 2950, loss = 0.13585273\n",
      "Iteration 2951, loss = 0.13582952\n",
      "Iteration 2952, loss = 0.13580634\n",
      "Iteration 2953, loss = 0.13578318\n",
      "Iteration 2954, loss = 0.13576004\n",
      "Iteration 2955, loss = 0.13573692\n",
      "Iteration 2956, loss = 0.13571381\n",
      "Iteration 2957, loss = 0.13569073\n",
      "Iteration 2958, loss = 0.13566766\n",
      "Iteration 2959, loss = 0.13564462\n",
      "Iteration 2960, loss = 0.13562159\n",
      "Iteration 2961, loss = 0.13559858\n",
      "Iteration 2962, loss = 0.13557559\n",
      "Iteration 2963, loss = 0.13555262\n",
      "Iteration 2964, loss = 0.13552967\n",
      "Iteration 2965, loss = 0.13550674\n",
      "Iteration 2966, loss = 0.13548383\n",
      "Iteration 2967, loss = 0.13546094\n",
      "Iteration 2968, loss = 0.13543806\n",
      "Iteration 2969, loss = 0.13541521\n",
      "Iteration 2970, loss = 0.13539237\n",
      "Iteration 2971, loss = 0.13536956\n",
      "Iteration 2972, loss = 0.13534676\n",
      "Iteration 2973, loss = 0.13532398\n",
      "Iteration 2974, loss = 0.13530122\n",
      "Iteration 2975, loss = 0.13527848\n",
      "Iteration 2976, loss = 0.13525576\n",
      "Iteration 2977, loss = 0.13523306\n",
      "Iteration 2978, loss = 0.13521037\n",
      "Iteration 2979, loss = 0.13518771\n",
      "Iteration 2980, loss = 0.13516506\n",
      "Iteration 2981, loss = 0.13514243\n",
      "Iteration 2982, loss = 0.13511982\n",
      "Iteration 2983, loss = 0.13509723\n",
      "Iteration 2984, loss = 0.13507466\n",
      "Iteration 2985, loss = 0.13505211\n",
      "Iteration 2986, loss = 0.13502957\n",
      "Iteration 2987, loss = 0.13500706\n",
      "Iteration 2988, loss = 0.13498456\n",
      "Iteration 2989, loss = 0.13496208\n",
      "Iteration 2990, loss = 0.13493962\n",
      "Iteration 2991, loss = 0.13491718\n",
      "Iteration 2992, loss = 0.13489476\n",
      "Iteration 2993, loss = 0.13487235\n",
      "Iteration 2994, loss = 0.13484997\n",
      "Iteration 2995, loss = 0.13482760\n",
      "Iteration 2996, loss = 0.13480525\n",
      "Iteration 2997, loss = 0.13478292\n",
      "Iteration 2998, loss = 0.13476061\n",
      "Iteration 2999, loss = 0.13473831\n",
      "Iteration 3000, loss = 0.13471604\n",
      "Iteration 3001, loss = 0.13469378\n",
      "Iteration 3002, loss = 0.13467154\n",
      "Iteration 3003, loss = 0.13464932\n",
      "Iteration 3004, loss = 0.13462712\n",
      "Iteration 3005, loss = 0.13460494\n",
      "Iteration 3006, loss = 0.13458277\n",
      "Iteration 3007, loss = 0.13456062\n",
      "Iteration 3008, loss = 0.13453850\n",
      "Iteration 3009, loss = 0.13451638\n",
      "Iteration 3010, loss = 0.13449429\n",
      "Iteration 3011, loss = 0.13447222\n",
      "Iteration 3012, loss = 0.13445016\n",
      "Iteration 3013, loss = 0.13442812\n",
      "Iteration 3014, loss = 0.13440610\n",
      "Iteration 3015, loss = 0.13438410\n",
      "Iteration 3016, loss = 0.13436211\n",
      "Iteration 3017, loss = 0.13434015\n",
      "Iteration 3018, loss = 0.13431820\n",
      "Iteration 3019, loss = 0.13429627\n",
      "Iteration 3020, loss = 0.13427436\n",
      "Iteration 3021, loss = 0.13425246\n",
      "Iteration 3022, loss = 0.13423058\n",
      "Iteration 3023, loss = 0.13420872\n",
      "Iteration 3024, loss = 0.13418688\n",
      "Iteration 3025, loss = 0.13416506\n",
      "Iteration 3026, loss = 0.13414326\n",
      "Iteration 3027, loss = 0.13412147\n",
      "Iteration 3028, loss = 0.13409970\n",
      "Iteration 3029, loss = 0.13407795\n",
      "Iteration 3030, loss = 0.13405621\n",
      "Iteration 3031, loss = 0.13403449\n",
      "Iteration 3032, loss = 0.13401280\n",
      "Iteration 3033, loss = 0.13399111\n",
      "Iteration 3034, loss = 0.13396945\n",
      "Iteration 3035, loss = 0.13394781\n",
      "Iteration 3036, loss = 0.13392618\n",
      "Iteration 3037, loss = 0.13390457\n",
      "Iteration 3038, loss = 0.13388297\n",
      "Iteration 3039, loss = 0.13386140\n",
      "Iteration 3040, loss = 0.13383984\n",
      "Iteration 3041, loss = 0.13381830\n",
      "Iteration 3042, loss = 0.13379678\n",
      "Iteration 3043, loss = 0.13377527\n",
      "Iteration 3044, loss = 0.13375378\n",
      "Iteration 3045, loss = 0.13373231\n",
      "Iteration 3046, loss = 0.13371086\n",
      "Iteration 3047, loss = 0.13368942\n",
      "Iteration 3048, loss = 0.13366800\n",
      "Iteration 3049, loss = 0.13364660\n",
      "Iteration 3050, loss = 0.13362522\n",
      "Iteration 3051, loss = 0.13360385\n",
      "Iteration 3052, loss = 0.13358250\n",
      "Iteration 3053, loss = 0.13356117\n",
      "Iteration 3054, loss = 0.13353985\n",
      "Iteration 3055, loss = 0.13351856\n",
      "Iteration 3056, loss = 0.13349728\n",
      "Iteration 3057, loss = 0.13347601\n",
      "Iteration 3058, loss = 0.13345477\n",
      "Iteration 3059, loss = 0.13343354\n",
      "Iteration 3060, loss = 0.13341233\n",
      "Iteration 3061, loss = 0.13339113\n",
      "Iteration 3062, loss = 0.13336995\n",
      "Iteration 3063, loss = 0.13334879\n",
      "Iteration 3064, loss = 0.13332765\n",
      "Iteration 3065, loss = 0.13330652\n",
      "Iteration 3066, loss = 0.13328542\n",
      "Iteration 3067, loss = 0.13326432\n",
      "Iteration 3068, loss = 0.13324325\n",
      "Iteration 3069, loss = 0.13322219\n",
      "Iteration 3070, loss = 0.13320115\n",
      "Iteration 3071, loss = 0.13318012\n",
      "Iteration 3072, loss = 0.13315912\n",
      "Iteration 3073, loss = 0.13313813\n",
      "Iteration 3074, loss = 0.13311715\n",
      "Iteration 3075, loss = 0.13309619\n",
      "Iteration 3076, loss = 0.13307525\n",
      "Iteration 3077, loss = 0.13305433\n",
      "Iteration 3078, loss = 0.13303342\n",
      "Iteration 3079, loss = 0.13301253\n",
      "Iteration 3080, loss = 0.13299166\n",
      "Iteration 3081, loss = 0.13297080\n",
      "Iteration 3082, loss = 0.13294997\n",
      "Iteration 3083, loss = 0.13292914\n",
      "Iteration 3084, loss = 0.13290834\n",
      "Iteration 3085, loss = 0.13288755\n",
      "Iteration 3086, loss = 0.13286677\n",
      "Iteration 3087, loss = 0.13284602\n",
      "Iteration 3088, loss = 0.13282528\n",
      "Iteration 3089, loss = 0.13280455\n",
      "Iteration 3090, loss = 0.13278385\n",
      "Iteration 3091, loss = 0.13276316\n",
      "Iteration 3092, loss = 0.13274248\n",
      "Iteration 3093, loss = 0.13272183\n",
      "Iteration 3094, loss = 0.13270119\n",
      "Iteration 3095, loss = 0.13268056\n",
      "Iteration 3096, loss = 0.13265995\n",
      "Iteration 3097, loss = 0.13263936\n",
      "Iteration 3098, loss = 0.13261879\n",
      "Iteration 3099, loss = 0.13259823\n",
      "Iteration 3100, loss = 0.13257769\n",
      "Iteration 3101, loss = 0.13255716\n",
      "Iteration 3102, loss = 0.13253665\n",
      "Iteration 3103, loss = 0.13251616\n",
      "Iteration 3104, loss = 0.13249568\n",
      "Iteration 3105, loss = 0.13247522\n",
      "Iteration 3106, loss = 0.13245478\n",
      "Iteration 3107, loss = 0.13243435\n",
      "Iteration 3108, loss = 0.13241394\n",
      "Iteration 3109, loss = 0.13239354\n",
      "Iteration 3110, loss = 0.13237316\n",
      "Iteration 3111, loss = 0.13235280\n",
      "Iteration 3112, loss = 0.13233245\n",
      "Iteration 3113, loss = 0.13231212\n",
      "Iteration 3114, loss = 0.13229181\n",
      "Iteration 3115, loss = 0.13227151\n",
      "Iteration 3116, loss = 0.13225123\n",
      "Iteration 3117, loss = 0.13223096\n",
      "Iteration 3118, loss = 0.13221071\n",
      "Iteration 3119, loss = 0.13219048\n",
      "Iteration 3120, loss = 0.13217026\n",
      "Iteration 3121, loss = 0.13215005\n",
      "Iteration 3122, loss = 0.13212987\n",
      "Iteration 3123, loss = 0.13210970\n",
      "Iteration 3124, loss = 0.13208954\n",
      "Iteration 3125, loss = 0.13206940\n",
      "Iteration 3126, loss = 0.13204928\n",
      "Iteration 3127, loss = 0.13202918\n",
      "Iteration 3128, loss = 0.13200908\n",
      "Iteration 3129, loss = 0.13198901\n",
      "Iteration 3130, loss = 0.13196895\n",
      "Iteration 3131, loss = 0.13194891\n",
      "Iteration 3132, loss = 0.13192888\n",
      "Iteration 3133, loss = 0.13190887\n",
      "Iteration 3134, loss = 0.13188887\n",
      "Iteration 3135, loss = 0.13186889\n",
      "Iteration 3136, loss = 0.13184893\n",
      "Iteration 3137, loss = 0.13182898\n",
      "Iteration 3138, loss = 0.13180904\n",
      "Iteration 3139, loss = 0.13178913\n",
      "Iteration 3140, loss = 0.13176922\n",
      "Iteration 3141, loss = 0.13174934\n",
      "Iteration 3142, loss = 0.13172947\n",
      "Iteration 3143, loss = 0.13170961\n",
      "Iteration 3144, loss = 0.13168977\n",
      "Iteration 3145, loss = 0.13166995\n",
      "Iteration 3146, loss = 0.13165014\n",
      "Iteration 3147, loss = 0.13163035\n",
      "Iteration 3148, loss = 0.13161057\n",
      "Iteration 3149, loss = 0.13159081\n",
      "Iteration 3150, loss = 0.13157106\n",
      "Iteration 3151, loss = 0.13155133\n",
      "Iteration 3152, loss = 0.13153162\n",
      "Iteration 3153, loss = 0.13151192\n",
      "Iteration 3154, loss = 0.13149223\n",
      "Iteration 3155, loss = 0.13147256\n",
      "Iteration 3156, loss = 0.13145291\n",
      "Iteration 3157, loss = 0.13143327\n",
      "Iteration 3158, loss = 0.13141365\n",
      "Iteration 3159, loss = 0.13139404\n",
      "Iteration 3160, loss = 0.13137445\n",
      "Iteration 3161, loss = 0.13135487\n",
      "Iteration 3162, loss = 0.13133531\n",
      "Iteration 3163, loss = 0.13131576\n",
      "Iteration 3164, loss = 0.13129623\n",
      "Iteration 3165, loss = 0.13127672\n",
      "Iteration 3166, loss = 0.13125722\n",
      "Iteration 3167, loss = 0.13123773\n",
      "Iteration 3168, loss = 0.13121826\n",
      "Iteration 3169, loss = 0.13119880\n",
      "Iteration 3170, loss = 0.13117936\n",
      "Iteration 3171, loss = 0.13115994\n",
      "Iteration 3172, loss = 0.13114053\n",
      "Iteration 3173, loss = 0.13112113\n",
      "Iteration 3174, loss = 0.13110175\n",
      "Iteration 3175, loss = 0.13108239\n",
      "Iteration 3176, loss = 0.13106304\n",
      "Iteration 3177, loss = 0.13104371\n",
      "Iteration 3178, loss = 0.13102439\n",
      "Iteration 3179, loss = 0.13100508\n",
      "Iteration 3180, loss = 0.13098579\n",
      "Iteration 3181, loss = 0.13096652\n",
      "Iteration 3182, loss = 0.13094726\n",
      "Iteration 3183, loss = 0.13092801\n",
      "Iteration 3184, loss = 0.13090878\n",
      "Iteration 3185, loss = 0.13088957\n",
      "Iteration 3186, loss = 0.13087037\n",
      "Iteration 3187, loss = 0.13085118\n",
      "Iteration 3188, loss = 0.13083201\n",
      "Iteration 3189, loss = 0.13081286\n",
      "Iteration 3190, loss = 0.13079372\n",
      "Iteration 3191, loss = 0.13077459\n",
      "Iteration 3192, loss = 0.13075548\n",
      "Iteration 3193, loss = 0.13073638\n",
      "Iteration 3194, loss = 0.13071730\n",
      "Iteration 3195, loss = 0.13069824\n",
      "Iteration 3196, loss = 0.13067918\n",
      "Iteration 3197, loss = 0.13066015\n",
      "Iteration 3198, loss = 0.13064112\n",
      "Iteration 3199, loss = 0.13062212\n",
      "Iteration 3200, loss = 0.13060312\n",
      "Iteration 3201, loss = 0.13058414\n",
      "Iteration 3202, loss = 0.13056518\n",
      "Iteration 3203, loss = 0.13054623\n",
      "Iteration 3204, loss = 0.13052730\n",
      "Iteration 3205, loss = 0.13050838\n",
      "Iteration 3206, loss = 0.13048947\n",
      "Iteration 3207, loss = 0.13047058\n",
      "Iteration 3208, loss = 0.13045170\n",
      "Iteration 3209, loss = 0.13043284\n",
      "Iteration 3210, loss = 0.13041399\n",
      "Iteration 3211, loss = 0.13039516\n",
      "Iteration 3212, loss = 0.13037634\n",
      "Iteration 3213, loss = 0.13035754\n",
      "Iteration 3214, loss = 0.13033875\n",
      "Iteration 3215, loss = 0.13031997\n",
      "Iteration 3216, loss = 0.13030121\n",
      "Iteration 3217, loss = 0.13028246\n",
      "Iteration 3218, loss = 0.13026373\n",
      "Iteration 3219, loss = 0.13024501\n",
      "Iteration 3220, loss = 0.13022631\n",
      "Iteration 3221, loss = 0.13020762\n",
      "Iteration 3222, loss = 0.13018895\n",
      "Iteration 3223, loss = 0.13017029\n",
      "Iteration 3224, loss = 0.13015164\n",
      "Iteration 3225, loss = 0.13013301\n",
      "Iteration 3226, loss = 0.13011439\n",
      "Iteration 3227, loss = 0.13009579\n",
      "Iteration 3228, loss = 0.13007720\n",
      "Iteration 3229, loss = 0.13005862\n",
      "Iteration 3230, loss = 0.13004006\n",
      "Iteration 3231, loss = 0.13002152\n",
      "Iteration 3232, loss = 0.13000298\n",
      "Iteration 3233, loss = 0.12998446\n",
      "Iteration 3234, loss = 0.12996596\n",
      "Iteration 3235, loss = 0.12994747\n",
      "Iteration 3236, loss = 0.12992899\n",
      "Iteration 3237, loss = 0.12991053\n",
      "Iteration 3238, loss = 0.12989208\n",
      "Iteration 3239, loss = 0.12987365\n",
      "Iteration 3240, loss = 0.12985523\n",
      "Iteration 3241, loss = 0.12983682\n",
      "Iteration 3242, loss = 0.12981843\n",
      "Iteration 3243, loss = 0.12980005\n",
      "Iteration 3244, loss = 0.12978169\n",
      "Iteration 3245, loss = 0.12976334\n",
      "Iteration 3246, loss = 0.12974501\n",
      "Iteration 3247, loss = 0.12972668\n",
      "Iteration 3248, loss = 0.12970838\n",
      "Iteration 3249, loss = 0.12969008\n",
      "Iteration 3250, loss = 0.12967180\n",
      "Iteration 3251, loss = 0.12965353\n",
      "Iteration 3252, loss = 0.12963528\n",
      "Iteration 3253, loss = 0.12961704\n",
      "Iteration 3254, loss = 0.12959882\n",
      "Iteration 3255, loss = 0.12958061\n",
      "Iteration 3256, loss = 0.12956241\n",
      "Iteration 3257, loss = 0.12954423\n",
      "Iteration 3258, loss = 0.12952606\n",
      "Iteration 3259, loss = 0.12950790\n",
      "Iteration 3260, loss = 0.12948976\n",
      "Iteration 3261, loss = 0.12947163\n",
      "Iteration 3262, loss = 0.12945352\n",
      "Iteration 3263, loss = 0.12943541\n",
      "Iteration 3264, loss = 0.12941733\n",
      "Iteration 3265, loss = 0.12939925\n",
      "Iteration 3266, loss = 0.12938119\n",
      "Iteration 3267, loss = 0.12936315\n",
      "Iteration 3268, loss = 0.12934511\n",
      "Iteration 3269, loss = 0.12932709\n",
      "Iteration 3270, loss = 0.12930909\n",
      "Iteration 3271, loss = 0.12929110\n",
      "Iteration 3272, loss = 0.12927312\n",
      "Iteration 3273, loss = 0.12925515\n",
      "Iteration 3274, loss = 0.12923720\n",
      "Iteration 3275, loss = 0.12921926\n",
      "Iteration 3276, loss = 0.12920134\n",
      "Iteration 3277, loss = 0.12918343\n",
      "Iteration 3278, loss = 0.12916553\n",
      "Iteration 3279, loss = 0.12914765\n",
      "Iteration 3280, loss = 0.12912978\n",
      "Iteration 3281, loss = 0.12911192\n",
      "Iteration 3282, loss = 0.12909408\n",
      "Iteration 3283, loss = 0.12907625\n",
      "Iteration 3284, loss = 0.12905843\n",
      "Iteration 3285, loss = 0.12904063\n",
      "Iteration 3286, loss = 0.12902284\n",
      "Iteration 3287, loss = 0.12900506\n",
      "Iteration 3288, loss = 0.12898730\n",
      "Iteration 3289, loss = 0.12896955\n",
      "Iteration 3290, loss = 0.12895181\n",
      "Iteration 3291, loss = 0.12893408\n",
      "Iteration 3292, loss = 0.12891637\n",
      "Iteration 3293, loss = 0.12889868\n",
      "Iteration 3294, loss = 0.12888099\n",
      "Iteration 3295, loss = 0.12886332\n",
      "Iteration 3296, loss = 0.12884567\n",
      "Iteration 3297, loss = 0.12882802\n",
      "Iteration 3298, loss = 0.12881039\n",
      "Iteration 3299, loss = 0.12879277\n",
      "Iteration 3300, loss = 0.12877517\n",
      "Iteration 3301, loss = 0.12875758\n",
      "Iteration 3302, loss = 0.12874000\n",
      "Iteration 3303, loss = 0.12872243\n",
      "Iteration 3304, loss = 0.12870488\n",
      "Iteration 3305, loss = 0.12868734\n",
      "Iteration 3306, loss = 0.12866982\n",
      "Iteration 3307, loss = 0.12865231\n",
      "Iteration 3308, loss = 0.12863481\n",
      "Iteration 3309, loss = 0.12861732\n",
      "Iteration 3310, loss = 0.12859985\n",
      "Iteration 3311, loss = 0.12858239\n",
      "Iteration 3312, loss = 0.12856494\n",
      "Iteration 3313, loss = 0.12854750\n",
      "Iteration 3314, loss = 0.12853008\n",
      "Iteration 3315, loss = 0.12851267\n",
      "Iteration 3316, loss = 0.12849528\n",
      "Iteration 3317, loss = 0.12847789\n",
      "Iteration 3318, loss = 0.12846052\n",
      "Iteration 3319, loss = 0.12844317\n",
      "Iteration 3320, loss = 0.12842582\n",
      "Iteration 3321, loss = 0.12840849\n",
      "Iteration 3322, loss = 0.12839117\n",
      "Iteration 3323, loss = 0.12837387\n",
      "Iteration 3324, loss = 0.12835658\n",
      "Iteration 3325, loss = 0.12833930\n",
      "Iteration 3326, loss = 0.12832203\n",
      "Iteration 3327, loss = 0.12830478\n",
      "Iteration 3328, loss = 0.12828753\n",
      "Iteration 3329, loss = 0.12827031\n",
      "Iteration 3330, loss = 0.12825309\n",
      "Iteration 3331, loss = 0.12823589\n",
      "Iteration 3332, loss = 0.12821870\n",
      "Iteration 3333, loss = 0.12820152\n",
      "Iteration 3334, loss = 0.12818435\n",
      "Iteration 3335, loss = 0.12816720\n",
      "Iteration 3336, loss = 0.12815006\n",
      "Iteration 3337, loss = 0.12813293\n",
      "Iteration 3338, loss = 0.12811582\n",
      "Iteration 3339, loss = 0.12809872\n",
      "Iteration 3340, loss = 0.12808163\n",
      "Iteration 3341, loss = 0.12806455\n",
      "Iteration 3342, loss = 0.12804749\n",
      "Iteration 3343, loss = 0.12803044\n",
      "Iteration 3344, loss = 0.12801340\n",
      "Iteration 3345, loss = 0.12799637\n",
      "Iteration 3346, loss = 0.12797936\n",
      "Iteration 3347, loss = 0.12796236\n",
      "Iteration 3348, loss = 0.12794537\n",
      "Iteration 3349, loss = 0.12792839\n",
      "Iteration 3350, loss = 0.12791143\n",
      "Iteration 3351, loss = 0.12789448\n",
      "Iteration 3352, loss = 0.12787754\n",
      "Iteration 3353, loss = 0.12786062\n",
      "Iteration 3354, loss = 0.12784370\n",
      "Iteration 3355, loss = 0.12782680\n",
      "Iteration 3356, loss = 0.12780991\n",
      "Iteration 3357, loss = 0.12779304\n",
      "Iteration 3358, loss = 0.12777617\n",
      "Iteration 3359, loss = 0.12775932\n",
      "Iteration 3360, loss = 0.12774248\n",
      "Iteration 3361, loss = 0.12772565\n",
      "Iteration 3362, loss = 0.12770884\n",
      "Iteration 3363, loss = 0.12769204\n",
      "Iteration 3364, loss = 0.12767525\n",
      "Iteration 3365, loss = 0.12765847\n",
      "Iteration 3366, loss = 0.12764170\n",
      "Iteration 3367, loss = 0.12762495\n",
      "Iteration 3368, loss = 0.12760821\n",
      "Iteration 3369, loss = 0.12759148\n",
      "Iteration 3370, loss = 0.12757477\n",
      "Iteration 3371, loss = 0.12755806\n",
      "Iteration 3372, loss = 0.12754137\n",
      "Iteration 3373, loss = 0.12752469\n",
      "Iteration 3374, loss = 0.12750802\n",
      "Iteration 3375, loss = 0.12749137\n",
      "Iteration 3376, loss = 0.12747473\n",
      "Iteration 3377, loss = 0.12745809\n",
      "Iteration 3378, loss = 0.12744148\n",
      "Iteration 3379, loss = 0.12742487\n",
      "Iteration 3380, loss = 0.12740827\n",
      "Iteration 3381, loss = 0.12739169\n",
      "Iteration 3382, loss = 0.12737512\n",
      "Iteration 3383, loss = 0.12735856\n",
      "Iteration 3384, loss = 0.12734202\n",
      "Iteration 3385, loss = 0.12732548\n",
      "Iteration 3386, loss = 0.12730896\n",
      "Iteration 3387, loss = 0.12729245\n",
      "Iteration 3388, loss = 0.12727595\n",
      "Iteration 3389, loss = 0.12725947\n",
      "Iteration 3390, loss = 0.12724299\n",
      "Iteration 3391, loss = 0.12722653\n",
      "Iteration 3392, loss = 0.12721008\n",
      "Iteration 3393, loss = 0.12719364\n",
      "Iteration 3394, loss = 0.12717722\n",
      "Iteration 3395, loss = 0.12716080\n",
      "Iteration 3396, loss = 0.12714440\n",
      "Iteration 3397, loss = 0.12712801\n",
      "Iteration 3398, loss = 0.12711163\n",
      "Iteration 3399, loss = 0.12709526\n",
      "Iteration 3400, loss = 0.12707891\n",
      "Iteration 3401, loss = 0.12706256\n",
      "Iteration 3402, loss = 0.12704623\n",
      "Iteration 3403, loss = 0.12702991\n",
      "Iteration 3404, loss = 0.12701361\n",
      "Iteration 3405, loss = 0.12699731\n",
      "Iteration 3406, loss = 0.12698103\n",
      "Iteration 3407, loss = 0.12696475\n",
      "Iteration 3408, loss = 0.12694849\n",
      "Iteration 3409, loss = 0.12693225\n",
      "Iteration 3410, loss = 0.12691601\n",
      "Iteration 3411, loss = 0.12689978\n",
      "Iteration 3412, loss = 0.12688357\n",
      "Iteration 3413, loss = 0.12686737\n",
      "Iteration 3414, loss = 0.12685118\n",
      "Iteration 3415, loss = 0.12683500\n",
      "Iteration 3416, loss = 0.12681883\n",
      "Iteration 3417, loss = 0.12680268\n",
      "Iteration 3418, loss = 0.12678653\n",
      "Iteration 3419, loss = 0.12677040\n",
      "Iteration 3420, loss = 0.12675428\n",
      "Iteration 3421, loss = 0.12673817\n",
      "Iteration 3422, loss = 0.12672208\n",
      "Iteration 3423, loss = 0.12670599\n",
      "Iteration 3424, loss = 0.12668992\n",
      "Iteration 3425, loss = 0.12667386\n",
      "Iteration 3426, loss = 0.12665781\n",
      "Iteration 3427, loss = 0.12664177\n",
      "Iteration 3428, loss = 0.12662574\n",
      "Iteration 3429, loss = 0.12660972\n",
      "Iteration 3430, loss = 0.12659372\n",
      "Iteration 3431, loss = 0.12657773\n",
      "Iteration 3432, loss = 0.12656174\n",
      "Iteration 3433, loss = 0.12654577\n",
      "Iteration 3434, loss = 0.12652982\n",
      "Iteration 3435, loss = 0.12651387\n",
      "Iteration 3436, loss = 0.12649793\n",
      "Iteration 3437, loss = 0.12648201\n",
      "Iteration 3438, loss = 0.12646610\n",
      "Iteration 3439, loss = 0.12645019\n",
      "Iteration 3440, loss = 0.12643430\n",
      "Iteration 3441, loss = 0.12641842\n",
      "Iteration 3442, loss = 0.12640256\n",
      "Iteration 3443, loss = 0.12638670\n",
      "Iteration 3444, loss = 0.12637086\n",
      "Iteration 3445, loss = 0.12635502\n",
      "Iteration 3446, loss = 0.12633920\n",
      "Iteration 3447, loss = 0.12632339\n",
      "Iteration 3448, loss = 0.12630759\n",
      "Iteration 3449, loss = 0.12629180\n",
      "Iteration 3450, loss = 0.12627603\n",
      "Iteration 3451, loss = 0.12626026\n",
      "Iteration 3452, loss = 0.12624451\n",
      "Iteration 3453, loss = 0.12622876\n",
      "Iteration 3454, loss = 0.12621303\n",
      "Iteration 3455, loss = 0.12619731\n",
      "Iteration 3456, loss = 0.12618160\n",
      "Iteration 3457, loss = 0.12616590\n",
      "Iteration 3458, loss = 0.12615022\n",
      "Iteration 3459, loss = 0.12613454\n",
      "Iteration 3460, loss = 0.12611888\n",
      "Iteration 3461, loss = 0.12610322\n",
      "Iteration 3462, loss = 0.12608758\n",
      "Iteration 3463, loss = 0.12607195\n",
      "Iteration 3464, loss = 0.12605633\n",
      "Iteration 3465, loss = 0.12604072\n",
      "Iteration 3466, loss = 0.12602512\n",
      "Iteration 3467, loss = 0.12600954\n",
      "Iteration 3468, loss = 0.12599396\n",
      "Iteration 3469, loss = 0.12597840\n",
      "Iteration 3470, loss = 0.12596284\n",
      "Iteration 3471, loss = 0.12594730\n",
      "Iteration 3472, loss = 0.12593177\n",
      "Iteration 3473, loss = 0.12591625\n",
      "Iteration 3474, loss = 0.12590074\n",
      "Iteration 3475, loss = 0.12588524\n",
      "Iteration 3476, loss = 0.12586975\n",
      "Iteration 3477, loss = 0.12585428\n",
      "Iteration 3478, loss = 0.12583881\n",
      "Iteration 3479, loss = 0.12582336\n",
      "Iteration 3480, loss = 0.12580791\n",
      "Iteration 3481, loss = 0.12579248\n",
      "Iteration 3482, loss = 0.12577706\n",
      "Iteration 3483, loss = 0.12576165\n",
      "Iteration 3484, loss = 0.12574625\n",
      "Iteration 3485, loss = 0.12573086\n",
      "Iteration 3486, loss = 0.12571548\n",
      "Iteration 3487, loss = 0.12570011\n",
      "Iteration 3488, loss = 0.12568476\n",
      "Iteration 3489, loss = 0.12566941\n",
      "Iteration 3490, loss = 0.12565408\n",
      "Iteration 3491, loss = 0.12563875\n",
      "Iteration 3492, loss = 0.12562344\n",
      "Iteration 3493, loss = 0.12560814\n",
      "Iteration 3494, loss = 0.12559284\n",
      "Iteration 3495, loss = 0.12557756\n",
      "Iteration 3496, loss = 0.12556229\n",
      "Iteration 3497, loss = 0.12554703\n",
      "Iteration 3498, loss = 0.12553179\n",
      "Iteration 3499, loss = 0.12551655\n",
      "Iteration 3500, loss = 0.12550132\n",
      "Iteration 3501, loss = 0.12548610\n",
      "Iteration 3502, loss = 0.12547090\n",
      "Iteration 3503, loss = 0.12545570\n",
      "Iteration 3504, loss = 0.12544052\n",
      "Iteration 3505, loss = 0.12542535\n",
      "Iteration 3506, loss = 0.12541018\n",
      "Iteration 3507, loss = 0.12539503\n",
      "Iteration 3508, loss = 0.12537989\n",
      "Iteration 3509, loss = 0.12536476\n",
      "Iteration 3510, loss = 0.12534964\n",
      "Iteration 3511, loss = 0.12533453\n",
      "Iteration 3512, loss = 0.12531943\n",
      "Iteration 3513, loss = 0.12530434\n",
      "Iteration 3514, loss = 0.12528926\n",
      "Iteration 3515, loss = 0.12527419\n",
      "Iteration 3516, loss = 0.12525914\n",
      "Iteration 3517, loss = 0.12524409\n",
      "Iteration 3518, loss = 0.12522906\n",
      "Iteration 3519, loss = 0.12521403\n",
      "Iteration 3520, loss = 0.12519902\n",
      "Iteration 3521, loss = 0.12518401\n",
      "Iteration 3522, loss = 0.12516902\n",
      "Iteration 3523, loss = 0.12515403\n",
      "Iteration 3524, loss = 0.12513906\n",
      "Iteration 3525, loss = 0.12512410\n",
      "Iteration 3526, loss = 0.12510915\n",
      "Iteration 3527, loss = 0.12509421\n",
      "Iteration 3528, loss = 0.12507927\n",
      "Iteration 3529, loss = 0.12506435\n",
      "Iteration 3530, loss = 0.12504944\n",
      "Iteration 3531, loss = 0.12503454\n",
      "Iteration 3532, loss = 0.12501965\n",
      "Iteration 3533, loss = 0.12500478\n",
      "Iteration 3534, loss = 0.12498991\n",
      "Iteration 3535, loss = 0.12497505\n",
      "Iteration 3536, loss = 0.12496020\n",
      "Iteration 3537, loss = 0.12494536\n",
      "Iteration 3538, loss = 0.12493054\n",
      "Iteration 3539, loss = 0.12491572\n",
      "Iteration 3540, loss = 0.12490091\n",
      "Iteration 3541, loss = 0.12488612\n",
      "Iteration 3542, loss = 0.12487133\n",
      "Iteration 3543, loss = 0.12485655\n",
      "Iteration 3544, loss = 0.12484179\n",
      "Iteration 3545, loss = 0.12482703\n",
      "Iteration 3546, loss = 0.12481229\n",
      "Iteration 3547, loss = 0.12479755\n",
      "Iteration 3548, loss = 0.12478283\n",
      "Iteration 3549, loss = 0.12476812\n",
      "Iteration 3550, loss = 0.12475341\n",
      "Iteration 3551, loss = 0.12473872\n",
      "Iteration 3552, loss = 0.12472403\n",
      "Iteration 3553, loss = 0.12470936\n",
      "Iteration 3554, loss = 0.12469470\n",
      "Iteration 3555, loss = 0.12468004\n",
      "Iteration 3556, loss = 0.12466540\n",
      "Iteration 3557, loss = 0.12465077\n",
      "Iteration 3558, loss = 0.12463614\n",
      "Iteration 3559, loss = 0.12462153\n",
      "Iteration 3560, loss = 0.12460693\n",
      "Iteration 3561, loss = 0.12459234\n",
      "Iteration 3562, loss = 0.12457775\n",
      "Iteration 3563, loss = 0.12456318\n",
      "Iteration 3564, loss = 0.12454862\n",
      "Iteration 3565, loss = 0.12453407\n",
      "Iteration 3566, loss = 0.12451953\n",
      "Iteration 3567, loss = 0.12450499\n",
      "Iteration 3568, loss = 0.12449047\n",
      "Iteration 3569, loss = 0.12447596\n",
      "Iteration 3570, loss = 0.12446146\n",
      "Iteration 3571, loss = 0.12444697\n",
      "Iteration 3572, loss = 0.12443248\n",
      "Iteration 3573, loss = 0.12441801\n",
      "Iteration 3574, loss = 0.12440355\n",
      "Iteration 3575, loss = 0.12438910\n",
      "Iteration 3576, loss = 0.12437466\n",
      "Iteration 3577, loss = 0.12436022\n",
      "Iteration 3578, loss = 0.12434580\n",
      "Iteration 3579, loss = 0.12433139\n",
      "Iteration 3580, loss = 0.12431699\n",
      "Iteration 3581, loss = 0.12430259\n",
      "Iteration 3582, loss = 0.12428821\n",
      "Iteration 3583, loss = 0.12427384\n",
      "Iteration 3584, loss = 0.12425948\n",
      "Iteration 3585, loss = 0.12424512\n",
      "Iteration 3586, loss = 0.12423078\n",
      "Iteration 3587, loss = 0.12421645\n",
      "Iteration 3588, loss = 0.12420212\n",
      "Iteration 3589, loss = 0.12418781\n",
      "Iteration 3590, loss = 0.12417351\n",
      "Iteration 3591, loss = 0.12415921\n",
      "Iteration 3592, loss = 0.12414493\n",
      "Iteration 3593, loss = 0.12413065\n",
      "Iteration 3594, loss = 0.12411639\n",
      "Iteration 3595, loss = 0.12410213\n",
      "Iteration 3596, loss = 0.12408789\n",
      "Iteration 3597, loss = 0.12407365\n",
      "Iteration 3598, loss = 0.12405943\n",
      "Iteration 3599, loss = 0.12404521\n",
      "Iteration 3600, loss = 0.12403101\n",
      "Iteration 3601, loss = 0.12401681\n",
      "Iteration 3602, loss = 0.12400262\n",
      "Iteration 3603, loss = 0.12398845\n",
      "Iteration 3604, loss = 0.12397428\n",
      "Iteration 3605, loss = 0.12396012\n",
      "Iteration 3606, loss = 0.12394597\n",
      "Iteration 3607, loss = 0.12393184\n",
      "Iteration 3608, loss = 0.12391771\n",
      "Iteration 3609, loss = 0.12390359\n",
      "Iteration 3610, loss = 0.12388948\n",
      "Iteration 3611, loss = 0.12387538\n",
      "Iteration 3612, loss = 0.12386129\n",
      "Iteration 3613, loss = 0.12384721\n",
      "Iteration 3614, loss = 0.12383314\n",
      "Iteration 3615, loss = 0.12381908\n",
      "Iteration 3616, loss = 0.12380502\n",
      "Iteration 3617, loss = 0.12379098\n",
      "Iteration 3618, loss = 0.12377695\n",
      "Iteration 3619, loss = 0.12376293\n",
      "Iteration 3620, loss = 0.12374891\n",
      "Iteration 3621, loss = 0.12373491\n",
      "Iteration 3622, loss = 0.12372091\n",
      "Iteration 3623, loss = 0.12370693\n",
      "Iteration 3624, loss = 0.12369295\n",
      "Iteration 3625, loss = 0.12367899\n",
      "Iteration 3626, loss = 0.12366503\n",
      "Iteration 3627, loss = 0.12365108\n",
      "Iteration 3628, loss = 0.12363715\n",
      "Iteration 3629, loss = 0.12362322\n",
      "Iteration 3630, loss = 0.12360930\n",
      "Iteration 3631, loss = 0.12359539\n",
      "Iteration 3632, loss = 0.12358149\n",
      "Iteration 3633, loss = 0.12356760\n",
      "Iteration 3634, loss = 0.12355372\n",
      "Iteration 3635, loss = 0.12353985\n",
      "Iteration 3636, loss = 0.12352598\n",
      "Iteration 3637, loss = 0.12351213\n",
      "Iteration 3638, loss = 0.12349829\n",
      "Iteration 3639, loss = 0.12348445\n",
      "Iteration 3640, loss = 0.12347063\n",
      "Iteration 3641, loss = 0.12345681\n",
      "Iteration 3642, loss = 0.12344301\n",
      "Iteration 3643, loss = 0.12342921\n",
      "Iteration 3644, loss = 0.12341542\n",
      "Iteration 3645, loss = 0.12340165\n",
      "Iteration 3646, loss = 0.12338788\n",
      "Iteration 3647, loss = 0.12337412\n",
      "Iteration 3648, loss = 0.12336037\n",
      "Iteration 3649, loss = 0.12334663\n",
      "Iteration 3650, loss = 0.12333289\n",
      "Iteration 3651, loss = 0.12331917\n",
      "Iteration 3652, loss = 0.12330546\n",
      "Iteration 3653, loss = 0.12329176\n",
      "Iteration 3654, loss = 0.12327806\n",
      "Iteration 3655, loss = 0.12326438\n",
      "Iteration 3656, loss = 0.12325070\n",
      "Iteration 3657, loss = 0.12323703\n",
      "Iteration 3658, loss = 0.12322337\n",
      "Iteration 3659, loss = 0.12320973\n",
      "Iteration 3660, loss = 0.12319609\n",
      "Iteration 3661, loss = 0.12318246\n",
      "Iteration 3662, loss = 0.12316883\n",
      "Iteration 3663, loss = 0.12315522\n",
      "Iteration 3664, loss = 0.12314162\n",
      "Iteration 3665, loss = 0.12312803\n",
      "Iteration 3666, loss = 0.12311444\n",
      "Iteration 3667, loss = 0.12310087\n",
      "Iteration 3668, loss = 0.12308730\n",
      "Iteration 3669, loss = 0.12307374\n",
      "Iteration 3670, loss = 0.12306019\n",
      "Iteration 3671, loss = 0.12304666\n",
      "Iteration 3672, loss = 0.12303313\n",
      "Iteration 3673, loss = 0.12301960\n",
      "Iteration 3674, loss = 0.12300609\n",
      "Iteration 3675, loss = 0.12299259\n",
      "Iteration 3676, loss = 0.12297910\n",
      "Iteration 3677, loss = 0.12296561\n",
      "Iteration 3678, loss = 0.12295214\n",
      "Iteration 3679, loss = 0.12293867\n",
      "Iteration 3680, loss = 0.12292521\n",
      "Iteration 3681, loss = 0.12291176\n",
      "Iteration 3682, loss = 0.12289832\n",
      "Iteration 3683, loss = 0.12288489\n",
      "Iteration 3684, loss = 0.12287147\n",
      "Iteration 3685, loss = 0.12285806\n",
      "Iteration 3686, loss = 0.12284465\n",
      "Iteration 3687, loss = 0.12283126\n",
      "Iteration 3688, loss = 0.12281787\n",
      "Iteration 3689, loss = 0.12280450\n",
      "Iteration 3690, loss = 0.12279113\n",
      "Iteration 3691, loss = 0.12277777\n",
      "Iteration 3692, loss = 0.12276442\n",
      "Iteration 3693, loss = 0.12275108\n",
      "Iteration 3694, loss = 0.12273774\n",
      "Iteration 3695, loss = 0.12272442\n",
      "Iteration 3696, loss = 0.12271111\n",
      "Iteration 3697, loss = 0.12269780\n",
      "Iteration 3698, loss = 0.12268450\n",
      "Iteration 3699, loss = 0.12267122\n",
      "Iteration 3700, loss = 0.12265794\n",
      "Iteration 3701, loss = 0.12264467\n",
      "Iteration 3702, loss = 0.12263140\n",
      "Iteration 3703, loss = 0.12261815\n",
      "Iteration 3704, loss = 0.12260491\n",
      "Iteration 3705, loss = 0.12259167\n",
      "Iteration 3706, loss = 0.12257845\n",
      "Iteration 3707, loss = 0.12256523\n",
      "Iteration 3708, loss = 0.12255202\n",
      "Iteration 3709, loss = 0.12253882\n",
      "Iteration 3710, loss = 0.12252563\n",
      "Iteration 3711, loss = 0.12251244\n",
      "Iteration 3712, loss = 0.12249927\n",
      "Iteration 3713, loss = 0.12248611\n",
      "Iteration 3714, loss = 0.12247295\n",
      "Iteration 3715, loss = 0.12245980\n",
      "Iteration 3716, loss = 0.12244666\n",
      "Iteration 3717, loss = 0.12243353\n",
      "Iteration 3718, loss = 0.12242041\n",
      "Iteration 3719, loss = 0.12240730\n",
      "Iteration 3720, loss = 0.12239419\n",
      "Iteration 3721, loss = 0.12238110\n",
      "Iteration 3722, loss = 0.12236801\n",
      "Iteration 3723, loss = 0.12235493\n",
      "Iteration 3724, loss = 0.12234186\n",
      "Iteration 3725, loss = 0.12232880\n",
      "Iteration 3726, loss = 0.12231575\n",
      "Iteration 3727, loss = 0.12230271\n",
      "Iteration 3728, loss = 0.12228967\n",
      "Iteration 3729, loss = 0.12227664\n",
      "Iteration 3730, loss = 0.12226363\n",
      "Iteration 3731, loss = 0.12225062\n",
      "Iteration 3732, loss = 0.12223762\n",
      "Iteration 3733, loss = 0.12222462\n",
      "Iteration 3734, loss = 0.12221164\n",
      "Iteration 3735, loss = 0.12219867\n",
      "Iteration 3736, loss = 0.12218570\n",
      "Iteration 3737, loss = 0.12217274\n",
      "Iteration 3738, loss = 0.12215979\n",
      "Iteration 3739, loss = 0.12214685\n",
      "Iteration 3740, loss = 0.12213392\n",
      "Iteration 3741, loss = 0.12212099\n",
      "Iteration 3742, loss = 0.12210808\n",
      "Iteration 3743, loss = 0.12209517\n",
      "Iteration 3744, loss = 0.12208227\n",
      "Iteration 3745, loss = 0.12206938\n",
      "Iteration 3746, loss = 0.12205650\n",
      "Iteration 3747, loss = 0.12204363\n",
      "Iteration 3748, loss = 0.12203076\n",
      "Iteration 3749, loss = 0.12201791\n",
      "Iteration 3750, loss = 0.12200506\n",
      "Iteration 3751, loss = 0.12199222\n",
      "Iteration 3752, loss = 0.12197939\n",
      "Iteration 3753, loss = 0.12196657\n",
      "Iteration 3754, loss = 0.12195375\n",
      "Iteration 3755, loss = 0.12194095\n",
      "Iteration 3756, loss = 0.12192815\n",
      "Iteration 3757, loss = 0.12191536\n",
      "Iteration 3758, loss = 0.12190258\n",
      "Iteration 3759, loss = 0.12188981\n",
      "Iteration 3760, loss = 0.12187704\n",
      "Iteration 3761, loss = 0.12186429\n",
      "Iteration 3762, loss = 0.12185154\n",
      "Iteration 3763, loss = 0.12183880\n",
      "Iteration 3764, loss = 0.12182607\n",
      "Iteration 3765, loss = 0.12181335\n",
      "Iteration 3766, loss = 0.12180064\n",
      "Iteration 3767, loss = 0.12178793\n",
      "Iteration 3768, loss = 0.12177523\n",
      "Iteration 3769, loss = 0.12176255\n",
      "Iteration 3770, loss = 0.12174986\n",
      "Iteration 3771, loss = 0.12173719\n",
      "Iteration 3772, loss = 0.12172453\n",
      "Iteration 3773, loss = 0.12171187\n",
      "Iteration 3774, loss = 0.12169922\n",
      "Iteration 3775, loss = 0.12168659\n",
      "Iteration 3776, loss = 0.12167395\n",
      "Iteration 3777, loss = 0.12166133\n",
      "Iteration 3778, loss = 0.12164872\n",
      "Iteration 3779, loss = 0.12163611\n",
      "Iteration 3780, loss = 0.12162351\n",
      "Iteration 3781, loss = 0.12161092\n",
      "Iteration 3782, loss = 0.12159834\n",
      "Iteration 3783, loss = 0.12158577\n",
      "Iteration 3784, loss = 0.12157320\n",
      "Iteration 3785, loss = 0.12156064\n",
      "Iteration 3786, loss = 0.12154809\n",
      "Iteration 3787, loss = 0.12153555\n",
      "Iteration 3788, loss = 0.12152302\n",
      "Iteration 3789, loss = 0.12151050\n",
      "Iteration 3790, loss = 0.12149798\n",
      "Iteration 3791, loss = 0.12148547\n",
      "Iteration 3792, loss = 0.12147297\n",
      "Iteration 3793, loss = 0.12146048\n",
      "Iteration 3794, loss = 0.12144799\n",
      "Iteration 3795, loss = 0.12143552\n",
      "Iteration 3796, loss = 0.12142305\n",
      "Iteration 3797, loss = 0.12141059\n",
      "Iteration 3798, loss = 0.12139814\n",
      "Iteration 3799, loss = 0.12138569\n",
      "Iteration 3800, loss = 0.12137326\n",
      "Iteration 3801, loss = 0.12136083\n",
      "Iteration 3802, loss = 0.12134841\n",
      "Iteration 3803, loss = 0.12133600\n",
      "Iteration 3804, loss = 0.12132360\n",
      "Iteration 3805, loss = 0.12131120\n",
      "Iteration 3806, loss = 0.12129881\n",
      "Iteration 3807, loss = 0.12128643\n",
      "Iteration 3808, loss = 0.12127406\n",
      "Iteration 3809, loss = 0.12126170\n",
      "Iteration 3810, loss = 0.12124934\n",
      "Iteration 3811, loss = 0.12123699\n",
      "Iteration 3812, loss = 0.12122465\n",
      "Iteration 3813, loss = 0.12121232\n",
      "Iteration 3814, loss = 0.12120000\n",
      "Iteration 3815, loss = 0.12118768\n",
      "Iteration 3816, loss = 0.12117538\n",
      "Iteration 3817, loss = 0.12116308\n",
      "Iteration 3818, loss = 0.12115078\n",
      "Iteration 3819, loss = 0.12113850\n",
      "Iteration 3820, loss = 0.12112622\n",
      "Iteration 3821, loss = 0.12111395\n",
      "Iteration 3822, loss = 0.12110169\n",
      "Iteration 3823, loss = 0.12108944\n",
      "Iteration 3824, loss = 0.12107720\n",
      "Iteration 3825, loss = 0.12106496\n",
      "Iteration 3826, loss = 0.12105273\n",
      "Iteration 3827, loss = 0.12104051\n",
      "Iteration 3828, loss = 0.12102830\n",
      "Iteration 3829, loss = 0.12101609\n",
      "Iteration 3830, loss = 0.12100389\n",
      "Iteration 3831, loss = 0.12099170\n",
      "Iteration 3832, loss = 0.12097952\n",
      "Iteration 3833, loss = 0.12096735\n",
      "Iteration 3834, loss = 0.12095518\n",
      "Iteration 3835, loss = 0.12094302\n",
      "Iteration 3836, loss = 0.12093087\n",
      "Iteration 3837, loss = 0.12091873\n",
      "Iteration 3838, loss = 0.12090660\n",
      "Iteration 3839, loss = 0.12089447\n",
      "Iteration 3840, loss = 0.12088235\n",
      "Iteration 3841, loss = 0.12087024\n",
      "Iteration 3842, loss = 0.12085813\n",
      "Iteration 3843, loss = 0.12084604\n",
      "Iteration 3844, loss = 0.12083395\n",
      "Iteration 3845, loss = 0.12082187\n",
      "Iteration 3846, loss = 0.12080979\n",
      "Iteration 3847, loss = 0.12079773\n",
      "Iteration 3848, loss = 0.12078567\n",
      "Iteration 3849, loss = 0.12077362\n",
      "Iteration 3850, loss = 0.12076158\n",
      "Iteration 3851, loss = 0.12074955\n",
      "Iteration 3852, loss = 0.12073752\n",
      "Iteration 3853, loss = 0.12072550\n",
      "Iteration 3854, loss = 0.12071349\n",
      "Iteration 3855, loss = 0.12070148\n",
      "Iteration 3856, loss = 0.12068949\n",
      "Iteration 3857, loss = 0.12067750\n",
      "Iteration 3858, loss = 0.12066552\n",
      "Iteration 3859, loss = 0.12065354\n",
      "Iteration 3860, loss = 0.12064158\n",
      "Iteration 3861, loss = 0.12062962\n",
      "Iteration 3862, loss = 0.12061767\n",
      "Iteration 3863, loss = 0.12060573\n",
      "Iteration 3864, loss = 0.12059379\n",
      "Iteration 3865, loss = 0.12058187\n",
      "Iteration 3866, loss = 0.12056995\n",
      "Iteration 3867, loss = 0.12055803\n",
      "Iteration 3868, loss = 0.12054613\n",
      "Iteration 3869, loss = 0.12053423\n",
      "Iteration 3870, loss = 0.12052234\n",
      "Iteration 3871, loss = 0.12051046\n",
      "Iteration 3872, loss = 0.12049858\n",
      "Iteration 3873, loss = 0.12048672\n",
      "Iteration 3874, loss = 0.12047486\n",
      "Iteration 3875, loss = 0.12046301\n",
      "Iteration 3876, loss = 0.12045116\n",
      "Iteration 3877, loss = 0.12043932\n",
      "Iteration 3878, loss = 0.12042749\n",
      "Iteration 3879, loss = 0.12041567\n",
      "Iteration 3880, loss = 0.12040386\n",
      "Iteration 3881, loss = 0.12039205\n",
      "Iteration 3882, loss = 0.12038025\n",
      "Iteration 3883, loss = 0.12036846\n",
      "Iteration 3884, loss = 0.12035667\n",
      "Iteration 3885, loss = 0.12034490\n",
      "Iteration 3886, loss = 0.12033313\n",
      "Iteration 3887, loss = 0.12032137\n",
      "Iteration 3888, loss = 0.12030961\n",
      "Iteration 3889, loss = 0.12029786\n",
      "Iteration 3890, loss = 0.12028612\n",
      "Iteration 3891, loss = 0.12027439\n",
      "Iteration 3892, loss = 0.12026267\n",
      "Iteration 3893, loss = 0.12025095\n",
      "Iteration 3894, loss = 0.12023924\n",
      "Iteration 3895, loss = 0.12022753\n",
      "Iteration 3896, loss = 0.12021584\n",
      "Iteration 3897, loss = 0.12020415\n",
      "Iteration 3898, loss = 0.12019247\n",
      "Iteration 3899, loss = 0.12018080\n",
      "Iteration 3900, loss = 0.12016913\n",
      "Iteration 3901, loss = 0.12015747\n",
      "Iteration 3902, loss = 0.12014582\n",
      "Iteration 3903, loss = 0.12013418\n",
      "Iteration 3904, loss = 0.12012254\n",
      "Iteration 3905, loss = 0.12011091\n",
      "Iteration 3906, loss = 0.12009929\n",
      "Iteration 3907, loss = 0.12008767\n",
      "Iteration 3908, loss = 0.12007607\n",
      "Iteration 3909, loss = 0.12006447\n",
      "Iteration 3910, loss = 0.12005287\n",
      "Iteration 3911, loss = 0.12004129\n",
      "Iteration 3912, loss = 0.12002971\n",
      "Iteration 3913, loss = 0.12001814\n",
      "Iteration 3914, loss = 0.12000657\n",
      "Iteration 3915, loss = 0.11999502\n",
      "Iteration 3916, loss = 0.11998347\n",
      "Iteration 3917, loss = 0.11997193\n",
      "Iteration 3918, loss = 0.11996039\n",
      "Iteration 3919, loss = 0.11994886\n",
      "Iteration 3920, loss = 0.11993734\n",
      "Iteration 3921, loss = 0.11992583\n",
      "Iteration 3922, loss = 0.11991433\n",
      "Iteration 3923, loss = 0.11990283\n",
      "Iteration 3924, loss = 0.11989134\n",
      "Iteration 3925, loss = 0.11987985\n",
      "Iteration 3926, loss = 0.11986837\n",
      "Iteration 3927, loss = 0.11985690\n",
      "Iteration 3928, loss = 0.11984544\n",
      "Iteration 3929, loss = 0.11983399\n",
      "Iteration 3930, loss = 0.11982254\n",
      "Iteration 3931, loss = 0.11981110\n",
      "Iteration 3932, loss = 0.11979966\n",
      "Iteration 3933, loss = 0.11978824\n",
      "Iteration 3934, loss = 0.11977682\n",
      "Iteration 3935, loss = 0.11976540\n",
      "Iteration 3936, loss = 0.11975400\n",
      "Iteration 3937, loss = 0.11974260\n",
      "Iteration 3938, loss = 0.11973121\n",
      "Iteration 3939, loss = 0.11971982\n",
      "Iteration 3940, loss = 0.11970845\n",
      "Iteration 3941, loss = 0.11969708\n",
      "Iteration 3942, loss = 0.11968571\n",
      "Iteration 3943, loss = 0.11967436\n",
      "Iteration 3944, loss = 0.11966301\n",
      "Iteration 3945, loss = 0.11965167\n",
      "Iteration 3946, loss = 0.11964033\n",
      "Iteration 3947, loss = 0.11962901\n",
      "Iteration 3948, loss = 0.11961769\n",
      "Iteration 3949, loss = 0.11960637\n",
      "Iteration 3950, loss = 0.11959507\n",
      "Iteration 3951, loss = 0.11958377\n",
      "Iteration 3952, loss = 0.11957247\n",
      "Iteration 3953, loss = 0.11956119\n",
      "Iteration 3954, loss = 0.11954991\n",
      "Iteration 3955, loss = 0.11953864\n",
      "Iteration 3956, loss = 0.11952737\n",
      "Iteration 3957, loss = 0.11951612\n",
      "Iteration 3958, loss = 0.11950487\n",
      "Iteration 3959, loss = 0.11949362\n",
      "Iteration 3960, loss = 0.11948239\n",
      "Iteration 3961, loss = 0.11947116\n",
      "Iteration 3962, loss = 0.11945994\n",
      "Iteration 3963, loss = 0.11944872\n",
      "Iteration 3964, loss = 0.11943751\n",
      "Iteration 3965, loss = 0.11942631\n",
      "Iteration 3966, loss = 0.11941512\n",
      "Iteration 3967, loss = 0.11940393\n",
      "Iteration 3968, loss = 0.11939275\n",
      "Iteration 3969, loss = 0.11938157\n",
      "Iteration 3970, loss = 0.11937041\n",
      "Iteration 3971, loss = 0.11935925\n",
      "Iteration 3972, loss = 0.11934809\n",
      "Iteration 3973, loss = 0.11933695\n",
      "Iteration 3974, loss = 0.11932581\n",
      "Iteration 3975, loss = 0.11931468\n",
      "Iteration 3976, loss = 0.11930355\n",
      "Iteration 3977, loss = 0.11929243\n",
      "Iteration 3978, loss = 0.11928132\n",
      "Iteration 3979, loss = 0.11927021\n",
      "Iteration 3980, loss = 0.11925912\n",
      "Iteration 3981, loss = 0.11924803\n",
      "Iteration 3982, loss = 0.11923694\n",
      "Iteration 3983, loss = 0.11922586\n",
      "Iteration 3984, loss = 0.11921479\n",
      "Iteration 3985, loss = 0.11920373\n",
      "Iteration 3986, loss = 0.11919267\n",
      "Iteration 3987, loss = 0.11918162\n",
      "Iteration 3988, loss = 0.11917058\n",
      "Iteration 3989, loss = 0.11915954\n",
      "Iteration 3990, loss = 0.11914851\n",
      "Iteration 3991, loss = 0.11913749\n",
      "Iteration 3992, loss = 0.11912647\n",
      "Iteration 3993, loss = 0.11911546\n",
      "Iteration 3994, loss = 0.11910446\n",
      "Iteration 3995, loss = 0.11909346\n",
      "Iteration 3996, loss = 0.11908248\n",
      "Iteration 3997, loss = 0.11907149\n",
      "Iteration 3998, loss = 0.11906052\n",
      "Iteration 3999, loss = 0.11904955\n",
      "Iteration 4000, loss = 0.11903859\n",
      "Iteration 4001, loss = 0.11902763\n",
      "Iteration 4002, loss = 0.11901668\n",
      "Iteration 4003, loss = 0.11900574\n",
      "Iteration 4004, loss = 0.11899481\n",
      "Iteration 4005, loss = 0.11898388\n",
      "Iteration 4006, loss = 0.11897295\n",
      "Iteration 4007, loss = 0.11896204\n",
      "Iteration 4008, loss = 0.11895113\n",
      "Iteration 4009, loss = 0.11894023\n",
      "Iteration 4010, loss = 0.11892933\n",
      "Iteration 4011, loss = 0.11891845\n",
      "Iteration 4012, loss = 0.11890756\n",
      "Iteration 4013, loss = 0.11889669\n",
      "Iteration 4014, loss = 0.11888582\n",
      "Iteration 4015, loss = 0.11887496\n",
      "Iteration 4016, loss = 0.11886410\n",
      "Iteration 4017, loss = 0.11885325\n",
      "Iteration 4018, loss = 0.11884241\n",
      "Iteration 4019, loss = 0.11883158\n",
      "Iteration 4020, loss = 0.11882075\n",
      "Iteration 4021, loss = 0.11880993\n",
      "Iteration 4022, loss = 0.11879911\n",
      "Iteration 4023, loss = 0.11878830\n",
      "Iteration 4024, loss = 0.11877750\n",
      "Iteration 4025, loss = 0.11876670\n",
      "Iteration 4026, loss = 0.11875592\n",
      "Iteration 4027, loss = 0.11874513\n",
      "Iteration 4028, loss = 0.11873436\n",
      "Iteration 4029, loss = 0.11872359\n",
      "Iteration 4030, loss = 0.11871283\n",
      "Iteration 4031, loss = 0.11870207\n",
      "Iteration 4032, loss = 0.11869132\n",
      "Iteration 4033, loss = 0.11868058\n",
      "Iteration 4034, loss = 0.11866984\n",
      "Iteration 4035, loss = 0.11865911\n",
      "Iteration 4036, loss = 0.11864839\n",
      "Iteration 4037, loss = 0.11863767\n",
      "Iteration 4038, loss = 0.11862696\n",
      "Iteration 4039, loss = 0.11861626\n",
      "Iteration 4040, loss = 0.11860556\n",
      "Iteration 4041, loss = 0.11859487\n",
      "Iteration 4042, loss = 0.11858418\n",
      "Iteration 4043, loss = 0.11857350\n",
      "Iteration 4044, loss = 0.11856283\n",
      "Iteration 4045, loss = 0.11855217\n",
      "Iteration 4046, loss = 0.11854151\n",
      "Iteration 4047, loss = 0.11853086\n",
      "Iteration 4048, loss = 0.11852021\n",
      "Iteration 4049, loss = 0.11850957\n",
      "Iteration 4050, loss = 0.11849894\n",
      "Iteration 4051, loss = 0.11848831\n",
      "Iteration 4052, loss = 0.11847769\n",
      "Iteration 4053, loss = 0.11846708\n",
      "Iteration 4054, loss = 0.11845647\n",
      "Iteration 4055, loss = 0.11844587\n",
      "Iteration 4056, loss = 0.11843528\n",
      "Iteration 4057, loss = 0.11842469\n",
      "Iteration 4058, loss = 0.11841411\n",
      "Iteration 4059, loss = 0.11840354\n",
      "Iteration 4060, loss = 0.11839297\n",
      "Iteration 4061, loss = 0.11838241\n",
      "Iteration 4062, loss = 0.11837185\n",
      "Iteration 4063, loss = 0.11836130\n",
      "Iteration 4064, loss = 0.11835076\n",
      "Iteration 4065, loss = 0.11834022\n",
      "Iteration 4066, loss = 0.11832969\n",
      "Iteration 4067, loss = 0.11831917\n",
      "Iteration 4068, loss = 0.11830865\n",
      "Iteration 4069, loss = 0.11829814\n",
      "Iteration 4070, loss = 0.11828763\n",
      "Iteration 4071, loss = 0.11827714\n",
      "Iteration 4072, loss = 0.11826664\n",
      "Iteration 4073, loss = 0.11825616\n",
      "Iteration 4074, loss = 0.11824568\n",
      "Iteration 4075, loss = 0.11823521\n",
      "Iteration 4076, loss = 0.11822474\n",
      "Iteration 4077, loss = 0.11821428\n",
      "Iteration 4078, loss = 0.11820382\n",
      "Iteration 4079, loss = 0.11819338\n",
      "Iteration 4080, loss = 0.11818293\n",
      "Iteration 4081, loss = 0.11817250\n",
      "Iteration 4082, loss = 0.11816207\n",
      "Iteration 4083, loss = 0.11815165\n",
      "Iteration 4084, loss = 0.11814123\n",
      "Iteration 4085, loss = 0.11813082\n",
      "Iteration 4086, loss = 0.11812042\n",
      "Iteration 4087, loss = 0.11811002\n",
      "Iteration 4088, loss = 0.11809963\n",
      "Iteration 4089, loss = 0.11808924\n",
      "Iteration 4090, loss = 0.11807886\n",
      "Iteration 4091, loss = 0.11806849\n",
      "Iteration 4092, loss = 0.11805812\n",
      "Iteration 4093, loss = 0.11804776\n",
      "Iteration 4094, loss = 0.11803741\n",
      "Iteration 4095, loss = 0.11802706\n",
      "Iteration 4096, loss = 0.11801672\n",
      "Iteration 4097, loss = 0.11800638\n",
      "Iteration 4098, loss = 0.11799605\n",
      "Iteration 4099, loss = 0.11798573\n",
      "Iteration 4100, loss = 0.11797541\n",
      "Iteration 4101, loss = 0.11796510\n",
      "Iteration 4102, loss = 0.11795480\n",
      "Iteration 4103, loss = 0.11794450\n",
      "Iteration 4104, loss = 0.11793421\n",
      "Iteration 4105, loss = 0.11792392\n",
      "Iteration 4106, loss = 0.11791364\n",
      "Iteration 4107, loss = 0.11790337\n",
      "Iteration 4108, loss = 0.11789310\n",
      "Iteration 4109, loss = 0.11788284\n",
      "Iteration 4110, loss = 0.11787258\n",
      "Iteration 4111, loss = 0.11786233\n",
      "Iteration 4112, loss = 0.11785209\n",
      "Iteration 4113, loss = 0.11784185\n",
      "Iteration 4114, loss = 0.11783162\n",
      "Iteration 4115, loss = 0.11782140\n",
      "Iteration 4116, loss = 0.11781118\n",
      "Iteration 4117, loss = 0.11780097\n",
      "Iteration 4118, loss = 0.11779076\n",
      "Iteration 4119, loss = 0.11778056\n",
      "Iteration 4120, loss = 0.11777037\n",
      "Iteration 4121, loss = 0.11776018\n",
      "Iteration 4122, loss = 0.11775000\n",
      "Iteration 4123, loss = 0.11773982\n",
      "Iteration 4124, loss = 0.11772965\n",
      "Iteration 4125, loss = 0.11771948\n",
      "Iteration 4126, loss = 0.11770933\n",
      "Iteration 4127, loss = 0.11769917\n",
      "Iteration 4128, loss = 0.11768903\n",
      "Iteration 4129, loss = 0.11767889\n",
      "Iteration 4130, loss = 0.11766875\n",
      "Iteration 4131, loss = 0.11765863\n",
      "Iteration 4132, loss = 0.11764851\n",
      "Iteration 4133, loss = 0.11763839\n",
      "Iteration 4134, loss = 0.11762828\n",
      "Iteration 4135, loss = 0.11761818\n",
      "Iteration 4136, loss = 0.11760808\n",
      "Iteration 4137, loss = 0.11759799\n",
      "Iteration 4138, loss = 0.11758790\n",
      "Iteration 4139, loss = 0.11757782\n",
      "Iteration 4140, loss = 0.11756775\n",
      "Iteration 4141, loss = 0.11755768\n",
      "Iteration 4142, loss = 0.11754762\n",
      "Iteration 4143, loss = 0.11753756\n",
      "Iteration 4144, loss = 0.11752751\n",
      "Iteration 4145, loss = 0.11751747\n",
      "Iteration 4146, loss = 0.11750743\n",
      "Iteration 4147, loss = 0.11749740\n",
      "Iteration 4148, loss = 0.11748737\n",
      "Iteration 4149, loss = 0.11747735\n",
      "Iteration 4150, loss = 0.11746733\n",
      "Iteration 4151, loss = 0.11745733\n",
      "Iteration 4152, loss = 0.11744732\n",
      "Iteration 4153, loss = 0.11743733\n",
      "Iteration 4154, loss = 0.11742734\n",
      "Iteration 4155, loss = 0.11741735\n",
      "Iteration 4156, loss = 0.11740737\n",
      "Iteration 4157, loss = 0.11739740\n",
      "Iteration 4158, loss = 0.11738743\n",
      "Iteration 4159, loss = 0.11737747\n",
      "Iteration 4160, loss = 0.11736752\n",
      "Iteration 4161, loss = 0.11735757\n",
      "Iteration 4162, loss = 0.11734762\n",
      "Iteration 4163, loss = 0.11733768\n",
      "Iteration 4164, loss = 0.11732775\n",
      "Iteration 4165, loss = 0.11731783\n",
      "Iteration 4166, loss = 0.11730791\n",
      "Iteration 4167, loss = 0.11729799\n",
      "Iteration 4168, loss = 0.11728808\n",
      "Iteration 4169, loss = 0.11727818\n",
      "Iteration 4170, loss = 0.11726828\n",
      "Iteration 4171, loss = 0.11725839\n",
      "Iteration 4172, loss = 0.11724851\n",
      "Iteration 4173, loss = 0.11723863\n",
      "Iteration 4174, loss = 0.11722875\n",
      "Iteration 4175, loss = 0.11721889\n",
      "Iteration 4176, loss = 0.11720902\n",
      "Iteration 4177, loss = 0.11719917\n",
      "Iteration 4178, loss = 0.11718932\n",
      "Iteration 4179, loss = 0.11717947\n",
      "Iteration 4180, loss = 0.11716963\n",
      "Iteration 4181, loss = 0.11715980\n",
      "Iteration 4182, loss = 0.11714997\n",
      "Iteration 4183, loss = 0.11714015\n",
      "Iteration 4184, loss = 0.11713034\n",
      "Iteration 4185, loss = 0.11712053\n",
      "Iteration 4186, loss = 0.11711072\n",
      "Iteration 4187, loss = 0.11710092\n",
      "Iteration 4188, loss = 0.11709113\n",
      "Iteration 4189, loss = 0.11708134\n",
      "Iteration 4190, loss = 0.11707156\n",
      "Iteration 4191, loss = 0.11706179\n",
      "Iteration 4192, loss = 0.11705202\n",
      "Iteration 4193, loss = 0.11704225\n",
      "Iteration 4194, loss = 0.11703249\n",
      "Iteration 4195, loss = 0.11702274\n",
      "Iteration 4196, loss = 0.11701299\n",
      "Iteration 4197, loss = 0.11700325\n",
      "Iteration 4198, loss = 0.11699352\n",
      "Iteration 4199, loss = 0.11698379\n",
      "Iteration 4200, loss = 0.11697406\n",
      "Iteration 4201, loss = 0.11696434\n",
      "Iteration 4202, loss = 0.11695463\n",
      "Iteration 4203, loss = 0.11694492\n",
      "Iteration 4204, loss = 0.11693522\n",
      "Iteration 4205, loss = 0.11692553\n",
      "Iteration 4206, loss = 0.11691584\n",
      "Iteration 4207, loss = 0.11690615\n",
      "Iteration 4208, loss = 0.11689647\n",
      "Iteration 4209, loss = 0.11688680\n",
      "Iteration 4210, loss = 0.11687713\n",
      "Iteration 4211, loss = 0.11686747\n",
      "Iteration 4212, loss = 0.11685781\n",
      "Iteration 4213, loss = 0.11684816\n",
      "Iteration 4214, loss = 0.11683852\n",
      "Iteration 4215, loss = 0.11682888\n",
      "Iteration 4216, loss = 0.11681924\n",
      "Iteration 4217, loss = 0.11680961\n",
      "Iteration 4218, loss = 0.11679999\n",
      "Iteration 4219, loss = 0.11679037\n",
      "Iteration 4220, loss = 0.11678076\n",
      "Iteration 4221, loss = 0.11677116\n",
      "Iteration 4222, loss = 0.11676156\n",
      "Iteration 4223, loss = 0.11675196\n",
      "Iteration 4224, loss = 0.11674237\n",
      "Iteration 4225, loss = 0.11673279\n",
      "Iteration 4226, loss = 0.11672321\n",
      "Iteration 4227, loss = 0.11671364\n",
      "Iteration 4228, loss = 0.11670407\n",
      "Iteration 4229, loss = 0.11669451\n",
      "Iteration 4230, loss = 0.11668495\n",
      "Iteration 4231, loss = 0.11667540\n",
      "Iteration 4232, loss = 0.11666586\n",
      "Iteration 4233, loss = 0.11665632\n",
      "Iteration 4234, loss = 0.11664678\n",
      "Iteration 4235, loss = 0.11663726\n",
      "Iteration 4236, loss = 0.11662773\n",
      "Iteration 4237, loss = 0.11661822\n",
      "Iteration 4238, loss = 0.11660870\n",
      "Iteration 4239, loss = 0.11659920\n",
      "Iteration 4240, loss = 0.11658970\n",
      "Iteration 4241, loss = 0.11658020\n",
      "Iteration 4242, loss = 0.11657071\n",
      "Iteration 4243, loss = 0.11656123\n",
      "Iteration 4244, loss = 0.11655175\n",
      "Iteration 4245, loss = 0.11654228\n",
      "Iteration 4246, loss = 0.11653281\n",
      "Iteration 4247, loss = 0.11652335\n",
      "Iteration 4248, loss = 0.11651389\n",
      "Iteration 4249, loss = 0.11650444\n",
      "Iteration 4250, loss = 0.11649499\n",
      "Iteration 4251, loss = 0.11648555\n",
      "Iteration 4252, loss = 0.11647612\n",
      "Iteration 4253, loss = 0.11646669\n",
      "Iteration 4254, loss = 0.11645726\n",
      "Iteration 4255, loss = 0.11644784\n",
      "Iteration 4256, loss = 0.11643843\n",
      "Iteration 4257, loss = 0.11642902\n",
      "Iteration 4258, loss = 0.11641962\n",
      "Iteration 4259, loss = 0.11641022\n",
      "Iteration 4260, loss = 0.11640083\n",
      "Iteration 4261, loss = 0.11639145\n",
      "Iteration 4262, loss = 0.11638206\n",
      "Iteration 4263, loss = 0.11637269\n",
      "Iteration 4264, loss = 0.11636332\n",
      "Iteration 4265, loss = 0.11635395\n",
      "Iteration 4266, loss = 0.11634459\n",
      "Iteration 4267, loss = 0.11633524\n",
      "Iteration 4268, loss = 0.11632589\n",
      "Iteration 4269, loss = 0.11631655\n",
      "Iteration 4270, loss = 0.11630721\n",
      "Iteration 4271, loss = 0.11629788\n",
      "Iteration 4272, loss = 0.11628855\n",
      "Iteration 4273, loss = 0.11627923\n",
      "Iteration 4274, loss = 0.11626991\n",
      "Iteration 4275, loss = 0.11626060\n",
      "Iteration 4276, loss = 0.11625130\n",
      "Iteration 4277, loss = 0.11624200\n",
      "Iteration 4278, loss = 0.11623270\n",
      "Iteration 4279, loss = 0.11622341\n",
      "Iteration 4280, loss = 0.11621413\n",
      "Iteration 4281, loss = 0.11620485\n",
      "Iteration 4282, loss = 0.11619557\n",
      "Iteration 4283, loss = 0.11618631\n",
      "Iteration 4284, loss = 0.11617704\n",
      "Iteration 4285, loss = 0.11616779\n",
      "Iteration 4286, loss = 0.11615853\n",
      "Iteration 4287, loss = 0.11614929\n",
      "Iteration 4288, loss = 0.11614004\n",
      "Iteration 4289, loss = 0.11613081\n",
      "Iteration 4290, loss = 0.11612158\n",
      "Iteration 4291, loss = 0.11611235\n",
      "Iteration 4292, loss = 0.11610313\n",
      "Iteration 4293, loss = 0.11609391\n",
      "Iteration 4294, loss = 0.11608470\n",
      "Iteration 4295, loss = 0.11607550\n",
      "Iteration 4296, loss = 0.11606630\n",
      "Iteration 4297, loss = 0.11605710\n",
      "Iteration 4298, loss = 0.11604791\n",
      "Iteration 4299, loss = 0.11603873\n",
      "Iteration 4300, loss = 0.11602955\n",
      "Iteration 4301, loss = 0.11602038\n",
      "Iteration 4302, loss = 0.11601121\n",
      "Iteration 4303, loss = 0.11600205\n",
      "Iteration 4304, loss = 0.11599289\n",
      "Iteration 4305, loss = 0.11598373\n",
      "Iteration 4306, loss = 0.11597459\n",
      "Iteration 4307, loss = 0.11596545\n",
      "Iteration 4308, loss = 0.11595631\n",
      "Iteration 4309, loss = 0.11594718\n",
      "Iteration 4310, loss = 0.11593805\n",
      "Iteration 4311, loss = 0.11592893\n",
      "Iteration 4312, loss = 0.11591981\n",
      "Iteration 4313, loss = 0.11591070\n",
      "Iteration 4314, loss = 0.11590159\n",
      "Iteration 4315, loss = 0.11589249\n",
      "Iteration 4316, loss = 0.11588340\n",
      "Iteration 4317, loss = 0.11587431\n",
      "Iteration 4318, loss = 0.11586522\n",
      "Iteration 4319, loss = 0.11585614\n",
      "Iteration 4320, loss = 0.11584707\n",
      "Iteration 4321, loss = 0.11583800\n",
      "Iteration 4322, loss = 0.11582893\n",
      "Iteration 4323, loss = 0.11581987\n",
      "Iteration 4324, loss = 0.11581082\n",
      "Iteration 4325, loss = 0.11580177\n",
      "Iteration 4326, loss = 0.11579273\n",
      "Iteration 4327, loss = 0.11578369\n",
      "Iteration 4328, loss = 0.11577465\n",
      "Iteration 4329, loss = 0.11576562\n",
      "Iteration 4330, loss = 0.11575660\n",
      "Iteration 4331, loss = 0.11574758\n",
      "Iteration 4332, loss = 0.11573857\n",
      "Iteration 4333, loss = 0.11572956\n",
      "Iteration 4334, loss = 0.11572056\n",
      "Iteration 4335, loss = 0.11571156\n",
      "Iteration 4336, loss = 0.11570257\n",
      "Iteration 4337, loss = 0.11569358\n",
      "Iteration 4338, loss = 0.11568459\n",
      "Iteration 4339, loss = 0.11567562\n",
      "Iteration 4340, loss = 0.11566664\n",
      "Iteration 4341, loss = 0.11565768\n",
      "Iteration 4342, loss = 0.11564871\n",
      "Iteration 4343, loss = 0.11563976\n",
      "Iteration 4344, loss = 0.11563080\n",
      "Iteration 4345, loss = 0.11562186\n",
      "Iteration 4346, loss = 0.11561291\n",
      "Iteration 4347, loss = 0.11560398\n",
      "Iteration 4348, loss = 0.11559504\n",
      "Iteration 4349, loss = 0.11558612\n",
      "Iteration 4350, loss = 0.11557719\n",
      "Iteration 4351, loss = 0.11556828\n",
      "Iteration 4352, loss = 0.11555936\n",
      "Iteration 4353, loss = 0.11555046\n",
      "Iteration 4354, loss = 0.11554155\n",
      "Iteration 4355, loss = 0.11553266\n",
      "Iteration 4356, loss = 0.11552377\n",
      "Iteration 4357, loss = 0.11551488\n",
      "Iteration 4358, loss = 0.11550600\n",
      "Iteration 4359, loss = 0.11549712\n",
      "Iteration 4360, loss = 0.11548825\n",
      "Iteration 4361, loss = 0.11547938\n",
      "Iteration 4362, loss = 0.11547052\n",
      "Iteration 4363, loss = 0.11546166\n",
      "Iteration 4364, loss = 0.11545281\n",
      "Iteration 4365, loss = 0.11544396\n",
      "Iteration 4366, loss = 0.11543512\n",
      "Iteration 4367, loss = 0.11542628\n",
      "Iteration 4368, loss = 0.11541745\n",
      "Iteration 4369, loss = 0.11540862\n",
      "Iteration 4370, loss = 0.11539980\n",
      "Iteration 4371, loss = 0.11539098\n",
      "Iteration 4372, loss = 0.11538217\n",
      "Iteration 4373, loss = 0.11537336\n",
      "Iteration 4374, loss = 0.11536456\n",
      "Iteration 4375, loss = 0.11535576\n",
      "Iteration 4376, loss = 0.11534697\n",
      "Iteration 4377, loss = 0.11533818\n",
      "Iteration 4378, loss = 0.11532940\n",
      "Iteration 4379, loss = 0.11532062\n",
      "Iteration 4380, loss = 0.11531184\n",
      "Iteration 4381, loss = 0.11530308\n",
      "Iteration 4382, loss = 0.11529431\n",
      "Iteration 4383, loss = 0.11528555\n",
      "Iteration 4384, loss = 0.11527680\n",
      "Iteration 4385, loss = 0.11526805\n",
      "Iteration 4386, loss = 0.11525931\n",
      "Iteration 4387, loss = 0.11525057\n",
      "Iteration 4388, loss = 0.11524183\n",
      "Iteration 4389, loss = 0.11523311\n",
      "Iteration 4390, loss = 0.11522438\n",
      "Iteration 4391, loss = 0.11521566\n",
      "Iteration 4392, loss = 0.11520695\n",
      "Iteration 4393, loss = 0.11519824\n",
      "Iteration 4394, loss = 0.11518953\n",
      "Iteration 4395, loss = 0.11518083\n",
      "Iteration 4396, loss = 0.11517214\n",
      "Iteration 4397, loss = 0.11516345\n",
      "Iteration 4398, loss = 0.11515476\n",
      "Iteration 4399, loss = 0.11514608\n",
      "Iteration 4400, loss = 0.11513741\n",
      "Iteration 4401, loss = 0.11512873\n",
      "Iteration 4402, loss = 0.11512007\n",
      "Iteration 4403, loss = 0.11511141\n",
      "Iteration 4404, loss = 0.11510275\n",
      "Iteration 4405, loss = 0.11509410\n",
      "Iteration 4406, loss = 0.11508545\n",
      "Iteration 4407, loss = 0.11507681\n",
      "Iteration 4408, loss = 0.11506817\n",
      "Iteration 4409, loss = 0.11505954\n",
      "Iteration 4410, loss = 0.11505091\n",
      "Iteration 4411, loss = 0.11504229\n",
      "Iteration 4412, loss = 0.11503367\n",
      "Iteration 4413, loss = 0.11502506\n",
      "Iteration 4414, loss = 0.11501645\n",
      "Iteration 4415, loss = 0.11500785\n",
      "Iteration 4416, loss = 0.11499925\n",
      "Iteration 4417, loss = 0.11499065\n",
      "Iteration 4418, loss = 0.11498206\n",
      "Iteration 4419, loss = 0.11497348\n",
      "Iteration 4420, loss = 0.11496490\n",
      "Iteration 4421, loss = 0.11495632\n",
      "Iteration 4422, loss = 0.11494775\n",
      "Iteration 4423, loss = 0.11493919\n",
      "Iteration 4424, loss = 0.11493063\n",
      "Iteration 4425, loss = 0.11492207\n",
      "Iteration 4426, loss = 0.11491352\n",
      "Iteration 4427, loss = 0.11490497\n",
      "Iteration 4428, loss = 0.11489643\n",
      "Iteration 4429, loss = 0.11488789\n",
      "Iteration 4430, loss = 0.11487936\n",
      "Iteration 4431, loss = 0.11487083\n",
      "Iteration 4432, loss = 0.11486231\n",
      "Iteration 4433, loss = 0.11485379\n",
      "Iteration 4434, loss = 0.11484528\n",
      "Iteration 4435, loss = 0.11483677\n",
      "Iteration 4436, loss = 0.11482826\n",
      "Iteration 4437, loss = 0.11481976\n",
      "Iteration 4438, loss = 0.11481127\n",
      "Iteration 4439, loss = 0.11480278\n",
      "Iteration 4440, loss = 0.11479429\n",
      "Iteration 4441, loss = 0.11478581\n",
      "Iteration 4442, loss = 0.11477734\n",
      "Iteration 4443, loss = 0.11476886\n",
      "Iteration 4444, loss = 0.11476040\n",
      "Iteration 4445, loss = 0.11475193\n",
      "Iteration 4446, loss = 0.11474348\n",
      "Iteration 4447, loss = 0.11473502\n",
      "Iteration 4448, loss = 0.11472658\n",
      "Iteration 4449, loss = 0.11471813\n",
      "Iteration 4450, loss = 0.11470969\n",
      "Iteration 4451, loss = 0.11470126\n",
      "Iteration 4452, loss = 0.11469283\n",
      "Iteration 4453, loss = 0.11468440\n",
      "Iteration 4454, loss = 0.11467598\n",
      "Iteration 4455, loss = 0.11466757\n",
      "Iteration 4456, loss = 0.11465916\n",
      "Iteration 4457, loss = 0.11465075\n",
      "Iteration 4458, loss = 0.11464235\n",
      "Iteration 4459, loss = 0.11463395\n",
      "Iteration 4460, loss = 0.11462556\n",
      "Iteration 4461, loss = 0.11461717\n",
      "Iteration 4462, loss = 0.11460879\n",
      "Iteration 4463, loss = 0.11460041\n",
      "Iteration 4464, loss = 0.11459203\n",
      "Iteration 4465, loss = 0.11458366\n",
      "Iteration 4466, loss = 0.11457530\n",
      "Iteration 4467, loss = 0.11456694\n",
      "Iteration 4468, loss = 0.11455858\n",
      "Iteration 4469, loss = 0.11455023\n",
      "Iteration 4470, loss = 0.11454189\n",
      "Iteration 4471, loss = 0.11453354\n",
      "Iteration 4472, loss = 0.11452521\n",
      "Iteration 4473, loss = 0.11451687\n",
      "Iteration 4474, loss = 0.11450854\n",
      "Iteration 4475, loss = 0.11450022\n",
      "Iteration 4476, loss = 0.11449190\n",
      "Iteration 4477, loss = 0.11448359\n",
      "Iteration 4478, loss = 0.11447528\n",
      "Iteration 4479, loss = 0.11446697\n",
      "Iteration 4480, loss = 0.11445867\n",
      "Iteration 4481, loss = 0.11445037\n",
      "Iteration 4482, loss = 0.11444208\n",
      "Iteration 4483, loss = 0.11443379\n",
      "Iteration 4484, loss = 0.11442551\n",
      "Iteration 4485, loss = 0.11441723\n",
      "Iteration 4486, loss = 0.11440896\n",
      "Iteration 4487, loss = 0.11440069\n",
      "Iteration 4488, loss = 0.11439242\n",
      "Iteration 4489, loss = 0.11438416\n",
      "Iteration 4490, loss = 0.11437591\n",
      "Iteration 4491, loss = 0.11436765\n",
      "Iteration 4492, loss = 0.11435941\n",
      "Iteration 4493, loss = 0.11435116\n",
      "Iteration 4494, loss = 0.11434293\n",
      "Iteration 4495, loss = 0.11433469\n",
      "Iteration 4496, loss = 0.11432646\n",
      "Iteration 4497, loss = 0.11431824\n",
      "Iteration 4498, loss = 0.11431002\n",
      "Iteration 4499, loss = 0.11430180\n",
      "Iteration 4500, loss = 0.11429359\n",
      "Iteration 4501, loss = 0.11428539\n",
      "Iteration 4502, loss = 0.11427718\n",
      "Iteration 4503, loss = 0.11426899\n",
      "Iteration 4504, loss = 0.11426079\n",
      "Iteration 4505, loss = 0.11425260\n",
      "Iteration 4506, loss = 0.11424442\n",
      "Iteration 4507, loss = 0.11423624\n",
      "Iteration 4508, loss = 0.11422806\n",
      "Iteration 4509, loss = 0.11421989\n",
      "Iteration 4510, loss = 0.11421173\n",
      "Iteration 4511, loss = 0.11420356\n",
      "Iteration 4512, loss = 0.11419541\n",
      "Iteration 4513, loss = 0.11418725\n",
      "Iteration 4514, loss = 0.11417910\n",
      "Iteration 4515, loss = 0.11417096\n",
      "Iteration 4516, loss = 0.11416282\n",
      "Iteration 4517, loss = 0.11415468\n",
      "Iteration 4518, loss = 0.11414655\n",
      "Iteration 4519, loss = 0.11413843\n",
      "Iteration 4520, loss = 0.11413030\n",
      "Iteration 4521, loss = 0.11412219\n",
      "Iteration 4522, loss = 0.11411407\n",
      "Iteration 4523, loss = 0.11410596\n",
      "Iteration 4524, loss = 0.11409786\n",
      "Iteration 4525, loss = 0.11408976\n",
      "Iteration 4526, loss = 0.11408166\n",
      "Iteration 4527, loss = 0.11407357\n",
      "Iteration 4528, loss = 0.11406548\n",
      "Iteration 4529, loss = 0.11405740\n",
      "Iteration 4530, loss = 0.11404932\n",
      "Iteration 4531, loss = 0.11404125\n",
      "Iteration 4532, loss = 0.11403318\n",
      "Iteration 4533, loss = 0.11402511\n",
      "Iteration 4534, loss = 0.11401705\n",
      "Iteration 4535, loss = 0.11400899\n",
      "Iteration 4536, loss = 0.11400094\n",
      "Iteration 4537, loss = 0.11399289\n",
      "Iteration 4538, loss = 0.11398485\n",
      "Iteration 4539, loss = 0.11397681\n",
      "Iteration 4540, loss = 0.11396877\n",
      "Iteration 4541, loss = 0.11396074\n",
      "Iteration 4542, loss = 0.11395272\n",
      "Iteration 4543, loss = 0.11394469\n",
      "Iteration 4544, loss = 0.11393668\n",
      "Iteration 4545, loss = 0.11392866\n",
      "Iteration 4546, loss = 0.11392065\n",
      "Iteration 4547, loss = 0.11391265\n",
      "Iteration 4548, loss = 0.11390465\n",
      "Iteration 4549, loss = 0.11389665\n",
      "Iteration 4550, loss = 0.11388866\n",
      "Iteration 4551, loss = 0.11388067\n",
      "Iteration 4552, loss = 0.11387269\n",
      "Iteration 4553, loss = 0.11386471\n",
      "Iteration 4554, loss = 0.11385673\n",
      "Iteration 4555, loss = 0.11384876\n",
      "Iteration 4556, loss = 0.11384080\n",
      "Iteration 4557, loss = 0.11383284\n",
      "Iteration 4558, loss = 0.11382488\n",
      "Iteration 4559, loss = 0.11381692\n",
      "Iteration 4560, loss = 0.11380898\n",
      "Iteration 4561, loss = 0.11380103\n",
      "Iteration 4562, loss = 0.11379309\n",
      "Iteration 4563, loss = 0.11378515\n",
      "Iteration 4564, loss = 0.11377722\n",
      "Iteration 4565, loss = 0.11376929\n",
      "Iteration 4566, loss = 0.11376137\n",
      "Iteration 4567, loss = 0.11375345\n",
      "Iteration 4568, loss = 0.11374553\n",
      "Iteration 4569, loss = 0.11373762\n",
      "Iteration 4570, loss = 0.11372972\n",
      "Iteration 4571, loss = 0.11372181\n",
      "Iteration 4572, loss = 0.11371392\n",
      "Iteration 4573, loss = 0.11370602\n",
      "Iteration 4574, loss = 0.11369813\n",
      "Iteration 4575, loss = 0.11369025\n",
      "Iteration 4576, loss = 0.11368237\n",
      "Iteration 4577, loss = 0.11367449\n",
      "Iteration 4578, loss = 0.11366662\n",
      "Iteration 4579, loss = 0.11365875\n",
      "Iteration 4580, loss = 0.11365088\n",
      "Iteration 4581, loss = 0.11364302\n",
      "Iteration 4582, loss = 0.11363517\n",
      "Iteration 4583, loss = 0.11362731\n",
      "Iteration 4584, loss = 0.11361947\n",
      "Iteration 4585, loss = 0.11361162\n",
      "Iteration 4586, loss = 0.11360378\n",
      "Iteration 4587, loss = 0.11359595\n",
      "Iteration 4588, loss = 0.11358812\n",
      "Iteration 4589, loss = 0.11358029\n",
      "Iteration 4590, loss = 0.11357247\n",
      "Iteration 4591, loss = 0.11356465\n",
      "Iteration 4592, loss = 0.11355684\n",
      "Iteration 4593, loss = 0.11354903\n",
      "Iteration 4594, loss = 0.11354122\n",
      "Iteration 4595, loss = 0.11353342\n",
      "Iteration 4596, loss = 0.11352562\n",
      "Iteration 4597, loss = 0.11351783\n",
      "Iteration 4598, loss = 0.11351004\n",
      "Iteration 4599, loss = 0.11350225\n",
      "Iteration 4600, loss = 0.11349447\n",
      "Iteration 4601, loss = 0.11348669\n",
      "Iteration 4602, loss = 0.11347892\n",
      "Iteration 4603, loss = 0.11347115\n",
      "Iteration 4604, loss = 0.11346339\n",
      "Iteration 4605, loss = 0.11345563\n",
      "Iteration 4606, loss = 0.11344787\n",
      "Iteration 4607, loss = 0.11344012\n",
      "Iteration 4608, loss = 0.11343237\n",
      "Iteration 4609, loss = 0.11342463\n",
      "Iteration 4610, loss = 0.11341689\n",
      "Iteration 4611, loss = 0.11340915\n",
      "Iteration 4612, loss = 0.11340142\n",
      "Iteration 4613, loss = 0.11339369\n",
      "Iteration 4614, loss = 0.11338597\n",
      "Iteration 4615, loss = 0.11337825\n",
      "Iteration 4616, loss = 0.11337053\n",
      "Iteration 4617, loss = 0.11336282\n",
      "Iteration 4618, loss = 0.11335512\n",
      "Iteration 4619, loss = 0.11334741\n",
      "Iteration 4620, loss = 0.11333971\n",
      "Iteration 4621, loss = 0.11333202\n",
      "Iteration 4622, loss = 0.11332433\n",
      "Iteration 4623, loss = 0.11331664\n",
      "Iteration 4624, loss = 0.11330896\n",
      "Iteration 4625, loss = 0.11330128\n",
      "Iteration 4626, loss = 0.11329361\n",
      "Iteration 4627, loss = 0.11328593\n",
      "Iteration 4628, loss = 0.11327827\n",
      "Iteration 4629, loss = 0.11327061\n",
      "Iteration 4630, loss = 0.11326295\n",
      "Iteration 4631, loss = 0.11325529\n",
      "Iteration 4632, loss = 0.11324764\n",
      "Iteration 4633, loss = 0.11324000\n",
      "Iteration 4634, loss = 0.11323235\n",
      "Iteration 4635, loss = 0.11322472\n",
      "Iteration 4636, loss = 0.11321708\n",
      "Iteration 4637, loss = 0.11320945\n",
      "Iteration 4638, loss = 0.11320183\n",
      "Iteration 4639, loss = 0.11319420\n",
      "Iteration 4640, loss = 0.11318659\n",
      "Iteration 4641, loss = 0.11317897\n",
      "Iteration 4642, loss = 0.11317136\n",
      "Iteration 4643, loss = 0.11316376\n",
      "Iteration 4644, loss = 0.11315615\n",
      "Iteration 4645, loss = 0.11314856\n",
      "Iteration 4646, loss = 0.11314096\n",
      "Iteration 4647, loss = 0.11313337\n",
      "Iteration 4648, loss = 0.11312579\n",
      "Iteration 4649, loss = 0.11311820\n",
      "Iteration 4650, loss = 0.11311063\n",
      "Iteration 4651, loss = 0.11310305\n",
      "Iteration 4652, loss = 0.11309548\n",
      "Iteration 4653, loss = 0.11308792\n",
      "Iteration 4654, loss = 0.11308035\n",
      "Iteration 4655, loss = 0.11307280\n",
      "Iteration 4656, loss = 0.11306524\n",
      "Iteration 4657, loss = 0.11305769\n",
      "Iteration 4658, loss = 0.11305014\n",
      "Iteration 4659, loss = 0.11304260\n",
      "Iteration 4660, loss = 0.11303506\n",
      "Iteration 4661, loss = 0.11302753\n",
      "Iteration 4662, loss = 0.11302000\n",
      "Iteration 4663, loss = 0.11301247\n",
      "Iteration 4664, loss = 0.11300495\n",
      "Iteration 4665, loss = 0.11299743\n",
      "Iteration 4666, loss = 0.11298992\n",
      "Iteration 4667, loss = 0.11298241\n",
      "Iteration 4668, loss = 0.11297490\n",
      "Iteration 4669, loss = 0.11296740\n",
      "Iteration 4670, loss = 0.11295990\n",
      "Iteration 4671, loss = 0.11295240\n",
      "Iteration 4672, loss = 0.11294491\n",
      "Iteration 4673, loss = 0.11293742\n",
      "Iteration 4674, loss = 0.11292994\n",
      "Iteration 4675, loss = 0.11292246\n",
      "Iteration 4676, loss = 0.11291499\n",
      "Iteration 4677, loss = 0.11290751\n",
      "Iteration 4678, loss = 0.11290005\n",
      "Iteration 4679, loss = 0.11289258\n",
      "Iteration 4680, loss = 0.11288512\n",
      "Iteration 4681, loss = 0.11287767\n",
      "Iteration 4682, loss = 0.11287022\n",
      "Iteration 4683, loss = 0.11286277\n",
      "Iteration 4684, loss = 0.11285532\n",
      "Iteration 4685, loss = 0.11284788\n",
      "Iteration 4686, loss = 0.11284045\n",
      "Iteration 4687, loss = 0.11283301\n",
      "Iteration 4688, loss = 0.11282558\n",
      "Iteration 4689, loss = 0.11281816\n",
      "Iteration 4690, loss = 0.11281074\n",
      "Iteration 4691, loss = 0.11280332\n",
      "Iteration 4692, loss = 0.11279591\n",
      "Iteration 4693, loss = 0.11278850\n",
      "Iteration 4694, loss = 0.11278109\n",
      "Iteration 4695, loss = 0.11277369\n",
      "Iteration 4696, loss = 0.11276629\n",
      "Iteration 4697, loss = 0.11275890\n",
      "Iteration 4698, loss = 0.11275151\n",
      "Iteration 4699, loss = 0.11274412\n",
      "Iteration 4700, loss = 0.11273674\n",
      "Iteration 4701, loss = 0.11272936\n",
      "Iteration 4702, loss = 0.11272199\n",
      "Iteration 4703, loss = 0.11271462\n",
      "Iteration 4704, loss = 0.11270725\n",
      "Iteration 4705, loss = 0.11269989\n",
      "Iteration 4706, loss = 0.11269253\n",
      "Iteration 4707, loss = 0.11268517\n",
      "Iteration 4708, loss = 0.11267782\n",
      "Iteration 4709, loss = 0.11267047\n",
      "Iteration 4710, loss = 0.11266313\n",
      "Iteration 4711, loss = 0.11265579\n",
      "Iteration 4712, loss = 0.11264845\n",
      "Iteration 4713, loss = 0.11264112\n",
      "Iteration 4714, loss = 0.11263379\n",
      "Iteration 4715, loss = 0.11262646\n",
      "Iteration 4716, loss = 0.11261914\n",
      "Iteration 4717, loss = 0.11261182\n",
      "Iteration 4718, loss = 0.11260451\n",
      "Iteration 4719, loss = 0.11259720\n",
      "Iteration 4720, loss = 0.11258989\n",
      "Iteration 4721, loss = 0.11258259\n",
      "Iteration 4722, loss = 0.11257529\n",
      "Iteration 4723, loss = 0.11256800\n",
      "Iteration 4724, loss = 0.11256071\n",
      "Iteration 4725, loss = 0.11255342\n",
      "Iteration 4726, loss = 0.11254614\n",
      "Iteration 4727, loss = 0.11253886\n",
      "Iteration 4728, loss = 0.11253158\n",
      "Iteration 4729, loss = 0.11252431\n",
      "Iteration 4730, loss = 0.11251704\n",
      "Iteration 4731, loss = 0.11250977\n",
      "Iteration 4732, loss = 0.11250251\n",
      "Iteration 4733, loss = 0.11249526\n",
      "Iteration 4734, loss = 0.11248800\n",
      "Iteration 4735, loss = 0.11248075\n",
      "Iteration 4736, loss = 0.11247351\n",
      "Iteration 4737, loss = 0.11246626\n",
      "Iteration 4738, loss = 0.11245903\n",
      "Iteration 4739, loss = 0.11245179\n",
      "Iteration 4740, loss = 0.11244456\n",
      "Iteration 4741, loss = 0.11243733\n",
      "Iteration 4742, loss = 0.11243011\n",
      "Iteration 4743, loss = 0.11242289\n",
      "Iteration 4744, loss = 0.11241567\n",
      "Iteration 4745, loss = 0.11240846\n",
      "Iteration 4746, loss = 0.11240125\n",
      "Iteration 4747, loss = 0.11239405\n",
      "Iteration 4748, loss = 0.11238684\n",
      "Iteration 4749, loss = 0.11237965\n",
      "Iteration 4750, loss = 0.11237245\n",
      "Iteration 4751, loss = 0.11236526\n",
      "Iteration 4752, loss = 0.11235808\n",
      "Iteration 4753, loss = 0.11235089\n",
      "Iteration 4754, loss = 0.11234371\n",
      "Iteration 4755, loss = 0.11233654\n",
      "Iteration 4756, loss = 0.11232937\n",
      "Iteration 4757, loss = 0.11232220\n",
      "Iteration 4758, loss = 0.11231503\n",
      "Iteration 4759, loss = 0.11230787\n",
      "Iteration 4760, loss = 0.11230071\n",
      "Iteration 4761, loss = 0.11229356\n",
      "Iteration 4762, loss = 0.11228641\n",
      "Iteration 4763, loss = 0.11227927\n",
      "Iteration 4764, loss = 0.11227212\n",
      "Iteration 4765, loss = 0.11226498\n",
      "Iteration 4766, loss = 0.11225785\n",
      "Iteration 4767, loss = 0.11225072\n",
      "Iteration 4768, loss = 0.11224359\n",
      "Iteration 4769, loss = 0.11223647\n",
      "Iteration 4770, loss = 0.11222934\n",
      "Iteration 4771, loss = 0.11222223\n",
      "Iteration 4772, loss = 0.11221511\n",
      "Iteration 4773, loss = 0.11220800\n",
      "Iteration 4774, loss = 0.11220090\n",
      "Iteration 4775, loss = 0.11219380\n",
      "Iteration 4776, loss = 0.11218670\n",
      "Iteration 4777, loss = 0.11217960\n",
      "Iteration 4778, loss = 0.11217251\n",
      "Iteration 4779, loss = 0.11216542\n",
      "Iteration 4780, loss = 0.11215834\n",
      "Iteration 4781, loss = 0.11215126\n",
      "Iteration 4782, loss = 0.11214418\n",
      "Iteration 4783, loss = 0.11213711\n",
      "Iteration 4784, loss = 0.11213004\n",
      "Iteration 4785, loss = 0.11212297\n",
      "Iteration 4786, loss = 0.11211591\n",
      "Iteration 4787, loss = 0.11210885\n",
      "Iteration 4788, loss = 0.11210180\n",
      "Iteration 4789, loss = 0.11209474\n",
      "Iteration 4790, loss = 0.11208770\n",
      "Iteration 4791, loss = 0.11208065\n",
      "Iteration 4792, loss = 0.11207361\n",
      "Iteration 4793, loss = 0.11206657\n",
      "Iteration 4794, loss = 0.11205954\n",
      "Iteration 4795, loss = 0.11205251\n",
      "Iteration 4796, loss = 0.11204548\n",
      "Iteration 4797, loss = 0.11203846\n",
      "Iteration 4798, loss = 0.11203144\n",
      "Iteration 4799, loss = 0.11202442\n",
      "Iteration 4800, loss = 0.11201741\n",
      "Iteration 4801, loss = 0.11201040\n",
      "Iteration 4802, loss = 0.11200340\n",
      "Iteration 4803, loss = 0.11199639\n",
      "Iteration 4804, loss = 0.11198940\n",
      "Iteration 4805, loss = 0.11198240\n",
      "Iteration 4806, loss = 0.11197541\n",
      "Iteration 4807, loss = 0.11196842\n",
      "Iteration 4808, loss = 0.11196144\n",
      "Iteration 4809, loss = 0.11195446\n",
      "Iteration 4810, loss = 0.11194748\n",
      "Iteration 4811, loss = 0.11194051\n",
      "Iteration 4812, loss = 0.11193354\n",
      "Iteration 4813, loss = 0.11192657\n",
      "Iteration 4814, loss = 0.11191961\n",
      "Iteration 4815, loss = 0.11191265\n",
      "Iteration 4816, loss = 0.11190570\n",
      "Iteration 4817, loss = 0.11189874\n",
      "Iteration 4818, loss = 0.11189180\n",
      "Iteration 4819, loss = 0.11188485\n",
      "Iteration 4820, loss = 0.11187791\n",
      "Iteration 4821, loss = 0.11187097\n",
      "Iteration 4822, loss = 0.11186404\n",
      "Iteration 4823, loss = 0.11185711\n",
      "Iteration 4824, loss = 0.11185018\n",
      "Iteration 4825, loss = 0.11184325\n",
      "Iteration 4826, loss = 0.11183633\n",
      "Iteration 4827, loss = 0.11182942\n",
      "Iteration 4828, loss = 0.11182250\n",
      "Iteration 4829, loss = 0.11181559\n",
      "Iteration 4830, loss = 0.11180869\n",
      "Iteration 4831, loss = 0.11180178\n",
      "Iteration 4832, loss = 0.11179488\n",
      "Iteration 4833, loss = 0.11178799\n",
      "Iteration 4834, loss = 0.11178110\n",
      "Iteration 4835, loss = 0.11177421\n",
      "Iteration 4836, loss = 0.11176732\n",
      "Iteration 4837, loss = 0.11176044\n",
      "Iteration 4838, loss = 0.11175356\n",
      "Iteration 4839, loss = 0.11174668\n",
      "Iteration 4840, loss = 0.11173981\n",
      "Iteration 4841, loss = 0.11173294\n",
      "Iteration 4842, loss = 0.11172608\n",
      "Iteration 4843, loss = 0.11171922\n",
      "Iteration 4844, loss = 0.11171236\n",
      "Iteration 4845, loss = 0.11170551\n",
      "Iteration 4846, loss = 0.11169865\n",
      "Iteration 4847, loss = 0.11169181\n",
      "Iteration 4848, loss = 0.11168496\n",
      "Iteration 4849, loss = 0.11167812\n",
      "Iteration 4850, loss = 0.11167129\n",
      "Iteration 4851, loss = 0.11166445\n",
      "Iteration 4852, loss = 0.11165762\n",
      "Iteration 4853, loss = 0.11165079\n",
      "Iteration 4854, loss = 0.11164397\n",
      "Iteration 4855, loss = 0.11163715\n",
      "Iteration 4856, loss = 0.11163033\n",
      "Iteration 4857, loss = 0.11162352\n",
      "Iteration 4858, loss = 0.11161671\n",
      "Iteration 4859, loss = 0.11160990\n",
      "Iteration 4860, loss = 0.11160310\n",
      "Iteration 4861, loss = 0.11159630\n",
      "Iteration 4862, loss = 0.11158951\n",
      "Iteration 4863, loss = 0.11158271\n",
      "Iteration 4864, loss = 0.11157592\n",
      "Iteration 4865, loss = 0.11156914\n",
      "Iteration 4866, loss = 0.11156236\n",
      "Iteration 4867, loss = 0.11155558\n",
      "Iteration 4868, loss = 0.11154880\n",
      "Iteration 4869, loss = 0.11154203\n",
      "Iteration 4870, loss = 0.11153526\n",
      "Iteration 4871, loss = 0.11152849\n",
      "Iteration 4872, loss = 0.11152173\n",
      "Iteration 4873, loss = 0.11151497\n",
      "Iteration 4874, loss = 0.11150822\n",
      "Iteration 4875, loss = 0.11150147\n",
      "Iteration 4876, loss = 0.11149472\n",
      "Iteration 4877, loss = 0.11148797\n",
      "Iteration 4878, loss = 0.11148123\n",
      "Iteration 4879, loss = 0.11147449\n",
      "Iteration 4880, loss = 0.11146776\n",
      "Iteration 4881, loss = 0.11146103\n",
      "Iteration 4882, loss = 0.11145430\n",
      "Iteration 4883, loss = 0.11144757\n",
      "Iteration 4884, loss = 0.11144085\n",
      "Iteration 4885, loss = 0.11143413\n",
      "Iteration 4886, loss = 0.11142742\n",
      "Iteration 4887, loss = 0.11142071\n",
      "Iteration 4888, loss = 0.11141400\n",
      "Iteration 4889, loss = 0.11140729\n",
      "Iteration 4890, loss = 0.11140059\n",
      "Iteration 4891, loss = 0.11139389\n",
      "Iteration 4892, loss = 0.11138720\n",
      "Iteration 4893, loss = 0.11138051\n",
      "Iteration 4894, loss = 0.11137382\n",
      "Iteration 4895, loss = 0.11136713\n",
      "Iteration 4896, loss = 0.11136045\n",
      "Iteration 4897, loss = 0.11135377\n",
      "Iteration 4898, loss = 0.11134710\n",
      "Iteration 4899, loss = 0.11134043\n",
      "Iteration 4900, loss = 0.11133376\n",
      "Iteration 4901, loss = 0.11132709\n",
      "Iteration 4902, loss = 0.11132043\n",
      "Iteration 4903, loss = 0.11131377\n",
      "Iteration 4904, loss = 0.11130712\n",
      "Iteration 4905, loss = 0.11130047\n",
      "Iteration 4906, loss = 0.11129382\n",
      "Iteration 4907, loss = 0.11128717\n",
      "Iteration 4908, loss = 0.11128053\n",
      "Iteration 4909, loss = 0.11127389\n",
      "Iteration 4910, loss = 0.11126726\n",
      "Iteration 4911, loss = 0.11126063\n",
      "Iteration 4912, loss = 0.11125400\n",
      "Iteration 4913, loss = 0.11124737\n",
      "Iteration 4914, loss = 0.11124075\n",
      "Iteration 4915, loss = 0.11123413\n",
      "Iteration 4916, loss = 0.11122752\n",
      "Iteration 4917, loss = 0.11122090\n",
      "Iteration 4918, loss = 0.11121430\n",
      "Iteration 4919, loss = 0.11120769\n",
      "Iteration 4920, loss = 0.11120109\n",
      "Iteration 4921, loss = 0.11119449\n",
      "Iteration 4922, loss = 0.11118789\n",
      "Iteration 4923, loss = 0.11118130\n",
      "Iteration 4924, loss = 0.11117471\n",
      "Iteration 4925, loss = 0.11116813\n",
      "Iteration 4926, loss = 0.11116154\n",
      "Iteration 4927, loss = 0.11115496\n",
      "Iteration 4928, loss = 0.11114839\n",
      "Iteration 4929, loss = 0.11114181\n",
      "Iteration 4930, loss = 0.11113524\n",
      "Iteration 4931, loss = 0.11112868\n",
      "Iteration 4932, loss = 0.11112211\n",
      "Iteration 4933, loss = 0.11111555\n",
      "Iteration 4934, loss = 0.11110900\n",
      "Iteration 4935, loss = 0.11110244\n",
      "Iteration 4936, loss = 0.11109589\n",
      "Iteration 4937, loss = 0.11108935\n",
      "Iteration 4938, loss = 0.11108280\n",
      "Iteration 4939, loss = 0.11107626\n",
      "Iteration 4940, loss = 0.11106973\n",
      "Iteration 4941, loss = 0.11106319\n",
      "Iteration 4942, loss = 0.11105666\n",
      "Iteration 4943, loss = 0.11105013\n",
      "Iteration 4944, loss = 0.11104361\n",
      "Iteration 4945, loss = 0.11103709\n",
      "Iteration 4946, loss = 0.11103057\n",
      "Iteration 4947, loss = 0.11102405\n",
      "Iteration 4948, loss = 0.11101754\n",
      "Iteration 4949, loss = 0.11101103\n",
      "Iteration 4950, loss = 0.11100453\n",
      "Iteration 4951, loss = 0.11099803\n",
      "Iteration 4952, loss = 0.11099153\n",
      "Iteration 4953, loss = 0.11098503\n",
      "Iteration 4954, loss = 0.11097854\n",
      "Iteration 4955, loss = 0.11097205\n",
      "Iteration 4956, loss = 0.11096557\n",
      "Iteration 4957, loss = 0.11095908\n",
      "Iteration 4958, loss = 0.11095260\n",
      "Iteration 4959, loss = 0.11094613\n",
      "Iteration 4960, loss = 0.11093965\n",
      "Iteration 4961, loss = 0.11093318\n",
      "Iteration 4962, loss = 0.11092672\n",
      "Iteration 4963, loss = 0.11092025\n",
      "Iteration 4964, loss = 0.11091379\n",
      "Iteration 4965, loss = 0.11090733\n",
      "Iteration 4966, loss = 0.11090088\n",
      "Iteration 4967, loss = 0.11089443\n",
      "Iteration 4968, loss = 0.11088798\n",
      "Iteration 4969, loss = 0.11088154\n",
      "Iteration 4970, loss = 0.11087509\n",
      "Iteration 4971, loss = 0.11086866\n",
      "Iteration 4972, loss = 0.11086222\n",
      "Iteration 4973, loss = 0.11085579\n",
      "Iteration 4974, loss = 0.11084936\n",
      "Iteration 4975, loss = 0.11084293\n",
      "Iteration 4976, loss = 0.11083651\n",
      "Iteration 4977, loss = 0.11083009\n",
      "Iteration 4978, loss = 0.11082368\n",
      "Iteration 4979, loss = 0.11081726\n",
      "Iteration 4980, loss = 0.11081085\n",
      "Iteration 4981, loss = 0.11080445\n",
      "Iteration 4982, loss = 0.11079804\n",
      "Iteration 4983, loss = 0.11079164\n",
      "Iteration 4984, loss = 0.11078524\n",
      "Iteration 4985, loss = 0.11077885\n",
      "Iteration 4986, loss = 0.11077246\n",
      "Iteration 4987, loss = 0.11076607\n",
      "Iteration 4988, loss = 0.11075968\n",
      "Iteration 4989, loss = 0.11075330\n",
      "Iteration 4990, loss = 0.11074692\n",
      "Iteration 4991, loss = 0.11074055\n",
      "Iteration 4992, loss = 0.11073418\n",
      "Iteration 4993, loss = 0.11072781\n",
      "Iteration 4994, loss = 0.11072144\n",
      "Iteration 4995, loss = 0.11071508\n",
      "Iteration 4996, loss = 0.11070872\n",
      "Iteration 4997, loss = 0.11070236\n",
      "Iteration 4998, loss = 0.11069601\n",
      "Iteration 4999, loss = 0.11068965\n",
      "Iteration 5000, loss = 0.11068331\n",
      "Iteration 5001, loss = 0.11067696\n",
      "Iteration 5002, loss = 0.11067062\n",
      "Iteration 5003, loss = 0.11066428\n",
      "Iteration 5004, loss = 0.11065795\n",
      "Iteration 5005, loss = 0.11065161\n",
      "Iteration 5006, loss = 0.11064528\n",
      "Iteration 5007, loss = 0.11063896\n",
      "Iteration 5008, loss = 0.11063264\n",
      "Iteration 5009, loss = 0.11062632\n",
      "Iteration 5010, loss = 0.11062000\n",
      "Iteration 5011, loss = 0.11061368\n",
      "Iteration 5012, loss = 0.11060737\n",
      "Iteration 5013, loss = 0.11060107\n",
      "Iteration 5014, loss = 0.11059476\n",
      "Iteration 5015, loss = 0.11058846\n",
      "Iteration 5016, loss = 0.11058216\n",
      "Iteration 5017, loss = 0.11057587\n",
      "Iteration 5018, loss = 0.11056957\n",
      "Iteration 5019, loss = 0.11056328\n",
      "Iteration 5020, loss = 0.11055700\n",
      "Iteration 5021, loss = 0.11055071\n",
      "Iteration 5022, loss = 0.11054443\n",
      "Iteration 5023, loss = 0.11053816\n",
      "Iteration 5024, loss = 0.11053188\n",
      "Iteration 5025, loss = 0.11052561\n",
      "Iteration 5026, loss = 0.11051934\n",
      "Iteration 5027, loss = 0.11051308\n",
      "Iteration 5028, loss = 0.11050682\n",
      "Iteration 5029, loss = 0.11050056\n",
      "Iteration 5030, loss = 0.11049430\n",
      "Iteration 5031, loss = 0.11048805\n",
      "Iteration 5032, loss = 0.11048180\n",
      "Iteration 5033, loss = 0.11047555\n",
      "Iteration 5034, loss = 0.11046931\n",
      "Iteration 5035, loss = 0.11046307\n",
      "Iteration 5036, loss = 0.11045683\n",
      "Iteration 5037, loss = 0.11045059\n",
      "Iteration 5038, loss = 0.11044436\n",
      "Iteration 5039, loss = 0.11043813\n",
      "Iteration 5040, loss = 0.11043191\n",
      "Iteration 5041, loss = 0.11042569\n",
      "Iteration 5042, loss = 0.11041947\n",
      "Iteration 5043, loss = 0.11041325\n",
      "Iteration 5044, loss = 0.11040703\n",
      "Iteration 5045, loss = 0.11040082\n",
      "Iteration 5046, loss = 0.11039462\n",
      "Iteration 5047, loss = 0.11038841\n",
      "Iteration 5048, loss = 0.11038221\n",
      "Iteration 5049, loss = 0.11037601\n",
      "Iteration 5050, loss = 0.11036982\n",
      "Iteration 5051, loss = 0.11036362\n",
      "Iteration 5052, loss = 0.11035743\n",
      "Iteration 5053, loss = 0.11035125\n",
      "Iteration 5054, loss = 0.11034506\n",
      "Iteration 5055, loss = 0.11033888\n",
      "Iteration 5056, loss = 0.11033270\n",
      "Iteration 5057, loss = 0.11032653\n",
      "Iteration 5058, loss = 0.11032036\n",
      "Iteration 5059, loss = 0.11031419\n",
      "Iteration 5060, loss = 0.11030802\n",
      "Iteration 5061, loss = 0.11030186\n",
      "Iteration 5062, loss = 0.11029570\n",
      "Iteration 5063, loss = 0.11028954\n",
      "Iteration 5064, loss = 0.11028339\n",
      "Iteration 5065, loss = 0.11027724\n",
      "Iteration 5066, loss = 0.11027109\n",
      "Iteration 5067, loss = 0.11026494\n",
      "Iteration 5068, loss = 0.11025880\n",
      "Iteration 5069, loss = 0.11025266\n",
      "Iteration 5070, loss = 0.11024652\n",
      "Iteration 5071, loss = 0.11024039\n",
      "Iteration 5072, loss = 0.11023426\n",
      "Iteration 5073, loss = 0.11022813\n",
      "Iteration 5074, loss = 0.11022201\n",
      "Iteration 5075, loss = 0.11021589\n",
      "Iteration 5076, loss = 0.11020977\n",
      "Iteration 5077, loss = 0.11020365\n",
      "Iteration 5078, loss = 0.11019754\n",
      "Iteration 5079, loss = 0.11019143\n",
      "Iteration 5080, loss = 0.11018532\n",
      "Iteration 5081, loss = 0.11017922\n",
      "Iteration 5082, loss = 0.11017312\n",
      "Iteration 5083, loss = 0.11016702\n",
      "Iteration 5084, loss = 0.11016092\n",
      "Iteration 5085, loss = 0.11015483\n",
      "Iteration 5086, loss = 0.11014874\n",
      "Iteration 5087, loss = 0.11014265\n",
      "Iteration 5088, loss = 0.11013657\n",
      "Iteration 5089, loss = 0.11013049\n",
      "Iteration 5090, loss = 0.11012441\n",
      "Iteration 5091, loss = 0.11011834\n",
      "Iteration 5092, loss = 0.11011226\n",
      "Iteration 5093, loss = 0.11010619\n",
      "Iteration 5094, loss = 0.11010013\n",
      "Iteration 5095, loss = 0.11009406\n",
      "Iteration 5096, loss = 0.11008800\n",
      "Iteration 5097, loss = 0.11008195\n",
      "Iteration 5098, loss = 0.11007589\n",
      "Iteration 5099, loss = 0.11006984\n",
      "Iteration 5100, loss = 0.11006379\n",
      "Iteration 5101, loss = 0.11005774\n",
      "Iteration 5102, loss = 0.11005170\n",
      "Iteration 5103, loss = 0.11004566\n",
      "Iteration 5104, loss = 0.11003962\n",
      "Iteration 5105, loss = 0.11003359\n",
      "Iteration 5106, loss = 0.11002756\n",
      "Iteration 5107, loss = 0.11002153\n",
      "Iteration 5108, loss = 0.11001550\n",
      "Iteration 5109, loss = 0.11000948\n",
      "Iteration 5110, loss = 0.11000346\n",
      "Iteration 5111, loss = 0.10999744\n",
      "Iteration 5112, loss = 0.10999143\n",
      "Iteration 5113, loss = 0.10998542\n",
      "Iteration 5114, loss = 0.10997941\n",
      "Iteration 5115, loss = 0.10997340\n",
      "Iteration 5116, loss = 0.10996740\n",
      "Iteration 5117, loss = 0.10996140\n",
      "Iteration 5118, loss = 0.10995540\n",
      "Iteration 5119, loss = 0.10994941\n",
      "Iteration 5120, loss = 0.10994341\n",
      "Iteration 5121, loss = 0.10993743\n",
      "Iteration 5122, loss = 0.10993144\n",
      "Iteration 5123, loss = 0.10992546\n",
      "Iteration 5124, loss = 0.10991948\n",
      "Iteration 5125, loss = 0.10991350\n",
      "Iteration 5126, loss = 0.10990752\n",
      "Iteration 5127, loss = 0.10990155\n",
      "Iteration 5128, loss = 0.10989558\n",
      "Iteration 5129, loss = 0.10988962\n",
      "Iteration 5130, loss = 0.10988365\n",
      "Iteration 5131, loss = 0.10987769\n",
      "Iteration 5132, loss = 0.10987174\n",
      "Iteration 5133, loss = 0.10986578\n",
      "Iteration 5134, loss = 0.10985983\n",
      "Iteration 5135, loss = 0.10985388\n",
      "Iteration 5136, loss = 0.10984793\n",
      "Iteration 5137, loss = 0.10984199\n",
      "Iteration 5138, loss = 0.10983605\n",
      "Iteration 5139, loss = 0.10983011\n",
      "Iteration 5140, loss = 0.10982418\n",
      "Iteration 5141, loss = 0.10981824\n",
      "Iteration 5142, loss = 0.10981232\n",
      "Iteration 5143, loss = 0.10980639\n",
      "Iteration 5144, loss = 0.10980046\n",
      "Iteration 5145, loss = 0.10979454\n",
      "Iteration 5146, loss = 0.10978863\n",
      "Iteration 5147, loss = 0.10978271\n",
      "Iteration 5148, loss = 0.10977680\n",
      "Iteration 5149, loss = 0.10977089\n",
      "Iteration 5150, loss = 0.10976498\n",
      "Iteration 5151, loss = 0.10975908\n",
      "Iteration 5152, loss = 0.10975317\n",
      "Iteration 5153, loss = 0.10974728\n",
      "Iteration 5154, loss = 0.10974138\n",
      "Iteration 5155, loss = 0.10973549\n",
      "Iteration 5156, loss = 0.10972960\n",
      "Iteration 5157, loss = 0.10972371\n",
      "Iteration 5158, loss = 0.10971782\n",
      "Iteration 5159, loss = 0.10971194\n",
      "Iteration 5160, loss = 0.10970606\n",
      "Iteration 5161, loss = 0.10970019\n",
      "Iteration 5162, loss = 0.10969431\n",
      "Iteration 5163, loss = 0.10968844\n",
      "Iteration 5164, loss = 0.10968257\n",
      "Iteration 5165, loss = 0.10967671\n",
      "Iteration 5166, loss = 0.10967084\n",
      "Iteration 5167, loss = 0.10966498\n",
      "Iteration 5168, loss = 0.10965913\n",
      "Iteration 5169, loss = 0.10965327\n",
      "Iteration 5170, loss = 0.10964742\n",
      "Iteration 5171, loss = 0.10964157\n",
      "Iteration 5172, loss = 0.10963573\n",
      "Iteration 5173, loss = 0.10962988\n",
      "Iteration 5174, loss = 0.10962404\n",
      "Iteration 5175, loss = 0.10961820\n",
      "Iteration 5176, loss = 0.10961237\n",
      "Iteration 5177, loss = 0.10960654\n",
      "Iteration 5178, loss = 0.10960071\n",
      "Iteration 5179, loss = 0.10959488\n",
      "Iteration 5180, loss = 0.10958906\n",
      "Iteration 5181, loss = 0.10958323\n",
      "Iteration 5182, loss = 0.10957742\n",
      "Iteration 5183, loss = 0.10957160\n",
      "Iteration 5184, loss = 0.10956579\n",
      "Iteration 5185, loss = 0.10955998\n",
      "Iteration 5186, loss = 0.10955417\n",
      "Iteration 5187, loss = 0.10954836\n",
      "Iteration 5188, loss = 0.10954256\n",
      "Iteration 5189, loss = 0.10953676\n",
      "Iteration 5190, loss = 0.10953096\n",
      "Iteration 5191, loss = 0.10952517\n",
      "Iteration 5192, loss = 0.10951938\n",
      "Iteration 5193, loss = 0.10951359\n",
      "Iteration 5194, loss = 0.10950780\n",
      "Iteration 5195, loss = 0.10950202\n",
      "Iteration 5196, loss = 0.10949624\n",
      "Iteration 5197, loss = 0.10949046\n",
      "Iteration 5198, loss = 0.10948468\n",
      "Iteration 5199, loss = 0.10947891\n",
      "Iteration 5200, loss = 0.10947314\n",
      "Iteration 5201, loss = 0.10946737\n",
      "Iteration 5202, loss = 0.10946161\n",
      "Iteration 5203, loss = 0.10945585\n",
      "Iteration 5204, loss = 0.10945009\n",
      "Iteration 5205, loss = 0.10944433\n",
      "Iteration 5206, loss = 0.10943858\n",
      "Iteration 5207, loss = 0.10943283\n",
      "Iteration 5208, loss = 0.10942708\n",
      "Iteration 5209, loss = 0.10942133\n",
      "Iteration 5210, loss = 0.10941559\n",
      "Iteration 5211, loss = 0.10940985\n",
      "Iteration 5212, loss = 0.10940411\n",
      "Iteration 5213, loss = 0.10939838\n",
      "Iteration 5214, loss = 0.10939264\n",
      "Iteration 5215, loss = 0.10938692\n",
      "Iteration 5216, loss = 0.10938119\n",
      "Iteration 5217, loss = 0.10937546\n",
      "Iteration 5218, loss = 0.10936974\n",
      "Iteration 5219, loss = 0.10936402\n",
      "Iteration 5220, loss = 0.10935831\n",
      "Iteration 5221, loss = 0.10935259\n",
      "Iteration 5222, loss = 0.10934688\n",
      "Iteration 5223, loss = 0.10934117\n",
      "Iteration 5224, loss = 0.10933547\n",
      "Iteration 5225, loss = 0.10932976\n",
      "Iteration 5226, loss = 0.10932406\n",
      "Iteration 5227, loss = 0.10931837\n",
      "Iteration 5228, loss = 0.10931267\n",
      "Iteration 5229, loss = 0.10930698\n",
      "Iteration 5230, loss = 0.10930129\n",
      "Iteration 5231, loss = 0.10929560\n",
      "Iteration 5232, loss = 0.10928992\n",
      "Iteration 5233, loss = 0.10928424\n",
      "Iteration 5234, loss = 0.10927856\n",
      "Iteration 5235, loss = 0.10927288\n",
      "Iteration 5236, loss = 0.10926721\n",
      "Iteration 5237, loss = 0.10926153\n",
      "Iteration 5238, loss = 0.10925586\n",
      "Iteration 5239, loss = 0.10925020\n",
      "Iteration 5240, loss = 0.10924454\n",
      "Iteration 5241, loss = 0.10923887\n",
      "Iteration 5242, loss = 0.10923322\n",
      "Iteration 5243, loss = 0.10922756\n",
      "Iteration 5244, loss = 0.10922191\n",
      "Iteration 5245, loss = 0.10921626\n",
      "Iteration 5246, loss = 0.10921061\n",
      "Iteration 5247, loss = 0.10920496\n",
      "Iteration 5248, loss = 0.10919932\n",
      "Iteration 5249, loss = 0.10919368\n",
      "Iteration 5250, loss = 0.10918804\n",
      "Iteration 5251, loss = 0.10918241\n",
      "Iteration 5252, loss = 0.10917678\n",
      "Iteration 5253, loss = 0.10917115\n",
      "Iteration 5254, loss = 0.10916552\n",
      "Iteration 5255, loss = 0.10915990\n",
      "Iteration 5256, loss = 0.10915427\n",
      "Iteration 5257, loss = 0.10914866\n",
      "Iteration 5258, loss = 0.10914304\n",
      "Iteration 5259, loss = 0.10913742\n",
      "Iteration 5260, loss = 0.10913181\n",
      "Iteration 5261, loss = 0.10912620\n",
      "Iteration 5262, loss = 0.10912060\n",
      "Iteration 5263, loss = 0.10911500\n",
      "Iteration 5264, loss = 0.10910939\n",
      "Iteration 5265, loss = 0.10910380\n",
      "Iteration 5266, loss = 0.10909820\n",
      "Iteration 5267, loss = 0.10909261\n",
      "Iteration 5268, loss = 0.10908702\n",
      "Iteration 5269, loss = 0.10908143\n",
      "Iteration 5270, loss = 0.10907584\n",
      "Iteration 5271, loss = 0.10907026\n",
      "Iteration 5272, loss = 0.10906468\n",
      "Iteration 5273, loss = 0.10905910\n",
      "Iteration 5274, loss = 0.10905353\n",
      "Iteration 5275, loss = 0.10904795\n",
      "Iteration 5276, loss = 0.10904238\n",
      "Iteration 5277, loss = 0.10903682\n",
      "Iteration 5278, loss = 0.10903125\n",
      "Iteration 5279, loss = 0.10902569\n",
      "Iteration 5280, loss = 0.10902013\n",
      "Iteration 5281, loss = 0.10901457\n",
      "Iteration 5282, loss = 0.10900902\n",
      "Iteration 5283, loss = 0.10900346\n",
      "Iteration 5284, loss = 0.10899791\n",
      "Iteration 5285, loss = 0.10899237\n",
      "Iteration 5286, loss = 0.10898682\n",
      "Iteration 5287, loss = 0.10898128\n",
      "Iteration 5288, loss = 0.10897574\n",
      "Iteration 5289, loss = 0.10897020\n",
      "Iteration 5290, loss = 0.10896467\n",
      "Iteration 5291, loss = 0.10895914\n",
      "Iteration 5292, loss = 0.10895361\n",
      "Iteration 5293, loss = 0.10894808\n",
      "Iteration 5294, loss = 0.10894256\n",
      "Iteration 5295, loss = 0.10893704\n",
      "Iteration 5296, loss = 0.10893152\n",
      "Iteration 5297, loss = 0.10892600\n",
      "Iteration 5298, loss = 0.10892049\n",
      "Iteration 5299, loss = 0.10891497\n",
      "Iteration 5300, loss = 0.10890946\n",
      "Iteration 5301, loss = 0.10890396\n",
      "Iteration 5302, loss = 0.10889845\n",
      "Iteration 5303, loss = 0.10889295\n",
      "Iteration 5304, loss = 0.10888745\n",
      "Iteration 5305, loss = 0.10888196\n",
      "Iteration 5306, loss = 0.10887646\n",
      "Iteration 5307, loss = 0.10887097\n",
      "Iteration 5308, loss = 0.10886548\n",
      "Iteration 5309, loss = 0.10886000\n",
      "Iteration 5310, loss = 0.10885451\n",
      "Iteration 5311, loss = 0.10884903\n",
      "Iteration 5312, loss = 0.10884355\n",
      "Iteration 5313, loss = 0.10883808\n",
      "Iteration 5314, loss = 0.10883260\n",
      "Iteration 5315, loss = 0.10882713\n",
      "Iteration 5316, loss = 0.10882166\n",
      "Iteration 5317, loss = 0.10881619\n",
      "Iteration 5318, loss = 0.10881073\n",
      "Iteration 5319, loss = 0.10880527\n",
      "Iteration 5320, loss = 0.10879981\n",
      "Iteration 5321, loss = 0.10879435\n",
      "Iteration 5322, loss = 0.10878890\n",
      "Iteration 5323, loss = 0.10878345\n",
      "Iteration 5324, loss = 0.10877800\n",
      "Iteration 5325, loss = 0.10877255\n",
      "Iteration 5326, loss = 0.10876711\n",
      "Iteration 5327, loss = 0.10876167\n",
      "Iteration 5328, loss = 0.10875623\n",
      "Iteration 5329, loss = 0.10875079\n",
      "Iteration 5330, loss = 0.10874536\n",
      "Iteration 5331, loss = 0.10873993\n",
      "Iteration 5332, loss = 0.10873450\n",
      "Iteration 5333, loss = 0.10872907\n",
      "Iteration 5334, loss = 0.10872365\n",
      "Iteration 5335, loss = 0.10871822\n",
      "Iteration 5336, loss = 0.10871280\n",
      "Iteration 5337, loss = 0.10870739\n",
      "Iteration 5338, loss = 0.10870197\n",
      "Iteration 5339, loss = 0.10869656\n",
      "Iteration 5340, loss = 0.10869115\n",
      "Iteration 5341, loss = 0.10868574\n",
      "Iteration 5342, loss = 0.10868034\n",
      "Iteration 5343, loss = 0.10867494\n",
      "Iteration 5344, loss = 0.10866954\n",
      "Iteration 5345, loss = 0.10866414\n",
      "Iteration 5346, loss = 0.10865875\n",
      "Iteration 5347, loss = 0.10865335\n",
      "Iteration 5348, loss = 0.10864796\n",
      "Iteration 5349, loss = 0.10864258\n",
      "Iteration 5350, loss = 0.10863719\n",
      "Iteration 5351, loss = 0.10863181\n",
      "Iteration 5352, loss = 0.10862643\n",
      "Iteration 5353, loss = 0.10862105\n",
      "Iteration 5354, loss = 0.10861568\n",
      "Iteration 5355, loss = 0.10861030\n",
      "Iteration 5356, loss = 0.10860493\n",
      "Iteration 5357, loss = 0.10859956\n",
      "Iteration 5358, loss = 0.10859420\n",
      "Iteration 5359, loss = 0.10858884\n",
      "Iteration 5360, loss = 0.10858347\n",
      "Iteration 5361, loss = 0.10857812\n",
      "Iteration 5362, loss = 0.10857276\n",
      "Iteration 5363, loss = 0.10856741\n",
      "Iteration 5364, loss = 0.10856206\n",
      "Iteration 5365, loss = 0.10855671\n",
      "Iteration 5366, loss = 0.10855136\n",
      "Iteration 5367, loss = 0.10854602\n",
      "Iteration 5368, loss = 0.10854068\n",
      "Iteration 5369, loss = 0.10853534\n",
      "Iteration 5370, loss = 0.10853000\n",
      "Iteration 5371, loss = 0.10852467\n",
      "Iteration 5372, loss = 0.10851934\n",
      "Iteration 5373, loss = 0.10851401\n",
      "Iteration 5374, loss = 0.10850868\n",
      "Iteration 5375, loss = 0.10850335\n",
      "Iteration 5376, loss = 0.10849803\n",
      "Iteration 5377, loss = 0.10849271\n",
      "Iteration 5378, loss = 0.10848740\n",
      "Iteration 5379, loss = 0.10848208\n",
      "Iteration 5380, loss = 0.10847677\n",
      "Iteration 5381, loss = 0.10847146\n",
      "Iteration 5382, loss = 0.10846615\n",
      "Iteration 5383, loss = 0.10846085\n",
      "Iteration 5384, loss = 0.10845554\n",
      "Iteration 5385, loss = 0.10845024\n",
      "Iteration 5386, loss = 0.10844494\n",
      "Iteration 5387, loss = 0.10843965\n",
      "Iteration 5388, loss = 0.10843435\n",
      "Iteration 5389, loss = 0.10842906\n",
      "Iteration 5390, loss = 0.10842377\n",
      "Iteration 5391, loss = 0.10841849\n",
      "Iteration 5392, loss = 0.10841320\n",
      "Iteration 5393, loss = 0.10840792\n",
      "Iteration 5394, loss = 0.10840264\n",
      "Iteration 5395, loss = 0.10839737\n",
      "Iteration 5396, loss = 0.10839209\n",
      "Iteration 5397, loss = 0.10838682\n",
      "Iteration 5398, loss = 0.10838155\n",
      "Iteration 5399, loss = 0.10837628\n",
      "Iteration 5400, loss = 0.10837102\n",
      "Iteration 5401, loss = 0.10836576\n",
      "Iteration 5402, loss = 0.10836050\n",
      "Iteration 5403, loss = 0.10835524\n",
      "Iteration 5404, loss = 0.10834998\n",
      "Iteration 5405, loss = 0.10834473\n",
      "Iteration 5406, loss = 0.10833948\n",
      "Iteration 5407, loss = 0.10833423\n",
      "Iteration 5408, loss = 0.10832898\n",
      "Iteration 5409, loss = 0.10832374\n",
      "Iteration 5410, loss = 0.10831850\n",
      "Iteration 5411, loss = 0.10831326\n",
      "Iteration 5412, loss = 0.10830802\n",
      "Iteration 5413, loss = 0.10830279\n",
      "Iteration 5414, loss = 0.10829756\n",
      "Iteration 5415, loss = 0.10829233\n",
      "Iteration 5416, loss = 0.10828710\n",
      "Iteration 5417, loss = 0.10828188\n",
      "Iteration 5418, loss = 0.10827665\n",
      "Iteration 5419, loss = 0.10827143\n",
      "Iteration 5420, loss = 0.10826621\n",
      "Iteration 5421, loss = 0.10826100\n",
      "Iteration 5422, loss = 0.10825579\n",
      "Iteration 5423, loss = 0.10825057\n",
      "Iteration 5424, loss = 0.10824537\n",
      "Iteration 5425, loss = 0.10824016\n",
      "Iteration 5426, loss = 0.10823496\n",
      "Iteration 5427, loss = 0.10822975\n",
      "Iteration 5428, loss = 0.10822455\n",
      "Iteration 5429, loss = 0.10821936\n",
      "Iteration 5430, loss = 0.10821416\n",
      "Iteration 5431, loss = 0.10820897\n",
      "Iteration 5432, loss = 0.10820378\n",
      "Iteration 5433, loss = 0.10819859\n",
      "Iteration 5434, loss = 0.10819341\n",
      "Iteration 5435, loss = 0.10818822\n",
      "Iteration 5436, loss = 0.10818304\n",
      "Iteration 5437, loss = 0.10817786\n",
      "Iteration 5438, loss = 0.10817269\n",
      "Iteration 5439, loss = 0.10816751\n",
      "Iteration 5440, loss = 0.10816234\n",
      "Iteration 5441, loss = 0.10815717\n",
      "Iteration 5442, loss = 0.10815201\n",
      "Iteration 5443, loss = 0.10814684\n",
      "Iteration 5444, loss = 0.10814168\n",
      "Iteration 5445, loss = 0.10813652\n",
      "Iteration 5446, loss = 0.10813136\n",
      "Iteration 5447, loss = 0.10812621\n",
      "Iteration 5448, loss = 0.10812105\n",
      "Iteration 5449, loss = 0.10811590\n",
      "Iteration 5450, loss = 0.10811075\n",
      "Iteration 5451, loss = 0.10810561\n",
      "Iteration 5452, loss = 0.10810046\n",
      "Iteration 5453, loss = 0.10809532\n",
      "Iteration 5454, loss = 0.10809018\n",
      "Iteration 5455, loss = 0.10808504\n",
      "Iteration 5456, loss = 0.10807991\n",
      "Iteration 5457, loss = 0.10807478\n",
      "Iteration 5458, loss = 0.10806965\n",
      "Iteration 5459, loss = 0.10806452\n",
      "Iteration 5460, loss = 0.10805939\n",
      "Iteration 5461, loss = 0.10805427\n",
      "Iteration 5462, loss = 0.10804915\n",
      "Iteration 5463, loss = 0.10804403\n",
      "Iteration 5464, loss = 0.10803891\n",
      "Iteration 5465, loss = 0.10803380\n",
      "Iteration 5466, loss = 0.10802869\n",
      "Iteration 5467, loss = 0.10802358\n",
      "Iteration 5468, loss = 0.10801847\n",
      "Iteration 5469, loss = 0.10801336\n",
      "Iteration 5470, loss = 0.10800826\n",
      "Iteration 5471, loss = 0.10800316\n",
      "Iteration 5472, loss = 0.10799806\n",
      "Iteration 5473, loss = 0.10799296\n",
      "Iteration 5474, loss = 0.10798787\n",
      "Iteration 5475, loss = 0.10798278\n",
      "Iteration 5476, loss = 0.10797769\n",
      "Iteration 5477, loss = 0.10797260\n",
      "Iteration 5478, loss = 0.10796751\n",
      "Iteration 5479, loss = 0.10796243\n",
      "Iteration 5480, loss = 0.10795735\n",
      "Iteration 5481, loss = 0.10795227\n",
      "Iteration 5482, loss = 0.10794720\n",
      "Iteration 5483, loss = 0.10794212\n",
      "Iteration 5484, loss = 0.10793705\n",
      "Iteration 5485, loss = 0.10793198\n",
      "Iteration 5486, loss = 0.10792691\n",
      "Iteration 5487, loss = 0.10792185\n",
      "Iteration 5488, loss = 0.10791679\n",
      "Iteration 5489, loss = 0.10791173\n",
      "Iteration 5490, loss = 0.10790667\n",
      "Iteration 5491, loss = 0.10790161\n",
      "Iteration 5492, loss = 0.10789656\n",
      "Iteration 5493, loss = 0.10789151\n",
      "Iteration 5494, loss = 0.10788646\n",
      "Iteration 5495, loss = 0.10788141\n",
      "Iteration 5496, loss = 0.10787636\n",
      "Iteration 5497, loss = 0.10787132\n",
      "Iteration 5498, loss = 0.10786628\n",
      "Iteration 5499, loss = 0.10786124\n",
      "Iteration 5500, loss = 0.10785621\n",
      "Iteration 5501, loss = 0.10785117\n",
      "Iteration 5502, loss = 0.10784614\n",
      "Iteration 5503, loss = 0.10784111\n",
      "Iteration 5504, loss = 0.10783608\n",
      "Iteration 5505, loss = 0.10783106\n",
      "Iteration 5506, loss = 0.10782604\n",
      "Iteration 5507, loss = 0.10782102\n",
      "Iteration 5508, loss = 0.10781600\n",
      "Iteration 5509, loss = 0.10781098\n",
      "Iteration 5510, loss = 0.10780597\n",
      "Iteration 5511, loss = 0.10780096\n",
      "Iteration 5512, loss = 0.10779595\n",
      "Iteration 5513, loss = 0.10779094\n",
      "Iteration 5514, loss = 0.10778593\n",
      "Iteration 5515, loss = 0.10778093\n",
      "Iteration 5516, loss = 0.10777593\n",
      "Iteration 5517, loss = 0.10777093\n",
      "Iteration 5518, loss = 0.10776593\n",
      "Iteration 5519, loss = 0.10776094\n",
      "Iteration 5520, loss = 0.10775595\n",
      "Iteration 5521, loss = 0.10775096\n",
      "Iteration 5522, loss = 0.10774597\n",
      "Iteration 5523, loss = 0.10774099\n",
      "Iteration 5524, loss = 0.10773600\n",
      "Iteration 5525, loss = 0.10773102\n",
      "Iteration 5526, loss = 0.10772604\n",
      "Iteration 5527, loss = 0.10772107\n",
      "Iteration 5528, loss = 0.10771609\n",
      "Iteration 5529, loss = 0.10771112\n",
      "Iteration 5530, loss = 0.10770615\n",
      "Iteration 5531, loss = 0.10770118\n",
      "Iteration 5532, loss = 0.10769621\n",
      "Iteration 5533, loss = 0.10769125\n",
      "Iteration 5534, loss = 0.10768629\n",
      "Iteration 5535, loss = 0.10768133\n",
      "Iteration 5536, loss = 0.10767637\n",
      "Iteration 5537, loss = 0.10767142\n",
      "Iteration 5538, loss = 0.10766646\n",
      "Iteration 5539, loss = 0.10766151\n",
      "Iteration 5540, loss = 0.10765657\n",
      "Iteration 5541, loss = 0.10765162\n",
      "Iteration 5542, loss = 0.10764667\n",
      "Iteration 5543, loss = 0.10764173\n",
      "Iteration 5544, loss = 0.10763679\n",
      "Iteration 5545, loss = 0.10763185\n",
      "Iteration 5546, loss = 0.10762692\n",
      "Iteration 5547, loss = 0.10762199\n",
      "Iteration 5548, loss = 0.10761705\n",
      "Iteration 5549, loss = 0.10761213\n",
      "Iteration 5550, loss = 0.10760720\n",
      "Iteration 5551, loss = 0.10760227\n",
      "Iteration 5552, loss = 0.10759735\n",
      "Iteration 5553, loss = 0.10759243\n",
      "Iteration 5554, loss = 0.10758751\n",
      "Iteration 5555, loss = 0.10758260\n",
      "Iteration 5556, loss = 0.10757768\n",
      "Iteration 5557, loss = 0.10757277\n",
      "Iteration 5558, loss = 0.10756786\n",
      "Iteration 5559, loss = 0.10756295\n",
      "Iteration 5560, loss = 0.10755805\n",
      "Iteration 5561, loss = 0.10755314\n",
      "Iteration 5562, loss = 0.10754824\n",
      "Iteration 5563, loss = 0.10754334\n",
      "Iteration 5564, loss = 0.10753844\n",
      "Iteration 5565, loss = 0.10753355\n",
      "Iteration 5566, loss = 0.10752866\n",
      "Iteration 5567, loss = 0.10752377\n",
      "Iteration 5568, loss = 0.10751888\n",
      "Iteration 5569, loss = 0.10751399\n",
      "Iteration 5570, loss = 0.10750911\n",
      "Iteration 5571, loss = 0.10750422\n",
      "Iteration 5572, loss = 0.10749934\n",
      "Iteration 5573, loss = 0.10749447\n",
      "Iteration 5574, loss = 0.10748959\n",
      "Iteration 5575, loss = 0.10748472\n",
      "Iteration 5576, loss = 0.10747984\n",
      "Iteration 5577, loss = 0.10747498\n",
      "Iteration 5578, loss = 0.10747011\n",
      "Iteration 5579, loss = 0.10746524\n",
      "Iteration 5580, loss = 0.10746038\n",
      "Iteration 5581, loss = 0.10745552\n",
      "Iteration 5582, loss = 0.10745066\n",
      "Iteration 5583, loss = 0.10744580\n",
      "Iteration 5584, loss = 0.10744095\n",
      "Iteration 5585, loss = 0.10743609\n",
      "Iteration 5586, loss = 0.10743124\n",
      "Iteration 5587, loss = 0.10742640\n",
      "Iteration 5588, loss = 0.10742155\n",
      "Iteration 5589, loss = 0.10741670\n",
      "Iteration 5590, loss = 0.10741186\n",
      "Iteration 5591, loss = 0.10740702\n",
      "Iteration 5592, loss = 0.10740218\n",
      "Iteration 5593, loss = 0.10739735\n",
      "Iteration 5594, loss = 0.10739251\n",
      "Iteration 5595, loss = 0.10738768\n",
      "Iteration 5596, loss = 0.10738285\n",
      "Iteration 5597, loss = 0.10737803\n",
      "Iteration 5598, loss = 0.10737320\n",
      "Iteration 5599, loss = 0.10736838\n",
      "Iteration 5600, loss = 0.10736356\n",
      "Iteration 5601, loss = 0.10735874\n",
      "Iteration 5602, loss = 0.10735392\n",
      "Iteration 5603, loss = 0.10734910\n",
      "Iteration 5604, loss = 0.10734429\n",
      "Iteration 5605, loss = 0.10733948\n",
      "Iteration 5606, loss = 0.10733467\n",
      "Iteration 5607, loss = 0.10732986\n",
      "Iteration 5608, loss = 0.10732506\n",
      "Iteration 5609, loss = 0.10732026\n",
      "Iteration 5610, loss = 0.10731546\n",
      "Iteration 5611, loss = 0.10731066\n",
      "Iteration 5612, loss = 0.10730586\n",
      "Iteration 5613, loss = 0.10730107\n",
      "Iteration 5614, loss = 0.10729627\n",
      "Iteration 5615, loss = 0.10729148\n",
      "Iteration 5616, loss = 0.10728670\n",
      "Iteration 5617, loss = 0.10728191\n",
      "Iteration 5618, loss = 0.10727713\n",
      "Iteration 5619, loss = 0.10727234\n",
      "Iteration 5620, loss = 0.10726756\n",
      "Iteration 5621, loss = 0.10726279\n",
      "Iteration 5622, loss = 0.10725801\n",
      "Iteration 5623, loss = 0.10725324\n",
      "Iteration 5624, loss = 0.10724846\n",
      "Iteration 5625, loss = 0.10724369\n",
      "Iteration 5626, loss = 0.10723893\n",
      "Iteration 5627, loss = 0.10723416\n",
      "Iteration 5628, loss = 0.10722940\n",
      "Iteration 5629, loss = 0.10722464\n",
      "Iteration 5630, loss = 0.10721988\n",
      "Iteration 5631, loss = 0.10721512\n",
      "Iteration 5632, loss = 0.10721036\n",
      "Iteration 5633, loss = 0.10720561\n",
      "Iteration 5634, loss = 0.10720086\n",
      "Iteration 5635, loss = 0.10719611\n",
      "Iteration 5636, loss = 0.10719136\n",
      "Iteration 5637, loss = 0.10718662\n",
      "Iteration 5638, loss = 0.10718187\n",
      "Iteration 5639, loss = 0.10717713\n",
      "Iteration 5640, loss = 0.10717239\n",
      "Iteration 5641, loss = 0.10716766\n",
      "Iteration 5642, loss = 0.10716292\n",
      "Iteration 5643, loss = 0.10715819\n",
      "Iteration 5644, loss = 0.10715346\n",
      "Iteration 5645, loss = 0.10714873\n",
      "Iteration 5646, loss = 0.10714400\n",
      "Iteration 5647, loss = 0.10713927\n",
      "Iteration 5648, loss = 0.10713455\n",
      "Iteration 5649, loss = 0.10712983\n",
      "Iteration 5650, loss = 0.10712511\n",
      "Iteration 5651, loss = 0.10712040\n",
      "Iteration 5652, loss = 0.10711568\n",
      "Iteration 5653, loss = 0.10711097\n",
      "Iteration 5654, loss = 0.10710626\n",
      "Iteration 5655, loss = 0.10710155\n",
      "Iteration 5656, loss = 0.10709684\n",
      "Iteration 5657, loss = 0.10709214\n",
      "Iteration 5658, loss = 0.10708743\n",
      "Iteration 5659, loss = 0.10708273\n",
      "Iteration 5660, loss = 0.10707803\n",
      "Iteration 5661, loss = 0.10707334\n",
      "Iteration 5662, loss = 0.10706864\n",
      "Iteration 5663, loss = 0.10706395\n",
      "Iteration 5664, loss = 0.10705926\n",
      "Iteration 5665, loss = 0.10705457\n",
      "Iteration 5666, loss = 0.10704988\n",
      "Iteration 5667, loss = 0.10704520\n",
      "Iteration 5668, loss = 0.10704051\n",
      "Iteration 5669, loss = 0.10703583\n",
      "Iteration 5670, loss = 0.10703115\n",
      "Iteration 5671, loss = 0.10702648\n",
      "Iteration 5672, loss = 0.10702180\n",
      "Iteration 5673, loss = 0.10701713\n",
      "Iteration 5674, loss = 0.10701246\n",
      "Iteration 5675, loss = 0.10700779\n",
      "Iteration 5676, loss = 0.10700312\n",
      "Iteration 5677, loss = 0.10699846\n",
      "Iteration 5678, loss = 0.10699380\n",
      "Iteration 5679, loss = 0.10698913\n",
      "Iteration 5680, loss = 0.10698448\n",
      "Iteration 5681, loss = 0.10697982\n",
      "Iteration 5682, loss = 0.10697516\n",
      "Iteration 5683, loss = 0.10697051\n",
      "Iteration 5684, loss = 0.10696586\n",
      "Iteration 5685, loss = 0.10696121\n",
      "Iteration 5686, loss = 0.10695656\n",
      "Iteration 5687, loss = 0.10695192\n",
      "Iteration 5688, loss = 0.10694727\n",
      "Iteration 5689, loss = 0.10694263\n",
      "Iteration 5690, loss = 0.10693799\n",
      "Iteration 5691, loss = 0.10693336\n",
      "Iteration 5692, loss = 0.10692872\n",
      "Iteration 5693, loss = 0.10692409\n",
      "Iteration 5694, loss = 0.10691946\n",
      "Iteration 5695, loss = 0.10691483\n",
      "Iteration 5696, loss = 0.10691020\n",
      "Iteration 5697, loss = 0.10690557\n",
      "Iteration 5698, loss = 0.10690095\n",
      "Iteration 5699, loss = 0.10689633\n",
      "Iteration 5700, loss = 0.10689171\n",
      "Iteration 5701, loss = 0.10688709\n",
      "Iteration 5702, loss = 0.10688248\n",
      "Iteration 5703, loss = 0.10687786\n",
      "Iteration 5704, loss = 0.10687325\n",
      "Iteration 5705, loss = 0.10686864\n",
      "Iteration 5706, loss = 0.10686403\n",
      "Iteration 5707, loss = 0.10685943\n",
      "Iteration 5708, loss = 0.10685482\n",
      "Iteration 5709, loss = 0.10685022\n",
      "Iteration 5710, loss = 0.10684562\n",
      "Iteration 5711, loss = 0.10684102\n",
      "Iteration 5712, loss = 0.10683642\n",
      "Iteration 5713, loss = 0.10683183\n",
      "Iteration 5714, loss = 0.10682724\n",
      "Iteration 5715, loss = 0.10682265\n",
      "Iteration 5716, loss = 0.10681806\n",
      "Iteration 5717, loss = 0.10681347\n",
      "Iteration 5718, loss = 0.10680889\n",
      "Iteration 5719, loss = 0.10680430\n",
      "Iteration 5720, loss = 0.10679972\n",
      "Iteration 5721, loss = 0.10679514\n",
      "Iteration 5722, loss = 0.10679057\n",
      "Iteration 5723, loss = 0.10678599\n",
      "Iteration 5724, loss = 0.10678142\n",
      "Iteration 5725, loss = 0.10677685\n",
      "Iteration 5726, loss = 0.10677228\n",
      "Iteration 5727, loss = 0.10676771\n",
      "Iteration 5728, loss = 0.10676315\n",
      "Iteration 5729, loss = 0.10675858\n",
      "Iteration 5730, loss = 0.10675402\n",
      "Iteration 5731, loss = 0.10674946\n",
      "Iteration 5732, loss = 0.10674490\n",
      "Iteration 5733, loss = 0.10674035\n",
      "Iteration 5734, loss = 0.10673579\n",
      "Iteration 5735, loss = 0.10673124\n",
      "Iteration 5736, loss = 0.10672669\n",
      "Iteration 5737, loss = 0.10672214\n",
      "Iteration 5738, loss = 0.10671760\n",
      "Iteration 5739, loss = 0.10671305\n",
      "Iteration 5740, loss = 0.10670851\n",
      "Iteration 5741, loss = 0.10670397\n",
      "Iteration 5742, loss = 0.10669943\n",
      "Iteration 5743, loss = 0.10669490\n",
      "Iteration 5744, loss = 0.10669036\n",
      "Iteration 5745, loss = 0.10668583\n",
      "Iteration 5746, loss = 0.10668130\n",
      "Iteration 5747, loss = 0.10667677\n",
      "Iteration 5748, loss = 0.10667224\n",
      "Iteration 5749, loss = 0.10666772\n",
      "Iteration 5750, loss = 0.10666319\n",
      "Iteration 5751, loss = 0.10665867\n",
      "Iteration 5752, loss = 0.10665415\n",
      "Iteration 5753, loss = 0.10664963\n",
      "Iteration 5754, loss = 0.10664512\n",
      "Iteration 5755, loss = 0.10664060\n",
      "Iteration 5756, loss = 0.10663609\n",
      "Iteration 5757, loss = 0.10663158\n",
      "Iteration 5758, loss = 0.10662707\n",
      "Iteration 5759, loss = 0.10662257\n",
      "Iteration 5760, loss = 0.10661806\n",
      "Iteration 5761, loss = 0.10661356\n",
      "Iteration 5762, loss = 0.10660906\n",
      "Iteration 5763, loss = 0.10660456\n",
      "Iteration 5764, loss = 0.10660006\n",
      "Iteration 5765, loss = 0.10659557\n",
      "Iteration 5766, loss = 0.10659107\n",
      "Iteration 5767, loss = 0.10658658\n",
      "Iteration 5768, loss = 0.10658209\n",
      "Iteration 5769, loss = 0.10657761\n",
      "Iteration 5770, loss = 0.10657312\n",
      "Iteration 5771, loss = 0.10656864\n",
      "Iteration 5772, loss = 0.10656415\n",
      "Iteration 5773, loss = 0.10655967\n",
      "Iteration 5774, loss = 0.10655520\n",
      "Iteration 5775, loss = 0.10655072\n",
      "Iteration 5776, loss = 0.10654624\n",
      "Iteration 5777, loss = 0.10654177\n",
      "Iteration 5778, loss = 0.10653730\n",
      "Iteration 5779, loss = 0.10653283\n",
      "Iteration 5780, loss = 0.10652837\n",
      "Iteration 5781, loss = 0.10652390\n",
      "Iteration 5782, loss = 0.10651944\n",
      "Iteration 5783, loss = 0.10651498\n",
      "Iteration 5784, loss = 0.10651052\n",
      "Iteration 5785, loss = 0.10650606\n",
      "Iteration 5786, loss = 0.10650160\n",
      "Iteration 5787, loss = 0.10649715\n",
      "Iteration 5788, loss = 0.10649270\n",
      "Iteration 5789, loss = 0.10648825\n",
      "Iteration 5790, loss = 0.10648380\n",
      "Iteration 5791, loss = 0.10647935\n",
      "Iteration 5792, loss = 0.10647491\n",
      "Iteration 5793, loss = 0.10647046\n",
      "Iteration 5794, loss = 0.10646602\n",
      "Iteration 5795, loss = 0.10646158\n",
      "Iteration 5796, loss = 0.10645714\n",
      "Iteration 5797, loss = 0.10645271\n",
      "Iteration 5798, loss = 0.10644828\n",
      "Iteration 5799, loss = 0.10644384\n",
      "Iteration 5800, loss = 0.10643941\n",
      "Iteration 5801, loss = 0.10643499\n",
      "Iteration 5802, loss = 0.10643056\n",
      "Iteration 5803, loss = 0.10642613\n",
      "Iteration 5804, loss = 0.10642171\n",
      "Iteration 5805, loss = 0.10641729\n",
      "Iteration 5806, loss = 0.10641287\n",
      "Iteration 5807, loss = 0.10640845\n",
      "Iteration 5808, loss = 0.10640404\n",
      "Iteration 5809, loss = 0.10639963\n",
      "Iteration 5810, loss = 0.10639521\n",
      "Iteration 5811, loss = 0.10639080\n",
      "Iteration 5812, loss = 0.10638640\n",
      "Iteration 5813, loss = 0.10638199\n",
      "Iteration 5814, loss = 0.10637759\n",
      "Iteration 5815, loss = 0.10637318\n",
      "Iteration 5816, loss = 0.10636878\n",
      "Iteration 5817, loss = 0.10636438\n",
      "Iteration 5818, loss = 0.10635999\n",
      "Iteration 5819, loss = 0.10635559\n",
      "Iteration 5820, loss = 0.10635120\n",
      "Iteration 5821, loss = 0.10634681\n",
      "Iteration 5822, loss = 0.10634242\n",
      "Iteration 5823, loss = 0.10633803\n",
      "Iteration 5824, loss = 0.10633364\n",
      "Iteration 5825, loss = 0.10632926\n",
      "Iteration 5826, loss = 0.10632487\n",
      "Iteration 5827, loss = 0.10632049\n",
      "Iteration 5828, loss = 0.10631611\n",
      "Iteration 5829, loss = 0.10631174\n",
      "Iteration 5830, loss = 0.10630736\n",
      "Iteration 5831, loss = 0.10630299\n",
      "Iteration 5832, loss = 0.10629862\n",
      "Iteration 5833, loss = 0.10629425\n",
      "Iteration 5834, loss = 0.10628988\n",
      "Iteration 5835, loss = 0.10628551\n",
      "Iteration 5836, loss = 0.10628115\n",
      "Iteration 5837, loss = 0.10627679\n",
      "Iteration 5838, loss = 0.10627242\n",
      "Iteration 5839, loss = 0.10626807\n",
      "Iteration 5840, loss = 0.10626371\n",
      "Iteration 5841, loss = 0.10625935\n",
      "Iteration 5842, loss = 0.10625500\n",
      "Iteration 5843, loss = 0.10625065\n",
      "Iteration 5844, loss = 0.10624630\n",
      "Iteration 5845, loss = 0.10624195\n",
      "Iteration 5846, loss = 0.10623760\n",
      "Iteration 5847, loss = 0.10623326\n",
      "Iteration 5848, loss = 0.10622891\n",
      "Iteration 5849, loss = 0.10622457\n",
      "Iteration 5850, loss = 0.10622023\n",
      "Iteration 5851, loss = 0.10621590\n",
      "Iteration 5852, loss = 0.10621156\n",
      "Iteration 5853, loss = 0.10620723\n",
      "Iteration 5854, loss = 0.10620289\n",
      "Iteration 5855, loss = 0.10619856\n",
      "Iteration 5856, loss = 0.10619423\n",
      "Iteration 5857, loss = 0.10618991\n",
      "Iteration 5858, loss = 0.10618558\n",
      "Iteration 5859, loss = 0.10618126\n",
      "Iteration 5860, loss = 0.10617694\n",
      "Iteration 5861, loss = 0.10617262\n",
      "Iteration 5862, loss = 0.10616830\n",
      "Iteration 5863, loss = 0.10616398\n",
      "Iteration 5864, loss = 0.10615967\n",
      "Iteration 5865, loss = 0.10615535\n",
      "Iteration 5866, loss = 0.10615104\n",
      "Iteration 5867, loss = 0.10614673\n",
      "Iteration 5868, loss = 0.10614243\n",
      "Iteration 5869, loss = 0.10613812\n",
      "Iteration 5870, loss = 0.10613382\n",
      "Iteration 5871, loss = 0.10612951\n",
      "Iteration 5872, loss = 0.10612521\n",
      "Iteration 5873, loss = 0.10612092\n",
      "Iteration 5874, loss = 0.10611662\n",
      "Iteration 5875, loss = 0.10611232\n",
      "Iteration 5876, loss = 0.10610803\n",
      "Iteration 5877, loss = 0.10610374\n",
      "Iteration 5878, loss = 0.10609945\n",
      "Iteration 5879, loss = 0.10609516\n",
      "Iteration 5880, loss = 0.10609087\n",
      "Iteration 5881, loss = 0.10608659\n",
      "Iteration 5882, loss = 0.10608231\n",
      "Iteration 5883, loss = 0.10607803\n",
      "Iteration 5884, loss = 0.10607375\n",
      "Iteration 5885, loss = 0.10606947\n",
      "Iteration 5886, loss = 0.10606519\n",
      "Iteration 5887, loss = 0.10606092\n",
      "Iteration 5888, loss = 0.10605665\n",
      "Iteration 5889, loss = 0.10605238\n",
      "Iteration 5890, loss = 0.10604811\n",
      "Iteration 5891, loss = 0.10604384\n",
      "Iteration 5892, loss = 0.10603957\n",
      "Iteration 5893, loss = 0.10603531\n",
      "Iteration 5894, loss = 0.10603105\n",
      "Iteration 5895, loss = 0.10602679\n",
      "Iteration 5896, loss = 0.10602253\n",
      "Iteration 5897, loss = 0.10601827\n",
      "Iteration 5898, loss = 0.10601402\n",
      "Iteration 5899, loss = 0.10600976\n",
      "Iteration 5900, loss = 0.10600551\n",
      "Iteration 5901, loss = 0.10600126\n",
      "Iteration 5902, loss = 0.10599701\n",
      "Iteration 5903, loss = 0.10599277\n",
      "Iteration 5904, loss = 0.10598852\n",
      "Iteration 5905, loss = 0.10598428\n",
      "Iteration 5906, loss = 0.10598004\n",
      "Iteration 5907, loss = 0.10597580\n",
      "Iteration 5908, loss = 0.10597156\n",
      "Iteration 5909, loss = 0.10596733\n",
      "Iteration 5910, loss = 0.10596309\n",
      "Iteration 5911, loss = 0.10595886\n",
      "Iteration 5912, loss = 0.10595463\n",
      "Iteration 5913, loss = 0.10595040\n",
      "Iteration 5914, loss = 0.10594617\n",
      "Iteration 5915, loss = 0.10594195\n",
      "Iteration 5916, loss = 0.10593772\n",
      "Iteration 5917, loss = 0.10593350\n",
      "Iteration 5918, loss = 0.10592928\n",
      "Iteration 5919, loss = 0.10592506\n",
      "Iteration 5920, loss = 0.10592084\n",
      "Iteration 5921, loss = 0.10591663\n",
      "Iteration 5922, loss = 0.10591241\n",
      "Iteration 5923, loss = 0.10590820\n",
      "Iteration 5924, loss = 0.10590399\n",
      "Iteration 5925, loss = 0.10589978\n",
      "Iteration 5926, loss = 0.10589558\n",
      "Iteration 5927, loss = 0.10589137\n",
      "Iteration 5928, loss = 0.10588717\n",
      "Iteration 5929, loss = 0.10588297\n",
      "Iteration 5930, loss = 0.10587877\n",
      "Iteration 5931, loss = 0.10587457\n",
      "Iteration 5932, loss = 0.10587037\n",
      "Iteration 5933, loss = 0.10586618\n",
      "Iteration 5934, loss = 0.10586198\n",
      "Iteration 5935, loss = 0.10585779\n",
      "Iteration 5936, loss = 0.10585360\n",
      "Iteration 5937, loss = 0.10584941\n",
      "Iteration 5938, loss = 0.10584523\n",
      "Iteration 5939, loss = 0.10584104\n",
      "Iteration 5940, loss = 0.10583686\n",
      "Iteration 5941, loss = 0.10583268\n",
      "Iteration 5942, loss = 0.10582850\n",
      "Iteration 5943, loss = 0.10582432\n",
      "Iteration 5944, loss = 0.10582014\n",
      "Iteration 5945, loss = 0.10581597\n",
      "Iteration 5946, loss = 0.10581180\n",
      "Iteration 5947, loss = 0.10580763\n",
      "Iteration 5948, loss = 0.10580346\n",
      "Iteration 5949, loss = 0.10579929\n",
      "Iteration 5950, loss = 0.10579512\n",
      "Iteration 5951, loss = 0.10579096\n",
      "Iteration 5952, loss = 0.10578679\n",
      "Iteration 5953, loss = 0.10578263\n",
      "Iteration 5954, loss = 0.10577847\n",
      "Iteration 5955, loss = 0.10577432\n",
      "Iteration 5956, loss = 0.10577016\n",
      "Iteration 5957, loss = 0.10576600\n",
      "Iteration 5958, loss = 0.10576185\n",
      "Iteration 5959, loss = 0.10575770\n",
      "Iteration 5960, loss = 0.10575355\n",
      "Iteration 5961, loss = 0.10574940\n",
      "Iteration 5962, loss = 0.10574526\n",
      "Iteration 5963, loss = 0.10574111\n",
      "Iteration 5964, loss = 0.10573697\n",
      "Iteration 5965, loss = 0.10573283\n",
      "Iteration 5966, loss = 0.10572869\n",
      "Iteration 5967, loss = 0.10572455\n",
      "Iteration 5968, loss = 0.10572042\n",
      "Iteration 5969, loss = 0.10571628\n",
      "Iteration 5970, loss = 0.10571215\n",
      "Iteration 5971, loss = 0.10570802\n",
      "Iteration 5972, loss = 0.10570389\n",
      "Iteration 5973, loss = 0.10569976\n",
      "Iteration 5974, loss = 0.10569563\n",
      "Iteration 5975, loss = 0.10569151\n",
      "Iteration 5976, loss = 0.10568739\n",
      "Iteration 5977, loss = 0.10568327\n",
      "Iteration 5978, loss = 0.10567915\n",
      "Iteration 5979, loss = 0.10567503\n",
      "Iteration 5980, loss = 0.10567091\n",
      "Iteration 5981, loss = 0.10566680\n",
      "Iteration 5982, loss = 0.10566268\n",
      "Iteration 5983, loss = 0.10565857\n",
      "Iteration 5984, loss = 0.10565446\n",
      "Iteration 5985, loss = 0.10565036\n",
      "Iteration 5986, loss = 0.10564625\n",
      "Iteration 5987, loss = 0.10564214\n",
      "Iteration 5988, loss = 0.10563804\n",
      "Iteration 5989, loss = 0.10563394\n",
      "Iteration 5990, loss = 0.10562984\n",
      "Iteration 5991, loss = 0.10562574\n",
      "Iteration 5992, loss = 0.10562165\n",
      "Iteration 5993, loss = 0.10561755\n",
      "Iteration 5994, loss = 0.10561346\n",
      "Iteration 5995, loss = 0.10560937\n",
      "Iteration 5996, loss = 0.10560528\n",
      "Iteration 5997, loss = 0.10560119\n",
      "Iteration 5998, loss = 0.10559710\n",
      "Iteration 5999, loss = 0.10559302\n",
      "Iteration 6000, loss = 0.10558893\n",
      "Iteration 6001, loss = 0.10558485\n",
      "Iteration 6002, loss = 0.10558077\n",
      "Iteration 6003, loss = 0.10557669\n",
      "Iteration 6004, loss = 0.10557262\n",
      "Iteration 6005, loss = 0.10556854\n",
      "Iteration 6006, loss = 0.10556447\n",
      "Iteration 6007, loss = 0.10556039\n",
      "Iteration 6008, loss = 0.10555632\n",
      "Iteration 6009, loss = 0.10555226\n",
      "Iteration 6010, loss = 0.10554819\n",
      "Iteration 6011, loss = 0.10554412\n",
      "Iteration 6012, loss = 0.10554006\n",
      "Iteration 6013, loss = 0.10553600\n",
      "Iteration 6014, loss = 0.10553194\n",
      "Iteration 6015, loss = 0.10552788\n",
      "Iteration 6016, loss = 0.10552382\n",
      "Iteration 6017, loss = 0.10551976\n",
      "Iteration 6018, loss = 0.10551571\n",
      "Iteration 6019, loss = 0.10551166\n",
      "Iteration 6020, loss = 0.10550761\n",
      "Iteration 6021, loss = 0.10550356\n",
      "Iteration 6022, loss = 0.10549951\n",
      "Iteration 6023, loss = 0.10549546\n",
      "Iteration 6024, loss = 0.10549142\n",
      "Iteration 6025, loss = 0.10548738\n",
      "Iteration 6026, loss = 0.10548334\n",
      "Iteration 6027, loss = 0.10547930\n",
      "Iteration 6028, loss = 0.10547526\n",
      "Iteration 6029, loss = 0.10547122\n",
      "Iteration 6030, loss = 0.10546719\n",
      "Iteration 6031, loss = 0.10546315\n",
      "Iteration 6032, loss = 0.10545912\n",
      "Iteration 6033, loss = 0.10545509\n",
      "Iteration 6034, loss = 0.10545106\n",
      "Iteration 6035, loss = 0.10544704\n",
      "Iteration 6036, loss = 0.10544301\n",
      "Iteration 6037, loss = 0.10543899\n",
      "Iteration 6038, loss = 0.10543497\n",
      "Iteration 6039, loss = 0.10543094\n",
      "Iteration 6040, loss = 0.10542693\n",
      "Iteration 6041, loss = 0.10542291\n",
      "Iteration 6042, loss = 0.10541889\n",
      "Iteration 6043, loss = 0.10541488\n",
      "Iteration 6044, loss = 0.10541087\n",
      "Iteration 6045, loss = 0.10540686\n",
      "Iteration 6046, loss = 0.10540285\n",
      "Iteration 6047, loss = 0.10539884\n",
      "Iteration 6048, loss = 0.10539483\n",
      "Iteration 6049, loss = 0.10539083\n",
      "Iteration 6050, loss = 0.10538683\n",
      "Iteration 6051, loss = 0.10538282\n",
      "Iteration 6052, loss = 0.10537882\n",
      "Iteration 6053, loss = 0.10537483\n",
      "Iteration 6054, loss = 0.10537083\n",
      "Iteration 6055, loss = 0.10536683\n",
      "Iteration 6056, loss = 0.10536284\n",
      "Iteration 6057, loss = 0.10535885\n",
      "Iteration 6058, loss = 0.10535486\n",
      "Iteration 6059, loss = 0.10535087\n",
      "Iteration 6060, loss = 0.10534688\n",
      "Iteration 6061, loss = 0.10534290\n",
      "Iteration 6062, loss = 0.10533891\n",
      "Iteration 6063, loss = 0.10533493\n",
      "Iteration 6064, loss = 0.10533095\n",
      "Iteration 6065, loss = 0.10532697\n",
      "Iteration 6066, loss = 0.10532299\n",
      "Iteration 6067, loss = 0.10531902\n",
      "Iteration 6068, loss = 0.10531504\n",
      "Iteration 6069, loss = 0.10531107\n",
      "Iteration 6070, loss = 0.10530710\n",
      "Iteration 6071, loss = 0.10530313\n",
      "Iteration 6072, loss = 0.10529916\n",
      "Iteration 6073, loss = 0.10529519\n",
      "Iteration 6074, loss = 0.10529123\n",
      "Iteration 6075, loss = 0.10528727\n",
      "Iteration 6076, loss = 0.10528330\n",
      "Iteration 6077, loss = 0.10527934\n",
      "Iteration 6078, loss = 0.10527538\n",
      "Iteration 6079, loss = 0.10527143\n",
      "Iteration 6080, loss = 0.10526747\n",
      "Iteration 6081, loss = 0.10526352\n",
      "Iteration 6082, loss = 0.10525956\n",
      "Iteration 6083, loss = 0.10525561\n",
      "Iteration 6084, loss = 0.10525166\n",
      "Iteration 6085, loss = 0.10524772\n",
      "Iteration 6086, loss = 0.10524377\n",
      "Iteration 6087, loss = 0.10523982\n",
      "Iteration 6088, loss = 0.10523588\n",
      "Iteration 6089, loss = 0.10523194\n",
      "Iteration 6090, loss = 0.10522800\n",
      "Iteration 6091, loss = 0.10522406\n",
      "Iteration 6092, loss = 0.10522012\n",
      "Iteration 6093, loss = 0.10521619\n",
      "Iteration 6094, loss = 0.10521225\n",
      "Iteration 6095, loss = 0.10520832\n",
      "Iteration 6096, loss = 0.10520439\n",
      "Iteration 6097, loss = 0.10520046\n",
      "Iteration 6098, loss = 0.10519653\n",
      "Iteration 6099, loss = 0.10519261\n",
      "Iteration 6100, loss = 0.10518868\n",
      "Iteration 6101, loss = 0.10518476\n",
      "Iteration 6102, loss = 0.10518084\n",
      "Iteration 6103, loss = 0.10517692\n",
      "Iteration 6104, loss = 0.10517300\n",
      "Iteration 6105, loss = 0.10516908\n",
      "Iteration 6106, loss = 0.10516516\n",
      "Iteration 6107, loss = 0.10516125\n",
      "Iteration 6108, loss = 0.10515734\n",
      "Iteration 6109, loss = 0.10515343\n",
      "Iteration 6110, loss = 0.10514952\n",
      "Iteration 6111, loss = 0.10514561\n",
      "Iteration 6112, loss = 0.10514170\n",
      "Iteration 6113, loss = 0.10513780\n",
      "Iteration 6114, loss = 0.10513390\n",
      "Iteration 6115, loss = 0.10512999\n",
      "Iteration 6116, loss = 0.10512609\n",
      "Iteration 6117, loss = 0.10512219\n",
      "Iteration 6118, loss = 0.10511830\n",
      "Iteration 6119, loss = 0.10511440\n",
      "Iteration 6120, loss = 0.10511051\n",
      "Iteration 6121, loss = 0.10510662\n",
      "Iteration 6122, loss = 0.10510272\n",
      "Iteration 6123, loss = 0.10509883\n",
      "Iteration 6124, loss = 0.10509495\n",
      "Iteration 6125, loss = 0.10509106\n",
      "Iteration 6126, loss = 0.10508718\n",
      "Iteration 6127, loss = 0.10508329\n",
      "Iteration 6128, loss = 0.10507941\n",
      "Iteration 6129, loss = 0.10507553\n",
      "Iteration 6130, loss = 0.10507165\n",
      "Iteration 6131, loss = 0.10506777\n",
      "Iteration 6132, loss = 0.10506390\n",
      "Iteration 6133, loss = 0.10506002\n",
      "Iteration 6134, loss = 0.10505615\n",
      "Iteration 6135, loss = 0.10505228\n",
      "Iteration 6136, loss = 0.10504841\n",
      "Iteration 6137, loss = 0.10504454\n",
      "Iteration 6138, loss = 0.10504067\n",
      "Iteration 6139, loss = 0.10503681\n",
      "Iteration 6140, loss = 0.10503294\n",
      "Iteration 6141, loss = 0.10502908\n",
      "Iteration 6142, loss = 0.10502522\n",
      "Iteration 6143, loss = 0.10502136\n",
      "Iteration 6144, loss = 0.10501750\n",
      "Iteration 6145, loss = 0.10501365\n",
      "Iteration 6146, loss = 0.10500979\n",
      "Iteration 6147, loss = 0.10500594\n",
      "Iteration 6148, loss = 0.10500209\n",
      "Iteration 6149, loss = 0.10499824\n",
      "Iteration 6150, loss = 0.10499439\n",
      "Iteration 6151, loss = 0.10499054\n",
      "Iteration 6152, loss = 0.10498669\n",
      "Iteration 6153, loss = 0.10498285\n",
      "Iteration 6154, loss = 0.10497901\n",
      "Iteration 6155, loss = 0.10497517\n",
      "Iteration 6156, loss = 0.10497133\n",
      "Iteration 6157, loss = 0.10496749\n",
      "Iteration 6158, loss = 0.10496365\n",
      "Iteration 6159, loss = 0.10495982\n",
      "Iteration 6160, loss = 0.10495598\n",
      "Iteration 6161, loss = 0.10495215\n",
      "Iteration 6162, loss = 0.10494832\n",
      "Iteration 6163, loss = 0.10494449\n",
      "Iteration 6164, loss = 0.10494066\n",
      "Iteration 6165, loss = 0.10493683\n",
      "Iteration 6166, loss = 0.10493301\n",
      "Iteration 6167, loss = 0.10492919\n",
      "Iteration 6168, loss = 0.10492536\n",
      "Iteration 6169, loss = 0.10492154\n",
      "Iteration 6170, loss = 0.10491772\n",
      "Iteration 6171, loss = 0.10491391\n",
      "Iteration 6172, loss = 0.10491009\n",
      "Iteration 6173, loss = 0.10490628\n",
      "Iteration 6174, loss = 0.10490246\n",
      "Iteration 6175, loss = 0.10489865\n",
      "Iteration 6176, loss = 0.10489484\n",
      "Iteration 6177, loss = 0.10489103\n",
      "Iteration 6178, loss = 0.10488722\n",
      "Iteration 6179, loss = 0.10488342\n",
      "Iteration 6180, loss = 0.10487961\n",
      "Iteration 6181, loss = 0.10487581\n",
      "Iteration 6182, loss = 0.10487201\n",
      "Iteration 6183, loss = 0.10486821\n",
      "Iteration 6184, loss = 0.10486441\n",
      "Iteration 6185, loss = 0.10486061\n",
      "Iteration 6186, loss = 0.10485682\n",
      "Iteration 6187, loss = 0.10485302\n",
      "Iteration 6188, loss = 0.10484923\n",
      "Iteration 6189, loss = 0.10484544\n",
      "Iteration 6190, loss = 0.10484165\n",
      "Iteration 6191, loss = 0.10483786\n",
      "Iteration 6192, loss = 0.10483408\n",
      "Iteration 6193, loss = 0.10483029\n",
      "Iteration 6194, loss = 0.10482651\n",
      "Iteration 6195, loss = 0.10482273\n",
      "Iteration 6196, loss = 0.10481894\n",
      "Iteration 6197, loss = 0.10481516\n",
      "Iteration 6198, loss = 0.10481139\n",
      "Iteration 6199, loss = 0.10480761\n",
      "Iteration 6200, loss = 0.10480384\n",
      "Iteration 6201, loss = 0.10480006\n",
      "Iteration 6202, loss = 0.10479629\n",
      "Iteration 6203, loss = 0.10479252\n",
      "Iteration 6204, loss = 0.10478875\n",
      "Iteration 6205, loss = 0.10478498\n",
      "Iteration 6206, loss = 0.10478122\n",
      "Iteration 6207, loss = 0.10477745\n",
      "Iteration 6208, loss = 0.10477369\n",
      "Iteration 6209, loss = 0.10476992\n",
      "Iteration 6210, loss = 0.10476616\n",
      "Iteration 6211, loss = 0.10476241\n",
      "Iteration 6212, loss = 0.10475865\n",
      "Iteration 6213, loss = 0.10475489\n",
      "Iteration 6214, loss = 0.10475114\n",
      "Iteration 6215, loss = 0.10474738\n",
      "Iteration 6216, loss = 0.10474363\n",
      "Iteration 6217, loss = 0.10473988\n",
      "Iteration 6218, loss = 0.10473613\n",
      "Iteration 6219, loss = 0.10473238\n",
      "Iteration 6220, loss = 0.10472864\n",
      "Iteration 6221, loss = 0.10472489\n",
      "Iteration 6222, loss = 0.10472115\n",
      "Iteration 6223, loss = 0.10471741\n",
      "Iteration 6224, loss = 0.10471367\n",
      "Iteration 6225, loss = 0.10470993\n",
      "Iteration 6226, loss = 0.10470619\n",
      "Iteration 6227, loss = 0.10470246\n",
      "Iteration 6228, loss = 0.10469872\n",
      "Iteration 6229, loss = 0.10469499\n",
      "Iteration 6230, loss = 0.10469126\n",
      "Iteration 6231, loss = 0.10468753\n",
      "Iteration 6232, loss = 0.10468380\n",
      "Iteration 6233, loss = 0.10468007\n",
      "Iteration 6234, loss = 0.10467634\n",
      "Iteration 6235, loss = 0.10467262\n",
      "Iteration 6236, loss = 0.10466889\n",
      "Iteration 6237, loss = 0.10466517\n",
      "Iteration 6238, loss = 0.10466145\n",
      "Iteration 6239, loss = 0.10465773\n",
      "Iteration 6240, loss = 0.10465402\n",
      "Iteration 6241, loss = 0.10465030\n",
      "Iteration 6242, loss = 0.10464659\n",
      "Iteration 6243, loss = 0.10464287\n",
      "Iteration 6244, loss = 0.10463916\n",
      "Iteration 6245, loss = 0.10463545\n",
      "Iteration 6246, loss = 0.10463174\n",
      "Iteration 6247, loss = 0.10462803\n",
      "Iteration 6248, loss = 0.10462433\n",
      "Iteration 6249, loss = 0.10462062\n",
      "Iteration 6250, loss = 0.10461692\n",
      "Iteration 6251, loss = 0.10461322\n",
      "Iteration 6252, loss = 0.10460952\n",
      "Iteration 6253, loss = 0.10460582\n",
      "Iteration 6254, loss = 0.10460212\n",
      "Iteration 6255, loss = 0.10459842\n",
      "Iteration 6256, loss = 0.10459473\n",
      "Iteration 6257, loss = 0.10459104\n",
      "Iteration 6258, loss = 0.10458734\n",
      "Iteration 6259, loss = 0.10458365\n",
      "Iteration 6260, loss = 0.10457996\n",
      "Iteration 6261, loss = 0.10457628\n",
      "Iteration 6262, loss = 0.10457259\n",
      "Iteration 6263, loss = 0.10456890\n",
      "Iteration 6264, loss = 0.10456522\n",
      "Iteration 6265, loss = 0.10456154\n",
      "Iteration 6266, loss = 0.10455786\n",
      "Iteration 6267, loss = 0.10455418\n",
      "Iteration 6268, loss = 0.10455050\n",
      "Iteration 6269, loss = 0.10454682\n",
      "Iteration 6270, loss = 0.10454315\n",
      "Iteration 6271, loss = 0.10453947\n",
      "Iteration 6272, loss = 0.10453580\n",
      "Iteration 6273, loss = 0.10453213\n",
      "Iteration 6274, loss = 0.10452846\n",
      "Iteration 6275, loss = 0.10452479\n",
      "Iteration 6276, loss = 0.10452113\n",
      "Iteration 6277, loss = 0.10451746\n",
      "Iteration 6278, loss = 0.10451380\n",
      "Iteration 6279, loss = 0.10451013\n",
      "Iteration 6280, loss = 0.10450647\n",
      "Iteration 6281, loss = 0.10450281\n",
      "Iteration 6282, loss = 0.10449915\n",
      "Iteration 6283, loss = 0.10449550\n",
      "Iteration 6284, loss = 0.10449184\n",
      "Iteration 6285, loss = 0.10448819\n",
      "Iteration 6286, loss = 0.10448453\n",
      "Iteration 6287, loss = 0.10448088\n",
      "Iteration 6288, loss = 0.10447723\n",
      "Iteration 6289, loss = 0.10447358\n",
      "Iteration 6290, loss = 0.10446994\n",
      "Iteration 6291, loss = 0.10446629\n",
      "Iteration 6292, loss = 0.10446265\n",
      "Iteration 6293, loss = 0.10445900\n",
      "Iteration 6294, loss = 0.10445536\n",
      "Iteration 6295, loss = 0.10445172\n",
      "Iteration 6296, loss = 0.10444808\n",
      "Iteration 6297, loss = 0.10444444\n",
      "Iteration 6298, loss = 0.10444081\n",
      "Iteration 6299, loss = 0.10443717\n",
      "Iteration 6300, loss = 0.10443354\n",
      "Iteration 6301, loss = 0.10442991\n",
      "Iteration 6302, loss = 0.10442627\n",
      "Iteration 6303, loss = 0.10442264\n",
      "Iteration 6304, loss = 0.10441902\n",
      "Iteration 6305, loss = 0.10441539\n",
      "Iteration 6306, loss = 0.10441176\n",
      "Iteration 6307, loss = 0.10440814\n",
      "Iteration 6308, loss = 0.10440452\n",
      "Iteration 6309, loss = 0.10440090\n",
      "Iteration 6310, loss = 0.10439728\n",
      "Iteration 6311, loss = 0.10439366\n",
      "Iteration 6312, loss = 0.10439004\n",
      "Iteration 6313, loss = 0.10438643\n",
      "Iteration 6314, loss = 0.10438281\n",
      "Iteration 6315, loss = 0.10437920\n",
      "Iteration 6316, loss = 0.10437559\n",
      "Iteration 6317, loss = 0.10437198\n",
      "Iteration 6318, loss = 0.10436837\n",
      "Iteration 6319, loss = 0.10436476\n",
      "Iteration 6320, loss = 0.10436115\n",
      "Iteration 6321, loss = 0.10435755\n",
      "Iteration 6322, loss = 0.10435394\n",
      "Iteration 6323, loss = 0.10435034\n",
      "Iteration 6324, loss = 0.10434674\n",
      "Iteration 6325, loss = 0.10434314\n",
      "Iteration 6326, loss = 0.10433954\n",
      "Iteration 6327, loss = 0.10433595\n",
      "Iteration 6328, loss = 0.10433235\n",
      "Iteration 6329, loss = 0.10432876\n",
      "Iteration 6330, loss = 0.10432516\n",
      "Iteration 6331, loss = 0.10432157\n",
      "Iteration 6332, loss = 0.10431798\n",
      "Iteration 6333, loss = 0.10431439\n",
      "Iteration 6334, loss = 0.10431081\n",
      "Iteration 6335, loss = 0.10430722\n",
      "Iteration 6336, loss = 0.10430364\n",
      "Iteration 6337, loss = 0.10430005\n",
      "Iteration 6338, loss = 0.10429647\n",
      "Iteration 6339, loss = 0.10429289\n",
      "Iteration 6340, loss = 0.10428931\n",
      "Iteration 6341, loss = 0.10428573\n",
      "Iteration 6342, loss = 0.10428216\n",
      "Iteration 6343, loss = 0.10427858\n",
      "Iteration 6344, loss = 0.10427501\n",
      "Iteration 6345, loss = 0.10427144\n",
      "Iteration 6346, loss = 0.10426786\n",
      "Iteration 6347, loss = 0.10426429\n",
      "Iteration 6348, loss = 0.10426073\n",
      "Iteration 6349, loss = 0.10425716\n",
      "Iteration 6350, loss = 0.10425359\n",
      "Iteration 6351, loss = 0.10425003\n",
      "Iteration 6352, loss = 0.10424647\n",
      "Iteration 6353, loss = 0.10424290\n",
      "Iteration 6354, loss = 0.10423934\n",
      "Iteration 6355, loss = 0.10423578\n",
      "Iteration 6356, loss = 0.10423223\n",
      "Iteration 6357, loss = 0.10422867\n",
      "Iteration 6358, loss = 0.10422511\n",
      "Iteration 6359, loss = 0.10422156\n",
      "Iteration 6360, loss = 0.10421801\n",
      "Iteration 6361, loss = 0.10421446\n",
      "Iteration 6362, loss = 0.10421091\n",
      "Iteration 6363, loss = 0.10420736\n",
      "Iteration 6364, loss = 0.10420381\n",
      "Iteration 6365, loss = 0.10420027\n",
      "Iteration 6366, loss = 0.10419672\n",
      "Iteration 6367, loss = 0.10419318\n",
      "Iteration 6368, loss = 0.10418964\n",
      "Iteration 6369, loss = 0.10418610\n",
      "Iteration 6370, loss = 0.10418256\n",
      "Iteration 6371, loss = 0.10417902\n",
      "Iteration 6372, loss = 0.10417548\n",
      "Iteration 6373, loss = 0.10417195\n",
      "Iteration 6374, loss = 0.10416841\n",
      "Iteration 6375, loss = 0.10416488\n",
      "Iteration 6376, loss = 0.10416135\n",
      "Iteration 6377, loss = 0.10415782\n",
      "Iteration 6378, loss = 0.10415429\n",
      "Iteration 6379, loss = 0.10415076\n",
      "Iteration 6380, loss = 0.10414724\n",
      "Iteration 6381, loss = 0.10414371\n",
      "Iteration 6382, loss = 0.10414019\n",
      "Iteration 6383, loss = 0.10413667\n",
      "Iteration 6384, loss = 0.10413315\n",
      "Iteration 6385, loss = 0.10412963\n",
      "Iteration 6386, loss = 0.10412611\n",
      "Iteration 6387, loss = 0.10412259\n",
      "Iteration 6388, loss = 0.10411908\n",
      "Iteration 6389, loss = 0.10411556\n",
      "Iteration 6390, loss = 0.10411205\n",
      "Iteration 6391, loss = 0.10410854\n",
      "Iteration 6392, loss = 0.10410503\n",
      "Iteration 6393, loss = 0.10410152\n",
      "Iteration 6394, loss = 0.10409801\n",
      "Iteration 6395, loss = 0.10409450\n",
      "Iteration 6396, loss = 0.10409100\n",
      "Iteration 6397, loss = 0.10408749\n",
      "Iteration 6398, loss = 0.10408399\n",
      "Iteration 6399, loss = 0.10408049\n",
      "Iteration 6400, loss = 0.10407699\n",
      "Iteration 6401, loss = 0.10407349\n",
      "Iteration 6402, loss = 0.10407000\n",
      "Iteration 6403, loss = 0.10406650\n",
      "Iteration 6404, loss = 0.10406300\n",
      "Iteration 6405, loss = 0.10405951\n",
      "Iteration 6406, loss = 0.10405602\n",
      "Iteration 6407, loss = 0.10405253\n",
      "Iteration 6408, loss = 0.10404904\n",
      "Iteration 6409, loss = 0.10404555\n",
      "Iteration 6410, loss = 0.10404206\n",
      "Iteration 6411, loss = 0.10403858\n",
      "Iteration 6412, loss = 0.10403509\n",
      "Iteration 6413, loss = 0.10403161\n",
      "Iteration 6414, loss = 0.10402813\n",
      "Iteration 6415, loss = 0.10402465\n",
      "Iteration 6416, loss = 0.10402117\n",
      "Iteration 6417, loss = 0.10401769\n",
      "Iteration 6418, loss = 0.10401422\n",
      "Iteration 6419, loss = 0.10401074\n",
      "Iteration 6420, loss = 0.10400727\n",
      "Iteration 6421, loss = 0.10400379\n",
      "Iteration 6422, loss = 0.10400032\n",
      "Iteration 6423, loss = 0.10399685\n",
      "Iteration 6424, loss = 0.10399338\n",
      "Iteration 6425, loss = 0.10398992\n",
      "Iteration 6426, loss = 0.10398645\n",
      "Iteration 6427, loss = 0.10398298\n",
      "Iteration 6428, loss = 0.10397952\n",
      "Iteration 6429, loss = 0.10397606\n",
      "Iteration 6430, loss = 0.10397260\n",
      "Iteration 6431, loss = 0.10396914\n",
      "Iteration 6432, loss = 0.10396568\n",
      "Iteration 6433, loss = 0.10396222\n",
      "Iteration 6434, loss = 0.10395876\n",
      "Iteration 6435, loss = 0.10395531\n",
      "Iteration 6436, loss = 0.10395186\n",
      "Iteration 6437, loss = 0.10394840\n",
      "Iteration 6438, loss = 0.10394495\n",
      "Iteration 6439, loss = 0.10394150\n",
      "Iteration 6440, loss = 0.10393805\n",
      "Iteration 6441, loss = 0.10393461\n",
      "Iteration 6442, loss = 0.10393116\n",
      "Iteration 6443, loss = 0.10392772\n",
      "Iteration 6444, loss = 0.10392427\n",
      "Iteration 6445, loss = 0.10392083\n",
      "Iteration 6446, loss = 0.10391739\n",
      "Iteration 6447, loss = 0.10391395\n",
      "Iteration 6448, loss = 0.10391051\n",
      "Iteration 6449, loss = 0.10390708\n",
      "Iteration 6450, loss = 0.10390364\n",
      "Iteration 6451, loss = 0.10390021\n",
      "Iteration 6452, loss = 0.10389677\n",
      "Iteration 6453, loss = 0.10389334\n",
      "Iteration 6454, loss = 0.10388991\n",
      "Iteration 6455, loss = 0.10388648\n",
      "Iteration 6456, loss = 0.10388305\n",
      "Iteration 6457, loss = 0.10387963\n",
      "Iteration 6458, loss = 0.10387620\n",
      "Iteration 6459, loss = 0.10387278\n",
      "Iteration 6460, loss = 0.10386935\n",
      "Iteration 6461, loss = 0.10386593\n",
      "Iteration 6462, loss = 0.10386251\n",
      "Iteration 6463, loss = 0.10385909\n",
      "Iteration 6464, loss = 0.10385567\n",
      "Iteration 6465, loss = 0.10385226\n",
      "Iteration 6466, loss = 0.10384884\n",
      "Iteration 6467, loss = 0.10384543\n",
      "Iteration 6468, loss = 0.10384201\n",
      "Iteration 6469, loss = 0.10383860\n",
      "Iteration 6470, loss = 0.10383519\n",
      "Iteration 6471, loss = 0.10383178\n",
      "Iteration 6472, loss = 0.10382837\n",
      "Iteration 6473, loss = 0.10382497\n",
      "Iteration 6474, loss = 0.10382156\n",
      "Iteration 6475, loss = 0.10381816\n",
      "Iteration 6476, loss = 0.10381475\n",
      "Iteration 6477, loss = 0.10381135\n",
      "Iteration 6478, loss = 0.10380795\n",
      "Iteration 6479, loss = 0.10380455\n",
      "Iteration 6480, loss = 0.10380115\n",
      "Iteration 6481, loss = 0.10379776\n",
      "Iteration 6482, loss = 0.10379436\n",
      "Iteration 6483, loss = 0.10379097\n",
      "Iteration 6484, loss = 0.10378757\n",
      "Iteration 6485, loss = 0.10378418\n",
      "Iteration 6486, loss = 0.10378079\n",
      "Iteration 6487, loss = 0.10377740\n",
      "Iteration 6488, loss = 0.10377401\n",
      "Iteration 6489, loss = 0.10377063\n",
      "Iteration 6490, loss = 0.10376724\n",
      "Iteration 6491, loss = 0.10376386\n",
      "Iteration 6492, loss = 0.10376047\n",
      "Iteration 6493, loss = 0.10375709\n",
      "Iteration 6494, loss = 0.10375371\n",
      "Iteration 6495, loss = 0.10375033\n",
      "Iteration 6496, loss = 0.10374695\n",
      "Iteration 6497, loss = 0.10374358\n",
      "Iteration 6498, loss = 0.10374020\n",
      "Iteration 6499, loss = 0.10373683\n",
      "Iteration 6500, loss = 0.10373345\n",
      "Iteration 6501, loss = 0.10373008\n",
      "Iteration 6502, loss = 0.10372671\n",
      "Iteration 6503, loss = 0.10372334\n",
      "Iteration 6504, loss = 0.10371997\n",
      "Iteration 6505, loss = 0.10371661\n",
      "Iteration 6506, loss = 0.10371324\n",
      "Iteration 6507, loss = 0.10370987\n",
      "Iteration 6508, loss = 0.10370651\n",
      "Iteration 6509, loss = 0.10370315\n",
      "Iteration 6510, loss = 0.10369979\n",
      "Iteration 6511, loss = 0.10369643\n",
      "Iteration 6512, loss = 0.10369307\n",
      "Iteration 6513, loss = 0.10368971\n",
      "Iteration 6514, loss = 0.10368636\n",
      "Iteration 6515, loss = 0.10368300\n",
      "Iteration 6516, loss = 0.10367965\n",
      "Iteration 6517, loss = 0.10367630\n",
      "Iteration 6518, loss = 0.10367294\n",
      "Iteration 6519, loss = 0.10366959\n",
      "Iteration 6520, loss = 0.10366625\n",
      "Iteration 6521, loss = 0.10366290\n",
      "Iteration 6522, loss = 0.10365955\n",
      "Iteration 6523, loss = 0.10365621\n",
      "Iteration 6524, loss = 0.10365286\n",
      "Iteration 6525, loss = 0.10364952\n",
      "Iteration 6526, loss = 0.10364618\n",
      "Iteration 6527, loss = 0.10364284\n",
      "Iteration 6528, loss = 0.10363950\n",
      "Iteration 6529, loss = 0.10363616\n",
      "Iteration 6530, loss = 0.10363282\n",
      "Iteration 6531, loss = 0.10362949\n",
      "Iteration 6532, loss = 0.10362615\n",
      "Iteration 6533, loss = 0.10362282\n",
      "Iteration 6534, loss = 0.10361949\n",
      "Iteration 6535, loss = 0.10361616\n",
      "Iteration 6536, loss = 0.10361283\n",
      "Iteration 6537, loss = 0.10360950\n",
      "Iteration 6538, loss = 0.10360617\n",
      "Iteration 6539, loss = 0.10360285\n",
      "Iteration 6540, loss = 0.10359952\n",
      "Iteration 6541, loss = 0.10359620\n",
      "Iteration 6542, loss = 0.10359288\n",
      "Iteration 6543, loss = 0.10358956\n",
      "Iteration 6544, loss = 0.10358624\n",
      "Iteration 6545, loss = 0.10358292\n",
      "Iteration 6546, loss = 0.10357960\n",
      "Iteration 6547, loss = 0.10357629\n",
      "Iteration 6548, loss = 0.10357297\n",
      "Iteration 6549, loss = 0.10356966\n",
      "Iteration 6550, loss = 0.10356634\n",
      "Iteration 6551, loss = 0.10356303\n",
      "Iteration 6552, loss = 0.10355972\n",
      "Iteration 6553, loss = 0.10355641\n",
      "Iteration 6554, loss = 0.10355311\n",
      "Iteration 6555, loss = 0.10354980\n",
      "Iteration 6556, loss = 0.10354649\n",
      "Iteration 6557, loss = 0.10354319\n",
      "Iteration 6558, loss = 0.10353989\n",
      "Iteration 6559, loss = 0.10353659\n",
      "Iteration 6560, loss = 0.10353329\n",
      "Iteration 6561, loss = 0.10352999\n",
      "Iteration 6562, loss = 0.10352669\n",
      "Iteration 6563, loss = 0.10352339\n",
      "Iteration 6564, loss = 0.10352009\n",
      "Iteration 6565, loss = 0.10351680\n",
      "Iteration 6566, loss = 0.10351351\n",
      "Iteration 6567, loss = 0.10351021\n",
      "Iteration 6568, loss = 0.10350692\n",
      "Iteration 6569, loss = 0.10350363\n",
      "Iteration 6570, loss = 0.10350034\n",
      "Iteration 6571, loss = 0.10349706\n",
      "Iteration 6572, loss = 0.10349377\n",
      "Iteration 6573, loss = 0.10349049\n",
      "Iteration 6574, loss = 0.10348720\n",
      "Iteration 6575, loss = 0.10348392\n",
      "Iteration 6576, loss = 0.10348064\n",
      "Iteration 6577, loss = 0.10347736\n",
      "Iteration 6578, loss = 0.10347408\n",
      "Iteration 6579, loss = 0.10347080\n",
      "Iteration 6580, loss = 0.10346752\n",
      "Iteration 6581, loss = 0.10346425\n",
      "Iteration 6582, loss = 0.10346097\n",
      "Iteration 6583, loss = 0.10345770\n",
      "Iteration 6584, loss = 0.10345443\n",
      "Iteration 6585, loss = 0.10345116\n",
      "Iteration 6586, loss = 0.10344789\n",
      "Iteration 6587, loss = 0.10344462\n",
      "Iteration 6588, loss = 0.10344135\n",
      "Iteration 6589, loss = 0.10343808\n",
      "Iteration 6590, loss = 0.10343482\n",
      "Iteration 6591, loss = 0.10343155\n",
      "Iteration 6592, loss = 0.10342829\n",
      "Iteration 6593, loss = 0.10342503\n",
      "Iteration 6594, loss = 0.10342177\n",
      "Iteration 6595, loss = 0.10341851\n",
      "Iteration 6596, loss = 0.10341525\n",
      "Iteration 6597, loss = 0.10341200\n",
      "Iteration 6598, loss = 0.10340874\n",
      "Iteration 6599, loss = 0.10340549\n",
      "Iteration 6600, loss = 0.10340223\n",
      "Iteration 6601, loss = 0.10339898\n",
      "Iteration 6602, loss = 0.10339573\n",
      "Iteration 6603, loss = 0.10339248\n",
      "Iteration 6604, loss = 0.10338923\n",
      "Iteration 6605, loss = 0.10338598\n",
      "Iteration 6606, loss = 0.10338274\n",
      "Iteration 6607, loss = 0.10337949\n",
      "Iteration 6608, loss = 0.10337625\n",
      "Iteration 6609, loss = 0.10337300\n",
      "Iteration 6610, loss = 0.10336976\n",
      "Iteration 6611, loss = 0.10336652\n",
      "Iteration 6612, loss = 0.10336328\n",
      "Iteration 6613, loss = 0.10336004\n",
      "Iteration 6614, loss = 0.10335681\n",
      "Iteration 6615, loss = 0.10335357\n",
      "Iteration 6616, loss = 0.10335034\n",
      "Iteration 6617, loss = 0.10334710\n",
      "Iteration 6618, loss = 0.10334387\n",
      "Iteration 6619, loss = 0.10334064\n",
      "Iteration 6620, loss = 0.10333741\n",
      "Iteration 6621, loss = 0.10333418\n",
      "Iteration 6622, loss = 0.10333095\n",
      "Iteration 6623, loss = 0.10332772\n",
      "Iteration 6624, loss = 0.10332450\n",
      "Iteration 6625, loss = 0.10332127\n",
      "Iteration 6626, loss = 0.10331805\n",
      "Iteration 6627, loss = 0.10331483\n",
      "Iteration 6628, loss = 0.10331161\n",
      "Iteration 6629, loss = 0.10330839\n",
      "Iteration 6630, loss = 0.10330517\n",
      "Iteration 6631, loss = 0.10330195\n",
      "Iteration 6632, loss = 0.10329874\n",
      "Iteration 6633, loss = 0.10329552\n",
      "Iteration 6634, loss = 0.10329231\n",
      "Iteration 6635, loss = 0.10328909\n",
      "Iteration 6636, loss = 0.10328588\n",
      "Iteration 6637, loss = 0.10328267\n",
      "Iteration 6638, loss = 0.10327946\n",
      "Iteration 6639, loss = 0.10327625\n",
      "Iteration 6640, loss = 0.10327305\n",
      "Iteration 6641, loss = 0.10326984\n",
      "Iteration 6642, loss = 0.10326663\n",
      "Iteration 6643, loss = 0.10326343\n",
      "Iteration 6644, loss = 0.10326023\n",
      "Iteration 6645, loss = 0.10325703\n",
      "Iteration 6646, loss = 0.10325383\n",
      "Iteration 6647, loss = 0.10325063\n",
      "Iteration 6648, loss = 0.10324743\n",
      "Iteration 6649, loss = 0.10324423\n",
      "Iteration 6650, loss = 0.10324104\n",
      "Iteration 6651, loss = 0.10323784\n",
      "Iteration 6652, loss = 0.10323465\n",
      "Iteration 6653, loss = 0.10323146\n",
      "Iteration 6654, loss = 0.10322826\n",
      "Iteration 6655, loss = 0.10322507\n",
      "Iteration 6656, loss = 0.10322189\n",
      "Iteration 6657, loss = 0.10321870\n",
      "Iteration 6658, loss = 0.10321551\n",
      "Iteration 6659, loss = 0.10321233\n",
      "Iteration 6660, loss = 0.10320914\n",
      "Iteration 6661, loss = 0.10320596\n",
      "Iteration 6662, loss = 0.10320278\n",
      "Iteration 6663, loss = 0.10319959\n",
      "Iteration 6664, loss = 0.10319641\n",
      "Iteration 6665, loss = 0.10319324\n",
      "Iteration 6666, loss = 0.10319006\n",
      "Iteration 6667, loss = 0.10318688\n",
      "Iteration 6668, loss = 0.10318371\n",
      "Iteration 6669, loss = 0.10318053\n",
      "Iteration 6670, loss = 0.10317736\n",
      "Iteration 6671, loss = 0.10317419\n",
      "Iteration 6672, loss = 0.10317101\n",
      "Iteration 6673, loss = 0.10316784\n",
      "Iteration 6674, loss = 0.10316468\n",
      "Iteration 6675, loss = 0.10316151\n",
      "Iteration 6676, loss = 0.10315834\n",
      "Iteration 6677, loss = 0.10315518\n",
      "Iteration 6678, loss = 0.10315201\n",
      "Iteration 6679, loss = 0.10314885\n",
      "Iteration 6680, loss = 0.10314569\n",
      "Iteration 6681, loss = 0.10314253\n",
      "Iteration 6682, loss = 0.10313937\n",
      "Iteration 6683, loss = 0.10313621\n",
      "Iteration 6684, loss = 0.10313305\n",
      "Iteration 6685, loss = 0.10312989\n",
      "Iteration 6686, loss = 0.10312674\n",
      "Iteration 6687, loss = 0.10312358\n",
      "Iteration 6688, loss = 0.10312043\n",
      "Iteration 6689, loss = 0.10311728\n",
      "Iteration 6690, loss = 0.10311413\n",
      "Iteration 6691, loss = 0.10311098\n",
      "Iteration 6692, loss = 0.10310783\n",
      "Iteration 6693, loss = 0.10310468\n",
      "Iteration 6694, loss = 0.10310154\n",
      "Iteration 6695, loss = 0.10309839\n",
      "Iteration 6696, loss = 0.10309525\n",
      "Iteration 6697, loss = 0.10309210\n",
      "Iteration 6698, loss = 0.10308896\n",
      "Iteration 6699, loss = 0.10308582\n",
      "Iteration 6700, loss = 0.10308268\n",
      "Iteration 6701, loss = 0.10307954\n",
      "Iteration 6702, loss = 0.10307640\n",
      "Iteration 6703, loss = 0.10307327\n",
      "Iteration 6704, loss = 0.10307013\n",
      "Iteration 6705, loss = 0.10306700\n",
      "Iteration 6706, loss = 0.10306387\n",
      "Iteration 6707, loss = 0.10306073\n",
      "Iteration 6708, loss = 0.10305760\n",
      "Iteration 6709, loss = 0.10305447\n",
      "Iteration 6710, loss = 0.10305134\n",
      "Iteration 6711, loss = 0.10304822\n",
      "Iteration 6712, loss = 0.10304509\n",
      "Iteration 6713, loss = 0.10304196\n",
      "Iteration 6714, loss = 0.10303884\n",
      "Iteration 6715, loss = 0.10303572\n",
      "Iteration 6716, loss = 0.10303259\n",
      "Iteration 6717, loss = 0.10302947\n",
      "Iteration 6718, loss = 0.10302635\n",
      "Iteration 6719, loss = 0.10302324\n",
      "Iteration 6720, loss = 0.10302012\n",
      "Iteration 6721, loss = 0.10301700\n",
      "Iteration 6722, loss = 0.10301389\n",
      "Iteration 6723, loss = 0.10301077\n",
      "Iteration 6724, loss = 0.10300766\n",
      "Iteration 6725, loss = 0.10300455\n",
      "Iteration 6726, loss = 0.10300143\n",
      "Iteration 6727, loss = 0.10299832\n",
      "Iteration 6728, loss = 0.10299521\n",
      "Iteration 6729, loss = 0.10299211\n",
      "Iteration 6730, loss = 0.10298900\n",
      "Iteration 6731, loss = 0.10298589\n",
      "Iteration 6732, loss = 0.10298279\n",
      "Iteration 6733, loss = 0.10297969\n",
      "Iteration 6734, loss = 0.10297658\n",
      "Iteration 6735, loss = 0.10297348\n",
      "Iteration 6736, loss = 0.10297038\n",
      "Iteration 6737, loss = 0.10296728\n",
      "Iteration 6738, loss = 0.10296418\n",
      "Iteration 6739, loss = 0.10296109\n",
      "Iteration 6740, loss = 0.10295799\n",
      "Iteration 6741, loss = 0.10295490\n",
      "Iteration 6742, loss = 0.10295180\n",
      "Iteration 6743, loss = 0.10294871\n",
      "Iteration 6744, loss = 0.10294562\n",
      "Iteration 6745, loss = 0.10294253\n",
      "Iteration 6746, loss = 0.10293944\n",
      "Iteration 6747, loss = 0.10293635\n",
      "Iteration 6748, loss = 0.10293326\n",
      "Iteration 6749, loss = 0.10293018\n",
      "Iteration 6750, loss = 0.10292709\n",
      "Iteration 6751, loss = 0.10292401\n",
      "Iteration 6752, loss = 0.10292092\n",
      "Iteration 6753, loss = 0.10291784\n",
      "Iteration 6754, loss = 0.10291476\n",
      "Iteration 6755, loss = 0.10291168\n",
      "Iteration 6756, loss = 0.10290860\n",
      "Iteration 6757, loss = 0.10290553\n",
      "Iteration 6758, loss = 0.10290245\n",
      "Iteration 6759, loss = 0.10289937\n",
      "Iteration 6760, loss = 0.10289630\n",
      "Iteration 6761, loss = 0.10289323\n",
      "Iteration 6762, loss = 0.10289015\n",
      "Iteration 6763, loss = 0.10288708\n",
      "Iteration 6764, loss = 0.10288401\n",
      "Iteration 6765, loss = 0.10288094\n",
      "Iteration 6766, loss = 0.10287788\n",
      "Iteration 6767, loss = 0.10287481\n",
      "Iteration 6768, loss = 0.10287174\n",
      "Iteration 6769, loss = 0.10286868\n",
      "Iteration 6770, loss = 0.10286561\n",
      "Iteration 6771, loss = 0.10286255\n",
      "Iteration 6772, loss = 0.10285949\n",
      "Iteration 6773, loss = 0.10285643\n",
      "Iteration 6774, loss = 0.10285337\n",
      "Iteration 6775, loss = 0.10285031\n",
      "Iteration 6776, loss = 0.10284725\n",
      "Iteration 6777, loss = 0.10284420\n",
      "Iteration 6778, loss = 0.10284114\n",
      "Iteration 6779, loss = 0.10283809\n",
      "Iteration 6780, loss = 0.10283504\n",
      "Iteration 6781, loss = 0.10283198\n",
      "Iteration 6782, loss = 0.10282893\n",
      "Iteration 6783, loss = 0.10282588\n",
      "Iteration 6784, loss = 0.10282283\n",
      "Iteration 6785, loss = 0.10281979\n",
      "Iteration 6786, loss = 0.10281674\n",
      "Iteration 6787, loss = 0.10281369\n",
      "Iteration 6788, loss = 0.10281065\n",
      "Iteration 6789, loss = 0.10280761\n",
      "Iteration 6790, loss = 0.10280456\n",
      "Iteration 6791, loss = 0.10280152\n",
      "Iteration 6792, loss = 0.10279848\n",
      "Iteration 6793, loss = 0.10279544\n",
      "Iteration 6794, loss = 0.10279240\n",
      "Iteration 6795, loss = 0.10278937\n",
      "Iteration 6796, loss = 0.10278633\n",
      "Iteration 6797, loss = 0.10278330\n",
      "Iteration 6798, loss = 0.10278026\n",
      "Iteration 6799, loss = 0.10277723\n",
      "Iteration 6800, loss = 0.10277420\n",
      "Iteration 6801, loss = 0.10277117\n",
      "Iteration 6802, loss = 0.10276814\n",
      "Iteration 6803, loss = 0.10276511\n",
      "Iteration 6804, loss = 0.10276208\n",
      "Iteration 6805, loss = 0.10275905\n",
      "Iteration 6806, loss = 0.10275603\n",
      "Iteration 6807, loss = 0.10275300\n",
      "Iteration 6808, loss = 0.10274998\n",
      "Iteration 6809, loss = 0.10274696\n",
      "Iteration 6810, loss = 0.10274393\n",
      "Iteration 6811, loss = 0.10274091\n",
      "Iteration 6812, loss = 0.10273789\n",
      "Iteration 6813, loss = 0.10273488\n",
      "Iteration 6814, loss = 0.10273186\n",
      "Iteration 6815, loss = 0.10272884\n",
      "Iteration 6816, loss = 0.10272583\n",
      "Iteration 6817, loss = 0.10272281\n",
      "Iteration 6818, loss = 0.10271980\n",
      "Iteration 6819, loss = 0.10271679\n",
      "Iteration 6820, loss = 0.10271378\n",
      "Iteration 6821, loss = 0.10271077\n",
      "Iteration 6822, loss = 0.10270776\n",
      "Iteration 6823, loss = 0.10270475\n",
      "Iteration 6824, loss = 0.10270174\n",
      "Iteration 6825, loss = 0.10269874\n",
      "Iteration 6826, loss = 0.10269573\n",
      "Iteration 6827, loss = 0.10269273\n",
      "Iteration 6828, loss = 0.10268973\n",
      "Iteration 6829, loss = 0.10268672\n",
      "Iteration 6830, loss = 0.10268372\n",
      "Iteration 6831, loss = 0.10268072\n",
      "Iteration 6832, loss = 0.10267773\n",
      "Iteration 6833, loss = 0.10267473\n",
      "Iteration 6834, loss = 0.10267173\n",
      "Iteration 6835, loss = 0.10266874\n",
      "Iteration 6836, loss = 0.10266574\n",
      "Iteration 6837, loss = 0.10266275\n",
      "Iteration 6838, loss = 0.10265976\n",
      "Iteration 6839, loss = 0.10265676\n",
      "Iteration 6840, loss = 0.10265377\n",
      "Iteration 6841, loss = 0.10265078\n",
      "Iteration 6842, loss = 0.10264780\n",
      "Iteration 6843, loss = 0.10264481\n",
      "Iteration 6844, loss = 0.10264182\n",
      "Iteration 6845, loss = 0.10263884\n",
      "Iteration 6846, loss = 0.10263585\n",
      "Iteration 6847, loss = 0.10263287\n",
      "Iteration 6848, loss = 0.10262989\n",
      "Iteration 6849, loss = 0.10262691\n",
      "Iteration 6850, loss = 0.10262393\n",
      "Iteration 6851, loss = 0.10262095\n",
      "Iteration 6852, loss = 0.10261797\n",
      "Iteration 6853, loss = 0.10261499\n",
      "Iteration 6854, loss = 0.10261202\n",
      "Iteration 6855, loss = 0.10260904\n",
      "Iteration 6856, loss = 0.10260607\n",
      "Iteration 6857, loss = 0.10260309\n",
      "Iteration 6858, loss = 0.10260012\n",
      "Iteration 6859, loss = 0.10259715\n",
      "Iteration 6860, loss = 0.10259418\n",
      "Iteration 6861, loss = 0.10259121\n",
      "Iteration 6862, loss = 0.10258824\n",
      "Iteration 6863, loss = 0.10258528\n",
      "Iteration 6864, loss = 0.10258231\n",
      "Iteration 6865, loss = 0.10257935\n",
      "Iteration 6866, loss = 0.10257638\n",
      "Iteration 6867, loss = 0.10257342\n",
      "Iteration 6868, loss = 0.10257046\n",
      "Iteration 6869, loss = 0.10256750\n",
      "Iteration 6870, loss = 0.10256454\n",
      "Iteration 6871, loss = 0.10256158\n",
      "Iteration 6872, loss = 0.10255862\n",
      "Iteration 6873, loss = 0.10255566\n",
      "Iteration 6874, loss = 0.10255271\n",
      "Iteration 6875, loss = 0.10254975\n",
      "Iteration 6876, loss = 0.10254680\n",
      "Iteration 6877, loss = 0.10254385\n",
      "Iteration 6878, loss = 0.10254090\n",
      "Iteration 6879, loss = 0.10253795\n",
      "Iteration 6880, loss = 0.10253500\n",
      "Iteration 6881, loss = 0.10253205\n",
      "Iteration 6882, loss = 0.10252910\n",
      "Iteration 6883, loss = 0.10252615\n",
      "Iteration 6884, loss = 0.10252321\n",
      "Iteration 6885, loss = 0.10252026\n",
      "Iteration 6886, loss = 0.10251732\n",
      "Iteration 6887, loss = 0.10251438\n",
      "Iteration 6888, loss = 0.10251143\n",
      "Iteration 6889, loss = 0.10250849\n",
      "Iteration 6890, loss = 0.10250555\n",
      "Iteration 6891, loss = 0.10250262\n",
      "Iteration 6892, loss = 0.10249968\n",
      "Iteration 6893, loss = 0.10249674\n",
      "Iteration 6894, loss = 0.10249381\n",
      "Iteration 6895, loss = 0.10249087\n",
      "Iteration 6896, loss = 0.10248794\n",
      "Iteration 6897, loss = 0.10248501\n",
      "Iteration 6898, loss = 0.10248207\n",
      "Iteration 6899, loss = 0.10247914\n",
      "Iteration 6900, loss = 0.10247621\n",
      "Iteration 6901, loss = 0.10247329\n",
      "Iteration 6902, loss = 0.10247036\n",
      "Iteration 6903, loss = 0.10246743\n",
      "Iteration 6904, loss = 0.10246451\n",
      "Iteration 6905, loss = 0.10246158\n",
      "Iteration 6906, loss = 0.10245866\n",
      "Iteration 6907, loss = 0.10245573\n",
      "Iteration 6908, loss = 0.10245281\n",
      "Iteration 6909, loss = 0.10244989\n",
      "Iteration 6910, loss = 0.10244697\n",
      "Iteration 6911, loss = 0.10244405\n",
      "Iteration 6912, loss = 0.10244114\n",
      "Iteration 6913, loss = 0.10243822\n",
      "Iteration 6914, loss = 0.10243530\n",
      "Iteration 6915, loss = 0.10243239\n",
      "Iteration 6916, loss = 0.10242948\n",
      "Iteration 6917, loss = 0.10242656\n",
      "Iteration 6918, loss = 0.10242365\n",
      "Iteration 6919, loss = 0.10242074\n",
      "Iteration 6920, loss = 0.10241783\n",
      "Iteration 6921, loss = 0.10241492\n",
      "Iteration 6922, loss = 0.10241201\n",
      "Iteration 6923, loss = 0.10240911\n",
      "Iteration 6924, loss = 0.10240620\n",
      "Iteration 6925, loss = 0.10240330\n",
      "Iteration 6926, loss = 0.10240039\n",
      "Iteration 6927, loss = 0.10239749\n",
      "Iteration 6928, loss = 0.10239459\n",
      "Iteration 6929, loss = 0.10239169\n",
      "Iteration 6930, loss = 0.10238879\n",
      "Iteration 6931, loss = 0.10238589\n",
      "Iteration 6932, loss = 0.10238299\n",
      "Iteration 6933, loss = 0.10238009\n",
      "Iteration 6934, loss = 0.10237720\n",
      "Iteration 6935, loss = 0.10237430\n",
      "Iteration 6936, loss = 0.10237141\n",
      "Iteration 6937, loss = 0.10236851\n",
      "Iteration 6938, loss = 0.10236562\n",
      "Iteration 6939, loss = 0.10236273\n",
      "Iteration 6940, loss = 0.10235984\n",
      "Iteration 6941, loss = 0.10235695\n",
      "Iteration 6942, loss = 0.10235406\n",
      "Iteration 6943, loss = 0.10235118\n",
      "Iteration 6944, loss = 0.10234829\n",
      "Iteration 6945, loss = 0.10234541\n",
      "Iteration 6946, loss = 0.10234252\n",
      "Iteration 6947, loss = 0.10233964\n",
      "Iteration 6948, loss = 0.10233676\n",
      "Iteration 6949, loss = 0.10233387\n",
      "Iteration 6950, loss = 0.10233099\n",
      "Iteration 6951, loss = 0.10232811\n",
      "Iteration 6952, loss = 0.10232524\n",
      "Iteration 6953, loss = 0.10232236\n",
      "Iteration 6954, loss = 0.10231948\n",
      "Iteration 6955, loss = 0.10231661\n",
      "Iteration 6956, loss = 0.10231373\n",
      "Iteration 6957, loss = 0.10231086\n",
      "Iteration 6958, loss = 0.10230799\n",
      "Iteration 6959, loss = 0.10230511\n",
      "Iteration 6960, loss = 0.10230224\n",
      "Iteration 6961, loss = 0.10229937\n",
      "Iteration 6962, loss = 0.10229651\n",
      "Iteration 6963, loss = 0.10229364\n",
      "Iteration 6964, loss = 0.10229077\n",
      "Iteration 6965, loss = 0.10228790\n",
      "Iteration 6966, loss = 0.10228504\n",
      "Iteration 6967, loss = 0.10228218\n",
      "Iteration 6968, loss = 0.10227931\n",
      "Iteration 6969, loss = 0.10227645\n",
      "Iteration 6970, loss = 0.10227359\n",
      "Iteration 6971, loss = 0.10227073\n",
      "Iteration 6972, loss = 0.10226787\n",
      "Iteration 6973, loss = 0.10226501\n",
      "Iteration 6974, loss = 0.10226216\n",
      "Iteration 6975, loss = 0.10225930\n",
      "Iteration 6976, loss = 0.10225644\n",
      "Iteration 6977, loss = 0.10225359\n",
      "Iteration 6978, loss = 0.10225074\n",
      "Iteration 6979, loss = 0.10224788\n",
      "Iteration 6980, loss = 0.10224503\n",
      "Iteration 6981, loss = 0.10224218\n",
      "Iteration 6982, loss = 0.10223933\n",
      "Iteration 6983, loss = 0.10223648\n",
      "Iteration 6984, loss = 0.10223364\n",
      "Iteration 6985, loss = 0.10223079\n",
      "Iteration 6986, loss = 0.10222794\n",
      "Iteration 6987, loss = 0.10222510\n",
      "Iteration 6988, loss = 0.10222226\n",
      "Iteration 6989, loss = 0.10221941\n",
      "Iteration 6990, loss = 0.10221657\n",
      "Iteration 6991, loss = 0.10221373\n",
      "Iteration 6992, loss = 0.10221089\n",
      "Iteration 6993, loss = 0.10220805\n",
      "Iteration 6994, loss = 0.10220521\n",
      "Iteration 6995, loss = 0.10220238\n",
      "Iteration 6996, loss = 0.10219954\n",
      "Iteration 6997, loss = 0.10219670\n",
      "Iteration 6998, loss = 0.10219387\n",
      "Iteration 6999, loss = 0.10219104\n",
      "Iteration 7000, loss = 0.10218820\n",
      "Iteration 7001, loss = 0.10218537\n",
      "Iteration 7002, loss = 0.10218254\n",
      "Iteration 7003, loss = 0.10217971\n",
      "Iteration 7004, loss = 0.10217688\n",
      "Iteration 7005, loss = 0.10217406\n",
      "Iteration 7006, loss = 0.10217123\n",
      "Iteration 7007, loss = 0.10216840\n",
      "Iteration 7008, loss = 0.10216558\n",
      "Iteration 7009, loss = 0.10216276\n",
      "Iteration 7010, loss = 0.10215993\n",
      "Iteration 7011, loss = 0.10215711\n",
      "Iteration 7012, loss = 0.10215429\n",
      "Iteration 7013, loss = 0.10215147\n",
      "Iteration 7014, loss = 0.10214865\n",
      "Iteration 7015, loss = 0.10214583\n",
      "Iteration 7016, loss = 0.10214301\n",
      "Iteration 7017, loss = 0.10214020\n",
      "Iteration 7018, loss = 0.10213738\n",
      "Iteration 7019, loss = 0.10213457\n",
      "Iteration 7020, loss = 0.10213176\n",
      "Iteration 7021, loss = 0.10212894\n",
      "Iteration 7022, loss = 0.10212613\n",
      "Iteration 7023, loss = 0.10212332\n",
      "Iteration 7024, loss = 0.10212051\n",
      "Iteration 7025, loss = 0.10211770\n",
      "Iteration 7026, loss = 0.10211489\n",
      "Iteration 7027, loss = 0.10211209\n",
      "Iteration 7028, loss = 0.10210928\n",
      "Iteration 7029, loss = 0.10210648\n",
      "Iteration 7030, loss = 0.10210367\n",
      "Iteration 7031, loss = 0.10210087\n",
      "Iteration 7032, loss = 0.10209807\n",
      "Iteration 7033, loss = 0.10209526\n",
      "Iteration 7034, loss = 0.10209246\n",
      "Iteration 7035, loss = 0.10208966\n",
      "Iteration 7036, loss = 0.10208687\n",
      "Iteration 7037, loss = 0.10208407\n",
      "Iteration 7038, loss = 0.10208127\n",
      "Iteration 7039, loss = 0.10207848\n",
      "Iteration 7040, loss = 0.10207568\n",
      "Iteration 7041, loss = 0.10207289\n",
      "Iteration 7042, loss = 0.10207009\n",
      "Iteration 7043, loss = 0.10206730\n",
      "Iteration 7044, loss = 0.10206451\n",
      "Iteration 7045, loss = 0.10206172\n",
      "Iteration 7046, loss = 0.10205893\n",
      "Iteration 7047, loss = 0.10205614\n",
      "Iteration 7048, loss = 0.10205336\n",
      "Iteration 7049, loss = 0.10205057\n",
      "Iteration 7050, loss = 0.10204778\n",
      "Iteration 7051, loss = 0.10204500\n",
      "Iteration 7052, loss = 0.10204222\n",
      "Iteration 7053, loss = 0.10203943\n",
      "Iteration 7054, loss = 0.10203665\n",
      "Iteration 7055, loss = 0.10203387\n",
      "Iteration 7056, loss = 0.10203109\n",
      "Iteration 7057, loss = 0.10202831\n",
      "Iteration 7058, loss = 0.10202553\n",
      "Iteration 7059, loss = 0.10202275\n",
      "Iteration 7060, loss = 0.10201998\n",
      "Iteration 7061, loss = 0.10201720\n",
      "Iteration 7062, loss = 0.10201443\n",
      "Iteration 7063, loss = 0.10201166\n",
      "Iteration 7064, loss = 0.10200888\n",
      "Iteration 7065, loss = 0.10200611\n",
      "Iteration 7066, loss = 0.10200334\n",
      "Iteration 7067, loss = 0.10200057\n",
      "Iteration 7068, loss = 0.10199780\n",
      "Iteration 7069, loss = 0.10199503\n",
      "Iteration 7070, loss = 0.10199227\n",
      "Iteration 7071, loss = 0.10198950\n",
      "Iteration 7072, loss = 0.10198673\n",
      "Iteration 7073, loss = 0.10198397\n",
      "Iteration 7074, loss = 0.10198121\n",
      "Iteration 7075, loss = 0.10197844\n",
      "Iteration 7076, loss = 0.10197568\n",
      "Iteration 7077, loss = 0.10197292\n",
      "Iteration 7078, loss = 0.10197016\n",
      "Iteration 7079, loss = 0.10196740\n",
      "Iteration 7080, loss = 0.10196464\n",
      "Iteration 7081, loss = 0.10196189\n",
      "Iteration 7082, loss = 0.10195913\n",
      "Iteration 7083, loss = 0.10195638\n",
      "Iteration 7084, loss = 0.10195362\n",
      "Iteration 7085, loss = 0.10195087\n",
      "Iteration 7086, loss = 0.10194811\n",
      "Iteration 7087, loss = 0.10194536\n",
      "Iteration 7088, loss = 0.10194261\n",
      "Iteration 7089, loss = 0.10193986\n",
      "Iteration 7090, loss = 0.10193711\n",
      "Iteration 7091, loss = 0.10193437\n",
      "Iteration 7092, loss = 0.10193162\n",
      "Iteration 7093, loss = 0.10192887\n",
      "Iteration 7094, loss = 0.10192613\n",
      "Iteration 7095, loss = 0.10192338\n",
      "Iteration 7096, loss = 0.10192064\n",
      "Iteration 7097, loss = 0.10191790\n",
      "Iteration 7098, loss = 0.10191515\n",
      "Iteration 7099, loss = 0.10191241\n",
      "Iteration 7100, loss = 0.10190967\n",
      "Iteration 7101, loss = 0.10190693\n",
      "Iteration 7102, loss = 0.10190420\n",
      "Iteration 7103, loss = 0.10190146\n",
      "Iteration 7104, loss = 0.10189872\n",
      "Iteration 7105, loss = 0.10189599\n",
      "Iteration 7106, loss = 0.10189325\n",
      "Iteration 7107, loss = 0.10189052\n",
      "Iteration 7108, loss = 0.10188779\n",
      "Iteration 7109, loss = 0.10188505\n",
      "Iteration 7110, loss = 0.10188232\n",
      "Iteration 7111, loss = 0.10187959\n",
      "Iteration 7112, loss = 0.10187686\n",
      "Iteration 7113, loss = 0.10187414\n",
      "Iteration 7114, loss = 0.10187141\n",
      "Iteration 7115, loss = 0.10186868\n",
      "Iteration 7116, loss = 0.10186596\n",
      "Iteration 7117, loss = 0.10186323\n",
      "Iteration 7118, loss = 0.10186051\n",
      "Iteration 7119, loss = 0.10185778\n",
      "Iteration 7120, loss = 0.10185506\n",
      "Iteration 7121, loss = 0.10185234\n",
      "Iteration 7122, loss = 0.10184962\n",
      "Iteration 7123, loss = 0.10184690\n",
      "Iteration 7124, loss = 0.10184418\n",
      "Iteration 7125, loss = 0.10184147\n",
      "Iteration 7126, loss = 0.10183875\n",
      "Iteration 7127, loss = 0.10183603\n",
      "Iteration 7128, loss = 0.10183332\n",
      "Iteration 7129, loss = 0.10183060\n",
      "Iteration 7130, loss = 0.10182789\n",
      "Iteration 7131, loss = 0.10182518\n",
      "Iteration 7132, loss = 0.10182247\n",
      "Iteration 7133, loss = 0.10181976\n",
      "Iteration 7134, loss = 0.10181705\n",
      "Iteration 7135, loss = 0.10181434\n",
      "Iteration 7136, loss = 0.10181163\n",
      "Iteration 7137, loss = 0.10180892\n",
      "Iteration 7138, loss = 0.10180622\n",
      "Iteration 7139, loss = 0.10180351\n",
      "Iteration 7140, loss = 0.10180081\n",
      "Iteration 7141, loss = 0.10179811\n",
      "Iteration 7142, loss = 0.10179540\n",
      "Iteration 7143, loss = 0.10179270\n",
      "Iteration 7144, loss = 0.10179000\n",
      "Iteration 7145, loss = 0.10178730\n",
      "Iteration 7146, loss = 0.10178460\n",
      "Iteration 7147, loss = 0.10178190\n",
      "Iteration 7148, loss = 0.10177921\n",
      "Iteration 7149, loss = 0.10177651\n",
      "Iteration 7150, loss = 0.10177382\n",
      "Iteration 7151, loss = 0.10177112\n",
      "Iteration 7152, loss = 0.10176843\n",
      "Iteration 7153, loss = 0.10176573\n",
      "Iteration 7154, loss = 0.10176304\n",
      "Iteration 7155, loss = 0.10176035\n",
      "Iteration 7156, loss = 0.10175766\n",
      "Iteration 7157, loss = 0.10175497\n",
      "Iteration 7158, loss = 0.10175228\n",
      "Iteration 7159, loss = 0.10174960\n",
      "Iteration 7160, loss = 0.10174691\n",
      "Iteration 7161, loss = 0.10174422\n",
      "Iteration 7162, loss = 0.10174154\n",
      "Iteration 7163, loss = 0.10173885\n",
      "Iteration 7164, loss = 0.10173617\n",
      "Iteration 7165, loss = 0.10173349\n",
      "Iteration 7166, loss = 0.10173081\n",
      "Iteration 7167, loss = 0.10172813\n",
      "Iteration 7168, loss = 0.10172545\n",
      "Iteration 7169, loss = 0.10172277\n",
      "Iteration 7170, loss = 0.10172009\n",
      "Iteration 7171, loss = 0.10171741\n",
      "Iteration 7172, loss = 0.10171474\n",
      "Iteration 7173, loss = 0.10171206\n",
      "Iteration 7174, loss = 0.10170939\n",
      "Iteration 7175, loss = 0.10170671\n",
      "Iteration 7176, loss = 0.10170404\n",
      "Iteration 7177, loss = 0.10170137\n",
      "Iteration 7178, loss = 0.10169870\n",
      "Iteration 7179, loss = 0.10169603\n",
      "Iteration 7180, loss = 0.10169336\n",
      "Iteration 7181, loss = 0.10169069\n",
      "Iteration 7182, loss = 0.10168802\n",
      "Iteration 7183, loss = 0.10168536\n",
      "Iteration 7184, loss = 0.10168269\n",
      "Iteration 7185, loss = 0.10168003\n",
      "Iteration 7186, loss = 0.10167736\n",
      "Iteration 7187, loss = 0.10167470\n",
      "Iteration 7188, loss = 0.10167204\n",
      "Iteration 7189, loss = 0.10166937\n",
      "Iteration 7190, loss = 0.10166671\n",
      "Iteration 7191, loss = 0.10166405\n",
      "Iteration 7192, loss = 0.10166140\n",
      "Iteration 7193, loss = 0.10165874\n",
      "Iteration 7194, loss = 0.10165608\n",
      "Iteration 7195, loss = 0.10165342\n",
      "Iteration 7196, loss = 0.10165077\n",
      "Iteration 7197, loss = 0.10164811\n",
      "Iteration 7198, loss = 0.10164546\n",
      "Iteration 7199, loss = 0.10164281\n",
      "Iteration 7200, loss = 0.10164016\n",
      "Iteration 7201, loss = 0.10163750\n",
      "Iteration 7202, loss = 0.10163485\n",
      "Iteration 7203, loss = 0.10163220\n",
      "Iteration 7204, loss = 0.10162956\n",
      "Iteration 7205, loss = 0.10162691\n",
      "Iteration 7206, loss = 0.10162426\n",
      "Iteration 7207, loss = 0.10162162\n",
      "Iteration 7208, loss = 0.10161897\n",
      "Iteration 7209, loss = 0.10161633\n",
      "Iteration 7210, loss = 0.10161368\n",
      "Iteration 7211, loss = 0.10161104\n",
      "Iteration 7212, loss = 0.10160840\n",
      "Iteration 7213, loss = 0.10160576\n",
      "Iteration 7214, loss = 0.10160312\n",
      "Iteration 7215, loss = 0.10160048\n",
      "Iteration 7216, loss = 0.10159784\n",
      "Iteration 7217, loss = 0.10159520\n",
      "Iteration 7218, loss = 0.10159257\n",
      "Iteration 7219, loss = 0.10158993\n",
      "Iteration 7220, loss = 0.10158729\n",
      "Iteration 7221, loss = 0.10158466\n",
      "Iteration 7222, loss = 0.10158203\n",
      "Iteration 7223, loss = 0.10157940\n",
      "Iteration 7224, loss = 0.10157676\n",
      "Iteration 7225, loss = 0.10157413\n",
      "Iteration 7226, loss = 0.10157150\n",
      "Iteration 7227, loss = 0.10156887\n",
      "Iteration 7228, loss = 0.10156625\n",
      "Iteration 7229, loss = 0.10156362\n",
      "Iteration 7230, loss = 0.10156099\n",
      "Iteration 7231, loss = 0.10155837\n",
      "Iteration 7232, loss = 0.10155574\n",
      "Iteration 7233, loss = 0.10155312\n",
      "Iteration 7234, loss = 0.10155049\n",
      "Iteration 7235, loss = 0.10154787\n",
      "Iteration 7236, loss = 0.10154525\n",
      "Iteration 7237, loss = 0.10154263\n",
      "Iteration 7238, loss = 0.10154001\n",
      "Iteration 7239, loss = 0.10153739\n",
      "Iteration 7240, loss = 0.10153477\n",
      "Iteration 7241, loss = 0.10153216\n",
      "Iteration 7242, loss = 0.10152954\n",
      "Iteration 7243, loss = 0.10152692\n",
      "Iteration 7244, loss = 0.10152431\n",
      "Iteration 7245, loss = 0.10152170\n",
      "Iteration 7246, loss = 0.10151908\n",
      "Iteration 7247, loss = 0.10151647\n",
      "Iteration 7248, loss = 0.10151386\n",
      "Iteration 7249, loss = 0.10151125\n",
      "Iteration 7250, loss = 0.10150864\n",
      "Iteration 7251, loss = 0.10150603\n",
      "Iteration 7252, loss = 0.10150342\n",
      "Iteration 7253, loss = 0.10150082\n",
      "Iteration 7254, loss = 0.10149821\n",
      "Iteration 7255, loss = 0.10149560\n",
      "Iteration 7256, loss = 0.10149300\n",
      "Iteration 7257, loss = 0.10149040\n",
      "Iteration 7258, loss = 0.10148779\n",
      "Iteration 7259, loss = 0.10148519\n",
      "Iteration 7260, loss = 0.10148259\n",
      "Iteration 7261, loss = 0.10147999\n",
      "Iteration 7262, loss = 0.10147739\n",
      "Iteration 7263, loss = 0.10147479\n",
      "Iteration 7264, loss = 0.10147219\n",
      "Iteration 7265, loss = 0.10146960\n",
      "Iteration 7266, loss = 0.10146700\n",
      "Iteration 7267, loss = 0.10146440\n",
      "Iteration 7268, loss = 0.10146181\n",
      "Iteration 7269, loss = 0.10145921\n",
      "Iteration 7270, loss = 0.10145662\n",
      "Iteration 7271, loss = 0.10145403\n",
      "Iteration 7272, loss = 0.10145144\n",
      "Iteration 7273, loss = 0.10144885\n",
      "Iteration 7274, loss = 0.10144626\n",
      "Iteration 7275, loss = 0.10144367\n",
      "Iteration 7276, loss = 0.10144108\n",
      "Iteration 7277, loss = 0.10143849\n",
      "Iteration 7278, loss = 0.10143591\n",
      "Iteration 7279, loss = 0.10143332\n",
      "Iteration 7280, loss = 0.10143074\n",
      "Iteration 7281, loss = 0.10142815\n",
      "Iteration 7282, loss = 0.10142557\n",
      "Iteration 7283, loss = 0.10142299\n",
      "Iteration 7284, loss = 0.10142041\n",
      "Iteration 7285, loss = 0.10141783\n",
      "Iteration 7286, loss = 0.10141525\n",
      "Iteration 7287, loss = 0.10141267\n",
      "Iteration 7288, loss = 0.10141009\n",
      "Iteration 7289, loss = 0.10140751\n",
      "Iteration 7290, loss = 0.10140494\n",
      "Iteration 7291, loss = 0.10140236\n",
      "Iteration 7292, loss = 0.10139979\n",
      "Iteration 7293, loss = 0.10139721\n",
      "Iteration 7294, loss = 0.10139464\n",
      "Iteration 7295, loss = 0.10139207\n",
      "Iteration 7296, loss = 0.10138949\n",
      "Iteration 7297, loss = 0.10138692\n",
      "Iteration 7298, loss = 0.10138435\n",
      "Iteration 7299, loss = 0.10138178\n",
      "Iteration 7300, loss = 0.10137922\n",
      "Iteration 7301, loss = 0.10137665\n",
      "Iteration 7302, loss = 0.10137408\n",
      "Iteration 7303, loss = 0.10137152\n",
      "Iteration 7304, loss = 0.10136895\n",
      "Iteration 7305, loss = 0.10136639\n",
      "Iteration 7306, loss = 0.10136382\n",
      "Iteration 7307, loss = 0.10136126\n",
      "Iteration 7308, loss = 0.10135870\n",
      "Iteration 7309, loss = 0.10135614\n",
      "Iteration 7310, loss = 0.10135358\n",
      "Iteration 7311, loss = 0.10135102\n",
      "Iteration 7312, loss = 0.10134846\n",
      "Iteration 7313, loss = 0.10134590\n",
      "Iteration 7314, loss = 0.10134335\n",
      "Iteration 7315, loss = 0.10134079\n",
      "Iteration 7316, loss = 0.10133823\n",
      "Iteration 7317, loss = 0.10133568\n",
      "Iteration 7318, loss = 0.10133313\n",
      "Iteration 7319, loss = 0.10133057\n",
      "Iteration 7320, loss = 0.10132802\n",
      "Iteration 7321, loss = 0.10132547\n",
      "Iteration 7322, loss = 0.10132292\n",
      "Iteration 7323, loss = 0.10132037\n",
      "Iteration 7324, loss = 0.10131782\n",
      "Iteration 7325, loss = 0.10131527\n",
      "Iteration 7326, loss = 0.10131273\n",
      "Iteration 7327, loss = 0.10131018\n",
      "Iteration 7328, loss = 0.10130763\n",
      "Iteration 7329, loss = 0.10130509\n",
      "Iteration 7330, loss = 0.10130254\n",
      "Iteration 7331, loss = 0.10130000\n",
      "Iteration 7332, loss = 0.10129746\n",
      "Iteration 7333, loss = 0.10129492\n",
      "Iteration 7334, loss = 0.10129238\n",
      "Iteration 7335, loss = 0.10128984\n",
      "Iteration 7336, loss = 0.10128730\n",
      "Iteration 7337, loss = 0.10128476\n",
      "Iteration 7338, loss = 0.10128222\n",
      "Iteration 7339, loss = 0.10127969\n",
      "Iteration 7340, loss = 0.10127715\n",
      "Iteration 7341, loss = 0.10127461\n",
      "Iteration 7342, loss = 0.10127208\n",
      "Iteration 7343, loss = 0.10126955\n",
      "Iteration 7344, loss = 0.10126701\n",
      "Iteration 7345, loss = 0.10126448\n",
      "Iteration 7346, loss = 0.10126195\n",
      "Iteration 7347, loss = 0.10125942\n",
      "Iteration 7348, loss = 0.10125689\n",
      "Iteration 7349, loss = 0.10125436\n",
      "Iteration 7350, loss = 0.10125183\n",
      "Iteration 7351, loss = 0.10124931\n",
      "Iteration 7352, loss = 0.10124678\n",
      "Iteration 7353, loss = 0.10124425\n",
      "Iteration 7354, loss = 0.10124173\n",
      "Iteration 7355, loss = 0.10123921\n",
      "Iteration 7356, loss = 0.10123668\n",
      "Iteration 7357, loss = 0.10123416\n",
      "Iteration 7358, loss = 0.10123164\n",
      "Iteration 7359, loss = 0.10122912\n",
      "Iteration 7360, loss = 0.10122660\n",
      "Iteration 7361, loss = 0.10122408\n",
      "Iteration 7362, loss = 0.10122156\n",
      "Iteration 7363, loss = 0.10121904\n",
      "Iteration 7364, loss = 0.10121653\n",
      "Iteration 7365, loss = 0.10121401\n",
      "Iteration 7366, loss = 0.10121149\n",
      "Iteration 7367, loss = 0.10120898\n",
      "Iteration 7368, loss = 0.10120647\n",
      "Iteration 7369, loss = 0.10120395\n",
      "Iteration 7370, loss = 0.10120144\n",
      "Iteration 7371, loss = 0.10119893\n",
      "Iteration 7372, loss = 0.10119642\n",
      "Iteration 7373, loss = 0.10119391\n",
      "Iteration 7374, loss = 0.10119140\n",
      "Iteration 7375, loss = 0.10118889\n",
      "Iteration 7376, loss = 0.10118639\n",
      "Iteration 7377, loss = 0.10118388\n",
      "Iteration 7378, loss = 0.10118137\n",
      "Iteration 7379, loss = 0.10117887\n",
      "Iteration 7380, loss = 0.10117636\n",
      "Iteration 7381, loss = 0.10117386\n",
      "Iteration 7382, loss = 0.10117136\n",
      "Iteration 7383, loss = 0.10116886\n",
      "Iteration 7384, loss = 0.10116635\n",
      "Iteration 7385, loss = 0.10116385\n",
      "Iteration 7386, loss = 0.10116136\n",
      "Iteration 7387, loss = 0.10115886\n",
      "Iteration 7388, loss = 0.10115636\n",
      "Iteration 7389, loss = 0.10115386\n",
      "Iteration 7390, loss = 0.10115137\n",
      "Iteration 7391, loss = 0.10114887\n",
      "Iteration 7392, loss = 0.10114637\n",
      "Iteration 7393, loss = 0.10114388\n",
      "Iteration 7394, loss = 0.10114139\n",
      "Iteration 7395, loss = 0.10113889\n",
      "Iteration 7396, loss = 0.10113640\n",
      "Iteration 7397, loss = 0.10113391\n",
      "Iteration 7398, loss = 0.10113142\n",
      "Iteration 7399, loss = 0.10112893\n",
      "Iteration 7400, loss = 0.10112644\n",
      "Iteration 7401, loss = 0.10112396\n",
      "Iteration 7402, loss = 0.10112147\n",
      "Iteration 7403, loss = 0.10111898\n",
      "Iteration 7404, loss = 0.10111650\n",
      "Iteration 7405, loss = 0.10111401\n",
      "Iteration 7406, loss = 0.10111153\n",
      "Iteration 7407, loss = 0.10110905\n",
      "Iteration 7408, loss = 0.10110656\n",
      "Iteration 7409, loss = 0.10110408\n",
      "Iteration 7410, loss = 0.10110160\n",
      "Iteration 7411, loss = 0.10109912\n",
      "Iteration 7412, loss = 0.10109664\n",
      "Iteration 7413, loss = 0.10109416\n",
      "Iteration 7414, loss = 0.10109169\n",
      "Iteration 7415, loss = 0.10108921\n",
      "Iteration 7416, loss = 0.10108673\n",
      "Iteration 7417, loss = 0.10108426\n",
      "Iteration 7418, loss = 0.10108178\n",
      "Iteration 7419, loss = 0.10107931\n",
      "Iteration 7420, loss = 0.10107684\n",
      "Iteration 7421, loss = 0.10107436\n",
      "Iteration 7422, loss = 0.10107189\n",
      "Iteration 7423, loss = 0.10106942\n",
      "Iteration 7424, loss = 0.10106695\n",
      "Iteration 7425, loss = 0.10106448\n",
      "Iteration 7426, loss = 0.10106201\n",
      "Iteration 7427, loss = 0.10105955\n",
      "Iteration 7428, loss = 0.10105708\n",
      "Iteration 7429, loss = 0.10105461\n",
      "Iteration 7430, loss = 0.10105215\n",
      "Iteration 7431, loss = 0.10104968\n",
      "Iteration 7432, loss = 0.10104722\n",
      "Iteration 7433, loss = 0.10104476\n",
      "Iteration 7434, loss = 0.10104229\n",
      "Iteration 7435, loss = 0.10103983\n",
      "Iteration 7436, loss = 0.10103737\n",
      "Iteration 7437, loss = 0.10103491\n",
      "Iteration 7438, loss = 0.10103245\n",
      "Iteration 7439, loss = 0.10102999\n",
      "Iteration 7440, loss = 0.10102754\n",
      "Iteration 7441, loss = 0.10102508\n",
      "Iteration 7442, loss = 0.10102262\n",
      "Iteration 7443, loss = 0.10102017\n",
      "Iteration 7444, loss = 0.10101771\n",
      "Iteration 7445, loss = 0.10101526\n",
      "Iteration 7446, loss = 0.10101281\n",
      "Iteration 7447, loss = 0.10101035\n",
      "Iteration 7448, loss = 0.10100790\n",
      "Iteration 7449, loss = 0.10100545\n",
      "Iteration 7450, loss = 0.10100300\n",
      "Iteration 7451, loss = 0.10100055\n",
      "Iteration 7452, loss = 0.10099810\n",
      "Iteration 7453, loss = 0.10099565\n",
      "Iteration 7454, loss = 0.10099321\n",
      "Iteration 7455, loss = 0.10099076\n",
      "Iteration 7456, loss = 0.10098832\n",
      "Iteration 7457, loss = 0.10098587\n",
      "Iteration 7458, loss = 0.10098343\n",
      "Iteration 7459, loss = 0.10098098\n",
      "Iteration 7460, loss = 0.10097854\n",
      "Iteration 7461, loss = 0.10097610\n",
      "Iteration 7462, loss = 0.10097366\n",
      "Iteration 7463, loss = 0.10097122\n",
      "Iteration 7464, loss = 0.10096878\n",
      "Iteration 7465, loss = 0.10096634\n",
      "Iteration 7466, loss = 0.10096390\n",
      "Iteration 7467, loss = 0.10096146\n",
      "Iteration 7468, loss = 0.10095903\n",
      "Iteration 7469, loss = 0.10095659\n",
      "Iteration 7470, loss = 0.10095416\n",
      "Iteration 7471, loss = 0.10095172\n",
      "Iteration 7472, loss = 0.10094929\n",
      "Iteration 7473, loss = 0.10094686\n",
      "Iteration 7474, loss = 0.10094442\n",
      "Iteration 7475, loss = 0.10094199\n",
      "Iteration 7476, loss = 0.10093956\n",
      "Iteration 7477, loss = 0.10093713\n",
      "Iteration 7478, loss = 0.10093470\n",
      "Iteration 7479, loss = 0.10093228\n",
      "Iteration 7480, loss = 0.10092985\n",
      "Iteration 7481, loss = 0.10092742\n",
      "Iteration 7482, loss = 0.10092500\n",
      "Iteration 7483, loss = 0.10092257\n",
      "Iteration 7484, loss = 0.10092015\n",
      "Iteration 7485, loss = 0.10091772\n",
      "Iteration 7486, loss = 0.10091530\n",
      "Iteration 7487, loss = 0.10091288\n",
      "Iteration 7488, loss = 0.10091046\n",
      "Iteration 7489, loss = 0.10090803\n",
      "Iteration 7490, loss = 0.10090561\n",
      "Iteration 7491, loss = 0.10090320\n",
      "Iteration 7492, loss = 0.10090078\n",
      "Iteration 7493, loss = 0.10089836\n",
      "Iteration 7494, loss = 0.10089594\n",
      "Iteration 7495, loss = 0.10089353\n",
      "Iteration 7496, loss = 0.10089111\n",
      "Iteration 7497, loss = 0.10088870\n",
      "Iteration 7498, loss = 0.10088628\n",
      "Iteration 7499, loss = 0.10088387\n",
      "Iteration 7500, loss = 0.10088146\n",
      "Iteration 7501, loss = 0.10087904\n",
      "Iteration 7502, loss = 0.10087663\n",
      "Iteration 7503, loss = 0.10087422\n",
      "Iteration 7504, loss = 0.10087181\n",
      "Iteration 7505, loss = 0.10086940\n",
      "Iteration 7506, loss = 0.10086700\n",
      "Iteration 7507, loss = 0.10086459\n",
      "Iteration 7508, loss = 0.10086218\n",
      "Iteration 7509, loss = 0.10085978\n",
      "Iteration 7510, loss = 0.10085737\n",
      "Iteration 7511, loss = 0.10085497\n",
      "Iteration 7512, loss = 0.10085256\n",
      "Iteration 7513, loss = 0.10085016\n",
      "Iteration 7514, loss = 0.10084776\n",
      "Iteration 7515, loss = 0.10084536\n",
      "Iteration 7516, loss = 0.10084296\n",
      "Iteration 7517, loss = 0.10084056\n",
      "Iteration 7518, loss = 0.10083816\n",
      "Iteration 7519, loss = 0.10083576\n",
      "Iteration 7520, loss = 0.10083336\n",
      "Iteration 7521, loss = 0.10083096\n",
      "Iteration 7522, loss = 0.10082857\n",
      "Iteration 7523, loss = 0.10082617\n",
      "Iteration 7524, loss = 0.10082378\n",
      "Iteration 7525, loss = 0.10082138\n",
      "Iteration 7526, loss = 0.10081899\n",
      "Iteration 7527, loss = 0.10081660\n",
      "Iteration 7528, loss = 0.10081420\n",
      "Iteration 7529, loss = 0.10081181\n",
      "Iteration 7530, loss = 0.10080942\n",
      "Iteration 7531, loss = 0.10080703\n",
      "Iteration 7532, loss = 0.10080464\n",
      "Iteration 7533, loss = 0.10080226\n",
      "Iteration 7534, loss = 0.10079987\n",
      "Iteration 7535, loss = 0.10079748\n",
      "Iteration 7536, loss = 0.10079509\n",
      "Iteration 7537, loss = 0.10079271\n",
      "Iteration 7538, loss = 0.10079032\n",
      "Iteration 7539, loss = 0.10078794\n",
      "Iteration 7540, loss = 0.10078556\n",
      "Iteration 7541, loss = 0.10078318\n",
      "Iteration 7542, loss = 0.10078079\n",
      "Iteration 7543, loss = 0.10077841\n",
      "Iteration 7544, loss = 0.10077603\n",
      "Iteration 7545, loss = 0.10077365\n",
      "Iteration 7546, loss = 0.10077127\n",
      "Iteration 7547, loss = 0.10076890\n",
      "Iteration 7548, loss = 0.10076652\n",
      "Iteration 7549, loss = 0.10076414\n",
      "Iteration 7550, loss = 0.10076177\n",
      "Iteration 7551, loss = 0.10075939\n",
      "Iteration 7552, loss = 0.10075702\n",
      "Iteration 7553, loss = 0.10075464\n",
      "Iteration 7554, loss = 0.10075227\n",
      "Iteration 7555, loss = 0.10074990\n",
      "Iteration 7556, loss = 0.10074753\n",
      "Iteration 7557, loss = 0.10074516\n",
      "Iteration 7558, loss = 0.10074279\n",
      "Iteration 7559, loss = 0.10074042\n",
      "Iteration 7560, loss = 0.10073805\n",
      "Iteration 7561, loss = 0.10073568\n",
      "Iteration 7562, loss = 0.10073331\n",
      "Iteration 7563, loss = 0.10073095\n",
      "Iteration 7564, loss = 0.10072858\n",
      "Iteration 7565, loss = 0.10072621\n",
      "Iteration 7566, loss = 0.10072385\n",
      "Iteration 7567, loss = 0.10072149\n",
      "Iteration 7568, loss = 0.10071912\n",
      "Iteration 7569, loss = 0.10071676\n",
      "Iteration 7570, loss = 0.10071440\n",
      "Iteration 7571, loss = 0.10071204\n",
      "Iteration 7572, loss = 0.10070968\n",
      "Iteration 7573, loss = 0.10070732\n",
      "Iteration 7574, loss = 0.10070496\n",
      "Iteration 7575, loss = 0.10070260\n",
      "Iteration 7576, loss = 0.10070025\n",
      "Iteration 7577, loss = 0.10069789\n",
      "Iteration 7578, loss = 0.10069553\n",
      "Iteration 7579, loss = 0.10069318\n",
      "Iteration 7580, loss = 0.10069082\n",
      "Iteration 7581, loss = 0.10068847\n",
      "Iteration 7582, loss = 0.10068612\n",
      "Iteration 7583, loss = 0.10068377\n",
      "Iteration 7584, loss = 0.10068141\n",
      "Iteration 7585, loss = 0.10067906\n",
      "Iteration 7586, loss = 0.10067671\n",
      "Iteration 7587, loss = 0.10067436\n",
      "Iteration 7588, loss = 0.10067202\n",
      "Iteration 7589, loss = 0.10066967\n",
      "Iteration 7590, loss = 0.10066732\n",
      "Iteration 7591, loss = 0.10066497\n",
      "Iteration 7592, loss = 0.10066263\n",
      "Iteration 7593, loss = 0.10066028\n",
      "Iteration 7594, loss = 0.10065794\n",
      "Iteration 7595, loss = 0.10065560\n",
      "Iteration 7596, loss = 0.10065325\n",
      "Iteration 7597, loss = 0.10065091\n",
      "Iteration 7598, loss = 0.10064857\n",
      "Iteration 7599, loss = 0.10064623\n",
      "Iteration 7600, loss = 0.10064389\n",
      "Iteration 7601, loss = 0.10064155\n",
      "Iteration 7602, loss = 0.10063921\n",
      "Iteration 7603, loss = 0.10063687\n",
      "Iteration 7604, loss = 0.10063453\n",
      "Iteration 7605, loss = 0.10063220\n",
      "Iteration 7606, loss = 0.10062986\n",
      "Iteration 7607, loss = 0.10062753\n",
      "Iteration 7608, loss = 0.10062519\n",
      "Iteration 7609, loss = 0.10062286\n",
      "Iteration 7610, loss = 0.10062053\n",
      "Iteration 7611, loss = 0.10061819\n",
      "Iteration 7612, loss = 0.10061586\n",
      "Iteration 7613, loss = 0.10061353\n",
      "Iteration 7614, loss = 0.10061120\n",
      "Iteration 7615, loss = 0.10060887\n",
      "Iteration 7616, loss = 0.10060654\n",
      "Iteration 7617, loss = 0.10060421\n",
      "Iteration 7618, loss = 0.10060189\n",
      "Iteration 7619, loss = 0.10059956\n",
      "Iteration 7620, loss = 0.10059723\n",
      "Iteration 7621, loss = 0.10059491\n",
      "Iteration 7622, loss = 0.10059258\n",
      "Iteration 7623, loss = 0.10059026\n",
      "Iteration 7624, loss = 0.10058794\n",
      "Iteration 7625, loss = 0.10058562\n",
      "Iteration 7626, loss = 0.10058329\n",
      "Iteration 7627, loss = 0.10058097\n",
      "Iteration 7628, loss = 0.10057865\n",
      "Iteration 7629, loss = 0.10057633\n",
      "Iteration 7630, loss = 0.10057401\n",
      "Iteration 7631, loss = 0.10057169\n",
      "Iteration 7632, loss = 0.10056938\n",
      "Iteration 7633, loss = 0.10056706\n",
      "Iteration 7634, loss = 0.10056474\n",
      "Iteration 7635, loss = 0.10056243\n",
      "Iteration 7636, loss = 0.10056011\n",
      "Iteration 7637, loss = 0.10055780\n",
      "Iteration 7638, loss = 0.10055549\n",
      "Iteration 7639, loss = 0.10055317\n",
      "Iteration 7640, loss = 0.10055086\n",
      "Iteration 7641, loss = 0.10054855\n",
      "Iteration 7642, loss = 0.10054624\n",
      "Iteration 7643, loss = 0.10054393\n",
      "Iteration 7644, loss = 0.10054162\n",
      "Iteration 7645, loss = 0.10053931\n",
      "Iteration 7646, loss = 0.10053700\n",
      "Iteration 7647, loss = 0.10053470\n",
      "Iteration 7648, loss = 0.10053239\n",
      "Iteration 7649, loss = 0.10053008\n",
      "Iteration 7650, loss = 0.10052778\n",
      "Iteration 7651, loss = 0.10052548\n",
      "Iteration 7652, loss = 0.10052317\n",
      "Iteration 7653, loss = 0.10052087\n",
      "Iteration 7654, loss = 0.10051857\n",
      "Iteration 7655, loss = 0.10051626\n",
      "Iteration 7656, loss = 0.10051396\n",
      "Iteration 7657, loss = 0.10051166\n",
      "Iteration 7658, loss = 0.10050936\n",
      "Iteration 7659, loss = 0.10050706\n",
      "Iteration 7660, loss = 0.10050477\n",
      "Iteration 7661, loss = 0.10050247\n",
      "Iteration 7662, loss = 0.10050017\n",
      "Iteration 7663, loss = 0.10049788\n",
      "Iteration 7664, loss = 0.10049558\n",
      "Iteration 7665, loss = 0.10049329\n",
      "Iteration 7666, loss = 0.10049099\n",
      "Iteration 7667, loss = 0.10048870\n",
      "Iteration 7668, loss = 0.10048641\n",
      "Iteration 7669, loss = 0.10048411\n",
      "Iteration 7670, loss = 0.10048182\n",
      "Iteration 7671, loss = 0.10047953\n",
      "Iteration 7672, loss = 0.10047724\n",
      "Iteration 7673, loss = 0.10047495\n",
      "Iteration 7674, loss = 0.10047266\n",
      "Iteration 7675, loss = 0.10047038\n",
      "Iteration 7676, loss = 0.10046809\n",
      "Iteration 7677, loss = 0.10046580\n",
      "Iteration 7678, loss = 0.10046352\n",
      "Iteration 7679, loss = 0.10046123\n",
      "Iteration 7680, loss = 0.10045895\n",
      "Iteration 7681, loss = 0.10045666\n",
      "Iteration 7682, loss = 0.10045438\n",
      "Iteration 7683, loss = 0.10045210\n",
      "Iteration 7684, loss = 0.10044982\n",
      "Iteration 7685, loss = 0.10044753\n",
      "Iteration 7686, loss = 0.10044525\n",
      "Iteration 7687, loss = 0.10044297\n",
      "Iteration 7688, loss = 0.10044070\n",
      "Iteration 7689, loss = 0.10043842\n",
      "Iteration 7690, loss = 0.10043614\n",
      "Iteration 7691, loss = 0.10043386\n",
      "Iteration 7692, loss = 0.10043159\n",
      "Iteration 7693, loss = 0.10042931\n",
      "Iteration 7694, loss = 0.10042704\n",
      "Iteration 7695, loss = 0.10042476\n",
      "Iteration 7696, loss = 0.10042249\n",
      "Iteration 7697, loss = 0.10042021\n",
      "Iteration 7698, loss = 0.10041794\n",
      "Iteration 7699, loss = 0.10041567\n",
      "Iteration 7700, loss = 0.10041340\n",
      "Iteration 7701, loss = 0.10041113\n",
      "Iteration 7702, loss = 0.10040886\n",
      "Iteration 7703, loss = 0.10040659\n",
      "Iteration 7704, loss = 0.10040432\n",
      "Iteration 7705, loss = 0.10040206\n",
      "Iteration 7706, loss = 0.10039979\n",
      "Iteration 7707, loss = 0.10039752\n",
      "Iteration 7708, loss = 0.10039526\n",
      "Iteration 7709, loss = 0.10039299\n",
      "Iteration 7710, loss = 0.10039073\n",
      "Iteration 7711, loss = 0.10038846\n",
      "Iteration 7712, loss = 0.10038620\n",
      "Iteration 7713, loss = 0.10038394\n",
      "Iteration 7714, loss = 0.10038168\n",
      "Iteration 7715, loss = 0.10037942\n",
      "Iteration 7716, loss = 0.10037716\n",
      "Iteration 7717, loss = 0.10037490\n",
      "Iteration 7718, loss = 0.10037264\n",
      "Iteration 7719, loss = 0.10037038\n",
      "Iteration 7720, loss = 0.10036812\n",
      "Iteration 7721, loss = 0.10036587\n",
      "Iteration 7722, loss = 0.10036361\n",
      "Iteration 7723, loss = 0.10036135\n",
      "Iteration 7724, loss = 0.10035910\n",
      "Iteration 7725, loss = 0.10035685\n",
      "Iteration 7726, loss = 0.10035459\n",
      "Iteration 7727, loss = 0.10035234\n",
      "Iteration 7728, loss = 0.10035009\n",
      "Iteration 7729, loss = 0.10034784\n",
      "Iteration 7730, loss = 0.10034559\n",
      "Iteration 7731, loss = 0.10034334\n",
      "Iteration 7732, loss = 0.10034109\n",
      "Iteration 7733, loss = 0.10033884\n",
      "Iteration 7734, loss = 0.10033659\n",
      "Iteration 7735, loss = 0.10033434\n",
      "Iteration 7736, loss = 0.10033209\n",
      "Iteration 7737, loss = 0.10032985\n",
      "Iteration 7738, loss = 0.10032760\n",
      "Iteration 7739, loss = 0.10032536\n",
      "Iteration 7740, loss = 0.10032311\n",
      "Iteration 7741, loss = 0.10032087\n",
      "Iteration 7742, loss = 0.10031863\n",
      "Iteration 7743, loss = 0.10031639\n",
      "Iteration 7744, loss = 0.10031414\n",
      "Iteration 7745, loss = 0.10031190\n",
      "Iteration 7746, loss = 0.10030966\n",
      "Iteration 7747, loss = 0.10030742\n",
      "Iteration 7748, loss = 0.10030518\n",
      "Iteration 7749, loss = 0.10030295\n",
      "Iteration 7750, loss = 0.10030071\n",
      "Iteration 7751, loss = 0.10029847\n",
      "Iteration 7752, loss = 0.10029624\n",
      "Iteration 7753, loss = 0.10029400\n",
      "Iteration 7754, loss = 0.10029177\n",
      "Iteration 7755, loss = 0.10028953\n",
      "Iteration 7756, loss = 0.10028730\n",
      "Iteration 7757, loss = 0.10028507\n",
      "Iteration 7758, loss = 0.10028283\n",
      "Iteration 7759, loss = 0.10028060\n",
      "Iteration 7760, loss = 0.10027837\n",
      "Iteration 7761, loss = 0.10027614\n",
      "Iteration 7762, loss = 0.10027391\n",
      "Iteration 7763, loss = 0.10027168\n",
      "Iteration 7764, loss = 0.10026945\n",
      "Iteration 7765, loss = 0.10026723\n",
      "Iteration 7766, loss = 0.10026500\n",
      "Iteration 7767, loss = 0.10026277\n",
      "Iteration 7768, loss = 0.10026055\n",
      "Iteration 7769, loss = 0.10025832\n",
      "Iteration 7770, loss = 0.10025610\n",
      "Iteration 7771, loss = 0.10025387\n",
      "Iteration 7772, loss = 0.10025165\n",
      "Iteration 7773, loss = 0.10024943\n",
      "Iteration 7774, loss = 0.10024721\n",
      "Iteration 7775, loss = 0.10024499\n",
      "Iteration 7776, loss = 0.10024276\n",
      "Iteration 7777, loss = 0.10024055\n",
      "Iteration 7778, loss = 0.10023833\n",
      "Iteration 7779, loss = 0.10023611\n",
      "Iteration 7780, loss = 0.10023389\n",
      "Iteration 7781, loss = 0.10023167\n",
      "Iteration 7782, loss = 0.10022946\n",
      "Iteration 7783, loss = 0.10022724\n",
      "Iteration 7784, loss = 0.10022502\n",
      "Iteration 7785, loss = 0.10022281\n",
      "Iteration 7786, loss = 0.10022060\n",
      "Iteration 7787, loss = 0.10021838\n",
      "Iteration 7788, loss = 0.10021617\n",
      "Iteration 7789, loss = 0.10021396\n",
      "Iteration 7790, loss = 0.10021175\n",
      "Iteration 7791, loss = 0.10020954\n",
      "Iteration 7792, loss = 0.10020733\n",
      "Iteration 7793, loss = 0.10020512\n",
      "Iteration 7794, loss = 0.10020291\n",
      "Iteration 7795, loss = 0.10020070\n",
      "Iteration 7796, loss = 0.10019849\n",
      "Iteration 7797, loss = 0.10019629\n",
      "Iteration 7798, loss = 0.10019408\n",
      "Iteration 7799, loss = 0.10019187\n",
      "Iteration 7800, loss = 0.10018967\n",
      "Iteration 7801, loss = 0.10018747\n",
      "Iteration 7802, loss = 0.10018526\n",
      "Iteration 7803, loss = 0.10018306\n",
      "Iteration 7804, loss = 0.10018086\n",
      "Iteration 7805, loss = 0.10017865\n",
      "Iteration 7806, loss = 0.10017645\n",
      "Iteration 7807, loss = 0.10017425\n",
      "Iteration 7808, loss = 0.10017205\n",
      "Iteration 7809, loss = 0.10016985\n",
      "Iteration 7810, loss = 0.10016766\n",
      "Iteration 7811, loss = 0.10016546\n",
      "Iteration 7812, loss = 0.10016326\n",
      "Iteration 7813, loss = 0.10016106\n",
      "Iteration 7814, loss = 0.10015887\n",
      "Iteration 7815, loss = 0.10015667\n",
      "Iteration 7816, loss = 0.10015448\n",
      "Iteration 7817, loss = 0.10015229\n",
      "Iteration 7818, loss = 0.10015009\n",
      "Iteration 7819, loss = 0.10014790\n",
      "Iteration 7820, loss = 0.10014571\n",
      "Iteration 7821, loss = 0.10014352\n",
      "Iteration 7822, loss = 0.10014133\n",
      "Iteration 7823, loss = 0.10013914\n",
      "Iteration 7824, loss = 0.10013695\n",
      "Iteration 7825, loss = 0.10013476\n",
      "Iteration 7826, loss = 0.10013257\n",
      "Iteration 7827, loss = 0.10013038\n",
      "Iteration 7828, loss = 0.10012819\n",
      "Iteration 7829, loss = 0.10012601\n",
      "Iteration 7830, loss = 0.10012382\n",
      "Iteration 7831, loss = 0.10012164\n",
      "Iteration 7832, loss = 0.10011945\n",
      "Iteration 7833, loss = 0.10011727\n",
      "Iteration 7834, loss = 0.10011509\n",
      "Iteration 7835, loss = 0.10011290\n",
      "Iteration 7836, loss = 0.10011072\n",
      "Iteration 7837, loss = 0.10010854\n",
      "Iteration 7838, loss = 0.10010636\n",
      "Iteration 7839, loss = 0.10010418\n",
      "Iteration 7840, loss = 0.10010200\n",
      "Iteration 7841, loss = 0.10009982\n",
      "Iteration 7842, loss = 0.10009764\n",
      "Iteration 7843, loss = 0.10009547\n",
      "Iteration 7844, loss = 0.10009329\n",
      "Iteration 7845, loss = 0.10009111\n",
      "Iteration 7846, loss = 0.10008894\n",
      "Iteration 7847, loss = 0.10008676\n",
      "Iteration 7848, loss = 0.10008459\n",
      "Iteration 7849, loss = 0.10008242\n",
      "Iteration 7850, loss = 0.10008024\n",
      "Iteration 7851, loss = 0.10007807\n",
      "Iteration 7852, loss = 0.10007590\n",
      "Iteration 7853, loss = 0.10007373\n",
      "Iteration 7854, loss = 0.10007156\n",
      "Iteration 7855, loss = 0.10006939\n",
      "Iteration 7856, loss = 0.10006722\n",
      "Iteration 7857, loss = 0.10006505\n",
      "Iteration 7858, loss = 0.10006288\n",
      "Iteration 7859, loss = 0.10006071\n",
      "Iteration 7860, loss = 0.10005855\n",
      "Iteration 7861, loss = 0.10005638\n",
      "Iteration 7862, loss = 0.10005422\n",
      "Iteration 7863, loss = 0.10005205\n",
      "Iteration 7864, loss = 0.10004989\n",
      "Iteration 7865, loss = 0.10004772\n",
      "Iteration 7866, loss = 0.10004556\n",
      "Iteration 7867, loss = 0.10004340\n",
      "Iteration 7868, loss = 0.10004124\n",
      "Iteration 7869, loss = 0.10003908\n",
      "Iteration 7870, loss = 0.10003692\n",
      "Iteration 7871, loss = 0.10003476\n",
      "Iteration 7872, loss = 0.10003260\n",
      "Iteration 7873, loss = 0.10003044\n",
      "Iteration 7874, loss = 0.10002828\n",
      "Iteration 7875, loss = 0.10002612\n",
      "Iteration 7876, loss = 0.10002397\n",
      "Iteration 7877, loss = 0.10002181\n",
      "Iteration 7878, loss = 0.10001966\n",
      "Iteration 7879, loss = 0.10001750\n",
      "Iteration 7880, loss = 0.10001535\n",
      "Iteration 7881, loss = 0.10001319\n",
      "Iteration 7882, loss = 0.10001104\n",
      "Iteration 7883, loss = 0.10000889\n",
      "Iteration 7884, loss = 0.10000674\n",
      "Iteration 7885, loss = 0.10000458\n",
      "Iteration 7886, loss = 0.10000243\n",
      "Iteration 7887, loss = 0.10000028\n",
      "Iteration 7888, loss = 0.09999814\n",
      "Iteration 7889, loss = 0.09999599\n",
      "Iteration 7890, loss = 0.09999384\n",
      "Iteration 7891, loss = 0.09999169\n",
      "Iteration 7892, loss = 0.09998954\n",
      "Iteration 7893, loss = 0.09998740\n",
      "Iteration 7894, loss = 0.09998525\n",
      "Iteration 7895, loss = 0.09998311\n",
      "Iteration 7896, loss = 0.09998096\n",
      "Iteration 7897, loss = 0.09997882\n",
      "Iteration 7898, loss = 0.09997668\n",
      "Iteration 7899, loss = 0.09997454\n",
      "Iteration 7900, loss = 0.09997239\n",
      "Iteration 7901, loss = 0.09997025\n",
      "Iteration 7902, loss = 0.09996811\n",
      "Iteration 7903, loss = 0.09996597\n",
      "Iteration 7904, loss = 0.09996383\n",
      "Iteration 7905, loss = 0.09996169\n",
      "Iteration 7906, loss = 0.09995956\n",
      "Iteration 7907, loss = 0.09995742\n",
      "Iteration 7908, loss = 0.09995528\n",
      "Iteration 7909, loss = 0.09995315\n",
      "Iteration 7910, loss = 0.09995101\n",
      "Iteration 7911, loss = 0.09994887\n",
      "Iteration 7912, loss = 0.09994674\n",
      "Iteration 7913, loss = 0.09994461\n",
      "Iteration 7914, loss = 0.09994247\n",
      "Iteration 7915, loss = 0.09994034\n",
      "Iteration 7916, loss = 0.09993821\n",
      "Iteration 7917, loss = 0.09993608\n",
      "Iteration 7918, loss = 0.09993395\n",
      "Iteration 7919, loss = 0.09993182\n",
      "Iteration 7920, loss = 0.09992969\n",
      "Iteration 7921, loss = 0.09992756\n",
      "Iteration 7922, loss = 0.09992543\n",
      "Iteration 7923, loss = 0.09992330\n",
      "Iteration 7924, loss = 0.09992118\n",
      "Iteration 7925, loss = 0.09991905\n",
      "Iteration 7926, loss = 0.09991692\n",
      "Iteration 7927, loss = 0.09991480\n",
      "Iteration 7928, loss = 0.09991267\n",
      "Iteration 7929, loss = 0.09991055\n",
      "Iteration 7930, loss = 0.09990843\n",
      "Iteration 7931, loss = 0.09990630\n",
      "Iteration 7932, loss = 0.09990418\n",
      "Iteration 7933, loss = 0.09990206\n",
      "Iteration 7934, loss = 0.09989994\n",
      "Iteration 7935, loss = 0.09989782\n",
      "Iteration 7936, loss = 0.09989570\n",
      "Iteration 7937, loss = 0.09989358\n",
      "Iteration 7938, loss = 0.09989146\n",
      "Iteration 7939, loss = 0.09988934\n",
      "Iteration 7940, loss = 0.09988723\n",
      "Iteration 7941, loss = 0.09988511\n",
      "Iteration 7942, loss = 0.09988299\n",
      "Iteration 7943, loss = 0.09988088\n",
      "Iteration 7944, loss = 0.09987876\n",
      "Iteration 7945, loss = 0.09987665\n",
      "Iteration 7946, loss = 0.09987453\n",
      "Iteration 7947, loss = 0.09987242\n",
      "Iteration 7948, loss = 0.09987031\n",
      "Iteration 7949, loss = 0.09986820\n",
      "Iteration 7950, loss = 0.09986609\n",
      "Iteration 7951, loss = 0.09986398\n",
      "Iteration 7952, loss = 0.09986187\n",
      "Iteration 7953, loss = 0.09985976\n",
      "Iteration 7954, loss = 0.09985765\n",
      "Iteration 7955, loss = 0.09985554\n",
      "Iteration 7956, loss = 0.09985343\n",
      "Iteration 7957, loss = 0.09985132\n",
      "Iteration 7958, loss = 0.09984922\n",
      "Iteration 7959, loss = 0.09984711\n",
      "Iteration 7960, loss = 0.09984501\n",
      "Iteration 7961, loss = 0.09984290\n",
      "Iteration 7962, loss = 0.09984080\n",
      "Iteration 7963, loss = 0.09983869\n",
      "Iteration 7964, loss = 0.09983659\n",
      "Iteration 7965, loss = 0.09983449\n",
      "Iteration 7966, loss = 0.09983239\n",
      "Iteration 7967, loss = 0.09983029\n",
      "Iteration 7968, loss = 0.09982819\n",
      "Iteration 7969, loss = 0.09982609\n",
      "Iteration 7970, loss = 0.09982399\n",
      "Iteration 7971, loss = 0.09982189\n",
      "Iteration 7972, loss = 0.09981979\n",
      "Iteration 7973, loss = 0.09981769\n",
      "Iteration 7974, loss = 0.09981560\n",
      "Iteration 7975, loss = 0.09981350\n",
      "Iteration 7976, loss = 0.09981140\n",
      "Iteration 7977, loss = 0.09980931\n",
      "Iteration 7978, loss = 0.09980721\n",
      "Iteration 7979, loss = 0.09980512\n",
      "Iteration 7980, loss = 0.09980303\n",
      "Iteration 7981, loss = 0.09980093\n",
      "Iteration 7982, loss = 0.09979884\n",
      "Iteration 7983, loss = 0.09979675\n",
      "Iteration 7984, loss = 0.09979466\n",
      "Iteration 7985, loss = 0.09979257\n",
      "Iteration 7986, loss = 0.09979048\n",
      "Iteration 7987, loss = 0.09978839\n",
      "Iteration 7988, loss = 0.09978630\n",
      "Iteration 7989, loss = 0.09978421\n",
      "Iteration 7990, loss = 0.09978213\n",
      "Iteration 7991, loss = 0.09978004\n",
      "Iteration 7992, loss = 0.09977795\n",
      "Iteration 7993, loss = 0.09977587\n",
      "Iteration 7994, loss = 0.09977378\n",
      "Iteration 7995, loss = 0.09977170\n",
      "Iteration 7996, loss = 0.09976962\n",
      "Iteration 7997, loss = 0.09976753\n",
      "Iteration 7998, loss = 0.09976545\n",
      "Iteration 7999, loss = 0.09976337\n",
      "Iteration 8000, loss = 0.09976129\n",
      "Iteration 8001, loss = 0.09975920\n",
      "Iteration 8002, loss = 0.09975712\n",
      "Iteration 8003, loss = 0.09975504\n",
      "Iteration 8004, loss = 0.09975297\n",
      "Iteration 8005, loss = 0.09975089\n",
      "Iteration 8006, loss = 0.09974881\n",
      "Iteration 8007, loss = 0.09974673\n",
      "Iteration 8008, loss = 0.09974466\n",
      "Iteration 8009, loss = 0.09974258\n",
      "Iteration 8010, loss = 0.09974050\n",
      "Iteration 8011, loss = 0.09973843\n",
      "Iteration 8012, loss = 0.09973635\n",
      "Iteration 8013, loss = 0.09973428\n",
      "Iteration 8014, loss = 0.09973221\n",
      "Iteration 8015, loss = 0.09973013\n",
      "Iteration 8016, loss = 0.09972806\n",
      "Iteration 8017, loss = 0.09972599\n",
      "Iteration 8018, loss = 0.09972392\n",
      "Iteration 8019, loss = 0.09972185\n",
      "Iteration 8020, loss = 0.09971978\n",
      "Iteration 8021, loss = 0.09971771\n",
      "Iteration 8022, loss = 0.09971564\n",
      "Iteration 8023, loss = 0.09971357\n",
      "Iteration 8024, loss = 0.09971151\n",
      "Iteration 8025, loss = 0.09970944\n",
      "Iteration 8026, loss = 0.09970737\n",
      "Iteration 8027, loss = 0.09970531\n",
      "Iteration 8028, loss = 0.09970324\n",
      "Iteration 8029, loss = 0.09970118\n",
      "Iteration 8030, loss = 0.09969912\n",
      "Iteration 8031, loss = 0.09969705\n",
      "Iteration 8032, loss = 0.09969499\n",
      "Iteration 8033, loss = 0.09969293\n",
      "Iteration 8034, loss = 0.09969087\n",
      "Iteration 8035, loss = 0.09968880\n",
      "Iteration 8036, loss = 0.09968674\n",
      "Iteration 8037, loss = 0.09968468\n",
      "Iteration 8038, loss = 0.09968263\n",
      "Iteration 8039, loss = 0.09968057\n",
      "Iteration 8040, loss = 0.09967851\n",
      "Iteration 8041, loss = 0.09967645\n",
      "Iteration 8042, loss = 0.09967439\n",
      "Iteration 8043, loss = 0.09967234\n",
      "Iteration 8044, loss = 0.09967028\n",
      "Iteration 8045, loss = 0.09966823\n",
      "Iteration 8046, loss = 0.09966617\n",
      "Iteration 8047, loss = 0.09966412\n",
      "Iteration 8048, loss = 0.09966207\n",
      "Iteration 8049, loss = 0.09966001\n",
      "Iteration 8050, loss = 0.09965796\n",
      "Iteration 8051, loss = 0.09965591\n",
      "Iteration 8052, loss = 0.09965386\n",
      "Iteration 8053, loss = 0.09965181\n",
      "Iteration 8054, loss = 0.09964976\n",
      "Iteration 8055, loss = 0.09964771\n",
      "Iteration 8056, loss = 0.09964566\n",
      "Iteration 8057, loss = 0.09964361\n",
      "Iteration 8058, loss = 0.09964156\n",
      "Iteration 8059, loss = 0.09963952\n",
      "Iteration 8060, loss = 0.09963747\n",
      "Iteration 8061, loss = 0.09963542\n",
      "Iteration 8062, loss = 0.09963338\n",
      "Iteration 8063, loss = 0.09963133\n",
      "Iteration 8064, loss = 0.09962929\n",
      "Iteration 8065, loss = 0.09962724\n",
      "Iteration 8066, loss = 0.09962520\n",
      "Iteration 8067, loss = 0.09962316\n",
      "Iteration 8068, loss = 0.09962112\n",
      "Iteration 8069, loss = 0.09961908\n",
      "Iteration 8070, loss = 0.09961704\n",
      "Iteration 8071, loss = 0.09961499\n",
      "Iteration 8072, loss = 0.09961296\n",
      "Iteration 8073, loss = 0.09961092\n",
      "Iteration 8074, loss = 0.09960888\n",
      "Iteration 8075, loss = 0.09960684\n",
      "Iteration 8076, loss = 0.09960480\n",
      "Iteration 8077, loss = 0.09960277\n",
      "Iteration 8078, loss = 0.09960073\n",
      "Iteration 8079, loss = 0.09959869\n",
      "Iteration 8080, loss = 0.09959666\n",
      "Iteration 8081, loss = 0.09959462\n",
      "Iteration 8082, loss = 0.09959259\n",
      "Iteration 8083, loss = 0.09959056\n",
      "Iteration 8084, loss = 0.09958852\n",
      "Iteration 8085, loss = 0.09958649\n",
      "Iteration 8086, loss = 0.09958446\n",
      "Iteration 8087, loss = 0.09958243\n",
      "Iteration 8088, loss = 0.09958040\n",
      "Iteration 8089, loss = 0.09957837\n",
      "Iteration 8090, loss = 0.09957634\n",
      "Iteration 8091, loss = 0.09957431\n",
      "Iteration 8092, loss = 0.09957228\n",
      "Iteration 8093, loss = 0.09957025\n",
      "Iteration 8094, loss = 0.09956823\n",
      "Iteration 8095, loss = 0.09956620\n",
      "Iteration 8096, loss = 0.09956417\n",
      "Iteration 8097, loss = 0.09956215\n",
      "Iteration 8098, loss = 0.09956012\n",
      "Iteration 8099, loss = 0.09955810\n",
      "Iteration 8100, loss = 0.09955608\n",
      "Iteration 8101, loss = 0.09955405\n",
      "Iteration 8102, loss = 0.09955203\n",
      "Iteration 8103, loss = 0.09955001\n",
      "Iteration 8104, loss = 0.09954799\n",
      "Iteration 8105, loss = 0.09954597\n",
      "Iteration 8106, loss = 0.09954395\n",
      "Iteration 8107, loss = 0.09954193\n",
      "Iteration 8108, loss = 0.09953991\n",
      "Iteration 8109, loss = 0.09953789\n",
      "Iteration 8110, loss = 0.09953587\n",
      "Iteration 8111, loss = 0.09953385\n",
      "Iteration 8112, loss = 0.09953183\n",
      "Iteration 8113, loss = 0.09952982\n",
      "Iteration 8114, loss = 0.09952780\n",
      "Iteration 8115, loss = 0.09952579\n",
      "Iteration 8116, loss = 0.09952377\n",
      "Iteration 8117, loss = 0.09952176\n",
      "Iteration 8118, loss = 0.09951974\n",
      "Iteration 8119, loss = 0.09951773\n",
      "Iteration 8120, loss = 0.09951572\n",
      "Iteration 8121, loss = 0.09951371\n",
      "Iteration 8122, loss = 0.09951170\n",
      "Iteration 8123, loss = 0.09950968\n",
      "Iteration 8124, loss = 0.09950767\n",
      "Iteration 8125, loss = 0.09950566\n",
      "Iteration 8126, loss = 0.09950366\n",
      "Iteration 8127, loss = 0.09950165\n",
      "Iteration 8128, loss = 0.09949964\n",
      "Iteration 8129, loss = 0.09949763\n",
      "Iteration 8130, loss = 0.09949562\n",
      "Iteration 8131, loss = 0.09949362\n",
      "Iteration 8132, loss = 0.09949161\n",
      "Iteration 8133, loss = 0.09948961\n",
      "Iteration 8134, loss = 0.09948760\n",
      "Iteration 8135, loss = 0.09948560\n",
      "Iteration 8136, loss = 0.09948359\n",
      "Iteration 8137, loss = 0.09948159\n",
      "Iteration 8138, loss = 0.09947959\n",
      "Iteration 8139, loss = 0.09947759\n",
      "Iteration 8140, loss = 0.09947559\n",
      "Iteration 8141, loss = 0.09947358\n",
      "Iteration 8142, loss = 0.09947158\n",
      "Iteration 8143, loss = 0.09946958\n",
      "Iteration 8144, loss = 0.09946759\n",
      "Iteration 8145, loss = 0.09946559\n",
      "Iteration 8146, loss = 0.09946359\n",
      "Iteration 8147, loss = 0.09946159\n",
      "Iteration 8148, loss = 0.09945959\n",
      "Iteration 8149, loss = 0.09945760\n",
      "Iteration 8150, loss = 0.09945560\n",
      "Iteration 8151, loss = 0.09945361\n",
      "Iteration 8152, loss = 0.09945161\n",
      "Iteration 8153, loss = 0.09944962\n",
      "Iteration 8154, loss = 0.09944762\n",
      "Iteration 8155, loss = 0.09944563\n",
      "Iteration 8156, loss = 0.09944364\n",
      "Iteration 8157, loss = 0.09944165\n",
      "Iteration 8158, loss = 0.09943966\n",
      "Iteration 8159, loss = 0.09943766\n",
      "Iteration 8160, loss = 0.09943567\n",
      "Iteration 8161, loss = 0.09943368\n",
      "Iteration 8162, loss = 0.09943169\n",
      "Iteration 8163, loss = 0.09942971\n",
      "Iteration 8164, loss = 0.09942772\n",
      "Iteration 8165, loss = 0.09942573\n",
      "Iteration 8166, loss = 0.09942374\n",
      "Iteration 8167, loss = 0.09942176\n",
      "Iteration 8168, loss = 0.09941977\n",
      "Iteration 8169, loss = 0.09941779\n",
      "Iteration 8170, loss = 0.09941580\n",
      "Iteration 8171, loss = 0.09941382\n",
      "Iteration 8172, loss = 0.09941183\n",
      "Iteration 8173, loss = 0.09940985\n",
      "Iteration 8174, loss = 0.09940787\n",
      "Iteration 8175, loss = 0.09940588\n",
      "Iteration 8176, loss = 0.09940390\n",
      "Iteration 8177, loss = 0.09940192\n",
      "Iteration 8178, loss = 0.09939994\n",
      "Iteration 8179, loss = 0.09939796\n",
      "Iteration 8180, loss = 0.09939598\n",
      "Iteration 8181, loss = 0.09939400\n",
      "Iteration 8182, loss = 0.09939202\n",
      "Iteration 8183, loss = 0.09939005\n",
      "Iteration 8184, loss = 0.09938807\n",
      "Iteration 8185, loss = 0.09938609\n",
      "Iteration 8186, loss = 0.09938412\n",
      "Iteration 8187, loss = 0.09938214\n",
      "Iteration 8188, loss = 0.09938017\n",
      "Iteration 8189, loss = 0.09937819\n",
      "Iteration 8190, loss = 0.09937622\n",
      "Iteration 8191, loss = 0.09937424\n",
      "Iteration 8192, loss = 0.09937227\n",
      "Iteration 8193, loss = 0.09937030\n",
      "Iteration 8194, loss = 0.09936833\n",
      "Iteration 8195, loss = 0.09936636\n",
      "Iteration 8196, loss = 0.09936439\n",
      "Iteration 8197, loss = 0.09936242\n",
      "Iteration 8198, loss = 0.09936045\n",
      "Iteration 8199, loss = 0.09935848\n",
      "Iteration 8200, loss = 0.09935651\n",
      "Iteration 8201, loss = 0.09935454\n",
      "Iteration 8202, loss = 0.09935257\n",
      "Iteration 8203, loss = 0.09935061\n",
      "Iteration 8204, loss = 0.09934864\n",
      "Iteration 8205, loss = 0.09934667\n",
      "Iteration 8206, loss = 0.09934471\n",
      "Iteration 8207, loss = 0.09934274\n",
      "Iteration 8208, loss = 0.09934078\n",
      "Iteration 8209, loss = 0.09933882\n",
      "Iteration 8210, loss = 0.09933685\n",
      "Iteration 8211, loss = 0.09933489\n",
      "Iteration 8212, loss = 0.09933293\n",
      "Iteration 8213, loss = 0.09933097\n",
      "Iteration 8214, loss = 0.09932900\n",
      "Iteration 8215, loss = 0.09932704\n",
      "Iteration 8216, loss = 0.09932508\n",
      "Iteration 8217, loss = 0.09932313\n",
      "Iteration 8218, loss = 0.09932117\n",
      "Iteration 8219, loss = 0.09931921\n",
      "Iteration 8220, loss = 0.09931725\n",
      "Iteration 8221, loss = 0.09931529\n",
      "Iteration 8222, loss = 0.09931334\n",
      "Iteration 8223, loss = 0.09931138\n",
      "Iteration 8224, loss = 0.09930942\n",
      "Iteration 8225, loss = 0.09930747\n",
      "Iteration 8226, loss = 0.09930551\n",
      "Iteration 8227, loss = 0.09930356\n",
      "Iteration 8228, loss = 0.09930161\n",
      "Iteration 8229, loss = 0.09929965\n",
      "Iteration 8230, loss = 0.09929770\n",
      "Iteration 8231, loss = 0.09929575\n",
      "Iteration 8232, loss = 0.09929380\n",
      "Iteration 8233, loss = 0.09929185\n",
      "Iteration 8234, loss = 0.09928990\n",
      "Iteration 8235, loss = 0.09928795\n",
      "Iteration 8236, loss = 0.09928600\n",
      "Iteration 8237, loss = 0.09928405\n",
      "Iteration 8238, loss = 0.09928210\n",
      "Iteration 8239, loss = 0.09928015\n",
      "Iteration 8240, loss = 0.09927821\n",
      "Iteration 8241, loss = 0.09927626\n",
      "Iteration 8242, loss = 0.09927431\n",
      "Iteration 8243, loss = 0.09927237\n",
      "Iteration 8244, loss = 0.09927042\n",
      "Iteration 8245, loss = 0.09926848\n",
      "Iteration 8246, loss = 0.09926653\n",
      "Iteration 8247, loss = 0.09926459\n",
      "Iteration 8248, loss = 0.09926265\n",
      "Iteration 8249, loss = 0.09926071\n",
      "Iteration 8250, loss = 0.09925876\n",
      "Iteration 8251, loss = 0.09925682\n",
      "Iteration 8252, loss = 0.09925488\n",
      "Iteration 8253, loss = 0.09925294\n",
      "Iteration 8254, loss = 0.09925100\n",
      "Iteration 8255, loss = 0.09924906\n",
      "Iteration 8256, loss = 0.09924712\n",
      "Iteration 8257, loss = 0.09924519\n",
      "Iteration 8258, loss = 0.09924325\n",
      "Iteration 8259, loss = 0.09924131\n",
      "Iteration 8260, loss = 0.09923937\n",
      "Iteration 8261, loss = 0.09923744\n",
      "Iteration 8262, loss = 0.09923550\n",
      "Iteration 8263, loss = 0.09923357\n",
      "Iteration 8264, loss = 0.09923163\n",
      "Iteration 8265, loss = 0.09922970\n",
      "Iteration 8266, loss = 0.09922777\n",
      "Iteration 8267, loss = 0.09922583\n",
      "Iteration 8268, loss = 0.09922390\n",
      "Iteration 8269, loss = 0.09922197\n",
      "Iteration 8270, loss = 0.09922004\n",
      "Iteration 8271, loss = 0.09921811\n",
      "Iteration 8272, loss = 0.09921618\n",
      "Iteration 8273, loss = 0.09921425\n",
      "Iteration 8274, loss = 0.09921232\n",
      "Iteration 8275, loss = 0.09921039\n",
      "Iteration 8276, loss = 0.09920846\n",
      "Iteration 8277, loss = 0.09920653\n",
      "Iteration 8278, loss = 0.09920461\n",
      "Iteration 8279, loss = 0.09920268\n",
      "Iteration 8280, loss = 0.09920075\n",
      "Iteration 8281, loss = 0.09919883\n",
      "Iteration 8282, loss = 0.09919690\n",
      "Iteration 8283, loss = 0.09919498\n",
      "Iteration 8284, loss = 0.09919305\n",
      "Iteration 8285, loss = 0.09919113\n",
      "Iteration 8286, loss = 0.09918921\n",
      "Iteration 8287, loss = 0.09918729\n",
      "Iteration 8288, loss = 0.09918536\n",
      "Iteration 8289, loss = 0.09918344\n",
      "Iteration 8290, loss = 0.09918152\n",
      "Iteration 8291, loss = 0.09917960\n",
      "Iteration 8292, loss = 0.09917768\n",
      "Iteration 8293, loss = 0.09917576\n",
      "Iteration 8294, loss = 0.09917384\n",
      "Iteration 8295, loss = 0.09917192\n",
      "Iteration 8296, loss = 0.09917001\n",
      "Iteration 8297, loss = 0.09916809\n",
      "Iteration 8298, loss = 0.09916617\n",
      "Iteration 8299, loss = 0.09916426\n",
      "Iteration 8300, loss = 0.09916234\n",
      "Iteration 8301, loss = 0.09916043\n",
      "Iteration 8302, loss = 0.09915851\n",
      "Iteration 8303, loss = 0.09915660\n",
      "Iteration 8304, loss = 0.09915468\n",
      "Iteration 8305, loss = 0.09915277\n",
      "Iteration 8306, loss = 0.09915086\n",
      "Iteration 8307, loss = 0.09914895\n",
      "Iteration 8308, loss = 0.09914703\n",
      "Iteration 8309, loss = 0.09914512\n",
      "Iteration 8310, loss = 0.09914321\n",
      "Iteration 8311, loss = 0.09914130\n",
      "Iteration 8312, loss = 0.09913939\n",
      "Iteration 8313, loss = 0.09913748\n",
      "Iteration 8314, loss = 0.09913558\n",
      "Iteration 8315, loss = 0.09913367\n",
      "Iteration 8316, loss = 0.09913176\n",
      "Iteration 8317, loss = 0.09912985\n",
      "Iteration 8318, loss = 0.09912795\n",
      "Iteration 8319, loss = 0.09912604\n",
      "Iteration 8320, loss = 0.09912413\n",
      "Iteration 8321, loss = 0.09912223\n",
      "Iteration 8322, loss = 0.09912033\n",
      "Iteration 8323, loss = 0.09911842\n",
      "Iteration 8324, loss = 0.09911652\n",
      "Iteration 8325, loss = 0.09911462\n",
      "Iteration 8326, loss = 0.09911271\n",
      "Iteration 8327, loss = 0.09911081\n",
      "Iteration 8328, loss = 0.09910891\n",
      "Iteration 8329, loss = 0.09910701\n",
      "Iteration 8330, loss = 0.09910511\n",
      "Iteration 8331, loss = 0.09910321\n",
      "Iteration 8332, loss = 0.09910131\n",
      "Iteration 8333, loss = 0.09909941\n",
      "Iteration 8334, loss = 0.09909751\n",
      "Iteration 8335, loss = 0.09909562\n",
      "Iteration 8336, loss = 0.09909372\n",
      "Iteration 8337, loss = 0.09909182\n",
      "Iteration 8338, loss = 0.09908993\n",
      "Iteration 8339, loss = 0.09908803\n",
      "Iteration 8340, loss = 0.09908613\n",
      "Iteration 8341, loss = 0.09908424\n",
      "Iteration 8342, loss = 0.09908235\n",
      "Iteration 8343, loss = 0.09908045\n",
      "Iteration 8344, loss = 0.09907856\n",
      "Iteration 8345, loss = 0.09907667\n",
      "Iteration 8346, loss = 0.09907477\n",
      "Iteration 8347, loss = 0.09907288\n",
      "Iteration 8348, loss = 0.09907099\n",
      "Iteration 8349, loss = 0.09906910\n",
      "Iteration 8350, loss = 0.09906721\n",
      "Iteration 8351, loss = 0.09906532\n",
      "Iteration 8352, loss = 0.09906343\n",
      "Iteration 8353, loss = 0.09906154\n",
      "Iteration 8354, loss = 0.09905965\n",
      "Iteration 8355, loss = 0.09905777\n",
      "Iteration 8356, loss = 0.09905588\n",
      "Iteration 8357, loss = 0.09905399\n",
      "Iteration 8358, loss = 0.09905211\n",
      "Iteration 8359, loss = 0.09905022\n",
      "Iteration 8360, loss = 0.09904834\n",
      "Iteration 8361, loss = 0.09904645\n",
      "Iteration 8362, loss = 0.09904457\n",
      "Iteration 8363, loss = 0.09904268\n",
      "Iteration 8364, loss = 0.09904080\n",
      "Iteration 8365, loss = 0.09903892\n",
      "Iteration 8366, loss = 0.09903704\n",
      "Iteration 8367, loss = 0.09903516\n",
      "Iteration 8368, loss = 0.09903327\n",
      "Iteration 8369, loss = 0.09903139\n",
      "Iteration 8370, loss = 0.09902951\n",
      "Iteration 8371, loss = 0.09902763\n",
      "Iteration 8372, loss = 0.09902576\n",
      "Iteration 8373, loss = 0.09902388\n",
      "Iteration 8374, loss = 0.09902200\n",
      "Iteration 8375, loss = 0.09902012\n",
      "Iteration 8376, loss = 0.09901824\n",
      "Iteration 8377, loss = 0.09901637\n",
      "Iteration 8378, loss = 0.09901449\n",
      "Iteration 8379, loss = 0.09901262\n",
      "Iteration 8380, loss = 0.09901074\n",
      "Iteration 8381, loss = 0.09900887\n",
      "Iteration 8382, loss = 0.09900699\n",
      "Iteration 8383, loss = 0.09900512\n",
      "Iteration 8384, loss = 0.09900325\n",
      "Iteration 8385, loss = 0.09900137\n",
      "Iteration 8386, loss = 0.09899950\n",
      "Iteration 8387, loss = 0.09899763\n",
      "Iteration 8388, loss = 0.09899576\n",
      "Iteration 8389, loss = 0.09899389\n",
      "Iteration 8390, loss = 0.09899202\n",
      "Iteration 8391, loss = 0.09899015\n",
      "Iteration 8392, loss = 0.09898828\n",
      "Iteration 8393, loss = 0.09898641\n",
      "Iteration 8394, loss = 0.09898454\n",
      "Iteration 8395, loss = 0.09898268\n",
      "Iteration 8396, loss = 0.09898081\n",
      "Iteration 8397, loss = 0.09897894\n",
      "Iteration 8398, loss = 0.09897708\n",
      "Iteration 8399, loss = 0.09897521\n",
      "Iteration 8400, loss = 0.09897335\n",
      "Iteration 8401, loss = 0.09897148\n",
      "Iteration 8402, loss = 0.09896962\n",
      "Iteration 8403, loss = 0.09896775\n",
      "Iteration 8404, loss = 0.09896589\n",
      "Iteration 8405, loss = 0.09896403\n",
      "Iteration 8406, loss = 0.09896216\n",
      "Iteration 8407, loss = 0.09896030\n",
      "Iteration 8408, loss = 0.09895844\n",
      "Iteration 8409, loss = 0.09895658\n",
      "Iteration 8410, loss = 0.09895472\n",
      "Iteration 8411, loss = 0.09895286\n",
      "Iteration 8412, loss = 0.09895100\n",
      "Iteration 8413, loss = 0.09894914\n",
      "Iteration 8414, loss = 0.09894729\n",
      "Iteration 8415, loss = 0.09894543\n",
      "Iteration 8416, loss = 0.09894357\n",
      "Iteration 8417, loss = 0.09894171\n",
      "Iteration 8418, loss = 0.09893986\n",
      "Iteration 8419, loss = 0.09893800\n",
      "Iteration 8420, loss = 0.09893615\n",
      "Iteration 8421, loss = 0.09893429\n",
      "Iteration 8422, loss = 0.09893244\n",
      "Iteration 8423, loss = 0.09893058\n",
      "Iteration 8424, loss = 0.09892873\n",
      "Iteration 8425, loss = 0.09892688\n",
      "Iteration 8426, loss = 0.09892503\n",
      "Iteration 8427, loss = 0.09892317\n",
      "Iteration 8428, loss = 0.09892132\n",
      "Iteration 8429, loss = 0.09891947\n",
      "Iteration 8430, loss = 0.09891762\n",
      "Iteration 8431, loss = 0.09891577\n",
      "Iteration 8432, loss = 0.09891392\n",
      "Iteration 8433, loss = 0.09891207\n",
      "Iteration 8434, loss = 0.09891023\n",
      "Iteration 8435, loss = 0.09890838\n",
      "Iteration 8436, loss = 0.09890653\n",
      "Iteration 8437, loss = 0.09890468\n",
      "Iteration 8438, loss = 0.09890284\n",
      "Iteration 8439, loss = 0.09890099\n",
      "Iteration 8440, loss = 0.09889915\n",
      "Iteration 8441, loss = 0.09889730\n",
      "Iteration 8442, loss = 0.09889546\n",
      "Iteration 8443, loss = 0.09889361\n",
      "Iteration 8444, loss = 0.09889177\n",
      "Iteration 8445, loss = 0.09888993\n",
      "Iteration 8446, loss = 0.09888808\n",
      "Iteration 8447, loss = 0.09888624\n",
      "Iteration 8448, loss = 0.09888440\n",
      "Iteration 8449, loss = 0.09888256\n",
      "Iteration 8450, loss = 0.09888072\n",
      "Iteration 8451, loss = 0.09887888\n",
      "Iteration 8452, loss = 0.09887704\n",
      "Iteration 8453, loss = 0.09887520\n",
      "Iteration 8454, loss = 0.09887336\n",
      "Iteration 8455, loss = 0.09887152\n",
      "Iteration 8456, loss = 0.09886969\n",
      "Iteration 8457, loss = 0.09886785\n",
      "Iteration 8458, loss = 0.09886601\n",
      "Iteration 8459, loss = 0.09886418\n",
      "Iteration 8460, loss = 0.09886234\n",
      "Iteration 8461, loss = 0.09886050\n",
      "Iteration 8462, loss = 0.09885867\n",
      "Iteration 8463, loss = 0.09885684\n",
      "Iteration 8464, loss = 0.09885500\n",
      "Iteration 8465, loss = 0.09885317\n",
      "Iteration 8466, loss = 0.09885134\n",
      "Iteration 8467, loss = 0.09884950\n",
      "Iteration 8468, loss = 0.09884767\n",
      "Iteration 8469, loss = 0.09884584\n",
      "Iteration 8470, loss = 0.09884401\n",
      "Iteration 8471, loss = 0.09884218\n",
      "Iteration 8472, loss = 0.09884035\n",
      "Iteration 8473, loss = 0.09883852\n",
      "Iteration 8474, loss = 0.09883669\n",
      "Iteration 8475, loss = 0.09883486\n",
      "Iteration 8476, loss = 0.09883303\n",
      "Iteration 8477, loss = 0.09883121\n",
      "Iteration 8478, loss = 0.09882938\n",
      "Iteration 8479, loss = 0.09882755\n",
      "Iteration 8480, loss = 0.09882573\n",
      "Iteration 8481, loss = 0.09882390\n",
      "Iteration 8482, loss = 0.09882208\n",
      "Iteration 8483, loss = 0.09882025\n",
      "Iteration 8484, loss = 0.09881843\n",
      "Iteration 8485, loss = 0.09881660\n",
      "Iteration 8486, loss = 0.09881478\n",
      "Iteration 8487, loss = 0.09881296\n",
      "Iteration 8488, loss = 0.09881113\n",
      "Iteration 8489, loss = 0.09880931\n",
      "Iteration 8490, loss = 0.09880749\n",
      "Iteration 8491, loss = 0.09880567\n",
      "Iteration 8492, loss = 0.09880385\n",
      "Iteration 8493, loss = 0.09880203\n",
      "Iteration 8494, loss = 0.09880021\n",
      "Iteration 8495, loss = 0.09879839\n",
      "Iteration 8496, loss = 0.09879657\n",
      "Iteration 8497, loss = 0.09879476\n",
      "Iteration 8498, loss = 0.09879294\n",
      "Iteration 8499, loss = 0.09879112\n",
      "Iteration 8500, loss = 0.09878930\n",
      "Iteration 8501, loss = 0.09878749\n",
      "Iteration 8502, loss = 0.09878567\n",
      "Iteration 8503, loss = 0.09878386\n",
      "Iteration 8504, loss = 0.09878204\n",
      "Iteration 8505, loss = 0.09878023\n",
      "Iteration 8506, loss = 0.09877841\n",
      "Iteration 8507, loss = 0.09877660\n",
      "Iteration 8508, loss = 0.09877479\n",
      "Iteration 8509, loss = 0.09877298\n",
      "Iteration 8510, loss = 0.09877116\n",
      "Iteration 8511, loss = 0.09876935\n",
      "Iteration 8512, loss = 0.09876754\n",
      "Iteration 8513, loss = 0.09876573\n",
      "Iteration 8514, loss = 0.09876392\n",
      "Iteration 8515, loss = 0.09876211\n",
      "Iteration 8516, loss = 0.09876030\n",
      "Iteration 8517, loss = 0.09875850\n",
      "Iteration 8518, loss = 0.09875669\n",
      "Iteration 8519, loss = 0.09875488\n",
      "Iteration 8520, loss = 0.09875307\n",
      "Iteration 8521, loss = 0.09875127\n",
      "Iteration 8522, loss = 0.09874946\n",
      "Iteration 8523, loss = 0.09874765\n",
      "Iteration 8524, loss = 0.09874585\n",
      "Iteration 8525, loss = 0.09874404\n",
      "Iteration 8526, loss = 0.09874224\n",
      "Iteration 8527, loss = 0.09874044\n",
      "Iteration 8528, loss = 0.09873863\n",
      "Iteration 8529, loss = 0.09873683\n",
      "Iteration 8530, loss = 0.09873503\n",
      "Iteration 8531, loss = 0.09873323\n",
      "Iteration 8532, loss = 0.09873142\n",
      "Iteration 8533, loss = 0.09872962\n",
      "Iteration 8534, loss = 0.09872782\n",
      "Iteration 8535, loss = 0.09872602\n",
      "Iteration 8536, loss = 0.09872422\n",
      "Iteration 8537, loss = 0.09872242\n",
      "Iteration 8538, loss = 0.09872062\n",
      "Iteration 8539, loss = 0.09871883\n",
      "Iteration 8540, loss = 0.09871703\n",
      "Iteration 8541, loss = 0.09871523\n",
      "Iteration 8542, loss = 0.09871344\n",
      "Iteration 8543, loss = 0.09871164\n",
      "Iteration 8544, loss = 0.09870984\n",
      "Iteration 8545, loss = 0.09870805\n",
      "Iteration 8546, loss = 0.09870625\n",
      "Iteration 8547, loss = 0.09870446\n",
      "Iteration 8548, loss = 0.09870266\n",
      "Iteration 8549, loss = 0.09870087\n",
      "Iteration 8550, loss = 0.09869908\n",
      "Iteration 8551, loss = 0.09869728\n",
      "Iteration 8552, loss = 0.09869549\n",
      "Iteration 8553, loss = 0.09869370\n",
      "Iteration 8554, loss = 0.09869191\n",
      "Iteration 8555, loss = 0.09869012\n",
      "Iteration 8556, loss = 0.09868833\n",
      "Iteration 8557, loss = 0.09868654\n",
      "Iteration 8558, loss = 0.09868475\n",
      "Iteration 8559, loss = 0.09868296\n",
      "Iteration 8560, loss = 0.09868117\n",
      "Iteration 8561, loss = 0.09867938\n",
      "Iteration 8562, loss = 0.09867760\n",
      "Iteration 8563, loss = 0.09867581\n",
      "Iteration 8564, loss = 0.09867402\n",
      "Iteration 8565, loss = 0.09867224\n",
      "Iteration 8566, loss = 0.09867045\n",
      "Iteration 8567, loss = 0.09866867\n",
      "Iteration 8568, loss = 0.09866688\n",
      "Iteration 8569, loss = 0.09866510\n",
      "Iteration 8570, loss = 0.09866331\n",
      "Iteration 8571, loss = 0.09866153\n",
      "Iteration 8572, loss = 0.09865975\n",
      "Iteration 8573, loss = 0.09865797\n",
      "Iteration 8574, loss = 0.09865618\n",
      "Iteration 8575, loss = 0.09865440\n",
      "Iteration 8576, loss = 0.09865262\n",
      "Iteration 8577, loss = 0.09865084\n",
      "Iteration 8578, loss = 0.09864906\n",
      "Iteration 8579, loss = 0.09864728\n",
      "Iteration 8580, loss = 0.09864550\n",
      "Iteration 8581, loss = 0.09864372\n",
      "Iteration 8582, loss = 0.09864194\n",
      "Iteration 8583, loss = 0.09864017\n",
      "Iteration 8584, loss = 0.09863839\n",
      "Iteration 8585, loss = 0.09863661\n",
      "Iteration 8586, loss = 0.09863484\n",
      "Iteration 8587, loss = 0.09863306\n",
      "Iteration 8588, loss = 0.09863128\n",
      "Iteration 8589, loss = 0.09862951\n",
      "Iteration 8590, loss = 0.09862773\n",
      "Iteration 8591, loss = 0.09862596\n",
      "Iteration 8592, loss = 0.09862419\n",
      "Iteration 8593, loss = 0.09862241\n",
      "Iteration 8594, loss = 0.09862064\n",
      "Iteration 8595, loss = 0.09861887\n",
      "Iteration 8596, loss = 0.09861710\n",
      "Iteration 8597, loss = 0.09861532\n",
      "Iteration 8598, loss = 0.09861355\n",
      "Iteration 8599, loss = 0.09861178\n",
      "Iteration 8600, loss = 0.09861001\n",
      "Iteration 8601, loss = 0.09860824\n",
      "Iteration 8602, loss = 0.09860647\n",
      "Iteration 8603, loss = 0.09860471\n",
      "Iteration 8604, loss = 0.09860294\n",
      "Iteration 8605, loss = 0.09860117\n",
      "Iteration 8606, loss = 0.09859940\n",
      "Iteration 8607, loss = 0.09859764\n",
      "Iteration 8608, loss = 0.09859587\n",
      "Iteration 8609, loss = 0.09859410\n",
      "Iteration 8610, loss = 0.09859234\n",
      "Iteration 8611, loss = 0.09859057\n",
      "Iteration 8612, loss = 0.09858881\n",
      "Iteration 8613, loss = 0.09858704\n",
      "Iteration 8614, loss = 0.09858528\n",
      "Iteration 8615, loss = 0.09858352\n",
      "Iteration 8616, loss = 0.09858175\n",
      "Iteration 8617, loss = 0.09857999\n",
      "Iteration 8618, loss = 0.09857823\n",
      "Iteration 8619, loss = 0.09857647\n",
      "Iteration 8620, loss = 0.09857471\n",
      "Iteration 8621, loss = 0.09857295\n",
      "Iteration 8622, loss = 0.09857119\n",
      "Iteration 8623, loss = 0.09856943\n",
      "Iteration 8624, loss = 0.09856767\n",
      "Iteration 8625, loss = 0.09856591\n",
      "Iteration 8626, loss = 0.09856415\n",
      "Iteration 8627, loss = 0.09856239\n",
      "Iteration 8628, loss = 0.09856064\n",
      "Iteration 8629, loss = 0.09855888\n",
      "Iteration 8630, loss = 0.09855712\n",
      "Iteration 8631, loss = 0.09855537\n",
      "Iteration 8632, loss = 0.09855361\n",
      "Iteration 8633, loss = 0.09855186\n",
      "Iteration 8634, loss = 0.09855010\n",
      "Iteration 8635, loss = 0.09854835\n",
      "Iteration 8636, loss = 0.09854659\n",
      "Iteration 8637, loss = 0.09854484\n",
      "Iteration 8638, loss = 0.09854309\n",
      "Iteration 8639, loss = 0.09854133\n",
      "Iteration 8640, loss = 0.09853958\n",
      "Iteration 8641, loss = 0.09853783\n",
      "Iteration 8642, loss = 0.09853608\n",
      "Iteration 8643, loss = 0.09853433\n",
      "Iteration 8644, loss = 0.09853258\n",
      "Iteration 8645, loss = 0.09853083\n",
      "Iteration 8646, loss = 0.09852908\n",
      "Iteration 8647, loss = 0.09852733\n",
      "Iteration 8648, loss = 0.09852558\n",
      "Iteration 8649, loss = 0.09852384\n",
      "Iteration 8650, loss = 0.09852209\n",
      "Iteration 8651, loss = 0.09852034\n",
      "Iteration 8652, loss = 0.09851859\n",
      "Iteration 8653, loss = 0.09851685\n",
      "Iteration 8654, loss = 0.09851510\n",
      "Iteration 8655, loss = 0.09851336\n",
      "Iteration 8656, loss = 0.09851161\n",
      "Iteration 8657, loss = 0.09850987\n",
      "Iteration 8658, loss = 0.09850812\n",
      "Iteration 8659, loss = 0.09850638\n",
      "Iteration 8660, loss = 0.09850464\n",
      "Iteration 8661, loss = 0.09850289\n",
      "Iteration 8662, loss = 0.09850115\n",
      "Iteration 8663, loss = 0.09849941\n",
      "Iteration 8664, loss = 0.09849767\n",
      "Iteration 8665, loss = 0.09849593\n",
      "Iteration 8666, loss = 0.09849419\n",
      "Iteration 8667, loss = 0.09849245\n",
      "Iteration 8668, loss = 0.09849071\n",
      "Iteration 8669, loss = 0.09848897\n",
      "Iteration 8670, loss = 0.09848723\n",
      "Iteration 8671, loss = 0.09848549\n",
      "Iteration 8672, loss = 0.09848375\n",
      "Iteration 8673, loss = 0.09848202\n",
      "Iteration 8674, loss = 0.09848028\n",
      "Iteration 8675, loss = 0.09847854\n",
      "Iteration 8676, loss = 0.09847681\n",
      "Iteration 8677, loss = 0.09847507\n",
      "Iteration 8678, loss = 0.09847334\n",
      "Iteration 8679, loss = 0.09847160\n",
      "Iteration 8680, loss = 0.09846987\n",
      "Iteration 8681, loss = 0.09846814\n",
      "Iteration 8682, loss = 0.09846640\n",
      "Iteration 8683, loss = 0.09846467\n",
      "Iteration 8684, loss = 0.09846294\n",
      "Iteration 8685, loss = 0.09846120\n",
      "Iteration 8686, loss = 0.09845947\n",
      "Iteration 8687, loss = 0.09845774\n",
      "Iteration 8688, loss = 0.09845601\n",
      "Iteration 8689, loss = 0.09845428\n",
      "Iteration 8690, loss = 0.09845255\n",
      "Iteration 8691, loss = 0.09845082\n",
      "Iteration 8692, loss = 0.09844909\n",
      "Iteration 8693, loss = 0.09844736\n",
      "Iteration 8694, loss = 0.09844564\n",
      "Iteration 8695, loss = 0.09844391\n",
      "Iteration 8696, loss = 0.09844218\n",
      "Iteration 8697, loss = 0.09844045\n",
      "Iteration 8698, loss = 0.09843873\n",
      "Iteration 8699, loss = 0.09843700\n",
      "Iteration 8700, loss = 0.09843528\n",
      "Iteration 8701, loss = 0.09843355\n",
      "Iteration 8702, loss = 0.09843183\n",
      "Iteration 8703, loss = 0.09843010\n",
      "Iteration 8704, loss = 0.09842838\n",
      "Iteration 8705, loss = 0.09842666\n",
      "Iteration 8706, loss = 0.09842493\n",
      "Iteration 8707, loss = 0.09842321\n",
      "Iteration 8708, loss = 0.09842149\n",
      "Iteration 8709, loss = 0.09841977\n",
      "Iteration 8710, loss = 0.09841805\n",
      "Iteration 8711, loss = 0.09841633\n",
      "Iteration 8712, loss = 0.09841461\n",
      "Iteration 8713, loss = 0.09841289\n",
      "Iteration 8714, loss = 0.09841117\n",
      "Iteration 8715, loss = 0.09840945\n",
      "Iteration 8716, loss = 0.09840773\n",
      "Iteration 8717, loss = 0.09840601\n",
      "Iteration 8718, loss = 0.09840429\n",
      "Iteration 8719, loss = 0.09840258\n",
      "Iteration 8720, loss = 0.09840086\n",
      "Iteration 8721, loss = 0.09839914\n",
      "Iteration 8722, loss = 0.09839743\n",
      "Iteration 8723, loss = 0.09839571\n",
      "Iteration 8724, loss = 0.09839400\n",
      "Iteration 8725, loss = 0.09839228\n",
      "Iteration 8726, loss = 0.09839057\n",
      "Iteration 8727, loss = 0.09838886\n",
      "Iteration 8728, loss = 0.09838714\n",
      "Iteration 8729, loss = 0.09838543\n",
      "Iteration 8730, loss = 0.09838372\n",
      "Iteration 8731, loss = 0.09838200\n",
      "Iteration 8732, loss = 0.09838029\n",
      "Iteration 8733, loss = 0.09837858\n",
      "Iteration 8734, loss = 0.09837687\n",
      "Iteration 8735, loss = 0.09837516\n",
      "Iteration 8736, loss = 0.09837345\n",
      "Iteration 8737, loss = 0.09837174\n",
      "Iteration 8738, loss = 0.09837003\n",
      "Iteration 8739, loss = 0.09836832\n",
      "Iteration 8740, loss = 0.09836662\n",
      "Iteration 8741, loss = 0.09836491\n",
      "Iteration 8742, loss = 0.09836320\n",
      "Iteration 8743, loss = 0.09836149\n",
      "Iteration 8744, loss = 0.09835979\n",
      "Iteration 8745, loss = 0.09835808\n",
      "Iteration 8746, loss = 0.09835638\n",
      "Iteration 8747, loss = 0.09835467\n",
      "Iteration 8748, loss = 0.09835297\n",
      "Iteration 8749, loss = 0.09835126\n",
      "Iteration 8750, loss = 0.09834956\n",
      "Iteration 8751, loss = 0.09834786\n",
      "Iteration 8752, loss = 0.09834615\n",
      "Iteration 8753, loss = 0.09834445\n",
      "Iteration 8754, loss = 0.09834275\n",
      "Iteration 8755, loss = 0.09834105\n",
      "Iteration 8756, loss = 0.09833935\n",
      "Iteration 8757, loss = 0.09833764\n",
      "Iteration 8758, loss = 0.09833594\n",
      "Iteration 8759, loss = 0.09833424\n",
      "Iteration 8760, loss = 0.09833254\n",
      "Iteration 8761, loss = 0.09833085\n",
      "Iteration 8762, loss = 0.09832915\n",
      "Iteration 8763, loss = 0.09832745\n",
      "Iteration 8764, loss = 0.09832575\n",
      "Iteration 8765, loss = 0.09832405\n",
      "Iteration 8766, loss = 0.09832236\n",
      "Iteration 8767, loss = 0.09832066\n",
      "Iteration 8768, loss = 0.09831896\n",
      "Iteration 8769, loss = 0.09831727\n",
      "Iteration 8770, loss = 0.09831557\n",
      "Iteration 8771, loss = 0.09831388\n",
      "Iteration 8772, loss = 0.09831218\n",
      "Iteration 8773, loss = 0.09831049\n",
      "Iteration 8774, loss = 0.09830880\n",
      "Iteration 8775, loss = 0.09830710\n",
      "Iteration 8776, loss = 0.09830541\n",
      "Iteration 8777, loss = 0.09830372\n",
      "Iteration 8778, loss = 0.09830203\n",
      "Iteration 8779, loss = 0.09830033\n",
      "Iteration 8780, loss = 0.09829864\n",
      "Iteration 8781, loss = 0.09829695\n",
      "Iteration 8782, loss = 0.09829526\n",
      "Iteration 8783, loss = 0.09829357\n",
      "Iteration 8784, loss = 0.09829188\n",
      "Iteration 8785, loss = 0.09829019\n",
      "Iteration 8786, loss = 0.09828851\n",
      "Iteration 8787, loss = 0.09828682\n",
      "Iteration 8788, loss = 0.09828513\n",
      "Iteration 8789, loss = 0.09828344\n",
      "Iteration 8790, loss = 0.09828176\n",
      "Iteration 8791, loss = 0.09828007\n",
      "Iteration 8792, loss = 0.09827838\n",
      "Iteration 8793, loss = 0.09827670\n",
      "Iteration 8794, loss = 0.09827501\n",
      "Iteration 8795, loss = 0.09827333\n",
      "Iteration 8796, loss = 0.09827164\n",
      "Iteration 8797, loss = 0.09826996\n",
      "Iteration 8798, loss = 0.09826828\n",
      "Iteration 8799, loss = 0.09826659\n",
      "Iteration 8800, loss = 0.09826491\n",
      "Iteration 8801, loss = 0.09826323\n",
      "Iteration 8802, loss = 0.09826155\n",
      "Iteration 8803, loss = 0.09825987\n",
      "Iteration 8804, loss = 0.09825819\n",
      "Iteration 8805, loss = 0.09825650\n",
      "Iteration 8806, loss = 0.09825482\n",
      "Iteration 8807, loss = 0.09825314\n",
      "Iteration 8808, loss = 0.09825147\n",
      "Iteration 8809, loss = 0.09824979\n",
      "Iteration 8810, loss = 0.09824811\n",
      "Iteration 8811, loss = 0.09824643\n",
      "Iteration 8812, loss = 0.09824475\n",
      "Iteration 8813, loss = 0.09824308\n",
      "Iteration 8814, loss = 0.09824140\n",
      "Iteration 8815, loss = 0.09823972\n",
      "Iteration 8816, loss = 0.09823805\n",
      "Iteration 8817, loss = 0.09823637\n",
      "Iteration 8818, loss = 0.09823470\n",
      "Iteration 8819, loss = 0.09823302\n",
      "Iteration 8820, loss = 0.09823135\n",
      "Iteration 8821, loss = 0.09822967\n",
      "Iteration 8822, loss = 0.09822800\n",
      "Iteration 8823, loss = 0.09822633\n",
      "Iteration 8824, loss = 0.09822465\n",
      "Iteration 8825, loss = 0.09822298\n",
      "Iteration 8826, loss = 0.09822131\n",
      "Iteration 8827, loss = 0.09821964\n",
      "Iteration 8828, loss = 0.09821797\n",
      "Iteration 8829, loss = 0.09821630\n",
      "Iteration 8830, loss = 0.09821463\n",
      "Iteration 8831, loss = 0.09821296\n",
      "Iteration 8832, loss = 0.09821129\n",
      "Iteration 8833, loss = 0.09820962\n",
      "Iteration 8834, loss = 0.09820795\n",
      "Iteration 8835, loss = 0.09820628\n",
      "Iteration 8836, loss = 0.09820462\n",
      "Iteration 8837, loss = 0.09820295\n",
      "Iteration 8838, loss = 0.09820128\n",
      "Iteration 8839, loss = 0.09819961\n",
      "Iteration 8840, loss = 0.09819795\n",
      "Iteration 8841, loss = 0.09819628\n",
      "Iteration 8842, loss = 0.09819462\n",
      "Iteration 8843, loss = 0.09819295\n",
      "Iteration 8844, loss = 0.09819129\n",
      "Iteration 8845, loss = 0.09818963\n",
      "Iteration 8846, loss = 0.09818796\n",
      "Iteration 8847, loss = 0.09818630\n",
      "Iteration 8848, loss = 0.09818464\n",
      "Iteration 8849, loss = 0.09818297\n",
      "Iteration 8850, loss = 0.09818131\n",
      "Iteration 8851, loss = 0.09817965\n",
      "Iteration 8852, loss = 0.09817799\n",
      "Iteration 8853, loss = 0.09817633\n",
      "Iteration 8854, loss = 0.09817467\n",
      "Iteration 8855, loss = 0.09817301\n",
      "Iteration 8856, loss = 0.09817135\n",
      "Iteration 8857, loss = 0.09816969\n",
      "Iteration 8858, loss = 0.09816803\n",
      "Iteration 8859, loss = 0.09816637\n",
      "Iteration 8860, loss = 0.09816472\n",
      "Iteration 8861, loss = 0.09816306\n",
      "Iteration 8862, loss = 0.09816140\n",
      "Iteration 8863, loss = 0.09815975\n",
      "Iteration 8864, loss = 0.09815809\n",
      "Iteration 8865, loss = 0.09815643\n",
      "Iteration 8866, loss = 0.09815478\n",
      "Iteration 8867, loss = 0.09815312\n",
      "Iteration 8868, loss = 0.09815147\n",
      "Iteration 8869, loss = 0.09814981\n",
      "Iteration 8870, loss = 0.09814816\n",
      "Iteration 8871, loss = 0.09814651\n",
      "Iteration 8872, loss = 0.09814486\n",
      "Iteration 8873, loss = 0.09814320\n",
      "Iteration 8874, loss = 0.09814155\n",
      "Iteration 8875, loss = 0.09813990\n",
      "Iteration 8876, loss = 0.09813825\n",
      "Iteration 8877, loss = 0.09813660\n",
      "Iteration 8878, loss = 0.09813495\n",
      "Iteration 8879, loss = 0.09813330\n",
      "Iteration 8880, loss = 0.09813165\n",
      "Iteration 8881, loss = 0.09813000\n",
      "Iteration 8882, loss = 0.09812835\n",
      "Iteration 8883, loss = 0.09812670\n",
      "Iteration 8884, loss = 0.09812505\n",
      "Iteration 8885, loss = 0.09812341\n",
      "Iteration 8886, loss = 0.09812176\n",
      "Iteration 8887, loss = 0.09812011\n",
      "Iteration 8888, loss = 0.09811846\n",
      "Iteration 8889, loss = 0.09811682\n",
      "Iteration 8890, loss = 0.09811517\n",
      "Iteration 8891, loss = 0.09811353\n",
      "Iteration 8892, loss = 0.09811188\n",
      "Iteration 8893, loss = 0.09811024\n",
      "Iteration 8894, loss = 0.09810860\n",
      "Iteration 8895, loss = 0.09810695\n",
      "Iteration 8896, loss = 0.09810531\n",
      "Iteration 8897, loss = 0.09810367\n",
      "Iteration 8898, loss = 0.09810202\n",
      "Iteration 8899, loss = 0.09810038\n",
      "Iteration 8900, loss = 0.09809874\n",
      "Iteration 8901, loss = 0.09809710\n",
      "Iteration 8902, loss = 0.09809546\n",
      "Iteration 8903, loss = 0.09809382\n",
      "Iteration 8904, loss = 0.09809218\n",
      "Iteration 8905, loss = 0.09809054\n",
      "Iteration 8906, loss = 0.09808890\n",
      "Iteration 8907, loss = 0.09808726\n",
      "Iteration 8908, loss = 0.09808562\n",
      "Iteration 8909, loss = 0.09808398\n",
      "Iteration 8910, loss = 0.09808235\n",
      "Iteration 8911, loss = 0.09808071\n",
      "Iteration 8912, loss = 0.09807907\n",
      "Iteration 8913, loss = 0.09807744\n",
      "Iteration 8914, loss = 0.09807580\n",
      "Iteration 8915, loss = 0.09807417\n",
      "Iteration 8916, loss = 0.09807253\n",
      "Iteration 8917, loss = 0.09807090\n",
      "Iteration 8918, loss = 0.09806926\n",
      "Iteration 8919, loss = 0.09806763\n",
      "Iteration 8920, loss = 0.09806599\n",
      "Iteration 8921, loss = 0.09806436\n",
      "Iteration 8922, loss = 0.09806273\n",
      "Iteration 8923, loss = 0.09806110\n",
      "Iteration 8924, loss = 0.09805946\n",
      "Iteration 8925, loss = 0.09805783\n",
      "Iteration 8926, loss = 0.09805620\n",
      "Iteration 8927, loss = 0.09805457\n",
      "Iteration 8928, loss = 0.09805294\n",
      "Iteration 8929, loss = 0.09805131\n",
      "Iteration 8930, loss = 0.09804968\n",
      "Iteration 8931, loss = 0.09804805\n",
      "Iteration 8932, loss = 0.09804642\n",
      "Iteration 8933, loss = 0.09804480\n",
      "Iteration 8934, loss = 0.09804317\n",
      "Iteration 8935, loss = 0.09804154\n",
      "Iteration 8936, loss = 0.09803991\n",
      "Iteration 8937, loss = 0.09803829\n",
      "Iteration 8938, loss = 0.09803666\n",
      "Iteration 8939, loss = 0.09803503\n",
      "Iteration 8940, loss = 0.09803341\n",
      "Iteration 8941, loss = 0.09803178\n",
      "Iteration 8942, loss = 0.09803016\n",
      "Iteration 8943, loss = 0.09802853\n",
      "Iteration 8944, loss = 0.09802691\n",
      "Iteration 8945, loss = 0.09802529\n",
      "Iteration 8946, loss = 0.09802366\n",
      "Iteration 8947, loss = 0.09802204\n",
      "Iteration 8948, loss = 0.09802042\n",
      "Iteration 8949, loss = 0.09801880\n",
      "Iteration 8950, loss = 0.09801718\n",
      "Iteration 8951, loss = 0.09801555\n",
      "Iteration 8952, loss = 0.09801393\n",
      "Iteration 8953, loss = 0.09801231\n",
      "Iteration 8954, loss = 0.09801069\n",
      "Iteration 8955, loss = 0.09800907\n",
      "Iteration 8956, loss = 0.09800745\n",
      "Iteration 8957, loss = 0.09800584\n",
      "Iteration 8958, loss = 0.09800422\n",
      "Iteration 8959, loss = 0.09800260\n",
      "Iteration 8960, loss = 0.09800098\n",
      "Iteration 8961, loss = 0.09799936\n",
      "Iteration 8962, loss = 0.09799775\n",
      "Iteration 8963, loss = 0.09799613\n",
      "Iteration 8964, loss = 0.09799452\n",
      "Iteration 8965, loss = 0.09799290\n",
      "Iteration 8966, loss = 0.09799128\n",
      "Iteration 8967, loss = 0.09798967\n",
      "Iteration 8968, loss = 0.09798805\n",
      "Iteration 8969, loss = 0.09798644\n",
      "Iteration 8970, loss = 0.09798483\n",
      "Iteration 8971, loss = 0.09798321\n",
      "Iteration 8972, loss = 0.09798160\n",
      "Iteration 8973, loss = 0.09797999\n",
      "Iteration 8974, loss = 0.09797838\n",
      "Iteration 8975, loss = 0.09797676\n",
      "Iteration 8976, loss = 0.09797515\n",
      "Iteration 8977, loss = 0.09797354\n",
      "Iteration 8978, loss = 0.09797193\n",
      "Iteration 8979, loss = 0.09797032\n",
      "Iteration 8980, loss = 0.09796871\n",
      "Iteration 8981, loss = 0.09796710\n",
      "Iteration 8982, loss = 0.09796549\n",
      "Iteration 8983, loss = 0.09796388\n",
      "Iteration 8984, loss = 0.09796228\n",
      "Iteration 8985, loss = 0.09796067\n",
      "Iteration 8986, loss = 0.09795906\n",
      "Iteration 8987, loss = 0.09795745\n",
      "Iteration 8988, loss = 0.09795585\n",
      "Iteration 8989, loss = 0.09795424\n",
      "Iteration 8990, loss = 0.09795264\n",
      "Iteration 8991, loss = 0.09795103\n",
      "Iteration 8992, loss = 0.09794942\n",
      "Iteration 8993, loss = 0.09794782\n",
      "Iteration 8994, loss = 0.09794622\n",
      "Iteration 8995, loss = 0.09794461\n",
      "Iteration 8996, loss = 0.09794301\n",
      "Iteration 8997, loss = 0.09794140\n",
      "Iteration 8998, loss = 0.09793980\n",
      "Iteration 8999, loss = 0.09793820\n",
      "Iteration 9000, loss = 0.09793660\n",
      "Iteration 9001, loss = 0.09793500\n",
      "Iteration 9002, loss = 0.09793339\n",
      "Iteration 9003, loss = 0.09793179\n",
      "Iteration 9004, loss = 0.09793019\n",
      "Iteration 9005, loss = 0.09792859\n",
      "Iteration 9006, loss = 0.09792699\n",
      "Iteration 9007, loss = 0.09792539\n",
      "Iteration 9008, loss = 0.09792380\n",
      "Iteration 9009, loss = 0.09792220\n",
      "Iteration 9010, loss = 0.09792060\n",
      "Iteration 9011, loss = 0.09791900\n",
      "Iteration 9012, loss = 0.09791740\n",
      "Iteration 9013, loss = 0.09791581\n",
      "Iteration 9014, loss = 0.09791421\n",
      "Iteration 9015, loss = 0.09791261\n",
      "Iteration 9016, loss = 0.09791102\n",
      "Iteration 9017, loss = 0.09790942\n",
      "Iteration 9018, loss = 0.09790783\n",
      "Iteration 9019, loss = 0.09790623\n",
      "Iteration 9020, loss = 0.09790464\n",
      "Iteration 9021, loss = 0.09790304\n",
      "Iteration 9022, loss = 0.09790145\n",
      "Iteration 9023, loss = 0.09789986\n",
      "Iteration 9024, loss = 0.09789827\n",
      "Iteration 9025, loss = 0.09789667\n",
      "Iteration 9026, loss = 0.09789508\n",
      "Iteration 9027, loss = 0.09789349\n",
      "Iteration 9028, loss = 0.09789190\n",
      "Iteration 9029, loss = 0.09789031\n",
      "Iteration 9030, loss = 0.09788872\n",
      "Iteration 9031, loss = 0.09788713\n",
      "Iteration 9032, loss = 0.09788554\n",
      "Iteration 9033, loss = 0.09788395\n",
      "Iteration 9034, loss = 0.09788236\n",
      "Iteration 9035, loss = 0.09788077\n",
      "Iteration 9036, loss = 0.09787918\n",
      "Iteration 9037, loss = 0.09787759\n",
      "Iteration 9038, loss = 0.09787601\n",
      "Iteration 9039, loss = 0.09787442\n",
      "Iteration 9040, loss = 0.09787283\n",
      "Iteration 9041, loss = 0.09787125\n",
      "Iteration 9042, loss = 0.09786966\n",
      "Iteration 9043, loss = 0.09786808\n",
      "Iteration 9044, loss = 0.09786649\n",
      "Iteration 9045, loss = 0.09786491\n",
      "Iteration 9046, loss = 0.09786332\n",
      "Iteration 9047, loss = 0.09786174\n",
      "Iteration 9048, loss = 0.09786015\n",
      "Iteration 9049, loss = 0.09785857\n",
      "Iteration 9050, loss = 0.09785699\n",
      "Iteration 9051, loss = 0.09785540\n",
      "Iteration 9052, loss = 0.09785382\n",
      "Iteration 9053, loss = 0.09785224\n",
      "Iteration 9054, loss = 0.09785066\n",
      "Iteration 9055, loss = 0.09784908\n",
      "Iteration 9056, loss = 0.09784750\n",
      "Iteration 9057, loss = 0.09784592\n",
      "Iteration 9058, loss = 0.09784434\n",
      "Iteration 9059, loss = 0.09784276\n",
      "Iteration 9060, loss = 0.09784118\n",
      "Iteration 9061, loss = 0.09783960\n",
      "Iteration 9062, loss = 0.09783802\n",
      "Iteration 9063, loss = 0.09783644\n",
      "Iteration 9064, loss = 0.09783487\n",
      "Iteration 9065, loss = 0.09783329\n",
      "Iteration 9066, loss = 0.09783171\n",
      "Iteration 9067, loss = 0.09783014\n",
      "Iteration 9068, loss = 0.09782856\n",
      "Iteration 9069, loss = 0.09782698\n",
      "Iteration 9070, loss = 0.09782541\n",
      "Iteration 9071, loss = 0.09782383\n",
      "Iteration 9072, loss = 0.09782226\n",
      "Iteration 9073, loss = 0.09782069\n",
      "Iteration 9074, loss = 0.09781911\n",
      "Iteration 9075, loss = 0.09781754\n",
      "Iteration 9076, loss = 0.09781597\n",
      "Iteration 9077, loss = 0.09781439\n",
      "Iteration 9078, loss = 0.09781282\n",
      "Iteration 9079, loss = 0.09781125\n",
      "Iteration 9080, loss = 0.09780968\n",
      "Iteration 9081, loss = 0.09780811\n",
      "Iteration 9082, loss = 0.09780654\n",
      "Iteration 9083, loss = 0.09780496\n",
      "Iteration 9084, loss = 0.09780339\n",
      "Iteration 9085, loss = 0.09780183\n",
      "Iteration 9086, loss = 0.09780026\n",
      "Iteration 9087, loss = 0.09779869\n",
      "Iteration 9088, loss = 0.09779712\n",
      "Iteration 9089, loss = 0.09779555\n",
      "Iteration 9090, loss = 0.09779398\n",
      "Iteration 9091, loss = 0.09779242\n",
      "Iteration 9092, loss = 0.09779085\n",
      "Iteration 9093, loss = 0.09778928\n",
      "Iteration 9094, loss = 0.09778772\n",
      "Iteration 9095, loss = 0.09778615\n",
      "Iteration 9096, loss = 0.09778458\n",
      "Iteration 9097, loss = 0.09778302\n",
      "Iteration 9098, loss = 0.09778145\n",
      "Iteration 9099, loss = 0.09777989\n",
      "Iteration 9100, loss = 0.09777833\n",
      "Iteration 9101, loss = 0.09777676\n",
      "Iteration 9102, loss = 0.09777520\n",
      "Iteration 9103, loss = 0.09777364\n",
      "Iteration 9104, loss = 0.09777207\n",
      "Iteration 9105, loss = 0.09777051\n",
      "Iteration 9106, loss = 0.09776895\n",
      "Iteration 9107, loss = 0.09776739\n",
      "Iteration 9108, loss = 0.09776583\n",
      "Iteration 9109, loss = 0.09776427\n",
      "Iteration 9110, loss = 0.09776270\n",
      "Iteration 9111, loss = 0.09776114\n",
      "Iteration 9112, loss = 0.09775959\n",
      "Iteration 9113, loss = 0.09775803\n",
      "Iteration 9114, loss = 0.09775647\n",
      "Iteration 9115, loss = 0.09775491\n",
      "Iteration 9116, loss = 0.09775335\n",
      "Iteration 9117, loss = 0.09775179\n",
      "Iteration 9118, loss = 0.09775024\n",
      "Iteration 9119, loss = 0.09774868\n",
      "Iteration 9120, loss = 0.09774712\n",
      "Iteration 9121, loss = 0.09774557\n",
      "Iteration 9122, loss = 0.09774401\n",
      "Iteration 9123, loss = 0.09774245\n",
      "Iteration 9124, loss = 0.09774090\n",
      "Iteration 9125, loss = 0.09773934\n",
      "Iteration 9126, loss = 0.09773779\n",
      "Iteration 9127, loss = 0.09773624\n",
      "Iteration 9128, loss = 0.09773468\n",
      "Iteration 9129, loss = 0.09773313\n",
      "Iteration 9130, loss = 0.09773158\n",
      "Iteration 9131, loss = 0.09773002\n",
      "Iteration 9132, loss = 0.09772847\n",
      "Iteration 9133, loss = 0.09772692\n",
      "Iteration 9134, loss = 0.09772537\n",
      "Iteration 9135, loss = 0.09772382\n",
      "Iteration 9136, loss = 0.09772227\n",
      "Iteration 9137, loss = 0.09772071\n",
      "Iteration 9138, loss = 0.09771916\n",
      "Iteration 9139, loss = 0.09771761\n",
      "Iteration 9140, loss = 0.09771607\n",
      "Iteration 9141, loss = 0.09771452\n",
      "Iteration 9142, loss = 0.09771297\n",
      "Iteration 9143, loss = 0.09771142\n",
      "Iteration 9144, loss = 0.09770987\n",
      "Iteration 9145, loss = 0.09770832\n",
      "Iteration 9146, loss = 0.09770678\n",
      "Iteration 9147, loss = 0.09770523\n",
      "Iteration 9148, loss = 0.09770368\n",
      "Iteration 9149, loss = 0.09770214\n",
      "Iteration 9150, loss = 0.09770059\n",
      "Iteration 9151, loss = 0.09769905\n",
      "Iteration 9152, loss = 0.09769750\n",
      "Iteration 9153, loss = 0.09769596\n",
      "Iteration 9154, loss = 0.09769441\n",
      "Iteration 9155, loss = 0.09769287\n",
      "Iteration 9156, loss = 0.09769133\n",
      "Iteration 9157, loss = 0.09768978\n",
      "Iteration 9158, loss = 0.09768824\n",
      "Iteration 9159, loss = 0.09768670\n",
      "Iteration 9160, loss = 0.09768516\n",
      "Iteration 9161, loss = 0.09768361\n",
      "Iteration 9162, loss = 0.09768207\n",
      "Iteration 9163, loss = 0.09768053\n",
      "Iteration 9164, loss = 0.09767899\n",
      "Iteration 9165, loss = 0.09767745\n",
      "Iteration 9166, loss = 0.09767591\n",
      "Iteration 9167, loss = 0.09767437\n",
      "Iteration 9168, loss = 0.09767283\n",
      "Iteration 9169, loss = 0.09767129\n",
      "Iteration 9170, loss = 0.09766976\n",
      "Iteration 9171, loss = 0.09766822\n",
      "Iteration 9172, loss = 0.09766668\n",
      "Iteration 9173, loss = 0.09766514\n",
      "Iteration 9174, loss = 0.09766361\n",
      "Iteration 9175, loss = 0.09766207\n",
      "Iteration 9176, loss = 0.09766053\n",
      "Iteration 9177, loss = 0.09765900\n",
      "Iteration 9178, loss = 0.09765746\n",
      "Iteration 9179, loss = 0.09765593\n",
      "Iteration 9180, loss = 0.09765439\n",
      "Iteration 9181, loss = 0.09765286\n",
      "Iteration 9182, loss = 0.09765132\n",
      "Iteration 9183, loss = 0.09764979\n",
      "Iteration 9184, loss = 0.09764826\n",
      "Iteration 9185, loss = 0.09764672\n",
      "Iteration 9186, loss = 0.09764519\n",
      "Iteration 9187, loss = 0.09764366\n",
      "Iteration 9188, loss = 0.09764213\n",
      "Iteration 9189, loss = 0.09764060\n",
      "Iteration 9190, loss = 0.09763906\n",
      "Iteration 9191, loss = 0.09763753\n",
      "Iteration 9192, loss = 0.09763600\n",
      "Iteration 9193, loss = 0.09763447\n",
      "Iteration 9194, loss = 0.09763294\n",
      "Iteration 9195, loss = 0.09763141\n",
      "Iteration 9196, loss = 0.09762988\n",
      "Iteration 9197, loss = 0.09762836\n",
      "Iteration 9198, loss = 0.09762683\n",
      "Iteration 9199, loss = 0.09762530\n",
      "Iteration 9200, loss = 0.09762377\n",
      "Iteration 9201, loss = 0.09762224\n",
      "Iteration 9202, loss = 0.09762072\n",
      "Iteration 9203, loss = 0.09761919\n",
      "Iteration 9204, loss = 0.09761767\n",
      "Iteration 9205, loss = 0.09761614\n",
      "Iteration 9206, loss = 0.09761461\n",
      "Iteration 9207, loss = 0.09761309\n",
      "Iteration 9208, loss = 0.09761156\n",
      "Iteration 9209, loss = 0.09761004\n",
      "Iteration 9210, loss = 0.09760852\n",
      "Iteration 9211, loss = 0.09760699\n",
      "Iteration 9212, loss = 0.09760547\n",
      "Iteration 9213, loss = 0.09760395\n",
      "Iteration 9214, loss = 0.09760242\n",
      "Iteration 9215, loss = 0.09760090\n",
      "Iteration 9216, loss = 0.09759938\n",
      "Iteration 9217, loss = 0.09759786\n",
      "Iteration 9218, loss = 0.09759634\n",
      "Iteration 9219, loss = 0.09759482\n",
      "Iteration 9220, loss = 0.09759329\n",
      "Iteration 9221, loss = 0.09759177\n",
      "Iteration 9222, loss = 0.09759026\n",
      "Iteration 9223, loss = 0.09758874\n",
      "Iteration 9224, loss = 0.09758722\n",
      "Iteration 9225, loss = 0.09758570\n",
      "Iteration 9226, loss = 0.09758418\n",
      "Iteration 9227, loss = 0.09758266\n",
      "Iteration 9228, loss = 0.09758114\n",
      "Iteration 9229, loss = 0.09757963\n",
      "Iteration 9230, loss = 0.09757811\n",
      "Iteration 9231, loss = 0.09757659\n",
      "Iteration 9232, loss = 0.09757508\n",
      "Iteration 9233, loss = 0.09757356\n",
      "Iteration 9234, loss = 0.09757205\n",
      "Iteration 9235, loss = 0.09757053\n",
      "Iteration 9236, loss = 0.09756902\n",
      "Iteration 9237, loss = 0.09756750\n",
      "Iteration 9238, loss = 0.09756599\n",
      "Iteration 9239, loss = 0.09756447\n",
      "Iteration 9240, loss = 0.09756296\n",
      "Iteration 9241, loss = 0.09756145\n",
      "Iteration 9242, loss = 0.09755993\n",
      "Iteration 9243, loss = 0.09755842\n",
      "Iteration 9244, loss = 0.09755691\n",
      "Iteration 9245, loss = 0.09755540\n",
      "Iteration 9246, loss = 0.09755389\n",
      "Iteration 9247, loss = 0.09755238\n",
      "Iteration 9248, loss = 0.09755087\n",
      "Iteration 9249, loss = 0.09754936\n",
      "Iteration 9250, loss = 0.09754785\n",
      "Iteration 9251, loss = 0.09754634\n",
      "Iteration 9252, loss = 0.09754483\n",
      "Iteration 9253, loss = 0.09754332\n",
      "Iteration 9254, loss = 0.09754181\n",
      "Iteration 9255, loss = 0.09754030\n",
      "Iteration 9256, loss = 0.09753879\n",
      "Iteration 9257, loss = 0.09753729\n",
      "Iteration 9258, loss = 0.09753578\n",
      "Iteration 9259, loss = 0.09753427\n",
      "Iteration 9260, loss = 0.09753277\n",
      "Iteration 9261, loss = 0.09753126\n",
      "Iteration 9262, loss = 0.09752975\n",
      "Iteration 9263, loss = 0.09752825\n",
      "Iteration 9264, loss = 0.09752674\n",
      "Iteration 9265, loss = 0.09752524\n",
      "Iteration 9266, loss = 0.09752373\n",
      "Iteration 9267, loss = 0.09752223\n",
      "Iteration 9268, loss = 0.09752073\n",
      "Iteration 9269, loss = 0.09751922\n",
      "Iteration 9270, loss = 0.09751772\n",
      "Iteration 9271, loss = 0.09751622\n",
      "Iteration 9272, loss = 0.09751472\n",
      "Iteration 9273, loss = 0.09751321\n",
      "Iteration 9274, loss = 0.09751171\n",
      "Iteration 9275, loss = 0.09751021\n",
      "Iteration 9276, loss = 0.09750871\n",
      "Iteration 9277, loss = 0.09750721\n",
      "Iteration 9278, loss = 0.09750571\n",
      "Iteration 9279, loss = 0.09750421\n",
      "Iteration 9280, loss = 0.09750271\n",
      "Iteration 9281, loss = 0.09750121\n",
      "Iteration 9282, loss = 0.09749971\n",
      "Iteration 9283, loss = 0.09749821\n",
      "Iteration 9284, loss = 0.09749672\n",
      "Iteration 9285, loss = 0.09749522\n",
      "Iteration 9286, loss = 0.09749372\n",
      "Iteration 9287, loss = 0.09749222\n",
      "Iteration 9288, loss = 0.09749073\n",
      "Iteration 9289, loss = 0.09748923\n",
      "Iteration 9290, loss = 0.09748774\n",
      "Iteration 9291, loss = 0.09748624\n",
      "Iteration 9292, loss = 0.09748474\n",
      "Iteration 9293, loss = 0.09748325\n",
      "Iteration 9294, loss = 0.09748176\n",
      "Iteration 9295, loss = 0.09748026\n",
      "Iteration 9296, loss = 0.09747877\n",
      "Iteration 9297, loss = 0.09747727\n",
      "Iteration 9298, loss = 0.09747578\n",
      "Iteration 9299, loss = 0.09747429\n",
      "Iteration 9300, loss = 0.09747280\n",
      "Iteration 9301, loss = 0.09747130\n",
      "Iteration 9302, loss = 0.09746981\n",
      "Iteration 9303, loss = 0.09746832\n",
      "Iteration 9304, loss = 0.09746683\n",
      "Iteration 9305, loss = 0.09746534\n",
      "Iteration 9306, loss = 0.09746385\n",
      "Iteration 9307, loss = 0.09746236\n",
      "Iteration 9308, loss = 0.09746087\n",
      "Iteration 9309, loss = 0.09745938\n",
      "Iteration 9310, loss = 0.09745789\n",
      "Iteration 9311, loss = 0.09745640\n",
      "Iteration 9312, loss = 0.09745491\n",
      "Iteration 9313, loss = 0.09745342\n",
      "Iteration 9314, loss = 0.09745194\n",
      "Iteration 9315, loss = 0.09745045\n",
      "Iteration 9316, loss = 0.09744896\n",
      "Iteration 9317, loss = 0.09744747\n",
      "Iteration 9318, loss = 0.09744599\n",
      "Iteration 9319, loss = 0.09744450\n",
      "Iteration 9320, loss = 0.09744302\n",
      "Iteration 9321, loss = 0.09744153\n",
      "Iteration 9322, loss = 0.09744005\n",
      "Iteration 9323, loss = 0.09743856\n",
      "Iteration 9324, loss = 0.09743708\n",
      "Iteration 9325, loss = 0.09743559\n",
      "Iteration 9326, loss = 0.09743411\n",
      "Iteration 9327, loss = 0.09743263\n",
      "Iteration 9328, loss = 0.09743114\n",
      "Iteration 9329, loss = 0.09742966\n",
      "Iteration 9330, loss = 0.09742818\n",
      "Iteration 9331, loss = 0.09742670\n",
      "Iteration 9332, loss = 0.09742522\n",
      "Iteration 9333, loss = 0.09742373\n",
      "Iteration 9334, loss = 0.09742225\n",
      "Iteration 9335, loss = 0.09742077\n",
      "Iteration 9336, loss = 0.09741929\n",
      "Iteration 9337, loss = 0.09741781\n",
      "Iteration 9338, loss = 0.09741633\n",
      "Iteration 9339, loss = 0.09741485\n",
      "Iteration 9340, loss = 0.09741338\n",
      "Iteration 9341, loss = 0.09741190\n",
      "Iteration 9342, loss = 0.09741042\n",
      "Iteration 9343, loss = 0.09740894\n",
      "Iteration 9344, loss = 0.09740746\n",
      "Iteration 9345, loss = 0.09740599\n",
      "Iteration 9346, loss = 0.09740451\n",
      "Iteration 9347, loss = 0.09740303\n",
      "Iteration 9348, loss = 0.09740156\n",
      "Iteration 9349, loss = 0.09740008\n",
      "Iteration 9350, loss = 0.09739861\n",
      "Iteration 9351, loss = 0.09739713\n",
      "Iteration 9352, loss = 0.09739566\n",
      "Iteration 9353, loss = 0.09739418\n",
      "Iteration 9354, loss = 0.09739271\n",
      "Iteration 9355, loss = 0.09739123\n",
      "Iteration 9356, loss = 0.09738976\n",
      "Iteration 9357, loss = 0.09738829\n",
      "Iteration 9358, loss = 0.09738681\n",
      "Iteration 9359, loss = 0.09738534\n",
      "Iteration 9360, loss = 0.09738387\n",
      "Iteration 9361, loss = 0.09738240\n",
      "Iteration 9362, loss = 0.09738093\n",
      "Iteration 9363, loss = 0.09737945\n",
      "Iteration 9364, loss = 0.09737798\n",
      "Iteration 9365, loss = 0.09737651\n",
      "Iteration 9366, loss = 0.09737504\n",
      "Iteration 9367, loss = 0.09737357\n",
      "Iteration 9368, loss = 0.09737210\n",
      "Iteration 9369, loss = 0.09737063\n",
      "Iteration 9370, loss = 0.09736917\n",
      "Iteration 9371, loss = 0.09736770\n",
      "Iteration 9372, loss = 0.09736623\n",
      "Iteration 9373, loss = 0.09736476\n",
      "Iteration 9374, loss = 0.09736329\n",
      "Iteration 9375, loss = 0.09736183\n",
      "Iteration 9376, loss = 0.09736036\n",
      "Iteration 9377, loss = 0.09735889\n",
      "Iteration 9378, loss = 0.09735743\n",
      "Iteration 9379, loss = 0.09735596\n",
      "Iteration 9380, loss = 0.09735450\n",
      "Iteration 9381, loss = 0.09735303\n",
      "Iteration 9382, loss = 0.09735157\n",
      "Iteration 9383, loss = 0.09735010\n",
      "Iteration 9384, loss = 0.09734864\n",
      "Iteration 9385, loss = 0.09734717\n",
      "Iteration 9386, loss = 0.09734571\n",
      "Iteration 9387, loss = 0.09734425\n",
      "Iteration 9388, loss = 0.09734279\n",
      "Iteration 9389, loss = 0.09734132\n",
      "Iteration 9390, loss = 0.09733986\n",
      "Iteration 9391, loss = 0.09733840\n",
      "Iteration 9392, loss = 0.09733694\n",
      "Iteration 9393, loss = 0.09733548\n",
      "Iteration 9394, loss = 0.09733402\n",
      "Iteration 9395, loss = 0.09733256\n",
      "Iteration 9396, loss = 0.09733110\n",
      "Iteration 9397, loss = 0.09732964\n",
      "Iteration 9398, loss = 0.09732818\n",
      "Iteration 9399, loss = 0.09732672\n",
      "Iteration 9400, loss = 0.09732526\n",
      "Iteration 9401, loss = 0.09732380\n",
      "Iteration 9402, loss = 0.09732234\n",
      "Iteration 9403, loss = 0.09732088\n",
      "Iteration 9404, loss = 0.09731943\n",
      "Iteration 9405, loss = 0.09731797\n",
      "Iteration 9406, loss = 0.09731651\n",
      "Iteration 9407, loss = 0.09731506\n",
      "Iteration 9408, loss = 0.09731360\n",
      "Iteration 9409, loss = 0.09731214\n",
      "Iteration 9410, loss = 0.09731069\n",
      "Iteration 9411, loss = 0.09730923\n",
      "Iteration 9412, loss = 0.09730778\n",
      "Iteration 9413, loss = 0.09730632\n",
      "Iteration 9414, loss = 0.09730487\n",
      "Iteration 9415, loss = 0.09730342\n",
      "Iteration 9416, loss = 0.09730196\n",
      "Iteration 9417, loss = 0.09730051\n",
      "Iteration 9418, loss = 0.09729906\n",
      "Iteration 9419, loss = 0.09729760\n",
      "Iteration 9420, loss = 0.09729615\n",
      "Iteration 9421, loss = 0.09729470\n",
      "Iteration 9422, loss = 0.09729325\n",
      "Iteration 9423, loss = 0.09729180\n",
      "Iteration 9424, loss = 0.09729035\n",
      "Iteration 9425, loss = 0.09728890\n",
      "Iteration 9426, loss = 0.09728745\n",
      "Iteration 9427, loss = 0.09728600\n",
      "Iteration 9428, loss = 0.09728455\n",
      "Iteration 9429, loss = 0.09728310\n",
      "Iteration 9430, loss = 0.09728165\n",
      "Iteration 9431, loss = 0.09728020\n",
      "Iteration 9432, loss = 0.09727875\n",
      "Iteration 9433, loss = 0.09727730\n",
      "Iteration 9434, loss = 0.09727586\n",
      "Iteration 9435, loss = 0.09727441\n",
      "Iteration 9436, loss = 0.09727296\n",
      "Iteration 9437, loss = 0.09727152\n",
      "Iteration 9438, loss = 0.09727007\n",
      "Iteration 9439, loss = 0.09726862\n",
      "Iteration 9440, loss = 0.09726718\n",
      "Iteration 9441, loss = 0.09726573\n",
      "Iteration 9442, loss = 0.09726429\n",
      "Iteration 9443, loss = 0.09726284\n",
      "Iteration 9444, loss = 0.09726140\n",
      "Iteration 9445, loss = 0.09725995\n",
      "Iteration 9446, loss = 0.09725851\n",
      "Iteration 9447, loss = 0.09725707\n",
      "Iteration 9448, loss = 0.09725562\n",
      "Iteration 9449, loss = 0.09725418\n",
      "Iteration 9450, loss = 0.09725274\n",
      "Iteration 9451, loss = 0.09725130\n",
      "Iteration 9452, loss = 0.09724986\n",
      "Iteration 9453, loss = 0.09724841\n",
      "Iteration 9454, loss = 0.09724697\n",
      "Iteration 9455, loss = 0.09724553\n",
      "Iteration 9456, loss = 0.09724409\n",
      "Iteration 9457, loss = 0.09724265\n",
      "Iteration 9458, loss = 0.09724121\n",
      "Iteration 9459, loss = 0.09723977\n",
      "Iteration 9460, loss = 0.09723833\n",
      "Iteration 9461, loss = 0.09723689\n",
      "Iteration 9462, loss = 0.09723546\n",
      "Iteration 9463, loss = 0.09723402\n",
      "Iteration 9464, loss = 0.09723258\n",
      "Iteration 9465, loss = 0.09723114\n",
      "Iteration 9466, loss = 0.09722971\n",
      "Iteration 9467, loss = 0.09722827\n",
      "Iteration 9468, loss = 0.09722683\n",
      "Iteration 9469, loss = 0.09722540\n",
      "Iteration 9470, loss = 0.09722396\n",
      "Iteration 9471, loss = 0.09722252\n",
      "Iteration 9472, loss = 0.09722109\n",
      "Iteration 9473, loss = 0.09721965\n",
      "Iteration 9474, loss = 0.09721822\n",
      "Iteration 9475, loss = 0.09721679\n",
      "Iteration 9476, loss = 0.09721535\n",
      "Iteration 9477, loss = 0.09721392\n",
      "Iteration 9478, loss = 0.09721248\n",
      "Iteration 9479, loss = 0.09721105\n",
      "Iteration 9480, loss = 0.09720962\n",
      "Iteration 9481, loss = 0.09720819\n",
      "Iteration 9482, loss = 0.09720675\n",
      "Iteration 9483, loss = 0.09720532\n",
      "Iteration 9484, loss = 0.09720389\n",
      "Iteration 9485, loss = 0.09720246\n",
      "Iteration 9486, loss = 0.09720103\n",
      "Iteration 9487, loss = 0.09719960\n",
      "Iteration 9488, loss = 0.09719817\n",
      "Iteration 9489, loss = 0.09719674\n",
      "Iteration 9490, loss = 0.09719531\n",
      "Iteration 9491, loss = 0.09719388\n",
      "Iteration 9492, loss = 0.09719245\n",
      "Iteration 9493, loss = 0.09719102\n",
      "Iteration 9494, loss = 0.09718959\n",
      "Iteration 9495, loss = 0.09718817\n",
      "Iteration 9496, loss = 0.09718674\n",
      "Iteration 9497, loss = 0.09718531\n",
      "Iteration 9498, loss = 0.09718388\n",
      "Iteration 9499, loss = 0.09718246\n",
      "Iteration 9500, loss = 0.09718103\n",
      "Iteration 9501, loss = 0.09717961\n",
      "Iteration 9502, loss = 0.09717818\n",
      "Iteration 9503, loss = 0.09717675\n",
      "Iteration 9504, loss = 0.09717533\n",
      "Iteration 9505, loss = 0.09717391\n",
      "Iteration 9506, loss = 0.09717248\n",
      "Iteration 9507, loss = 0.09717106\n",
      "Iteration 9508, loss = 0.09716963\n",
      "Iteration 9509, loss = 0.09716821\n",
      "Iteration 9510, loss = 0.09716679\n",
      "Iteration 9511, loss = 0.09716536\n",
      "Iteration 9512, loss = 0.09716394\n",
      "Iteration 9513, loss = 0.09716252\n",
      "Iteration 9514, loss = 0.09716110\n",
      "Iteration 9515, loss = 0.09715968\n",
      "Iteration 9516, loss = 0.09715825\n",
      "Iteration 9517, loss = 0.09715683\n",
      "Iteration 9518, loss = 0.09715541\n",
      "Iteration 9519, loss = 0.09715399\n",
      "Iteration 9520, loss = 0.09715257\n",
      "Iteration 9521, loss = 0.09715115\n",
      "Iteration 9522, loss = 0.09714973\n",
      "Iteration 9523, loss = 0.09714831\n",
      "Iteration 9524, loss = 0.09714690\n",
      "Iteration 9525, loss = 0.09714548\n",
      "Iteration 9526, loss = 0.09714406\n",
      "Iteration 9527, loss = 0.09714264\n",
      "Iteration 9528, loss = 0.09714122\n",
      "Iteration 9529, loss = 0.09713981\n",
      "Iteration 9530, loss = 0.09713839\n",
      "Iteration 9531, loss = 0.09713697\n",
      "Iteration 9532, loss = 0.09713556\n",
      "Iteration 9533, loss = 0.09713414\n",
      "Iteration 9534, loss = 0.09713273\n",
      "Iteration 9535, loss = 0.09713131\n",
      "Iteration 9536, loss = 0.09712990\n",
      "Iteration 9537, loss = 0.09712848\n",
      "Iteration 9538, loss = 0.09712707\n",
      "Iteration 9539, loss = 0.09712565\n",
      "Iteration 9540, loss = 0.09712424\n",
      "Iteration 9541, loss = 0.09712283\n",
      "Iteration 9542, loss = 0.09712141\n",
      "Iteration 9543, loss = 0.09712000\n",
      "Iteration 9544, loss = 0.09711859\n",
      "Iteration 9545, loss = 0.09711718\n",
      "Iteration 9546, loss = 0.09711576\n",
      "Iteration 9547, loss = 0.09711435\n",
      "Iteration 9548, loss = 0.09711294\n",
      "Iteration 9549, loss = 0.09711153\n",
      "Iteration 9550, loss = 0.09711012\n",
      "Iteration 9551, loss = 0.09710871\n",
      "Iteration 9552, loss = 0.09710730\n",
      "Iteration 9553, loss = 0.09710589\n",
      "Iteration 9554, loss = 0.09710448\n",
      "Iteration 9555, loss = 0.09710307\n",
      "Iteration 9556, loss = 0.09710166\n",
      "Iteration 9557, loss = 0.09710026\n",
      "Iteration 9558, loss = 0.09709885\n",
      "Iteration 9559, loss = 0.09709744\n",
      "Iteration 9560, loss = 0.09709603\n",
      "Iteration 9561, loss = 0.09709463\n",
      "Iteration 9562, loss = 0.09709322\n",
      "Iteration 9563, loss = 0.09709181\n",
      "Iteration 9564, loss = 0.09709041\n",
      "Iteration 9565, loss = 0.09708900\n",
      "Iteration 9566, loss = 0.09708760\n",
      "Iteration 9567, loss = 0.09708619\n",
      "Iteration 9568, loss = 0.09708479\n",
      "Iteration 9569, loss = 0.09708338\n",
      "Iteration 9570, loss = 0.09708198\n",
      "Iteration 9571, loss = 0.09708057\n",
      "Iteration 9572, loss = 0.09707917\n",
      "Iteration 9573, loss = 0.09707777\n",
      "Iteration 9574, loss = 0.09707636\n",
      "Iteration 9575, loss = 0.09707496\n",
      "Iteration 9576, loss = 0.09707356\n",
      "Iteration 9577, loss = 0.09707216\n",
      "Iteration 9578, loss = 0.09707075\n",
      "Iteration 9579, loss = 0.09706935\n",
      "Iteration 9580, loss = 0.09706795\n",
      "Iteration 9581, loss = 0.09706655\n",
      "Iteration 9582, loss = 0.09706515\n",
      "Iteration 9583, loss = 0.09706375\n",
      "Iteration 9584, loss = 0.09706235\n",
      "Iteration 9585, loss = 0.09706095\n",
      "Iteration 9586, loss = 0.09705955\n",
      "Iteration 9587, loss = 0.09705815\n",
      "Iteration 9588, loss = 0.09705675\n",
      "Iteration 9589, loss = 0.09705535\n",
      "Iteration 9590, loss = 0.09705396\n",
      "Iteration 9591, loss = 0.09705256\n",
      "Iteration 9592, loss = 0.09705116\n",
      "Iteration 9593, loss = 0.09704976\n",
      "Iteration 9594, loss = 0.09704837\n",
      "Iteration 9595, loss = 0.09704697\n",
      "Iteration 9596, loss = 0.09704557\n",
      "Iteration 9597, loss = 0.09704418\n",
      "Iteration 9598, loss = 0.09704278\n",
      "Iteration 9599, loss = 0.09704139\n",
      "Iteration 9600, loss = 0.09703999\n",
      "Iteration 9601, loss = 0.09703860\n",
      "Iteration 9602, loss = 0.09703720\n",
      "Iteration 9603, loss = 0.09703581\n",
      "Iteration 9604, loss = 0.09703441\n",
      "Iteration 9605, loss = 0.09703302\n",
      "Iteration 9606, loss = 0.09703163\n",
      "Iteration 9607, loss = 0.09703024\n",
      "Iteration 9608, loss = 0.09702884\n",
      "Iteration 9609, loss = 0.09702745\n",
      "Iteration 9610, loss = 0.09702606\n",
      "Iteration 9611, loss = 0.09702467\n",
      "Iteration 9612, loss = 0.09702328\n",
      "Iteration 9613, loss = 0.09702188\n",
      "Iteration 9614, loss = 0.09702049\n",
      "Iteration 9615, loss = 0.09701910\n",
      "Iteration 9616, loss = 0.09701771\n",
      "Iteration 9617, loss = 0.09701632\n",
      "Iteration 9618, loss = 0.09701493\n",
      "Iteration 9619, loss = 0.09701354\n",
      "Iteration 9620, loss = 0.09701216\n",
      "Iteration 9621, loss = 0.09701077\n",
      "Iteration 9622, loss = 0.09700938\n",
      "Iteration 9623, loss = 0.09700799\n",
      "Iteration 9624, loss = 0.09700660\n",
      "Iteration 9625, loss = 0.09700522\n",
      "Iteration 9626, loss = 0.09700383\n",
      "Iteration 9627, loss = 0.09700244\n",
      "Iteration 9628, loss = 0.09700106\n",
      "Iteration 9629, loss = 0.09699967\n",
      "Iteration 9630, loss = 0.09699828\n",
      "Iteration 9631, loss = 0.09699690\n",
      "Iteration 9632, loss = 0.09699551\n",
      "Iteration 9633, loss = 0.09699413\n",
      "Iteration 9634, loss = 0.09699274\n",
      "Iteration 9635, loss = 0.09699136\n",
      "Iteration 9636, loss = 0.09698997\n",
      "Iteration 9637, loss = 0.09698859\n",
      "Iteration 9638, loss = 0.09698721\n",
      "Iteration 9639, loss = 0.09698582\n",
      "Iteration 9640, loss = 0.09698444\n",
      "Iteration 9641, loss = 0.09698306\n",
      "Iteration 9642, loss = 0.09698168\n",
      "Iteration 9643, loss = 0.09698029\n",
      "Iteration 9644, loss = 0.09697891\n",
      "Iteration 9645, loss = 0.09697753\n",
      "Iteration 9646, loss = 0.09697615\n",
      "Iteration 9647, loss = 0.09697477\n",
      "Iteration 9648, loss = 0.09697339\n",
      "Iteration 9649, loss = 0.09697201\n",
      "Iteration 9650, loss = 0.09697063\n",
      "Iteration 9651, loss = 0.09696925\n",
      "Iteration 9652, loss = 0.09696787\n",
      "Iteration 9653, loss = 0.09696649\n",
      "Iteration 9654, loss = 0.09696511\n",
      "Iteration 9655, loss = 0.09696374\n",
      "Iteration 9656, loss = 0.09696236\n",
      "Iteration 9657, loss = 0.09696098\n",
      "Iteration 9658, loss = 0.09695960\n",
      "Iteration 9659, loss = 0.09695822\n",
      "Iteration 9660, loss = 0.09695685\n",
      "Iteration 9661, loss = 0.09695547\n",
      "Iteration 9662, loss = 0.09695410\n",
      "Iteration 9663, loss = 0.09695272\n",
      "Iteration 9664, loss = 0.09695134\n",
      "Iteration 9665, loss = 0.09694997\n",
      "Iteration 9666, loss = 0.09694859\n",
      "Iteration 9667, loss = 0.09694722\n",
      "Iteration 9668, loss = 0.09694584\n",
      "Iteration 9669, loss = 0.09694447\n",
      "Iteration 9670, loss = 0.09694310\n",
      "Iteration 9671, loss = 0.09694172\n",
      "Iteration 9672, loss = 0.09694035\n",
      "Iteration 9673, loss = 0.09693898\n",
      "Iteration 9674, loss = 0.09693760\n",
      "Iteration 9675, loss = 0.09693623\n",
      "Iteration 9676, loss = 0.09693486\n",
      "Iteration 9677, loss = 0.09693349\n",
      "Iteration 9678, loss = 0.09693212\n",
      "Iteration 9679, loss = 0.09693075\n",
      "Iteration 9680, loss = 0.09692937\n",
      "Iteration 9681, loss = 0.09692800\n",
      "Iteration 9682, loss = 0.09692663\n",
      "Iteration 9683, loss = 0.09692526\n",
      "Iteration 9684, loss = 0.09692389\n",
      "Iteration 9685, loss = 0.09692252\n",
      "Iteration 9686, loss = 0.09692116\n",
      "Iteration 9687, loss = 0.09691979\n",
      "Iteration 9688, loss = 0.09691842\n",
      "Iteration 9689, loss = 0.09691705\n",
      "Iteration 9690, loss = 0.09691568\n",
      "Iteration 9691, loss = 0.09691431\n",
      "Iteration 9692, loss = 0.09691295\n",
      "Iteration 9693, loss = 0.09691158\n",
      "Iteration 9694, loss = 0.09691021\n",
      "Iteration 9695, loss = 0.09690885\n",
      "Iteration 9696, loss = 0.09690748\n",
      "Iteration 9697, loss = 0.09690612\n",
      "Iteration 9698, loss = 0.09690475\n",
      "Iteration 9699, loss = 0.09690339\n",
      "Iteration 9700, loss = 0.09690202\n",
      "Iteration 9701, loss = 0.09690066\n",
      "Iteration 9702, loss = 0.09689929\n",
      "Iteration 9703, loss = 0.09689793\n",
      "Iteration 9704, loss = 0.09689656\n",
      "Iteration 9705, loss = 0.09689520\n",
      "Iteration 9706, loss = 0.09689384\n",
      "Iteration 9707, loss = 0.09689247\n",
      "Iteration 9708, loss = 0.09689111\n",
      "Iteration 9709, loss = 0.09688975\n",
      "Iteration 9710, loss = 0.09688839\n",
      "Iteration 9711, loss = 0.09688703\n",
      "Iteration 9712, loss = 0.09688566\n",
      "Iteration 9713, loss = 0.09688430\n",
      "Iteration 9714, loss = 0.09688294\n",
      "Iteration 9715, loss = 0.09688158\n",
      "Iteration 9716, loss = 0.09688022\n",
      "Iteration 9717, loss = 0.09687886\n",
      "Iteration 9718, loss = 0.09687750\n",
      "Iteration 9719, loss = 0.09687614\n",
      "Iteration 9720, loss = 0.09687478\n",
      "Iteration 9721, loss = 0.09687343\n",
      "Iteration 9722, loss = 0.09687207\n",
      "Iteration 9723, loss = 0.09687071\n",
      "Iteration 9724, loss = 0.09686935\n",
      "Iteration 9725, loss = 0.09686799\n",
      "Iteration 9726, loss = 0.09686664\n",
      "Iteration 9727, loss = 0.09686528\n",
      "Iteration 9728, loss = 0.09686392\n",
      "Iteration 9729, loss = 0.09686257\n",
      "Iteration 9730, loss = 0.09686121\n",
      "Iteration 9731, loss = 0.09685985\n",
      "Iteration 9732, loss = 0.09685850\n",
      "Iteration 9733, loss = 0.09685714\n",
      "Iteration 9734, loss = 0.09685579\n",
      "Iteration 9735, loss = 0.09685443\n",
      "Iteration 9736, loss = 0.09685308\n",
      "Iteration 9737, loss = 0.09685173\n",
      "Iteration 9738, loss = 0.09685037\n",
      "Iteration 9739, loss = 0.09684902\n",
      "Iteration 9740, loss = 0.09684767\n",
      "Iteration 9741, loss = 0.09684631\n",
      "Iteration 9742, loss = 0.09684496\n",
      "Iteration 9743, loss = 0.09684361\n",
      "Iteration 9744, loss = 0.09684226\n",
      "Iteration 9745, loss = 0.09684090\n",
      "Iteration 9746, loss = 0.09683955\n",
      "Iteration 9747, loss = 0.09683820\n",
      "Iteration 9748, loss = 0.09683685\n",
      "Iteration 9749, loss = 0.09683550\n",
      "Iteration 9750, loss = 0.09683415\n",
      "Iteration 9751, loss = 0.09683280\n",
      "Iteration 9752, loss = 0.09683145\n",
      "Iteration 9753, loss = 0.09683010\n",
      "Iteration 9754, loss = 0.09682875\n",
      "Iteration 9755, loss = 0.09682740\n",
      "Iteration 9756, loss = 0.09682605\n",
      "Iteration 9757, loss = 0.09682471\n",
      "Iteration 9758, loss = 0.09682336\n",
      "Iteration 9759, loss = 0.09682201\n",
      "Iteration 9760, loss = 0.09682066\n",
      "Iteration 9761, loss = 0.09681932\n",
      "Iteration 9762, loss = 0.09681797\n",
      "Iteration 9763, loss = 0.09681662\n",
      "Iteration 9764, loss = 0.09681528\n",
      "Iteration 9765, loss = 0.09681393\n",
      "Iteration 9766, loss = 0.09681259\n",
      "Iteration 9767, loss = 0.09681124\n",
      "Iteration 9768, loss = 0.09680989\n",
      "Iteration 9769, loss = 0.09680855\n",
      "Iteration 9770, loss = 0.09680721\n",
      "Iteration 9771, loss = 0.09680586\n",
      "Iteration 9772, loss = 0.09680452\n",
      "Iteration 9773, loss = 0.09680317\n",
      "Iteration 9774, loss = 0.09680183\n",
      "Iteration 9775, loss = 0.09680049\n",
      "Iteration 9776, loss = 0.09679914\n",
      "Iteration 9777, loss = 0.09679780\n",
      "Iteration 9778, loss = 0.09679646\n",
      "Iteration 9779, loss = 0.09679512\n",
      "Iteration 9780, loss = 0.09679378\n",
      "Iteration 9781, loss = 0.09679244\n",
      "Iteration 9782, loss = 0.09679109\n",
      "Iteration 9783, loss = 0.09678975\n",
      "Iteration 9784, loss = 0.09678841\n",
      "Iteration 9785, loss = 0.09678707\n",
      "Iteration 9786, loss = 0.09678573\n",
      "Iteration 9787, loss = 0.09678439\n",
      "Iteration 9788, loss = 0.09678305\n",
      "Iteration 9789, loss = 0.09678172\n",
      "Iteration 9790, loss = 0.09678038\n",
      "Iteration 9791, loss = 0.09677904\n",
      "Iteration 9792, loss = 0.09677770\n",
      "Iteration 9793, loss = 0.09677636\n",
      "Iteration 9794, loss = 0.09677502\n",
      "Iteration 9795, loss = 0.09677369\n",
      "Iteration 9796, loss = 0.09677235\n",
      "Iteration 9797, loss = 0.09677101\n",
      "Iteration 9798, loss = 0.09676968\n",
      "Iteration 9799, loss = 0.09676834\n",
      "Iteration 9800, loss = 0.09676700\n",
      "Iteration 9801, loss = 0.09676567\n",
      "Iteration 9802, loss = 0.09676433\n",
      "Iteration 9803, loss = 0.09676300\n",
      "Iteration 9804, loss = 0.09676166\n",
      "Iteration 9805, loss = 0.09676033\n",
      "Iteration 9806, loss = 0.09675900\n",
      "Iteration 9807, loss = 0.09675766\n",
      "Iteration 9808, loss = 0.09675633\n",
      "Iteration 9809, loss = 0.09675499\n",
      "Iteration 9810, loss = 0.09675366\n",
      "Iteration 9811, loss = 0.09675233\n",
      "Iteration 9812, loss = 0.09675100\n",
      "Iteration 9813, loss = 0.09674966\n",
      "Iteration 9814, loss = 0.09674833\n",
      "Iteration 9815, loss = 0.09674700\n",
      "Iteration 9816, loss = 0.09674567\n",
      "Iteration 9817, loss = 0.09674434\n",
      "Iteration 9818, loss = 0.09674301\n",
      "Iteration 9819, loss = 0.09674168\n",
      "Iteration 9820, loss = 0.09674035\n",
      "Iteration 9821, loss = 0.09673902\n",
      "Iteration 9822, loss = 0.09673769\n",
      "Iteration 9823, loss = 0.09673636\n",
      "Iteration 9824, loss = 0.09673503\n",
      "Iteration 9825, loss = 0.09673370\n",
      "Iteration 9826, loss = 0.09673237\n",
      "Iteration 9827, loss = 0.09673104\n",
      "Iteration 9828, loss = 0.09672972\n",
      "Iteration 9829, loss = 0.09672839\n",
      "Iteration 9830, loss = 0.09672706\n",
      "Iteration 9831, loss = 0.09672573\n",
      "Iteration 9832, loss = 0.09672441\n",
      "Iteration 9833, loss = 0.09672308\n",
      "Iteration 9834, loss = 0.09672175\n",
      "Iteration 9835, loss = 0.09672043\n",
      "Iteration 9836, loss = 0.09671910\n",
      "Iteration 9837, loss = 0.09671778\n",
      "Iteration 9838, loss = 0.09671645\n",
      "Iteration 9839, loss = 0.09671513\n",
      "Iteration 9840, loss = 0.09671380\n",
      "Iteration 9841, loss = 0.09671248\n",
      "Iteration 9842, loss = 0.09671115\n",
      "Iteration 9843, loss = 0.09670983\n",
      "Iteration 9844, loss = 0.09670851\n",
      "Iteration 9845, loss = 0.09670718\n",
      "Iteration 9846, loss = 0.09670586\n",
      "Iteration 9847, loss = 0.09670454\n",
      "Iteration 9848, loss = 0.09670322\n",
      "Iteration 9849, loss = 0.09670189\n",
      "Iteration 9850, loss = 0.09670057\n",
      "Iteration 9851, loss = 0.09669925\n",
      "Iteration 9852, loss = 0.09669793\n",
      "Iteration 9853, loss = 0.09669661\n",
      "Iteration 9854, loss = 0.09669529\n",
      "Iteration 9855, loss = 0.09669397\n",
      "Iteration 9856, loss = 0.09669265\n",
      "Iteration 9857, loss = 0.09669133\n",
      "Iteration 9858, loss = 0.09669001\n",
      "Iteration 9859, loss = 0.09668869\n",
      "Iteration 9860, loss = 0.09668737\n",
      "Iteration 9861, loss = 0.09668605\n",
      "Iteration 9862, loss = 0.09668473\n",
      "Iteration 9863, loss = 0.09668341\n",
      "Iteration 9864, loss = 0.09668210\n",
      "Iteration 9865, loss = 0.09668078\n",
      "Iteration 9866, loss = 0.09667946\n",
      "Iteration 9867, loss = 0.09667814\n",
      "Iteration 9868, loss = 0.09667683\n",
      "Iteration 9869, loss = 0.09667551\n",
      "Iteration 9870, loss = 0.09667420\n",
      "Iteration 9871, loss = 0.09667288\n",
      "Iteration 9872, loss = 0.09667156\n",
      "Iteration 9873, loss = 0.09667025\n",
      "Iteration 9874, loss = 0.09666893\n",
      "Iteration 9875, loss = 0.09666762\n",
      "Iteration 9876, loss = 0.09666630\n",
      "Iteration 9877, loss = 0.09666499\n",
      "Iteration 9878, loss = 0.09666368\n",
      "Iteration 9879, loss = 0.09666236\n",
      "Iteration 9880, loss = 0.09666105\n",
      "Iteration 9881, loss = 0.09665974\n",
      "Iteration 9882, loss = 0.09665842\n",
      "Iteration 9883, loss = 0.09665711\n",
      "Iteration 9884, loss = 0.09665580\n",
      "Iteration 9885, loss = 0.09665449\n",
      "Iteration 9886, loss = 0.09665317\n",
      "Iteration 9887, loss = 0.09665186\n",
      "Iteration 9888, loss = 0.09665055\n",
      "Iteration 9889, loss = 0.09664924\n",
      "Iteration 9890, loss = 0.09664793\n",
      "Iteration 9891, loss = 0.09664662\n",
      "Iteration 9892, loss = 0.09664531\n",
      "Iteration 9893, loss = 0.09664400\n",
      "Iteration 9894, loss = 0.09664269\n",
      "Iteration 9895, loss = 0.09664138\n",
      "Iteration 9896, loss = 0.09664007\n",
      "Iteration 9897, loss = 0.09663876\n",
      "Iteration 9898, loss = 0.09663745\n",
      "Iteration 9899, loss = 0.09663615\n",
      "Iteration 9900, loss = 0.09663484\n",
      "Iteration 9901, loss = 0.09663353\n",
      "Iteration 9902, loss = 0.09663222\n",
      "Iteration 9903, loss = 0.09663092\n",
      "Iteration 9904, loss = 0.09662961\n",
      "Iteration 9905, loss = 0.09662830\n",
      "Iteration 9906, loss = 0.09662700\n",
      "Iteration 9907, loss = 0.09662569\n",
      "Iteration 9908, loss = 0.09662438\n",
      "Iteration 9909, loss = 0.09662308\n",
      "Iteration 9910, loss = 0.09662177\n",
      "Iteration 9911, loss = 0.09662047\n",
      "Iteration 9912, loss = 0.09661916\n",
      "Iteration 9913, loss = 0.09661786\n",
      "Iteration 9914, loss = 0.09661656\n",
      "Iteration 9915, loss = 0.09661525\n",
      "Iteration 9916, loss = 0.09661395\n",
      "Iteration 9917, loss = 0.09661265\n",
      "Iteration 9918, loss = 0.09661134\n",
      "Iteration 9919, loss = 0.09661004\n",
      "Iteration 9920, loss = 0.09660874\n",
      "Iteration 9921, loss = 0.09660744\n",
      "Iteration 9922, loss = 0.09660613\n",
      "Iteration 9923, loss = 0.09660483\n",
      "Iteration 9924, loss = 0.09660353\n",
      "Iteration 9925, loss = 0.09660223\n",
      "Iteration 9926, loss = 0.09660093\n",
      "Iteration 9927, loss = 0.09659963\n",
      "Iteration 9928, loss = 0.09659833\n",
      "Iteration 9929, loss = 0.09659703\n",
      "Iteration 9930, loss = 0.09659573\n",
      "Iteration 9931, loss = 0.09659443\n",
      "Iteration 9932, loss = 0.09659313\n",
      "Iteration 9933, loss = 0.09659183\n",
      "Iteration 9934, loss = 0.09659053\n",
      "Iteration 9935, loss = 0.09658923\n",
      "Iteration 9936, loss = 0.09658794\n",
      "Iteration 9937, loss = 0.09658664\n",
      "Iteration 9938, loss = 0.09658534\n",
      "Iteration 9939, loss = 0.09658404\n",
      "Iteration 9940, loss = 0.09658275\n",
      "Iteration 9941, loss = 0.09658145\n",
      "Iteration 9942, loss = 0.09658015\n",
      "Iteration 9943, loss = 0.09657886\n",
      "Iteration 9944, loss = 0.09657756\n",
      "Iteration 9945, loss = 0.09657626\n",
      "Iteration 9946, loss = 0.09657497\n",
      "Iteration 9947, loss = 0.09657367\n",
      "Iteration 9948, loss = 0.09657238\n",
      "Iteration 9949, loss = 0.09657109\n",
      "Iteration 9950, loss = 0.09656979\n",
      "Iteration 9951, loss = 0.09656850\n",
      "Iteration 9952, loss = 0.09656720\n",
      "Iteration 9953, loss = 0.09656591\n",
      "Iteration 9954, loss = 0.09656462\n",
      "Iteration 9955, loss = 0.09656332\n",
      "Iteration 9956, loss = 0.09656203\n",
      "Iteration 9957, loss = 0.09656074\n",
      "Iteration 9958, loss = 0.09655945\n",
      "Iteration 9959, loss = 0.09655815\n",
      "Iteration 9960, loss = 0.09655686\n",
      "Iteration 9961, loss = 0.09655557\n",
      "Iteration 9962, loss = 0.09655428\n",
      "Iteration 9963, loss = 0.09655299\n",
      "Iteration 9964, loss = 0.09655170\n",
      "Iteration 9965, loss = 0.09655041\n",
      "Iteration 9966, loss = 0.09654912\n",
      "Iteration 9967, loss = 0.09654783\n",
      "Iteration 9968, loss = 0.09654654\n",
      "Iteration 9969, loss = 0.09654525\n",
      "Iteration 9970, loss = 0.09654396\n",
      "Iteration 9971, loss = 0.09654267\n",
      "Iteration 9972, loss = 0.09654138\n",
      "Iteration 9973, loss = 0.09654010\n",
      "Iteration 9974, loss = 0.09653881\n",
      "Iteration 9975, loss = 0.09653752\n",
      "Iteration 9976, loss = 0.09653623\n",
      "Iteration 9977, loss = 0.09653495\n",
      "Iteration 9978, loss = 0.09653366\n",
      "Iteration 9979, loss = 0.09653237\n",
      "Iteration 9980, loss = 0.09653109\n",
      "Iteration 9981, loss = 0.09652980\n",
      "Iteration 9982, loss = 0.09652851\n",
      "Iteration 9983, loss = 0.09652723\n",
      "Iteration 9984, loss = 0.09652594\n",
      "Iteration 9985, loss = 0.09652466\n",
      "Iteration 9986, loss = 0.09652337\n",
      "Iteration 9987, loss = 0.09652209\n",
      "Iteration 9988, loss = 0.09652081\n",
      "Iteration 9989, loss = 0.09651952\n",
      "Iteration 9990, loss = 0.09651824\n",
      "Iteration 9991, loss = 0.09651696\n",
      "Iteration 9992, loss = 0.09651567\n",
      "Iteration 9993, loss = 0.09651439\n",
      "Iteration 9994, loss = 0.09651311\n",
      "Iteration 9995, loss = 0.09651182\n",
      "Iteration 9996, loss = 0.09651054\n",
      "Iteration 9997, loss = 0.09650926\n",
      "Iteration 9998, loss = 0.09650798\n",
      "Iteration 9999, loss = 0.09650670\n",
      "Iteration 10000, loss = 0.09650542\n",
      "Iteration 10001, loss = 0.09650414\n",
      "Iteration 10002, loss = 0.09650286\n",
      "Iteration 10003, loss = 0.09650158\n",
      "Iteration 10004, loss = 0.09650030\n",
      "Iteration 10005, loss = 0.09649902\n",
      "Iteration 10006, loss = 0.09649774\n",
      "Iteration 10007, loss = 0.09649646\n",
      "Iteration 10008, loss = 0.09649518\n",
      "Iteration 10009, loss = 0.09649390\n",
      "Iteration 10010, loss = 0.09649262\n",
      "Iteration 10011, loss = 0.09649134\n",
      "Iteration 10012, loss = 0.09649007\n",
      "Iteration 10013, loss = 0.09648879\n",
      "Iteration 10014, loss = 0.09648751\n",
      "Iteration 10015, loss = 0.09648624\n",
      "Iteration 10016, loss = 0.09648496\n",
      "Iteration 10017, loss = 0.09648368\n",
      "Iteration 10018, loss = 0.09648241\n",
      "Iteration 10019, loss = 0.09648113\n",
      "Iteration 10020, loss = 0.09647985\n",
      "Iteration 10021, loss = 0.09647858\n",
      "Iteration 10022, loss = 0.09647730\n",
      "Iteration 10023, loss = 0.09647603\n",
      "Iteration 10024, loss = 0.09647475\n",
      "Iteration 10025, loss = 0.09647348\n",
      "Iteration 10026, loss = 0.09647221\n",
      "Iteration 10027, loss = 0.09647093\n",
      "Iteration 10028, loss = 0.09646966\n",
      "Iteration 10029, loss = 0.09646839\n",
      "Iteration 10030, loss = 0.09646711\n",
      "Iteration 10031, loss = 0.09646584\n",
      "Iteration 10032, loss = 0.09646457\n",
      "Iteration 10033, loss = 0.09646329\n",
      "Iteration 10034, loss = 0.09646202\n",
      "Iteration 10035, loss = 0.09646075\n",
      "Iteration 10036, loss = 0.09645948\n",
      "Iteration 10037, loss = 0.09645821\n",
      "Iteration 10038, loss = 0.09645694\n",
      "Iteration 10039, loss = 0.09645567\n",
      "Iteration 10040, loss = 0.09645440\n",
      "Iteration 10041, loss = 0.09645313\n",
      "Iteration 10042, loss = 0.09645186\n",
      "Iteration 10043, loss = 0.09645059\n",
      "Iteration 10044, loss = 0.09644932\n",
      "Iteration 10045, loss = 0.09644805\n",
      "Iteration 10046, loss = 0.09644678\n",
      "Iteration 10047, loss = 0.09644551\n",
      "Iteration 10048, loss = 0.09644424\n",
      "Iteration 10049, loss = 0.09644297\n",
      "Iteration 10050, loss = 0.09644171\n",
      "Iteration 10051, loss = 0.09644044\n",
      "Iteration 10052, loss = 0.09643917\n",
      "Iteration 10053, loss = 0.09643790\n",
      "Iteration 10054, loss = 0.09643664\n",
      "Iteration 10055, loss = 0.09643537\n",
      "Iteration 10056, loss = 0.09643410\n",
      "Iteration 10057, loss = 0.09643284\n",
      "Iteration 10058, loss = 0.09643157\n",
      "Iteration 10059, loss = 0.09643031\n",
      "Iteration 10060, loss = 0.09642904\n",
      "Iteration 10061, loss = 0.09642778\n",
      "Iteration 10062, loss = 0.09642651\n",
      "Iteration 10063, loss = 0.09642525\n",
      "Iteration 10064, loss = 0.09642398\n",
      "Iteration 10065, loss = 0.09642272\n",
      "Iteration 10066, loss = 0.09642146\n",
      "Iteration 10067, loss = 0.09642019\n",
      "Iteration 10068, loss = 0.09641893\n",
      "Iteration 10069, loss = 0.09641767\n",
      "Iteration 10070, loss = 0.09641640\n",
      "Iteration 10071, loss = 0.09641514\n",
      "Iteration 10072, loss = 0.09641388\n",
      "Iteration 10073, loss = 0.09641262\n",
      "Iteration 10074, loss = 0.09641136\n",
      "Iteration 10075, loss = 0.09641010\n",
      "Iteration 10076, loss = 0.09640883\n",
      "Iteration 10077, loss = 0.09640757\n",
      "Iteration 10078, loss = 0.09640631\n",
      "Iteration 10079, loss = 0.09640505\n",
      "Iteration 10080, loss = 0.09640379\n",
      "Iteration 10081, loss = 0.09640253\n",
      "Iteration 10082, loss = 0.09640127\n",
      "Iteration 10083, loss = 0.09640001\n",
      "Iteration 10084, loss = 0.09639875\n",
      "Iteration 10085, loss = 0.09639750\n",
      "Iteration 10086, loss = 0.09639624\n",
      "Iteration 10087, loss = 0.09639498\n",
      "Iteration 10088, loss = 0.09639372\n",
      "Iteration 10089, loss = 0.09639246\n",
      "Iteration 10090, loss = 0.09639121\n",
      "Iteration 10091, loss = 0.09638995\n",
      "Iteration 10092, loss = 0.09638869\n",
      "Iteration 10093, loss = 0.09638744\n",
      "Iteration 10094, loss = 0.09638618\n",
      "Iteration 10095, loss = 0.09638492\n",
      "Iteration 10096, loss = 0.09638367\n",
      "Iteration 10097, loss = 0.09638241\n",
      "Iteration 10098, loss = 0.09638116\n",
      "Iteration 10099, loss = 0.09637990\n",
      "Iteration 10100, loss = 0.09637865\n",
      "Iteration 10101, loss = 0.09637739\n",
      "Iteration 10102, loss = 0.09637614\n",
      "Iteration 10103, loss = 0.09637488\n",
      "Iteration 10104, loss = 0.09637363\n",
      "Iteration 10105, loss = 0.09637237\n",
      "Iteration 10106, loss = 0.09637112\n",
      "Iteration 10107, loss = 0.09636987\n",
      "Iteration 10108, loss = 0.09636862\n",
      "Iteration 10109, loss = 0.09636736\n",
      "Iteration 10110, loss = 0.09636611\n",
      "Iteration 10111, loss = 0.09636486\n",
      "Iteration 10112, loss = 0.09636361\n",
      "Iteration 10113, loss = 0.09636235\n",
      "Iteration 10114, loss = 0.09636110\n",
      "Iteration 10115, loss = 0.09635985\n",
      "Iteration 10116, loss = 0.09635860\n",
      "Iteration 10117, loss = 0.09635735\n",
      "Iteration 10118, loss = 0.09635610\n",
      "Iteration 10119, loss = 0.09635485\n",
      "Iteration 10120, loss = 0.09635360\n",
      "Iteration 10121, loss = 0.09635235\n",
      "Iteration 10122, loss = 0.09635110\n",
      "Iteration 10123, loss = 0.09634985\n",
      "Iteration 10124, loss = 0.09634860\n",
      "Iteration 10125, loss = 0.09634736\n",
      "Iteration 10126, loss = 0.09634611\n",
      "Iteration 10127, loss = 0.09634486\n",
      "Iteration 10128, loss = 0.09634361\n",
      "Iteration 10129, loss = 0.09634236\n",
      "Iteration 10130, loss = 0.09634112\n",
      "Iteration 10131, loss = 0.09633987\n",
      "Iteration 10132, loss = 0.09633862\n",
      "Iteration 10133, loss = 0.09633738\n",
      "Iteration 10134, loss = 0.09633613\n",
      "Iteration 10135, loss = 0.09633488\n",
      "Iteration 10136, loss = 0.09633364\n",
      "Iteration 10137, loss = 0.09633239\n",
      "Iteration 10138, loss = 0.09633115\n",
      "Iteration 10139, loss = 0.09632990\n",
      "Iteration 10140, loss = 0.09632866\n",
      "Iteration 10141, loss = 0.09632741\n",
      "Iteration 10142, loss = 0.09632617\n",
      "Iteration 10143, loss = 0.09632492\n",
      "Iteration 10144, loss = 0.09632368\n",
      "Iteration 10145, loss = 0.09632244\n",
      "Iteration 10146, loss = 0.09632119\n",
      "Iteration 10147, loss = 0.09631995\n",
      "Iteration 10148, loss = 0.09631871\n",
      "Iteration 10149, loss = 0.09631747\n",
      "Iteration 10150, loss = 0.09631622\n",
      "Iteration 10151, loss = 0.09631498\n",
      "Iteration 10152, loss = 0.09631374\n",
      "Iteration 10153, loss = 0.09631250\n",
      "Iteration 10154, loss = 0.09631126\n",
      "Iteration 10155, loss = 0.09631002\n",
      "Iteration 10156, loss = 0.09630877\n",
      "Iteration 10157, loss = 0.09630753\n",
      "Iteration 10158, loss = 0.09630629\n",
      "Iteration 10159, loss = 0.09630505\n",
      "Iteration 10160, loss = 0.09630381\n",
      "Iteration 10161, loss = 0.09630257\n",
      "Iteration 10162, loss = 0.09630134\n",
      "Iteration 10163, loss = 0.09630010\n",
      "Iteration 10164, loss = 0.09629886\n",
      "Iteration 10165, loss = 0.09629762\n",
      "Iteration 10166, loss = 0.09629638\n",
      "Iteration 10167, loss = 0.09629514\n",
      "Iteration 10168, loss = 0.09629391\n",
      "Iteration 10169, loss = 0.09629267\n",
      "Iteration 10170, loss = 0.09629143\n",
      "Iteration 10171, loss = 0.09629019\n",
      "Iteration 10172, loss = 0.09628896\n",
      "Iteration 10173, loss = 0.09628772\n",
      "Iteration 10174, loss = 0.09628648\n",
      "Iteration 10175, loss = 0.09628525\n",
      "Iteration 10176, loss = 0.09628401\n",
      "Iteration 10177, loss = 0.09628278\n",
      "Iteration 10178, loss = 0.09628154\n",
      "Iteration 10179, loss = 0.09628031\n",
      "Iteration 10180, loss = 0.09627907\n",
      "Iteration 10181, loss = 0.09627784\n",
      "Iteration 10182, loss = 0.09627660\n",
      "Iteration 10183, loss = 0.09627537\n",
      "Iteration 10184, loss = 0.09627414\n",
      "Iteration 10185, loss = 0.09627290\n",
      "Iteration 10186, loss = 0.09627167\n",
      "Iteration 10187, loss = 0.09627044\n",
      "Iteration 10188, loss = 0.09626920\n",
      "Iteration 10189, loss = 0.09626797\n",
      "Iteration 10190, loss = 0.09626674\n",
      "Iteration 10191, loss = 0.09626551\n",
      "Iteration 10192, loss = 0.09626427\n",
      "Iteration 10193, loss = 0.09626304\n",
      "Iteration 10194, loss = 0.09626181\n",
      "Iteration 10195, loss = 0.09626058\n",
      "Iteration 10196, loss = 0.09625935\n",
      "Iteration 10197, loss = 0.09625812\n",
      "Iteration 10198, loss = 0.09625689\n",
      "Iteration 10199, loss = 0.09625566\n",
      "Iteration 10200, loss = 0.09625443\n",
      "Iteration 10201, loss = 0.09625320\n",
      "Iteration 10202, loss = 0.09625197\n",
      "Iteration 10203, loss = 0.09625074\n",
      "Iteration 10204, loss = 0.09624951\n",
      "Iteration 10205, loss = 0.09624828\n",
      "Iteration 10206, loss = 0.09624705\n",
      "Iteration 10207, loss = 0.09624583\n",
      "Iteration 10208, loss = 0.09624460\n",
      "Iteration 10209, loss = 0.09624337\n",
      "Iteration 10210, loss = 0.09624214\n",
      "Iteration 10211, loss = 0.09624092\n",
      "Iteration 10212, loss = 0.09623969\n",
      "Iteration 10213, loss = 0.09623846\n",
      "Iteration 10214, loss = 0.09623724\n",
      "Iteration 10215, loss = 0.09623601\n",
      "Iteration 10216, loss = 0.09623478\n",
      "Iteration 10217, loss = 0.09623356\n",
      "Iteration 10218, loss = 0.09623233\n",
      "Iteration 10219, loss = 0.09623111\n",
      "Iteration 10220, loss = 0.09622988\n",
      "Iteration 10221, loss = 0.09622866\n",
      "Iteration 10222, loss = 0.09622743\n",
      "Iteration 10223, loss = 0.09622621\n",
      "Iteration 10224, loss = 0.09622499\n",
      "Iteration 10225, loss = 0.09622376\n",
      "Iteration 10226, loss = 0.09622254\n",
      "Iteration 10227, loss = 0.09622132\n",
      "Iteration 10228, loss = 0.09622009\n",
      "Iteration 10229, loss = 0.09621887\n",
      "Iteration 10230, loss = 0.09621765\n",
      "Iteration 10231, loss = 0.09621643\n",
      "Iteration 10232, loss = 0.09621520\n",
      "Iteration 10233, loss = 0.09621398\n",
      "Iteration 10234, loss = 0.09621276\n",
      "Iteration 10235, loss = 0.09621154\n",
      "Iteration 10236, loss = 0.09621032\n",
      "Iteration 10237, loss = 0.09620910\n",
      "Iteration 10238, loss = 0.09620788\n",
      "Iteration 10239, loss = 0.09620666\n",
      "Iteration 10240, loss = 0.09620544\n",
      "Iteration 10241, loss = 0.09620422\n",
      "Iteration 10242, loss = 0.09620300\n",
      "Iteration 10243, loss = 0.09620178\n",
      "Iteration 10244, loss = 0.09620056\n",
      "Iteration 10245, loss = 0.09619934\n",
      "Iteration 10246, loss = 0.09619812\n",
      "Iteration 10247, loss = 0.09619690\n",
      "Iteration 10248, loss = 0.09619569\n",
      "Iteration 10249, loss = 0.09619447\n",
      "Iteration 10250, loss = 0.09619325\n",
      "Iteration 10251, loss = 0.09619203\n",
      "Iteration 10252, loss = 0.09619082\n",
      "Iteration 10253, loss = 0.09618960\n",
      "Iteration 10254, loss = 0.09618838\n",
      "Iteration 10255, loss = 0.09618717\n",
      "Iteration 10256, loss = 0.09618595\n",
      "Iteration 10257, loss = 0.09618473\n",
      "Iteration 10258, loss = 0.09618352\n",
      "Iteration 10259, loss = 0.09618230\n",
      "Iteration 10260, loss = 0.09618109\n",
      "Iteration 10261, loss = 0.09617987\n",
      "Iteration 10262, loss = 0.09617866\n",
      "Iteration 10263, loss = 0.09617744\n",
      "Iteration 10264, loss = 0.09617623\n",
      "Iteration 10265, loss = 0.09617501\n",
      "Iteration 10266, loss = 0.09617380\n",
      "Iteration 10267, loss = 0.09617259\n",
      "Iteration 10268, loss = 0.09617137\n",
      "Iteration 10269, loss = 0.09617016\n",
      "Iteration 10270, loss = 0.09616895\n",
      "Iteration 10271, loss = 0.09616774\n",
      "Iteration 10272, loss = 0.09616652\n",
      "Iteration 10273, loss = 0.09616531\n",
      "Iteration 10274, loss = 0.09616410\n",
      "Iteration 10275, loss = 0.09616289\n",
      "Iteration 10276, loss = 0.09616168\n",
      "Iteration 10277, loss = 0.09616047\n",
      "Iteration 10278, loss = 0.09615925\n",
      "Iteration 10279, loss = 0.09615804\n",
      "Iteration 10280, loss = 0.09615683\n",
      "Iteration 10281, loss = 0.09615562\n",
      "Iteration 10282, loss = 0.09615441\n",
      "Iteration 10283, loss = 0.09615320\n",
      "Iteration 10284, loss = 0.09615199\n",
      "Iteration 10285, loss = 0.09615079\n",
      "Iteration 10286, loss = 0.09614958\n",
      "Iteration 10287, loss = 0.09614837\n",
      "Iteration 10288, loss = 0.09614716\n",
      "Iteration 10289, loss = 0.09614595\n",
      "Iteration 10290, loss = 0.09614474\n",
      "Iteration 10291, loss = 0.09614354\n",
      "Iteration 10292, loss = 0.09614233\n",
      "Iteration 10293, loss = 0.09614112\n",
      "Iteration 10294, loss = 0.09613991\n",
      "Iteration 10295, loss = 0.09613871\n",
      "Iteration 10296, loss = 0.09613750\n",
      "Iteration 10297, loss = 0.09613629\n",
      "Iteration 10298, loss = 0.09613509\n",
      "Iteration 10299, loss = 0.09613388\n",
      "Iteration 10300, loss = 0.09613268\n",
      "Iteration 10301, loss = 0.09613147\n",
      "Iteration 10302, loss = 0.09613027\n",
      "Iteration 10303, loss = 0.09612906\n",
      "Iteration 10304, loss = 0.09612786\n",
      "Iteration 10305, loss = 0.09612665\n",
      "Iteration 10306, loss = 0.09612545\n",
      "Iteration 10307, loss = 0.09612424\n",
      "Iteration 10308, loss = 0.09612304\n",
      "Iteration 10309, loss = 0.09612184\n",
      "Iteration 10310, loss = 0.09612063\n",
      "Iteration 10311, loss = 0.09611943\n",
      "Iteration 10312, loss = 0.09611823\n",
      "Iteration 10313, loss = 0.09611703\n",
      "Iteration 10314, loss = 0.09611582\n",
      "Iteration 10315, loss = 0.09611462\n",
      "Iteration 10316, loss = 0.09611342\n",
      "Iteration 10317, loss = 0.09611222\n",
      "Iteration 10318, loss = 0.09611102\n",
      "Iteration 10319, loss = 0.09610982\n",
      "Iteration 10320, loss = 0.09610861\n",
      "Iteration 10321, loss = 0.09610741\n",
      "Iteration 10322, loss = 0.09610621\n",
      "Iteration 10323, loss = 0.09610501\n",
      "Iteration 10324, loss = 0.09610381\n",
      "Iteration 10325, loss = 0.09610261\n",
      "Iteration 10326, loss = 0.09610141\n",
      "Iteration 10327, loss = 0.09610022\n",
      "Iteration 10328, loss = 0.09609902\n",
      "Iteration 10329, loss = 0.09609782\n",
      "Iteration 10330, loss = 0.09609662\n",
      "Iteration 10331, loss = 0.09609542\n",
      "Iteration 10332, loss = 0.09609422\n",
      "Iteration 10333, loss = 0.09609302\n",
      "Iteration 10334, loss = 0.09609183\n",
      "Iteration 10335, loss = 0.09609063\n",
      "Iteration 10336, loss = 0.09608943\n",
      "Iteration 10337, loss = 0.09608824\n",
      "Iteration 10338, loss = 0.09608704\n",
      "Iteration 10339, loss = 0.09608584\n",
      "Iteration 10340, loss = 0.09608465\n",
      "Iteration 10341, loss = 0.09608345\n",
      "Iteration 10342, loss = 0.09608226\n",
      "Iteration 10343, loss = 0.09608106\n",
      "Iteration 10344, loss = 0.09607986\n",
      "Iteration 10345, loss = 0.09607867\n",
      "Iteration 10346, loss = 0.09607748\n",
      "Iteration 10347, loss = 0.09607628\n",
      "Iteration 10348, loss = 0.09607509\n",
      "Iteration 10349, loss = 0.09607389\n",
      "Iteration 10350, loss = 0.09607270\n",
      "Iteration 10351, loss = 0.09607150\n",
      "Iteration 10352, loss = 0.09607031\n",
      "Iteration 10353, loss = 0.09606912\n",
      "Iteration 10354, loss = 0.09606793\n",
      "Iteration 10355, loss = 0.09606673\n",
      "Iteration 10356, loss = 0.09606554\n",
      "Iteration 10357, loss = 0.09606435\n",
      "Iteration 10358, loss = 0.09606316\n",
      "Iteration 10359, loss = 0.09606196\n",
      "Iteration 10360, loss = 0.09606077\n",
      "Iteration 10361, loss = 0.09605958\n",
      "Iteration 10362, loss = 0.09605839\n",
      "Iteration 10363, loss = 0.09605720\n",
      "Iteration 10364, loss = 0.09605601\n",
      "Iteration 10365, loss = 0.09605482\n",
      "Iteration 10366, loss = 0.09605363\n",
      "Iteration 10367, loss = 0.09605244\n",
      "Iteration 10368, loss = 0.09605125\n",
      "Iteration 10369, loss = 0.09605006\n",
      "Iteration 10370, loss = 0.09604887\n",
      "Iteration 10371, loss = 0.09604768\n",
      "Iteration 10372, loss = 0.09604649\n",
      "Iteration 10373, loss = 0.09604531\n",
      "Iteration 10374, loss = 0.09604412\n",
      "Iteration 10375, loss = 0.09604293\n",
      "Iteration 10376, loss = 0.09604174\n",
      "Iteration 10377, loss = 0.09604055\n",
      "Iteration 10378, loss = 0.09603937\n",
      "Iteration 10379, loss = 0.09603818\n",
      "Iteration 10380, loss = 0.09603699\n",
      "Iteration 10381, loss = 0.09603581\n",
      "Iteration 10382, loss = 0.09603462\n",
      "Iteration 10383, loss = 0.09603343\n",
      "Iteration 10384, loss = 0.09603225\n",
      "Iteration 10385, loss = 0.09603106\n",
      "Iteration 10386, loss = 0.09602988\n",
      "Iteration 10387, loss = 0.09602869\n",
      "Iteration 10388, loss = 0.09602751\n",
      "Iteration 10389, loss = 0.09602632\n",
      "Iteration 10390, loss = 0.09602514\n",
      "Iteration 10391, loss = 0.09602395\n",
      "Iteration 10392, loss = 0.09602277\n",
      "Iteration 10393, loss = 0.09602159\n",
      "Iteration 10394, loss = 0.09602040\n",
      "Iteration 10395, loss = 0.09601922\n",
      "Iteration 10396, loss = 0.09601804\n",
      "Iteration 10397, loss = 0.09601685\n",
      "Iteration 10398, loss = 0.09601567\n",
      "Iteration 10399, loss = 0.09601449\n",
      "Iteration 10400, loss = 0.09601331\n",
      "Iteration 10401, loss = 0.09601212\n",
      "Iteration 10402, loss = 0.09601094\n",
      "Iteration 10403, loss = 0.09600976\n",
      "Iteration 10404, loss = 0.09600858\n",
      "Iteration 10405, loss = 0.09600740\n",
      "Iteration 10406, loss = 0.09600622\n",
      "Iteration 10407, loss = 0.09600504\n",
      "Iteration 10408, loss = 0.09600386\n",
      "Iteration 10409, loss = 0.09600268\n",
      "Iteration 10410, loss = 0.09600150\n",
      "Iteration 10411, loss = 0.09600032\n",
      "Iteration 10412, loss = 0.09599914\n",
      "Iteration 10413, loss = 0.09599796\n",
      "Iteration 10414, loss = 0.09599678\n",
      "Iteration 10415, loss = 0.09599560\n",
      "Iteration 10416, loss = 0.09599442\n",
      "Iteration 10417, loss = 0.09599324\n",
      "Iteration 10418, loss = 0.09599206\n",
      "Iteration 10419, loss = 0.09599089\n",
      "Iteration 10420, loss = 0.09598971\n",
      "Iteration 10421, loss = 0.09598853\n",
      "Iteration 10422, loss = 0.09598735\n",
      "Iteration 10423, loss = 0.09598618\n",
      "Iteration 10424, loss = 0.09598500\n",
      "Iteration 10425, loss = 0.09598382\n",
      "Iteration 10426, loss = 0.09598265\n",
      "Iteration 10427, loss = 0.09598147\n",
      "Iteration 10428, loss = 0.09598030\n",
      "Iteration 10429, loss = 0.09597912\n",
      "Iteration 10430, loss = 0.09597795\n",
      "Iteration 10431, loss = 0.09597677\n",
      "Iteration 10432, loss = 0.09597560\n",
      "Iteration 10433, loss = 0.09597442\n",
      "Iteration 10434, loss = 0.09597325\n",
      "Iteration 10435, loss = 0.09597207\n",
      "Iteration 10436, loss = 0.09597090\n",
      "Iteration 10437, loss = 0.09596972\n",
      "Iteration 10438, loss = 0.09596855\n",
      "Iteration 10439, loss = 0.09596738\n",
      "Iteration 10440, loss = 0.09596620\n",
      "Iteration 10441, loss = 0.09596503\n",
      "Iteration 10442, loss = 0.09596386\n",
      "Iteration 10443, loss = 0.09596269\n",
      "Iteration 10444, loss = 0.09596151\n",
      "Iteration 10445, loss = 0.09596034\n",
      "Iteration 10446, loss = 0.09595917\n",
      "Iteration 10447, loss = 0.09595800\n",
      "Iteration 10448, loss = 0.09595683\n",
      "Iteration 10449, loss = 0.09595566\n",
      "Iteration 10450, loss = 0.09595449\n",
      "Iteration 10451, loss = 0.09595332\n",
      "Iteration 10452, loss = 0.09595215\n",
      "Iteration 10453, loss = 0.09595097\n",
      "Iteration 10454, loss = 0.09594981\n",
      "Iteration 10455, loss = 0.09594864\n",
      "Iteration 10456, loss = 0.09594747\n",
      "Iteration 10457, loss = 0.09594630\n",
      "Iteration 10458, loss = 0.09594513\n",
      "Iteration 10459, loss = 0.09594396\n",
      "Iteration 10460, loss = 0.09594279\n",
      "Iteration 10461, loss = 0.09594162\n",
      "Iteration 10462, loss = 0.09594045\n",
      "Iteration 10463, loss = 0.09593929\n",
      "Iteration 10464, loss = 0.09593812\n",
      "Iteration 10465, loss = 0.09593695\n",
      "Iteration 10466, loss = 0.09593578\n",
      "Iteration 10467, loss = 0.09593462\n",
      "Iteration 10468, loss = 0.09593345\n",
      "Iteration 10469, loss = 0.09593228\n",
      "Iteration 10470, loss = 0.09593112\n",
      "Iteration 10471, loss = 0.09592995\n",
      "Iteration 10472, loss = 0.09592879\n",
      "Iteration 10473, loss = 0.09592762\n",
      "Iteration 10474, loss = 0.09592645\n",
      "Iteration 10475, loss = 0.09592529\n",
      "Iteration 10476, loss = 0.09592412\n",
      "Iteration 10477, loss = 0.09592296\n",
      "Iteration 10478, loss = 0.09592179\n",
      "Iteration 10479, loss = 0.09592063\n",
      "Iteration 10480, loss = 0.09591947\n",
      "Iteration 10481, loss = 0.09591830\n",
      "Iteration 10482, loss = 0.09591714\n",
      "Iteration 10483, loss = 0.09591598\n",
      "Iteration 10484, loss = 0.09591481\n",
      "Iteration 10485, loss = 0.09591365\n",
      "Iteration 10486, loss = 0.09591249\n",
      "Iteration 10487, loss = 0.09591132\n",
      "Iteration 10488, loss = 0.09591016\n",
      "Iteration 10489, loss = 0.09590900\n",
      "Iteration 10490, loss = 0.09590784\n",
      "Iteration 10491, loss = 0.09590668\n",
      "Iteration 10492, loss = 0.09590551\n",
      "Iteration 10493, loss = 0.09590435\n",
      "Iteration 10494, loss = 0.09590319\n",
      "Iteration 10495, loss = 0.09590203\n",
      "Iteration 10496, loss = 0.09590087\n",
      "Iteration 10497, loss = 0.09589971\n",
      "Iteration 10498, loss = 0.09589855\n",
      "Iteration 10499, loss = 0.09589739\n",
      "Iteration 10500, loss = 0.09589623\n",
      "Iteration 10501, loss = 0.09589507\n",
      "Iteration 10502, loss = 0.09589391\n",
      "Iteration 10503, loss = 0.09589275\n",
      "Iteration 10504, loss = 0.09589160\n",
      "Iteration 10505, loss = 0.09589044\n",
      "Iteration 10506, loss = 0.09588928\n",
      "Iteration 10507, loss = 0.09588812\n",
      "Iteration 10508, loss = 0.09588696\n",
      "Iteration 10509, loss = 0.09588581\n",
      "Iteration 10510, loss = 0.09588465\n",
      "Iteration 10511, loss = 0.09588349\n",
      "Iteration 10512, loss = 0.09588233\n",
      "Iteration 10513, loss = 0.09588118\n",
      "Iteration 10514, loss = 0.09588002\n",
      "Iteration 10515, loss = 0.09587886\n",
      "Iteration 10516, loss = 0.09587771\n",
      "Iteration 10517, loss = 0.09587655\n",
      "Iteration 10518, loss = 0.09587540\n",
      "Iteration 10519, loss = 0.09587424\n",
      "Iteration 10520, loss = 0.09587309\n",
      "Iteration 10521, loss = 0.09587193\n",
      "Iteration 10522, loss = 0.09587078\n",
      "Iteration 10523, loss = 0.09586962\n",
      "Iteration 10524, loss = 0.09586847\n",
      "Iteration 10525, loss = 0.09586731\n",
      "Iteration 10526, loss = 0.09586616\n",
      "Iteration 10527, loss = 0.09586501\n",
      "Iteration 10528, loss = 0.09586385\n",
      "Iteration 10529, loss = 0.09586270\n",
      "Iteration 10530, loss = 0.09586155\n",
      "Iteration 10531, loss = 0.09586039\n",
      "Iteration 10532, loss = 0.09585924\n",
      "Iteration 10533, loss = 0.09585809\n",
      "Iteration 10534, loss = 0.09585694\n",
      "Iteration 10535, loss = 0.09585578\n",
      "Iteration 10536, loss = 0.09585463\n",
      "Iteration 10537, loss = 0.09585348\n",
      "Iteration 10538, loss = 0.09585233\n",
      "Iteration 10539, loss = 0.09585118\n",
      "Iteration 10540, loss = 0.09585003\n",
      "Iteration 10541, loss = 0.09584888\n",
      "Iteration 10542, loss = 0.09584773\n",
      "Iteration 10543, loss = 0.09584658\n",
      "Iteration 10544, loss = 0.09584543\n",
      "Iteration 10545, loss = 0.09584428\n",
      "Iteration 10546, loss = 0.09584313\n",
      "Iteration 10547, loss = 0.09584198\n",
      "Iteration 10548, loss = 0.09584083\n",
      "Iteration 10549, loss = 0.09583968\n",
      "Iteration 10550, loss = 0.09583853\n",
      "Iteration 10551, loss = 0.09583738\n",
      "Iteration 10552, loss = 0.09583624\n",
      "Iteration 10553, loss = 0.09583509\n",
      "Iteration 10554, loss = 0.09583394\n",
      "Iteration 10555, loss = 0.09583279\n",
      "Iteration 10556, loss = 0.09583165\n",
      "Iteration 10557, loss = 0.09583050\n",
      "Iteration 10558, loss = 0.09582935\n",
      "Iteration 10559, loss = 0.09582820\n",
      "Iteration 10560, loss = 0.09582706\n",
      "Iteration 10561, loss = 0.09582591\n",
      "Iteration 10562, loss = 0.09582477\n",
      "Iteration 10563, loss = 0.09582362\n",
      "Iteration 10564, loss = 0.09582247\n",
      "Iteration 10565, loss = 0.09582133\n",
      "Iteration 10566, loss = 0.09582018\n",
      "Iteration 10567, loss = 0.09581904\n",
      "Iteration 10568, loss = 0.09581789\n",
      "Iteration 10569, loss = 0.09581675\n",
      "Iteration 10570, loss = 0.09581561\n",
      "Iteration 10571, loss = 0.09581446\n",
      "Iteration 10572, loss = 0.09581332\n",
      "Iteration 10573, loss = 0.09581217\n",
      "Iteration 10574, loss = 0.09581103\n",
      "Iteration 10575, loss = 0.09580989\n",
      "Iteration 10576, loss = 0.09580874\n",
      "Iteration 10577, loss = 0.09580760\n",
      "Iteration 10578, loss = 0.09580646\n",
      "Iteration 10579, loss = 0.09580532\n",
      "Iteration 10580, loss = 0.09580417\n",
      "Iteration 10581, loss = 0.09580303\n",
      "Iteration 10582, loss = 0.09580189\n",
      "Iteration 10583, loss = 0.09580075\n",
      "Iteration 10584, loss = 0.09579961\n",
      "Iteration 10585, loss = 0.09579847\n",
      "Iteration 10586, loss = 0.09579733\n",
      "Iteration 10587, loss = 0.09579619\n",
      "Iteration 10588, loss = 0.09579505\n",
      "Iteration 10589, loss = 0.09579391\n",
      "Iteration 10590, loss = 0.09579277\n",
      "Iteration 10591, loss = 0.09579163\n",
      "Iteration 10592, loss = 0.09579049\n",
      "Iteration 10593, loss = 0.09578935\n",
      "Iteration 10594, loss = 0.09578821\n",
      "Iteration 10595, loss = 0.09578707\n",
      "Iteration 10596, loss = 0.09578593\n",
      "Iteration 10597, loss = 0.09578479\n",
      "Iteration 10598, loss = 0.09578365\n",
      "Iteration 10599, loss = 0.09578252\n",
      "Iteration 10600, loss = 0.09578138\n",
      "Iteration 10601, loss = 0.09578024\n",
      "Iteration 10602, loss = 0.09577910\n",
      "Iteration 10603, loss = 0.09577797\n",
      "Iteration 10604, loss = 0.09577683\n",
      "Iteration 10605, loss = 0.09577569\n",
      "Iteration 10606, loss = 0.09577456\n",
      "Iteration 10607, loss = 0.09577342\n",
      "Iteration 10608, loss = 0.09577228\n",
      "Iteration 10609, loss = 0.09577115\n",
      "Iteration 10610, loss = 0.09577001\n",
      "Iteration 10611, loss = 0.09576888\n",
      "Iteration 10612, loss = 0.09576774\n",
      "Iteration 10613, loss = 0.09576661\n",
      "Iteration 10614, loss = 0.09576547\n",
      "Iteration 10615, loss = 0.09576434\n",
      "Iteration 10616, loss = 0.09576320\n",
      "Iteration 10617, loss = 0.09576207\n",
      "Iteration 10618, loss = 0.09576093\n",
      "Iteration 10619, loss = 0.09575980\n",
      "Iteration 10620, loss = 0.09575867\n",
      "Iteration 10621, loss = 0.09575753\n",
      "Iteration 10622, loss = 0.09575640\n",
      "Iteration 10623, loss = 0.09575527\n",
      "Iteration 10624, loss = 0.09575414\n",
      "Iteration 10625, loss = 0.09575300\n",
      "Iteration 10626, loss = 0.09575187\n",
      "Iteration 10627, loss = 0.09575074\n",
      "Iteration 10628, loss = 0.09574961\n",
      "Iteration 10629, loss = 0.09574848\n",
      "Iteration 10630, loss = 0.09574734\n",
      "Iteration 10631, loss = 0.09574621\n",
      "Iteration 10632, loss = 0.09574508\n",
      "Iteration 10633, loss = 0.09574395\n",
      "Iteration 10634, loss = 0.09574282\n",
      "Iteration 10635, loss = 0.09574169\n",
      "Iteration 10636, loss = 0.09574056\n",
      "Iteration 10637, loss = 0.09573943\n",
      "Iteration 10638, loss = 0.09573830\n",
      "Iteration 10639, loss = 0.09573717\n",
      "Iteration 10640, loss = 0.09573604\n",
      "Iteration 10641, loss = 0.09573491\n",
      "Iteration 10642, loss = 0.09573378\n",
      "Iteration 10643, loss = 0.09573266\n",
      "Iteration 10644, loss = 0.09573153\n",
      "Iteration 10645, loss = 0.09573040\n",
      "Iteration 10646, loss = 0.09572927\n",
      "Iteration 10647, loss = 0.09572814\n",
      "Iteration 10648, loss = 0.09572701\n",
      "Iteration 10649, loss = 0.09572589\n",
      "Iteration 10650, loss = 0.09572476\n",
      "Iteration 10651, loss = 0.09572363\n",
      "Iteration 10652, loss = 0.09572251\n",
      "Iteration 10653, loss = 0.09572138\n",
      "Iteration 10654, loss = 0.09572025\n",
      "Iteration 10655, loss = 0.09571913\n",
      "Iteration 10656, loss = 0.09571800\n",
      "Iteration 10657, loss = 0.09571688\n",
      "Iteration 10658, loss = 0.09571575\n",
      "Iteration 10659, loss = 0.09571463\n",
      "Iteration 10660, loss = 0.09571350\n",
      "Iteration 10661, loss = 0.09571238\n",
      "Iteration 10662, loss = 0.09571125\n",
      "Iteration 10663, loss = 0.09571013\n",
      "Iteration 10664, loss = 0.09570900\n",
      "Iteration 10665, loss = 0.09570788\n",
      "Iteration 10666, loss = 0.09570675\n",
      "Iteration 10667, loss = 0.09570563\n",
      "Iteration 10668, loss = 0.09570451\n",
      "Iteration 10669, loss = 0.09570338\n",
      "Iteration 10670, loss = 0.09570226\n",
      "Iteration 10671, loss = 0.09570114\n",
      "Iteration 10672, loss = 0.09570002\n",
      "Iteration 10673, loss = 0.09569889\n",
      "Iteration 10674, loss = 0.09569777\n",
      "Iteration 10675, loss = 0.09569665\n",
      "Iteration 10676, loss = 0.09569553\n",
      "Iteration 10677, loss = 0.09569441\n",
      "Iteration 10678, loss = 0.09569329\n",
      "Iteration 10679, loss = 0.09569216\n",
      "Iteration 10680, loss = 0.09569104\n",
      "Iteration 10681, loss = 0.09568992\n",
      "Iteration 10682, loss = 0.09568880\n",
      "Iteration 10683, loss = 0.09568768\n",
      "Iteration 10684, loss = 0.09568656\n",
      "Iteration 10685, loss = 0.09568544\n",
      "Iteration 10686, loss = 0.09568432\n",
      "Iteration 10687, loss = 0.09568320\n",
      "Iteration 10688, loss = 0.09568208\n",
      "Iteration 10689, loss = 0.09568096\n",
      "Iteration 10690, loss = 0.09567985\n",
      "Iteration 10691, loss = 0.09567873\n",
      "Iteration 10692, loss = 0.09567761\n",
      "Iteration 10693, loss = 0.09567649\n",
      "Iteration 10694, loss = 0.09567537\n",
      "Iteration 10695, loss = 0.09567426\n",
      "Iteration 10696, loss = 0.09567314\n",
      "Iteration 10697, loss = 0.09567202\n",
      "Iteration 10698, loss = 0.09567090\n",
      "Iteration 10699, loss = 0.09566979\n",
      "Iteration 10700, loss = 0.09566867\n",
      "Iteration 10701, loss = 0.09566755\n",
      "Iteration 10702, loss = 0.09566644\n",
      "Iteration 10703, loss = 0.09566532\n",
      "Iteration 10704, loss = 0.09566421\n",
      "Iteration 10705, loss = 0.09566309\n",
      "Iteration 10706, loss = 0.09566197\n",
      "Iteration 10707, loss = 0.09566086\n",
      "Iteration 10708, loss = 0.09565974\n",
      "Iteration 10709, loss = 0.09565863\n",
      "Iteration 10710, loss = 0.09565751\n",
      "Iteration 10711, loss = 0.09565640\n",
      "Iteration 10712, loss = 0.09565529\n",
      "Iteration 10713, loss = 0.09565417\n",
      "Iteration 10714, loss = 0.09565306\n",
      "Iteration 10715, loss = 0.09565194\n",
      "Iteration 10716, loss = 0.09565083\n",
      "Iteration 10717, loss = 0.09564972\n",
      "Iteration 10718, loss = 0.09564860\n",
      "Iteration 10719, loss = 0.09564749\n",
      "Iteration 10720, loss = 0.09564638\n",
      "Iteration 10721, loss = 0.09564527\n",
      "Iteration 10722, loss = 0.09564415\n",
      "Iteration 10723, loss = 0.09564304\n",
      "Iteration 10724, loss = 0.09564193\n",
      "Iteration 10725, loss = 0.09564082\n",
      "Iteration 10726, loss = 0.09563971\n",
      "Iteration 10727, loss = 0.09563860\n",
      "Iteration 10728, loss = 0.09563749\n",
      "Iteration 10729, loss = 0.09563638\n",
      "Iteration 10730, loss = 0.09563526\n",
      "Iteration 10731, loss = 0.09563415\n",
      "Iteration 10732, loss = 0.09563304\n",
      "Iteration 10733, loss = 0.09563193\n",
      "Iteration 10734, loss = 0.09563082\n",
      "Iteration 10735, loss = 0.09562972\n",
      "Iteration 10736, loss = 0.09562861\n",
      "Iteration 10737, loss = 0.09562750\n",
      "Iteration 10738, loss = 0.09562639\n",
      "Iteration 10739, loss = 0.09562528\n",
      "Iteration 10740, loss = 0.09562417\n",
      "Iteration 10741, loss = 0.09562306\n",
      "Iteration 10742, loss = 0.09562195\n",
      "Iteration 10743, loss = 0.09562085\n",
      "Iteration 10744, loss = 0.09561974\n",
      "Iteration 10745, loss = 0.09561863\n",
      "Iteration 10746, loss = 0.09561752\n",
      "Iteration 10747, loss = 0.09561642\n",
      "Iteration 10748, loss = 0.09561531\n",
      "Iteration 10749, loss = 0.09561420\n",
      "Iteration 10750, loss = 0.09561310\n",
      "Iteration 10751, loss = 0.09561199\n",
      "Iteration 10752, loss = 0.09561089\n",
      "Iteration 10753, loss = 0.09560978\n",
      "Iteration 10754, loss = 0.09560867\n",
      "Iteration 10755, loss = 0.09560757\n",
      "Iteration 10756, loss = 0.09560646\n",
      "Iteration 10757, loss = 0.09560536\n",
      "Iteration 10758, loss = 0.09560425\n",
      "Iteration 10759, loss = 0.09560315\n",
      "Iteration 10760, loss = 0.09560204\n",
      "Iteration 10761, loss = 0.09560094\n",
      "Iteration 10762, loss = 0.09559984\n",
      "Iteration 10763, loss = 0.09559873\n",
      "Iteration 10764, loss = 0.09559763\n",
      "Iteration 10765, loss = 0.09559653\n",
      "Iteration 10766, loss = 0.09559542\n",
      "Iteration 10767, loss = 0.09559432\n",
      "Iteration 10768, loss = 0.09559322\n",
      "Iteration 10769, loss = 0.09559211\n",
      "Iteration 10770, loss = 0.09559101\n",
      "Iteration 10771, loss = 0.09558991\n",
      "Iteration 10772, loss = 0.09558881\n",
      "Iteration 10773, loss = 0.09558771\n",
      "Iteration 10774, loss = 0.09558660\n",
      "Iteration 10775, loss = 0.09558550\n",
      "Iteration 10776, loss = 0.09558440\n",
      "Iteration 10777, loss = 0.09558330\n",
      "Iteration 10778, loss = 0.09558220\n",
      "Iteration 10779, loss = 0.09558110\n",
      "Iteration 10780, loss = 0.09558000\n",
      "Iteration 10781, loss = 0.09557890\n",
      "Iteration 10782, loss = 0.09557780\n",
      "Iteration 10783, loss = 0.09557670\n",
      "Iteration 10784, loss = 0.09557560\n",
      "Iteration 10785, loss = 0.09557450\n",
      "Iteration 10786, loss = 0.09557340\n",
      "Iteration 10787, loss = 0.09557230\n",
      "Iteration 10788, loss = 0.09557120\n",
      "Iteration 10789, loss = 0.09557011\n",
      "Iteration 10790, loss = 0.09556901\n",
      "Iteration 10791, loss = 0.09556791\n",
      "Iteration 10792, loss = 0.09556681\n",
      "Iteration 10793, loss = 0.09556571\n",
      "Iteration 10794, loss = 0.09556462\n",
      "Iteration 10795, loss = 0.09556352\n",
      "Iteration 10796, loss = 0.09556242\n",
      "Iteration 10797, loss = 0.09556132\n",
      "Iteration 10798, loss = 0.09556023\n",
      "Iteration 10799, loss = 0.09555913\n",
      "Iteration 10800, loss = 0.09555803\n",
      "Iteration 10801, loss = 0.09555694\n",
      "Iteration 10802, loss = 0.09555584\n",
      "Iteration 10803, loss = 0.09555475\n",
      "Iteration 10804, loss = 0.09555365\n",
      "Iteration 10805, loss = 0.09555256\n",
      "Iteration 10806, loss = 0.09555146\n",
      "Iteration 10807, loss = 0.09555037\n",
      "Iteration 10808, loss = 0.09554927\n",
      "Iteration 10809, loss = 0.09554818\n",
      "Iteration 10810, loss = 0.09554708\n",
      "Iteration 10811, loss = 0.09554599\n",
      "Iteration 10812, loss = 0.09554489\n",
      "Iteration 10813, loss = 0.09554380\n",
      "Iteration 10814, loss = 0.09554271\n",
      "Iteration 10815, loss = 0.09554161\n",
      "Iteration 10816, loss = 0.09554052\n",
      "Iteration 10817, loss = 0.09553943\n",
      "Iteration 10818, loss = 0.09553834\n",
      "Iteration 10819, loss = 0.09553724\n",
      "Iteration 10820, loss = 0.09553615\n",
      "Iteration 10821, loss = 0.09553506\n",
      "Iteration 10822, loss = 0.09553397\n",
      "Iteration 10823, loss = 0.09553287\n",
      "Iteration 10824, loss = 0.09553178\n",
      "Iteration 10825, loss = 0.09553069\n",
      "Iteration 10826, loss = 0.09552960\n",
      "Iteration 10827, loss = 0.09552851\n",
      "Iteration 10828, loss = 0.09552742\n",
      "Iteration 10829, loss = 0.09552633\n",
      "Iteration 10830, loss = 0.09552524\n",
      "Iteration 10831, loss = 0.09552415\n",
      "Iteration 10832, loss = 0.09552306\n",
      "Iteration 10833, loss = 0.09552197\n",
      "Iteration 10834, loss = 0.09552088\n",
      "Iteration 10835, loss = 0.09551979\n",
      "Iteration 10836, loss = 0.09551870\n",
      "Iteration 10837, loss = 0.09551761\n",
      "Iteration 10838, loss = 0.09551652\n",
      "Iteration 10839, loss = 0.09551543\n",
      "Iteration 10840, loss = 0.09551435\n",
      "Iteration 10841, loss = 0.09551326\n",
      "Iteration 10842, loss = 0.09551217\n",
      "Iteration 10843, loss = 0.09551108\n",
      "Iteration 10844, loss = 0.09550999\n",
      "Iteration 10845, loss = 0.09550891\n",
      "Iteration 10846, loss = 0.09550782\n",
      "Iteration 10847, loss = 0.09550673\n",
      "Iteration 10848, loss = 0.09550565\n",
      "Iteration 10849, loss = 0.09550456\n",
      "Iteration 10850, loss = 0.09550347\n",
      "Iteration 10851, loss = 0.09550239\n",
      "Iteration 10852, loss = 0.09550130\n",
      "Iteration 10853, loss = 0.09550021\n",
      "Iteration 10854, loss = 0.09549913\n",
      "Iteration 10855, loss = 0.09549804\n",
      "Iteration 10856, loss = 0.09549696\n",
      "Iteration 10857, loss = 0.09549587\n",
      "Iteration 10858, loss = 0.09549479\n",
      "Iteration 10859, loss = 0.09549370\n",
      "Iteration 10860, loss = 0.09549262\n",
      "Iteration 10861, loss = 0.09549154\n",
      "Iteration 10862, loss = 0.09549045\n",
      "Iteration 10863, loss = 0.09548937\n",
      "Iteration 10864, loss = 0.09548828\n",
      "Iteration 10865, loss = 0.09548720\n",
      "Iteration 10866, loss = 0.09548612\n",
      "Iteration 10867, loss = 0.09548503\n",
      "Iteration 10868, loss = 0.09548395\n",
      "Iteration 10869, loss = 0.09548287\n",
      "Iteration 10870, loss = 0.09548179\n",
      "Iteration 10871, loss = 0.09548070\n",
      "Iteration 10872, loss = 0.09547962\n",
      "Iteration 10873, loss = 0.09547854\n",
      "Iteration 10874, loss = 0.09547746\n",
      "Iteration 10875, loss = 0.09547638\n",
      "Iteration 10876, loss = 0.09547530\n",
      "Iteration 10877, loss = 0.09547421\n",
      "Iteration 10878, loss = 0.09547313\n",
      "Iteration 10879, loss = 0.09547205\n",
      "Iteration 10880, loss = 0.09547097\n",
      "Iteration 10881, loss = 0.09546989\n",
      "Iteration 10882, loss = 0.09546881\n",
      "Iteration 10883, loss = 0.09546773\n",
      "Iteration 10884, loss = 0.09546665\n",
      "Iteration 10885, loss = 0.09546557\n",
      "Iteration 10886, loss = 0.09546449\n",
      "Iteration 10887, loss = 0.09546341\n",
      "Iteration 10888, loss = 0.09546234\n",
      "Iteration 10889, loss = 0.09546126\n",
      "Iteration 10890, loss = 0.09546018\n",
      "Iteration 10891, loss = 0.09545910\n",
      "Iteration 10892, loss = 0.09545802\n",
      "Iteration 10893, loss = 0.09545694\n",
      "Iteration 10894, loss = 0.09545587\n",
      "Iteration 10895, loss = 0.09545479\n",
      "Iteration 10896, loss = 0.09545371\n",
      "Iteration 10897, loss = 0.09545263\n",
      "Iteration 10898, loss = 0.09545156\n",
      "Iteration 10899, loss = 0.09545048\n",
      "Iteration 10900, loss = 0.09544940\n",
      "Iteration 10901, loss = 0.09544833\n",
      "Iteration 10902, loss = 0.09544725\n",
      "Iteration 10903, loss = 0.09544618\n",
      "Iteration 10904, loss = 0.09544510\n",
      "Iteration 10905, loss = 0.09544402\n",
      "Iteration 10906, loss = 0.09544295\n",
      "Iteration 10907, loss = 0.09544187\n",
      "Iteration 10908, loss = 0.09544080\n",
      "Iteration 10909, loss = 0.09543972\n",
      "Iteration 10910, loss = 0.09543865\n",
      "Iteration 10911, loss = 0.09543757\n",
      "Iteration 10912, loss = 0.09543650\n",
      "Iteration 10913, loss = 0.09543543\n",
      "Iteration 10914, loss = 0.09543435\n",
      "Iteration 10915, loss = 0.09543328\n",
      "Iteration 10916, loss = 0.09543220\n",
      "Iteration 10917, loss = 0.09543113\n",
      "Iteration 10918, loss = 0.09543006\n",
      "Iteration 10919, loss = 0.09542899\n",
      "Iteration 10920, loss = 0.09542791\n",
      "Iteration 10921, loss = 0.09542684\n",
      "Iteration 10922, loss = 0.09542577\n",
      "Iteration 10923, loss = 0.09542470\n",
      "Iteration 10924, loss = 0.09542362\n",
      "Iteration 10925, loss = 0.09542255\n",
      "Iteration 10926, loss = 0.09542148\n",
      "Iteration 10927, loss = 0.09542041\n",
      "Iteration 10928, loss = 0.09541934\n",
      "Iteration 10929, loss = 0.09541827\n",
      "Iteration 10930, loss = 0.09541720\n",
      "Iteration 10931, loss = 0.09541613\n",
      "Iteration 10932, loss = 0.09541505\n",
      "Iteration 10933, loss = 0.09541398\n",
      "Iteration 10934, loss = 0.09541291\n",
      "Iteration 10935, loss = 0.09541184\n",
      "Iteration 10936, loss = 0.09541077\n",
      "Iteration 10937, loss = 0.09540971\n",
      "Iteration 10938, loss = 0.09540864\n",
      "Iteration 10939, loss = 0.09540757\n",
      "Iteration 10940, loss = 0.09540650\n",
      "Iteration 10941, loss = 0.09540543\n",
      "Iteration 10942, loss = 0.09540436\n",
      "Iteration 10943, loss = 0.09540329\n",
      "Iteration 10944, loss = 0.09540222\n",
      "Iteration 10945, loss = 0.09540116\n",
      "Iteration 10946, loss = 0.09540009\n",
      "Iteration 10947, loss = 0.09539902\n",
      "Iteration 10948, loss = 0.09539795\n",
      "Iteration 10949, loss = 0.09539689\n",
      "Iteration 10950, loss = 0.09539582\n",
      "Iteration 10951, loss = 0.09539475\n",
      "Iteration 10952, loss = 0.09539369\n",
      "Iteration 10953, loss = 0.09539262\n",
      "Iteration 10954, loss = 0.09539155\n",
      "Iteration 10955, loss = 0.09539049\n",
      "Iteration 10956, loss = 0.09538942\n",
      "Iteration 10957, loss = 0.09538836\n",
      "Iteration 10958, loss = 0.09538729\n",
      "Iteration 10959, loss = 0.09538623\n",
      "Iteration 10960, loss = 0.09538516\n",
      "Iteration 10961, loss = 0.09538410\n",
      "Iteration 10962, loss = 0.09538303\n",
      "Iteration 10963, loss = 0.09538197\n",
      "Iteration 10964, loss = 0.09538090\n",
      "Iteration 10965, loss = 0.09537984\n",
      "Iteration 10966, loss = 0.09537877\n",
      "Iteration 10967, loss = 0.09537771\n",
      "Iteration 10968, loss = 0.09537665\n",
      "Iteration 10969, loss = 0.09537558\n",
      "Iteration 10970, loss = 0.09537452\n",
      "Iteration 10971, loss = 0.09537346\n",
      "Iteration 10972, loss = 0.09537239\n",
      "Iteration 10973, loss = 0.09537133\n",
      "Iteration 10974, loss = 0.09537027\n",
      "Iteration 10975, loss = 0.09536921\n",
      "Iteration 10976, loss = 0.09536814\n",
      "Iteration 10977, loss = 0.09536708\n",
      "Iteration 10978, loss = 0.09536602\n",
      "Iteration 10979, loss = 0.09536496\n",
      "Iteration 10980, loss = 0.09536390\n",
      "Iteration 10981, loss = 0.09536284\n",
      "Iteration 10982, loss = 0.09536178\n",
      "Iteration 10983, loss = 0.09536072\n",
      "Iteration 10984, loss = 0.09535965\n",
      "Iteration 10985, loss = 0.09535859\n",
      "Iteration 10986, loss = 0.09535753\n",
      "Iteration 10987, loss = 0.09535647\n",
      "Iteration 10988, loss = 0.09535541\n",
      "Iteration 10989, loss = 0.09535435\n",
      "Iteration 10990, loss = 0.09535330\n",
      "Iteration 10991, loss = 0.09535224\n",
      "Iteration 10992, loss = 0.09535118\n",
      "Iteration 10993, loss = 0.09535012\n",
      "Iteration 10994, loss = 0.09534906\n",
      "Iteration 10995, loss = 0.09534800\n",
      "Iteration 10996, loss = 0.09534694\n",
      "Iteration 10997, loss = 0.09534588\n",
      "Iteration 10998, loss = 0.09534483\n",
      "Iteration 10999, loss = 0.09534377\n",
      "Iteration 11000, loss = 0.09534271\n",
      "Iteration 11001, loss = 0.09534165\n",
      "Iteration 11002, loss = 0.09534060\n",
      "Iteration 11003, loss = 0.09533954\n",
      "Iteration 11004, loss = 0.09533848\n",
      "Iteration 11005, loss = 0.09533743\n",
      "Iteration 11006, loss = 0.09533637\n",
      "Iteration 11007, loss = 0.09533531\n",
      "Iteration 11008, loss = 0.09533426\n",
      "Iteration 11009, loss = 0.09533320\n",
      "Iteration 11010, loss = 0.09533215\n",
      "Iteration 11011, loss = 0.09533109\n",
      "Iteration 11012, loss = 0.09533004\n",
      "Iteration 11013, loss = 0.09532898\n",
      "Iteration 11014, loss = 0.09532793\n",
      "Iteration 11015, loss = 0.09532687\n",
      "Iteration 11016, loss = 0.09532582\n",
      "Iteration 11017, loss = 0.09532476\n",
      "Iteration 11018, loss = 0.09532371\n",
      "Iteration 11019, loss = 0.09532265\n",
      "Iteration 11020, loss = 0.09532160\n",
      "Iteration 11021, loss = 0.09532055\n",
      "Iteration 11022, loss = 0.09531949\n",
      "Iteration 11023, loss = 0.09531844\n",
      "Iteration 11024, loss = 0.09531739\n",
      "Iteration 11025, loss = 0.09531633\n",
      "Iteration 11026, loss = 0.09531528\n",
      "Iteration 11027, loss = 0.09531423\n",
      "Iteration 11028, loss = 0.09531318\n",
      "Iteration 11029, loss = 0.09531212\n",
      "Iteration 11030, loss = 0.09531107\n",
      "Iteration 11031, loss = 0.09531002\n",
      "Iteration 11032, loss = 0.09530897\n",
      "Iteration 11033, loss = 0.09530792\n",
      "Iteration 11034, loss = 0.09530687\n",
      "Iteration 11035, loss = 0.09530582\n",
      "Iteration 11036, loss = 0.09530476\n",
      "Iteration 11037, loss = 0.09530371\n",
      "Iteration 11038, loss = 0.09530266\n",
      "Iteration 11039, loss = 0.09530161\n",
      "Iteration 11040, loss = 0.09530056\n",
      "Iteration 11041, loss = 0.09529951\n",
      "Iteration 11042, loss = 0.09529846\n",
      "Iteration 11043, loss = 0.09529741\n",
      "Iteration 11044, loss = 0.09529636\n",
      "Iteration 11045, loss = 0.09529532\n",
      "Iteration 11046, loss = 0.09529427\n",
      "Iteration 11047, loss = 0.09529322\n",
      "Iteration 11048, loss = 0.09529217\n",
      "Iteration 11049, loss = 0.09529112\n",
      "Iteration 11050, loss = 0.09529007\n",
      "Iteration 11051, loss = 0.09528902\n",
      "Iteration 11052, loss = 0.09528798\n",
      "Iteration 11053, loss = 0.09528693\n",
      "Iteration 11054, loss = 0.09528588\n",
      "Iteration 11055, loss = 0.09528483\n",
      "Iteration 11056, loss = 0.09528379\n",
      "Iteration 11057, loss = 0.09528274\n",
      "Iteration 11058, loss = 0.09528169\n",
      "Iteration 11059, loss = 0.09528065\n",
      "Iteration 11060, loss = 0.09527960\n",
      "Iteration 11061, loss = 0.09527855\n",
      "Iteration 11062, loss = 0.09527751\n",
      "Iteration 11063, loss = 0.09527646\n",
      "Iteration 11064, loss = 0.09527542\n",
      "Iteration 11065, loss = 0.09527437\n",
      "Iteration 11066, loss = 0.09527333\n",
      "Iteration 11067, loss = 0.09527228\n",
      "Iteration 11068, loss = 0.09527124\n",
      "Iteration 11069, loss = 0.09527019\n",
      "Iteration 11070, loss = 0.09526915\n",
      "Iteration 11071, loss = 0.09526810\n",
      "Iteration 11072, loss = 0.09526706\n",
      "Iteration 11073, loss = 0.09526601\n",
      "Iteration 11074, loss = 0.09526497\n",
      "Iteration 11075, loss = 0.09526393\n",
      "Iteration 11076, loss = 0.09526288\n",
      "Iteration 11077, loss = 0.09526184\n",
      "Iteration 11078, loss = 0.09526080\n",
      "Iteration 11079, loss = 0.09525975\n",
      "Iteration 11080, loss = 0.09525871\n",
      "Iteration 11081, loss = 0.09525767\n",
      "Iteration 11082, loss = 0.09525663\n",
      "Iteration 11083, loss = 0.09525558\n",
      "Iteration 11084, loss = 0.09525454\n",
      "Iteration 11085, loss = 0.09525350\n",
      "Iteration 11086, loss = 0.09525246\n",
      "Iteration 11087, loss = 0.09525142\n",
      "Iteration 11088, loss = 0.09525038\n",
      "Iteration 11089, loss = 0.09524933\n",
      "Iteration 11090, loss = 0.09524829\n",
      "Iteration 11091, loss = 0.09524725\n",
      "Iteration 11092, loss = 0.09524621\n",
      "Iteration 11093, loss = 0.09524517\n",
      "Iteration 11094, loss = 0.09524413\n",
      "Iteration 11095, loss = 0.09524309\n",
      "Iteration 11096, loss = 0.09524205\n",
      "Iteration 11097, loss = 0.09524101\n",
      "Iteration 11098, loss = 0.09523997\n",
      "Iteration 11099, loss = 0.09523893\n",
      "Iteration 11100, loss = 0.09523789\n",
      "Iteration 11101, loss = 0.09523686\n",
      "Iteration 11102, loss = 0.09523582\n",
      "Iteration 11103, loss = 0.09523478\n",
      "Iteration 11104, loss = 0.09523374\n",
      "Iteration 11105, loss = 0.09523270\n",
      "Iteration 11106, loss = 0.09523166\n",
      "Iteration 11107, loss = 0.09523063\n",
      "Iteration 11108, loss = 0.09522959\n",
      "Iteration 11109, loss = 0.09522855\n",
      "Iteration 11110, loss = 0.09522751\n",
      "Iteration 11111, loss = 0.09522648\n",
      "Iteration 11112, loss = 0.09522544\n",
      "Iteration 11113, loss = 0.09522440\n",
      "Iteration 11114, loss = 0.09522337\n",
      "Iteration 11115, loss = 0.09522233\n",
      "Iteration 11116, loss = 0.09522129\n",
      "Iteration 11117, loss = 0.09522026\n",
      "Iteration 11118, loss = 0.09521922\n",
      "Iteration 11119, loss = 0.09521819\n",
      "Iteration 11120, loss = 0.09521715\n",
      "Iteration 11121, loss = 0.09521612\n",
      "Iteration 11122, loss = 0.09521508\n",
      "Iteration 11123, loss = 0.09521405\n",
      "Iteration 11124, loss = 0.09521301\n",
      "Iteration 11125, loss = 0.09521198\n",
      "Iteration 11126, loss = 0.09521094\n",
      "Iteration 11127, loss = 0.09520991\n",
      "Iteration 11128, loss = 0.09520887\n",
      "Iteration 11129, loss = 0.09520784\n",
      "Iteration 11130, loss = 0.09520681\n",
      "Iteration 11131, loss = 0.09520577\n",
      "Iteration 11132, loss = 0.09520474\n",
      "Iteration 11133, loss = 0.09520371\n",
      "Iteration 11134, loss = 0.09520267\n",
      "Iteration 11135, loss = 0.09520164\n",
      "Iteration 11136, loss = 0.09520061\n",
      "Iteration 11137, loss = 0.09519958\n",
      "Iteration 11138, loss = 0.09519854\n",
      "Iteration 11139, loss = 0.09519751\n",
      "Iteration 11140, loss = 0.09519648\n",
      "Iteration 11141, loss = 0.09519545\n",
      "Iteration 11142, loss = 0.09519442\n",
      "Iteration 11143, loss = 0.09519338\n",
      "Iteration 11144, loss = 0.09519235\n",
      "Iteration 11145, loss = 0.09519132\n",
      "Iteration 11146, loss = 0.09519029\n",
      "Iteration 11147, loss = 0.09518926\n",
      "Iteration 11148, loss = 0.09518823\n",
      "Iteration 11149, loss = 0.09518720\n",
      "Iteration 11150, loss = 0.09518617\n",
      "Iteration 11151, loss = 0.09518514\n",
      "Iteration 11152, loss = 0.09518411\n",
      "Iteration 11153, loss = 0.09518308\n",
      "Iteration 11154, loss = 0.09518205\n",
      "Iteration 11155, loss = 0.09518102\n",
      "Iteration 11156, loss = 0.09517999\n",
      "Iteration 11157, loss = 0.09517896\n",
      "Iteration 11158, loss = 0.09517794\n",
      "Iteration 11159, loss = 0.09517691\n",
      "Iteration 11160, loss = 0.09517588\n",
      "Iteration 11161, loss = 0.09517485\n",
      "Iteration 11162, loss = 0.09517382\n",
      "Iteration 11163, loss = 0.09517280\n",
      "Iteration 11164, loss = 0.09517177\n",
      "Iteration 11165, loss = 0.09517074\n",
      "Iteration 11166, loss = 0.09516971\n",
      "Iteration 11167, loss = 0.09516869\n",
      "Iteration 11168, loss = 0.09516766\n",
      "Iteration 11169, loss = 0.09516663\n",
      "Iteration 11170, loss = 0.09516561\n",
      "Iteration 11171, loss = 0.09516458\n",
      "Iteration 11172, loss = 0.09516355\n",
      "Iteration 11173, loss = 0.09516253\n",
      "Iteration 11174, loss = 0.09516150\n",
      "Iteration 11175, loss = 0.09516048\n",
      "Iteration 11176, loss = 0.09515945\n",
      "Iteration 11177, loss = 0.09515843\n",
      "Iteration 11178, loss = 0.09515740\n",
      "Iteration 11179, loss = 0.09515638\n",
      "Iteration 11180, loss = 0.09515535\n",
      "Iteration 11181, loss = 0.09515433\n",
      "Iteration 11182, loss = 0.09515330\n",
      "Iteration 11183, loss = 0.09515228\n",
      "Iteration 11184, loss = 0.09515125\n",
      "Iteration 11185, loss = 0.09515023\n",
      "Iteration 11186, loss = 0.09514921\n",
      "Iteration 11187, loss = 0.09514818\n",
      "Iteration 11188, loss = 0.09514716\n",
      "Iteration 11189, loss = 0.09514614\n",
      "Iteration 11190, loss = 0.09514511\n",
      "Iteration 11191, loss = 0.09514409\n",
      "Iteration 11192, loss = 0.09514307\n",
      "Iteration 11193, loss = 0.09514204\n",
      "Iteration 11194, loss = 0.09514102\n",
      "Iteration 11195, loss = 0.09514000\n",
      "Iteration 11196, loss = 0.09513898\n",
      "Iteration 11197, loss = 0.09513796\n",
      "Iteration 11198, loss = 0.09513693\n",
      "Iteration 11199, loss = 0.09513591\n",
      "Iteration 11200, loss = 0.09513489\n",
      "Iteration 11201, loss = 0.09513387\n",
      "Iteration 11202, loss = 0.09513285\n",
      "Iteration 11203, loss = 0.09513183\n",
      "Iteration 11204, loss = 0.09513081\n",
      "Iteration 11205, loss = 0.09512979\n",
      "Iteration 11206, loss = 0.09512877\n",
      "Iteration 11207, loss = 0.09512775\n",
      "Iteration 11208, loss = 0.09512673\n",
      "Iteration 11209, loss = 0.09512571\n",
      "Iteration 11210, loss = 0.09512469\n",
      "Iteration 11211, loss = 0.09512367\n",
      "Iteration 11212, loss = 0.09512265\n",
      "Iteration 11213, loss = 0.09512163\n",
      "Iteration 11214, loss = 0.09512061\n",
      "Iteration 11215, loss = 0.09511959\n",
      "Iteration 11216, loss = 0.09511858\n",
      "Iteration 11217, loss = 0.09511756\n",
      "Iteration 11218, loss = 0.09511654\n",
      "Iteration 11219, loss = 0.09511552\n",
      "Iteration 11220, loss = 0.09511450\n",
      "Iteration 11221, loss = 0.09511349\n",
      "Iteration 11222, loss = 0.09511247\n",
      "Iteration 11223, loss = 0.09511145\n",
      "Iteration 11224, loss = 0.09511044\n",
      "Iteration 11225, loss = 0.09510942\n",
      "Iteration 11226, loss = 0.09510840\n",
      "Iteration 11227, loss = 0.09510739\n",
      "Iteration 11228, loss = 0.09510637\n",
      "Iteration 11229, loss = 0.09510535\n",
      "Iteration 11230, loss = 0.09510434\n",
      "Iteration 11231, loss = 0.09510332\n",
      "Iteration 11232, loss = 0.09510231\n",
      "Iteration 11233, loss = 0.09510129\n",
      "Iteration 11234, loss = 0.09510027\n",
      "Iteration 11235, loss = 0.09509926\n",
      "Iteration 11236, loss = 0.09509824\n",
      "Iteration 11237, loss = 0.09509723\n",
      "Iteration 11238, loss = 0.09509621\n",
      "Iteration 11239, loss = 0.09509520\n",
      "Iteration 11240, loss = 0.09509419\n",
      "Iteration 11241, loss = 0.09509317\n",
      "Iteration 11242, loss = 0.09509216\n",
      "Iteration 11243, loss = 0.09509114\n",
      "Iteration 11244, loss = 0.09509013\n",
      "Iteration 11245, loss = 0.09508912\n",
      "Iteration 11246, loss = 0.09508810\n",
      "Iteration 11247, loss = 0.09508709\n",
      "Iteration 11248, loss = 0.09508608\n",
      "Iteration 11249, loss = 0.09508507\n",
      "Iteration 11250, loss = 0.09508405\n",
      "Iteration 11251, loss = 0.09508304\n",
      "Iteration 11252, loss = 0.09508203\n",
      "Iteration 11253, loss = 0.09508102\n",
      "Iteration 11254, loss = 0.09508000\n",
      "Iteration 11255, loss = 0.09507899\n",
      "Iteration 11256, loss = 0.09507798\n",
      "Iteration 11257, loss = 0.09507697\n",
      "Iteration 11258, loss = 0.09507596\n",
      "Iteration 11259, loss = 0.09507495\n",
      "Iteration 11260, loss = 0.09507394\n",
      "Iteration 11261, loss = 0.09507293\n",
      "Iteration 11262, loss = 0.09507192\n",
      "Iteration 11263, loss = 0.09507091\n",
      "Iteration 11264, loss = 0.09506990\n",
      "Iteration 11265, loss = 0.09506889\n",
      "Iteration 11266, loss = 0.09506788\n",
      "Iteration 11267, loss = 0.09506687\n",
      "Iteration 11268, loss = 0.09506586\n",
      "Iteration 11269, loss = 0.09506485\n",
      "Iteration 11270, loss = 0.09506384\n",
      "Iteration 11271, loss = 0.09506283\n",
      "Iteration 11272, loss = 0.09506182\n",
      "Iteration 11273, loss = 0.09506081\n",
      "Iteration 11274, loss = 0.09505980\n",
      "Iteration 11275, loss = 0.09505880\n",
      "Iteration 11276, loss = 0.09505779\n",
      "Iteration 11277, loss = 0.09505678\n",
      "Iteration 11278, loss = 0.09505577\n",
      "Iteration 11279, loss = 0.09505476\n",
      "Iteration 11280, loss = 0.09505376\n",
      "Iteration 11281, loss = 0.09505275\n",
      "Iteration 11282, loss = 0.09505174\n",
      "Iteration 11283, loss = 0.09505074\n",
      "Iteration 11284, loss = 0.09504973\n",
      "Iteration 11285, loss = 0.09504872\n",
      "Iteration 11286, loss = 0.09504772\n",
      "Iteration 11287, loss = 0.09504671\n",
      "Iteration 11288, loss = 0.09504570\n",
      "Iteration 11289, loss = 0.09504470\n",
      "Iteration 11290, loss = 0.09504369\n",
      "Iteration 11291, loss = 0.09504269\n",
      "Iteration 11292, loss = 0.09504168\n",
      "Iteration 11293, loss = 0.09504068\n",
      "Iteration 11294, loss = 0.09503967\n",
      "Iteration 11295, loss = 0.09503867\n",
      "Iteration 11296, loss = 0.09503766\n",
      "Iteration 11297, loss = 0.09503666\n",
      "Iteration 11298, loss = 0.09503565\n",
      "Iteration 11299, loss = 0.09503465\n",
      "Iteration 11300, loss = 0.09503365\n",
      "Iteration 11301, loss = 0.09503264\n",
      "Iteration 11302, loss = 0.09503164\n",
      "Iteration 11303, loss = 0.09503064\n",
      "Iteration 11304, loss = 0.09502963\n",
      "Iteration 11305, loss = 0.09502863\n",
      "Iteration 11306, loss = 0.09502763\n",
      "Iteration 11307, loss = 0.09502662\n",
      "Iteration 11308, loss = 0.09502562\n",
      "Iteration 11309, loss = 0.09502462\n",
      "Iteration 11310, loss = 0.09502362\n",
      "Iteration 11311, loss = 0.09502261\n",
      "Iteration 11312, loss = 0.09502161\n",
      "Iteration 11313, loss = 0.09502061\n",
      "Iteration 11314, loss = 0.09501961\n",
      "Iteration 11315, loss = 0.09501861\n",
      "Iteration 11316, loss = 0.09501761\n",
      "Iteration 11317, loss = 0.09501660\n",
      "Iteration 11318, loss = 0.09501560\n",
      "Iteration 11319, loss = 0.09501460\n",
      "Iteration 11320, loss = 0.09501360\n",
      "Iteration 11321, loss = 0.09501260\n",
      "Iteration 11322, loss = 0.09501160\n",
      "Iteration 11323, loss = 0.09501060\n",
      "Iteration 11324, loss = 0.09500960\n",
      "Iteration 11325, loss = 0.09500860\n",
      "Iteration 11326, loss = 0.09500760\n",
      "Iteration 11327, loss = 0.09500660\n",
      "Iteration 11328, loss = 0.09500560\n",
      "Iteration 11329, loss = 0.09500461\n",
      "Iteration 11330, loss = 0.09500361\n",
      "Iteration 11331, loss = 0.09500261\n",
      "Iteration 11332, loss = 0.09500161\n",
      "Iteration 11333, loss = 0.09500061\n",
      "Iteration 11334, loss = 0.09499961\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 23.54465433\n",
      "Iteration 2, loss = 23.54465431\n",
      "Iteration 3, loss = 23.54465428\n",
      "Iteration 4, loss = 23.54465425\n",
      "Iteration 5, loss = 23.54465422\n",
      "Iteration 6, loss = 23.54465419\n",
      "Iteration 7, loss = 23.54465415\n",
      "Iteration 8, loss = 23.54465412\n",
      "Iteration 9, loss = 23.54465408\n",
      "Iteration 10, loss = 23.54465405\n",
      "Iteration 11, loss = 23.54465402\n",
      "Iteration 12, loss = 23.54465398\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.92402741\n",
      "Iteration 2, loss = 1.90287866\n",
      "Iteration 3, loss = 1.88186883\n",
      "Iteration 4, loss = 1.86098768\n",
      "Iteration 5, loss = 1.84019385\n",
      "Iteration 6, loss = 1.81954491\n",
      "Iteration 7, loss = 1.79903010\n",
      "Iteration 8, loss = 1.77868215\n",
      "Iteration 9, loss = 1.75847798\n",
      "Iteration 10, loss = 1.73841896\n",
      "Iteration 11, loss = 1.71853310\n",
      "Iteration 12, loss = 1.69879031\n",
      "Iteration 13, loss = 1.67919610\n",
      "Iteration 14, loss = 1.65975155\n",
      "Iteration 15, loss = 1.64046841\n",
      "Iteration 16, loss = 1.62137810\n",
      "Iteration 17, loss = 1.60245953\n",
      "Iteration 18, loss = 1.58365017\n",
      "Iteration 19, loss = 1.56495602\n",
      "Iteration 20, loss = 1.54640441\n",
      "Iteration 21, loss = 1.52798022\n",
      "Iteration 22, loss = 1.50971316\n",
      "Iteration 23, loss = 1.49160883\n",
      "Iteration 24, loss = 1.47365198\n",
      "Iteration 25, loss = 1.45582520\n",
      "Iteration 26, loss = 1.43815592\n",
      "Iteration 27, loss = 1.42064858\n",
      "Iteration 28, loss = 1.40329847\n",
      "Iteration 29, loss = 1.38610498\n",
      "Iteration 30, loss = 1.36906392\n",
      "Iteration 31, loss = 1.35215087\n",
      "Iteration 32, loss = 1.33539810\n",
      "Iteration 33, loss = 1.31875486\n",
      "Iteration 34, loss = 1.30224352\n",
      "Iteration 35, loss = 1.28587943\n",
      "Iteration 36, loss = 1.26966997\n",
      "Iteration 37, loss = 1.25361470\n",
      "Iteration 38, loss = 1.23771826\n",
      "Iteration 39, loss = 1.22198131\n",
      "Iteration 40, loss = 1.20639806\n",
      "Iteration 41, loss = 1.19095126\n",
      "Iteration 42, loss = 1.17564400\n",
      "Iteration 43, loss = 1.16047998\n",
      "Iteration 44, loss = 1.14545775\n",
      "Iteration 45, loss = 1.13057514\n",
      "Iteration 46, loss = 1.11583579\n",
      "Iteration 47, loss = 1.10122886\n",
      "Iteration 48, loss = 1.08676244\n",
      "Iteration 49, loss = 1.07244462\n",
      "Iteration 50, loss = 1.05826672\n",
      "Iteration 51, loss = 1.04422942\n",
      "Iteration 52, loss = 1.03033292\n",
      "Iteration 53, loss = 1.01657350\n",
      "Iteration 54, loss = 1.00294629\n",
      "Iteration 55, loss = 0.98945705\n",
      "Iteration 56, loss = 0.97610530\n",
      "Iteration 57, loss = 0.96288844\n",
      "Iteration 58, loss = 0.94980706\n",
      "Iteration 59, loss = 0.93685549\n",
      "Iteration 60, loss = 0.92403196\n",
      "Iteration 61, loss = 0.91133588\n",
      "Iteration 62, loss = 0.89876991\n",
      "Iteration 63, loss = 0.88632833\n",
      "Iteration 64, loss = 0.87402268\n",
      "Iteration 65, loss = 0.86184889\n",
      "Iteration 66, loss = 0.84980783\n",
      "Iteration 67, loss = 0.83789891\n",
      "Iteration 68, loss = 0.82612110\n",
      "Iteration 69, loss = 0.81447923\n",
      "Iteration 70, loss = 0.80297595\n",
      "Iteration 71, loss = 0.79159936\n",
      "Iteration 72, loss = 0.78035395\n",
      "Iteration 73, loss = 0.76925187\n",
      "Iteration 74, loss = 0.75828578\n",
      "Iteration 75, loss = 0.74745498\n",
      "Iteration 76, loss = 0.73676195\n",
      "Iteration 77, loss = 0.72620463\n",
      "Iteration 78, loss = 0.71579021\n",
      "Iteration 79, loss = 0.70552219\n",
      "Iteration 80, loss = 0.69539831\n",
      "Iteration 81, loss = 0.68541967\n",
      "Iteration 82, loss = 0.67557905\n",
      "Iteration 83, loss = 0.66587992\n",
      "Iteration 84, loss = 0.65632134\n",
      "Iteration 85, loss = 0.64690003\n",
      "Iteration 86, loss = 0.63761114\n",
      "Iteration 87, loss = 0.62846062\n",
      "Iteration 88, loss = 0.61944830\n",
      "Iteration 89, loss = 0.61057927\n",
      "Iteration 90, loss = 0.60185268\n",
      "Iteration 91, loss = 0.59326104\n",
      "Iteration 92, loss = 0.58480525\n",
      "Iteration 93, loss = 0.57648590\n",
      "Iteration 94, loss = 0.56830874\n",
      "Iteration 95, loss = 0.56026485\n",
      "Iteration 96, loss = 0.55235428\n",
      "Iteration 97, loss = 0.54457776\n",
      "Iteration 98, loss = 0.53693187\n",
      "Iteration 99, loss = 0.52941520\n",
      "Iteration 100, loss = 0.52202776\n",
      "Iteration 101, loss = 0.51476913\n",
      "Iteration 102, loss = 0.50763983\n",
      "Iteration 103, loss = 0.50063702\n",
      "Iteration 104, loss = 0.49376056\n",
      "Iteration 105, loss = 0.48701048\n",
      "Iteration 106, loss = 0.48038338\n",
      "Iteration 107, loss = 0.47388087\n",
      "Iteration 108, loss = 0.46750206\n",
      "Iteration 109, loss = 0.46124622\n",
      "Iteration 110, loss = 0.45511396\n",
      "Iteration 111, loss = 0.44910150\n",
      "Iteration 112, loss = 0.44320779\n",
      "Iteration 113, loss = 0.43743157\n",
      "Iteration 114, loss = 0.43177022\n",
      "Iteration 115, loss = 0.42622087\n",
      "Iteration 116, loss = 0.42078106\n",
      "Iteration 117, loss = 0.41545426\n",
      "Iteration 118, loss = 0.41023882\n",
      "Iteration 119, loss = 0.40512848\n",
      "Iteration 120, loss = 0.40012134\n",
      "Iteration 121, loss = 0.39521451\n",
      "Iteration 122, loss = 0.39040774\n",
      "Iteration 123, loss = 0.38569610\n",
      "Iteration 124, loss = 0.38107869\n",
      "Iteration 125, loss = 0.37655540\n",
      "Iteration 126, loss = 0.37212282\n",
      "Iteration 127, loss = 0.36777888\n",
      "Iteration 128, loss = 0.36352187\n",
      "Iteration 129, loss = 0.35935148\n",
      "Iteration 130, loss = 0.35526473\n",
      "Iteration 131, loss = 0.35126121\n",
      "Iteration 132, loss = 0.34733866\n",
      "Iteration 133, loss = 0.34349537\n",
      "Iteration 134, loss = 0.33972841\n",
      "Iteration 135, loss = 0.33603613\n",
      "Iteration 136, loss = 0.33241877\n",
      "Iteration 137, loss = 0.32887292\n",
      "Iteration 138, loss = 0.32539761\n",
      "Iteration 139, loss = 0.32199243\n",
      "Iteration 140, loss = 0.31865491\n",
      "Iteration 141, loss = 0.31538497\n",
      "Iteration 142, loss = 0.31218074\n",
      "Iteration 143, loss = 0.30904017\n",
      "Iteration 144, loss = 0.30596246\n",
      "Iteration 145, loss = 0.30294448\n",
      "Iteration 146, loss = 0.29998470\n",
      "Iteration 147, loss = 0.29708093\n",
      "Iteration 148, loss = 0.29423221\n",
      "Iteration 149, loss = 0.29143646\n",
      "Iteration 150, loss = 0.28869351\n",
      "Iteration 151, loss = 0.28600224\n",
      "Iteration 152, loss = 0.28336465\n",
      "Iteration 153, loss = 0.28077626\n",
      "Iteration 154, loss = 0.27823591\n",
      "Iteration 155, loss = 0.27573831\n",
      "Iteration 156, loss = 0.27328292\n",
      "Iteration 157, loss = 0.27087309\n",
      "Iteration 158, loss = 0.26850667\n",
      "Iteration 159, loss = 0.26618419\n",
      "Iteration 160, loss = 0.26390345\n",
      "Iteration 161, loss = 0.26166317\n",
      "Iteration 162, loss = 0.25946218\n",
      "Iteration 163, loss = 0.25729954\n",
      "Iteration 164, loss = 0.25517484\n",
      "Iteration 165, loss = 0.25308802\n",
      "Iteration 166, loss = 0.25103758\n",
      "Iteration 167, loss = 0.24902253\n",
      "Iteration 168, loss = 0.24704208\n",
      "Iteration 169, loss = 0.24509547\n",
      "Iteration 170, loss = 0.24318179\n",
      "Iteration 171, loss = 0.24130047\n",
      "Iteration 172, loss = 0.23945091\n",
      "Iteration 173, loss = 0.23763468\n",
      "Iteration 174, loss = 0.23584976\n",
      "Iteration 175, loss = 0.23409483\n",
      "Iteration 176, loss = 0.23237033\n",
      "Iteration 177, loss = 0.23067529\n",
      "Iteration 178, loss = 0.22900894\n",
      "Iteration 179, loss = 0.22737103\n",
      "Iteration 180, loss = 0.22575997\n",
      "Iteration 181, loss = 0.22417515\n",
      "Iteration 182, loss = 0.22261614\n",
      "Iteration 183, loss = 0.22108304\n",
      "Iteration 184, loss = 0.21957453\n",
      "Iteration 185, loss = 0.21809008\n",
      "Iteration 186, loss = 0.21662906\n",
      "Iteration 187, loss = 0.21519146\n",
      "Iteration 188, loss = 0.21377721\n",
      "Iteration 189, loss = 0.21238582\n",
      "Iteration 190, loss = 0.21101634\n",
      "Iteration 191, loss = 0.20966862\n",
      "Iteration 192, loss = 0.20834215\n",
      "Iteration 193, loss = 0.20703720\n",
      "Iteration 194, loss = 0.20575225\n",
      "Iteration 195, loss = 0.20448650\n",
      "Iteration 196, loss = 0.20324114\n",
      "Iteration 197, loss = 0.20201520\n",
      "Iteration 198, loss = 0.20080819\n",
      "Iteration 199, loss = 0.19961917\n",
      "Iteration 200, loss = 0.19844776\n",
      "Iteration 201, loss = 0.19729372\n",
      "Iteration 202, loss = 0.19615692\n",
      "Iteration 203, loss = 0.19503669\n",
      "Iteration 204, loss = 0.19393308\n",
      "Iteration 205, loss = 0.19284687\n",
      "Iteration 206, loss = 0.19177745\n",
      "Iteration 207, loss = 0.19072498\n",
      "Iteration 208, loss = 0.18968761\n",
      "Iteration 209, loss = 0.18866511\n",
      "Iteration 210, loss = 0.18765711\n",
      "Iteration 211, loss = 0.18666329\n",
      "Iteration 212, loss = 0.18568337\n",
      "Iteration 213, loss = 0.18471714\n",
      "Iteration 214, loss = 0.18376452\n",
      "Iteration 215, loss = 0.18282506\n",
      "Iteration 216, loss = 0.18189882\n",
      "Iteration 217, loss = 0.18098525\n",
      "Iteration 218, loss = 0.18008399\n",
      "Iteration 219, loss = 0.17919480\n",
      "Iteration 220, loss = 0.17831745\n",
      "Iteration 221, loss = 0.17745171\n",
      "Iteration 222, loss = 0.17659736\n",
      "Iteration 223, loss = 0.17575418\n",
      "Iteration 224, loss = 0.17492204\n",
      "Iteration 225, loss = 0.17410107\n",
      "Iteration 226, loss = 0.17329069\n",
      "Iteration 227, loss = 0.17249072\n",
      "Iteration 228, loss = 0.17170153\n",
      "Iteration 229, loss = 0.17092250\n",
      "Iteration 230, loss = 0.17015346\n",
      "Iteration 231, loss = 0.16939429\n",
      "Iteration 232, loss = 0.16864480\n",
      "Iteration 233, loss = 0.16790476\n",
      "Iteration 234, loss = 0.16717398\n",
      "Iteration 235, loss = 0.16645232\n",
      "Iteration 236, loss = 0.16573958\n",
      "Iteration 237, loss = 0.16503559\n",
      "Iteration 238, loss = 0.16434024\n",
      "Iteration 239, loss = 0.16365341\n",
      "Iteration 240, loss = 0.16297490\n",
      "Iteration 241, loss = 0.16230477\n",
      "Iteration 242, loss = 0.16164282\n",
      "Iteration 243, loss = 0.16098894\n",
      "Iteration 244, loss = 0.16034286\n",
      "Iteration 245, loss = 0.15970445\n",
      "Iteration 246, loss = 0.15907358\n",
      "Iteration 247, loss = 0.15845013\n",
      "Iteration 248, loss = 0.15783397\n",
      "Iteration 249, loss = 0.15722497\n",
      "Iteration 250, loss = 0.15662304\n",
      "Iteration 251, loss = 0.15602805\n",
      "Iteration 252, loss = 0.15543991\n",
      "Iteration 253, loss = 0.15485852\n",
      "Iteration 254, loss = 0.15428384\n",
      "Iteration 255, loss = 0.15371568\n",
      "Iteration 256, loss = 0.15315395\n",
      "Iteration 257, loss = 0.15259862\n",
      "Iteration 258, loss = 0.15204954\n",
      "Iteration 259, loss = 0.15150659\n",
      "Iteration 260, loss = 0.15096994\n",
      "Iteration 261, loss = 0.15043946\n",
      "Iteration 262, loss = 0.14991485\n",
      "Iteration 263, loss = 0.14939626\n",
      "Iteration 264, loss = 0.14888351\n",
      "Iteration 265, loss = 0.14837681\n",
      "Iteration 266, loss = 0.14787574\n",
      "Iteration 267, loss = 0.14738016\n",
      "Iteration 268, loss = 0.14688998\n",
      "Iteration 269, loss = 0.14640511\n",
      "Iteration 270, loss = 0.14592546\n",
      "Iteration 271, loss = 0.14545099\n",
      "Iteration 272, loss = 0.14498162\n",
      "Iteration 273, loss = 0.14451740\n",
      "Iteration 274, loss = 0.14405832\n",
      "Iteration 275, loss = 0.14360409\n",
      "Iteration 276, loss = 0.14315463\n",
      "Iteration 277, loss = 0.14270985\n",
      "Iteration 278, loss = 0.14226969\n",
      "Iteration 279, loss = 0.14183408\n",
      "Iteration 280, loss = 0.14140296\n",
      "Iteration 281, loss = 0.14097624\n",
      "Iteration 282, loss = 0.14055388\n",
      "Iteration 283, loss = 0.14013581\n",
      "Iteration 284, loss = 0.13972196\n",
      "Iteration 285, loss = 0.13931228\n",
      "Iteration 286, loss = 0.13890671\n",
      "Iteration 287, loss = 0.13850520\n",
      "Iteration 288, loss = 0.13810767\n",
      "Iteration 289, loss = 0.13771424\n",
      "Iteration 290, loss = 0.13732494\n",
      "Iteration 291, loss = 0.13693956\n",
      "Iteration 292, loss = 0.13655798\n",
      "Iteration 293, loss = 0.13618058\n",
      "Iteration 294, loss = 0.13580692\n",
      "Iteration 295, loss = 0.13543688\n",
      "Iteration 296, loss = 0.13507043\n",
      "Iteration 297, loss = 0.13470750\n",
      "Iteration 298, loss = 0.13434806\n",
      "Iteration 299, loss = 0.13399205\n",
      "Iteration 300, loss = 0.13363942\n",
      "Iteration 301, loss = 0.13329011\n",
      "Iteration 302, loss = 0.13294410\n",
      "Iteration 303, loss = 0.13260133\n",
      "Iteration 304, loss = 0.13226175\n",
      "Iteration 305, loss = 0.13192532\n",
      "Iteration 306, loss = 0.13159200\n",
      "Iteration 307, loss = 0.13126182\n",
      "Iteration 308, loss = 0.13093473\n",
      "Iteration 309, loss = 0.13061062\n",
      "Iteration 310, loss = 0.13028945\n",
      "Iteration 311, loss = 0.12997151\n",
      "Iteration 312, loss = 0.12965648\n",
      "Iteration 313, loss = 0.12934434\n",
      "Iteration 314, loss = 0.12903517\n",
      "Iteration 315, loss = 0.12872882\n",
      "Iteration 316, loss = 0.12842523\n",
      "Iteration 317, loss = 0.12812436\n",
      "Iteration 318, loss = 0.12782617\n",
      "Iteration 319, loss = 0.12753062\n",
      "Iteration 320, loss = 0.12723767\n",
      "Iteration 321, loss = 0.12694729\n",
      "Iteration 322, loss = 0.12665945\n",
      "Iteration 323, loss = 0.12637412\n",
      "Iteration 324, loss = 0.12609125\n",
      "Iteration 325, loss = 0.12581082\n",
      "Iteration 326, loss = 0.12553278\n",
      "Iteration 327, loss = 0.12525713\n",
      "Iteration 328, loss = 0.12498382\n",
      "Iteration 329, loss = 0.12471282\n",
      "Iteration 330, loss = 0.12444413\n",
      "Iteration 331, loss = 0.12417771\n",
      "Iteration 332, loss = 0.12391353\n",
      "Iteration 333, loss = 0.12365155\n",
      "Iteration 334, loss = 0.12339179\n",
      "Iteration 335, loss = 0.12313428\n",
      "Iteration 336, loss = 0.12287891\n",
      "Iteration 337, loss = 0.12262565\n",
      "Iteration 338, loss = 0.12237446\n",
      "Iteration 339, loss = 0.12212533\n",
      "Iteration 340, loss = 0.12187823\n",
      "Iteration 341, loss = 0.12163313\n",
      "Iteration 342, loss = 0.12139001\n",
      "Iteration 343, loss = 0.12114885\n",
      "Iteration 344, loss = 0.12090961\n",
      "Iteration 345, loss = 0.12067228\n",
      "Iteration 346, loss = 0.12043684\n",
      "Iteration 347, loss = 0.12020326\n",
      "Iteration 348, loss = 0.11997158\n",
      "Iteration 349, loss = 0.11974191\n",
      "Iteration 350, loss = 0.11951410\n",
      "Iteration 351, loss = 0.11928811\n",
      "Iteration 352, loss = 0.11906389\n",
      "Iteration 353, loss = 0.11884143\n",
      "Iteration 354, loss = 0.11862070\n",
      "Iteration 355, loss = 0.11840168\n",
      "Iteration 356, loss = 0.11818435\n",
      "Iteration 357, loss = 0.11796870\n",
      "Iteration 358, loss = 0.11775469\n",
      "Iteration 359, loss = 0.11754232\n",
      "Iteration 360, loss = 0.11733161\n",
      "Iteration 361, loss = 0.11712253\n",
      "Iteration 362, loss = 0.11691503\n",
      "Iteration 363, loss = 0.11670909\n",
      "Iteration 364, loss = 0.11650470\n",
      "Iteration 365, loss = 0.11630184\n",
      "Iteration 366, loss = 0.11610048\n",
      "Iteration 367, loss = 0.11590058\n",
      "Iteration 368, loss = 0.11570215\n",
      "Iteration 369, loss = 0.11550518\n",
      "Iteration 370, loss = 0.11530964\n",
      "Iteration 371, loss = 0.11511555\n",
      "Iteration 372, loss = 0.11492287\n",
      "Iteration 373, loss = 0.11473166\n",
      "Iteration 374, loss = 0.11454183\n",
      "Iteration 375, loss = 0.11435337\n",
      "Iteration 376, loss = 0.11416627\n",
      "Iteration 377, loss = 0.11398049\n",
      "Iteration 378, loss = 0.11379604\n",
      "Iteration 379, loss = 0.11361290\n",
      "Iteration 380, loss = 0.11343110\n",
      "Iteration 381, loss = 0.11325057\n",
      "Iteration 382, loss = 0.11307132\n",
      "Iteration 383, loss = 0.11289332\n",
      "Iteration 384, loss = 0.11271656\n",
      "Iteration 385, loss = 0.11254103\n",
      "Iteration 386, loss = 0.11236672\n",
      "Iteration 387, loss = 0.11219360\n",
      "Iteration 388, loss = 0.11202167\n",
      "Iteration 389, loss = 0.11185091\n",
      "Iteration 390, loss = 0.11168155\n",
      "Iteration 391, loss = 0.11151339\n",
      "Iteration 392, loss = 0.11134639\n",
      "Iteration 393, loss = 0.11118053\n",
      "Iteration 394, loss = 0.11101581\n",
      "Iteration 395, loss = 0.11085220\n",
      "Iteration 396, loss = 0.11068970\n",
      "Iteration 397, loss = 0.11052829\n",
      "Iteration 398, loss = 0.11036796\n",
      "Iteration 399, loss = 0.11020870\n",
      "Iteration 400, loss = 0.11005050\n",
      "Iteration 401, loss = 0.10989334\n",
      "Iteration 402, loss = 0.10973721\n",
      "Iteration 403, loss = 0.10958210\n",
      "Iteration 404, loss = 0.10942799\n",
      "Iteration 405, loss = 0.10927489\n",
      "Iteration 406, loss = 0.10912277\n",
      "Iteration 407, loss = 0.10897163\n",
      "Iteration 408, loss = 0.10882146\n",
      "Iteration 409, loss = 0.10867225\n",
      "Iteration 410, loss = 0.10852397\n",
      "Iteration 411, loss = 0.10837664\n",
      "Iteration 412, loss = 0.10823023\n",
      "Iteration 413, loss = 0.10808474\n",
      "Iteration 414, loss = 0.10794018\n",
      "Iteration 415, loss = 0.10779658\n",
      "Iteration 416, loss = 0.10765388\n",
      "Iteration 417, loss = 0.10751206\n",
      "Iteration 418, loss = 0.10737111\n",
      "Iteration 419, loss = 0.10723103\n",
      "Iteration 420, loss = 0.10709180\n",
      "Iteration 421, loss = 0.10695341\n",
      "Iteration 422, loss = 0.10681587\n",
      "Iteration 423, loss = 0.10667915\n",
      "Iteration 424, loss = 0.10654326\n",
      "Iteration 425, loss = 0.10640817\n",
      "Iteration 426, loss = 0.10627389\n",
      "Iteration 427, loss = 0.10614040\n",
      "Iteration 428, loss = 0.10600778\n",
      "Iteration 429, loss = 0.10587595\n",
      "Iteration 430, loss = 0.10574491\n",
      "Iteration 431, loss = 0.10561463\n",
      "Iteration 432, loss = 0.10548511\n",
      "Iteration 433, loss = 0.10535635\n",
      "Iteration 434, loss = 0.10522834\n",
      "Iteration 435, loss = 0.10510107\n",
      "Iteration 436, loss = 0.10497453\n",
      "Iteration 437, loss = 0.10484871\n",
      "Iteration 438, loss = 0.10472362\n",
      "Iteration 439, loss = 0.10459923\n",
      "Iteration 440, loss = 0.10447554\n",
      "Iteration 441, loss = 0.10435255\n",
      "Iteration 442, loss = 0.10423025\n",
      "Iteration 443, loss = 0.10410864\n",
      "Iteration 444, loss = 0.10398769\n",
      "Iteration 445, loss = 0.10386742\n",
      "Iteration 446, loss = 0.10374781\n",
      "Iteration 447, loss = 0.10362885\n",
      "Iteration 448, loss = 0.10351054\n",
      "Iteration 449, loss = 0.10339288\n",
      "Iteration 450, loss = 0.10327585\n",
      "Iteration 451, loss = 0.10315947\n",
      "Iteration 452, loss = 0.10304372\n",
      "Iteration 453, loss = 0.10292843\n",
      "Iteration 454, loss = 0.10281342\n",
      "Iteration 455, loss = 0.10269897\n",
      "Iteration 456, loss = 0.10258512\n",
      "Iteration 457, loss = 0.10247205\n",
      "Iteration 458, loss = 0.10235978\n",
      "Iteration 459, loss = 0.10224809\n",
      "Iteration 460, loss = 0.10213698\n",
      "Iteration 461, loss = 0.10202645\n",
      "Iteration 462, loss = 0.10191649\n",
      "Iteration 463, loss = 0.10180712\n",
      "Iteration 464, loss = 0.10169835\n",
      "Iteration 465, loss = 0.10159015\n",
      "Iteration 466, loss = 0.10148250\n",
      "Iteration 467, loss = 0.10137548\n",
      "Iteration 468, loss = 0.10126901\n",
      "Iteration 469, loss = 0.10116317\n",
      "Iteration 470, loss = 0.10105797\n",
      "Iteration 471, loss = 0.10095330\n",
      "Iteration 472, loss = 0.10084908\n",
      "Iteration 473, loss = 0.10074535\n",
      "Iteration 474, loss = 0.10064215\n",
      "Iteration 475, loss = 0.10053946\n",
      "Iteration 476, loss = 0.10043730\n",
      "Iteration 477, loss = 0.10033564\n",
      "Iteration 478, loss = 0.10023449\n",
      "Iteration 479, loss = 0.10013385\n",
      "Iteration 480, loss = 0.10003371\n",
      "Iteration 481, loss = 0.09993406\n",
      "Iteration 482, loss = 0.09983489\n",
      "Iteration 483, loss = 0.09973633\n",
      "Iteration 484, loss = 0.09963853\n",
      "Iteration 485, loss = 0.09954122\n",
      "Iteration 486, loss = 0.09944441\n",
      "Iteration 487, loss = 0.09934809\n",
      "Iteration 488, loss = 0.09925225\n",
      "Iteration 489, loss = 0.09915689\n",
      "Iteration 490, loss = 0.09906203\n",
      "Iteration 491, loss = 0.09896766\n",
      "Iteration 492, loss = 0.09887375\n",
      "Iteration 493, loss = 0.09878030\n",
      "Iteration 494, loss = 0.09868731\n",
      "Iteration 495, loss = 0.09859479\n",
      "Iteration 496, loss = 0.09850283\n",
      "Iteration 497, loss = 0.09841132\n",
      "Iteration 498, loss = 0.09832026\n",
      "Iteration 499, loss = 0.09822963\n",
      "Iteration 500, loss = 0.09813943\n",
      "Iteration 501, loss = 0.09804968\n",
      "Iteration 502, loss = 0.09796035\n",
      "Iteration 503, loss = 0.09787145\n",
      "Iteration 504, loss = 0.09778296\n",
      "Iteration 505, loss = 0.09769489\n",
      "Iteration 506, loss = 0.09760722\n",
      "Iteration 507, loss = 0.09751995\n",
      "Iteration 508, loss = 0.09743308\n",
      "Iteration 509, loss = 0.09734661\n",
      "Iteration 510, loss = 0.09726053\n",
      "Iteration 511, loss = 0.09717484\n",
      "Iteration 512, loss = 0.09708954\n",
      "Iteration 513, loss = 0.09700500\n",
      "Iteration 514, loss = 0.09692098\n",
      "Iteration 515, loss = 0.09683745\n",
      "Iteration 516, loss = 0.09675432\n",
      "Iteration 517, loss = 0.09667158\n",
      "Iteration 518, loss = 0.09658937\n",
      "Iteration 519, loss = 0.09650784\n",
      "Iteration 520, loss = 0.09642673\n",
      "Iteration 521, loss = 0.09634602\n",
      "Iteration 522, loss = 0.09626573\n",
      "Iteration 523, loss = 0.09618592\n",
      "Iteration 524, loss = 0.09610649\n",
      "Iteration 525, loss = 0.09602746\n",
      "Iteration 526, loss = 0.09594886\n",
      "Iteration 527, loss = 0.09587063\n",
      "Iteration 528, loss = 0.09579277\n",
      "Iteration 529, loss = 0.09571540\n",
      "Iteration 530, loss = 0.09563858\n",
      "Iteration 531, loss = 0.09556212\n",
      "Iteration 532, loss = 0.09548604\n",
      "Iteration 533, loss = 0.09541056\n",
      "Iteration 534, loss = 0.09533552\n",
      "Iteration 535, loss = 0.09526086\n",
      "Iteration 536, loss = 0.09518655\n",
      "Iteration 537, loss = 0.09511260\n",
      "Iteration 538, loss = 0.09503901\n",
      "Iteration 539, loss = 0.09496576\n",
      "Iteration 540, loss = 0.09489285\n",
      "Iteration 541, loss = 0.09482027\n",
      "Iteration 542, loss = 0.09474802\n",
      "Iteration 543, loss = 0.09467610\n",
      "Iteration 544, loss = 0.09460449\n",
      "Iteration 545, loss = 0.09453322\n",
      "Iteration 546, loss = 0.09446229\n",
      "Iteration 547, loss = 0.09439168\n",
      "Iteration 548, loss = 0.09432149\n",
      "Iteration 549, loss = 0.09425164\n",
      "Iteration 550, loss = 0.09418211\n",
      "Iteration 551, loss = 0.09411287\n",
      "Iteration 552, loss = 0.09404394\n",
      "Iteration 553, loss = 0.09397530\n",
      "Iteration 554, loss = 0.09390696\n",
      "Iteration 555, loss = 0.09383890\n",
      "Iteration 556, loss = 0.09377113\n",
      "Iteration 557, loss = 0.09370364\n",
      "Iteration 558, loss = 0.09363644\n",
      "Iteration 559, loss = 0.09356953\n",
      "Iteration 560, loss = 0.09350290\n",
      "Iteration 561, loss = 0.09343654\n",
      "Iteration 562, loss = 0.09337045\n",
      "Iteration 563, loss = 0.09330462\n",
      "Iteration 564, loss = 0.09323905\n",
      "Iteration 565, loss = 0.09317374\n",
      "Iteration 566, loss = 0.09310869\n",
      "Iteration 567, loss = 0.09304389\n",
      "Iteration 568, loss = 0.09297934\n",
      "Iteration 569, loss = 0.09291504\n",
      "Iteration 570, loss = 0.09285098\n",
      "Iteration 571, loss = 0.09278717\n",
      "Iteration 572, loss = 0.09272360\n",
      "Iteration 573, loss = 0.09266026\n",
      "Iteration 574, loss = 0.09259716\n",
      "Iteration 575, loss = 0.09253421\n",
      "Iteration 576, loss = 0.09247137\n",
      "Iteration 577, loss = 0.09240873\n",
      "Iteration 578, loss = 0.09234631\n",
      "Iteration 579, loss = 0.09228410\n",
      "Iteration 580, loss = 0.09222209\n",
      "Iteration 581, loss = 0.09216030\n",
      "Iteration 582, loss = 0.09209872\n",
      "Iteration 583, loss = 0.09203735\n",
      "Iteration 584, loss = 0.09197618\n",
      "Iteration 585, loss = 0.09191522\n",
      "Iteration 586, loss = 0.09185447\n",
      "Iteration 587, loss = 0.09179393\n",
      "Iteration 588, loss = 0.09173359\n",
      "Iteration 589, loss = 0.09167345\n",
      "Iteration 590, loss = 0.09161353\n",
      "Iteration 591, loss = 0.09155419\n",
      "Iteration 592, loss = 0.09149507\n",
      "Iteration 593, loss = 0.09143618\n",
      "Iteration 594, loss = 0.09137751\n",
      "Iteration 595, loss = 0.09131923\n",
      "Iteration 596, loss = 0.09126132\n",
      "Iteration 597, loss = 0.09120375\n",
      "Iteration 598, loss = 0.09114639\n",
      "Iteration 599, loss = 0.09108924\n",
      "Iteration 600, loss = 0.09103230\n",
      "Iteration 601, loss = 0.09097557\n",
      "Iteration 602, loss = 0.09091904\n",
      "Iteration 603, loss = 0.09086271\n",
      "Iteration 604, loss = 0.09080658\n",
      "Iteration 605, loss = 0.09075064\n",
      "Iteration 606, loss = 0.09069490\n",
      "Iteration 607, loss = 0.09063934\n",
      "Iteration 608, loss = 0.09058397\n",
      "Iteration 609, loss = 0.09052879\n",
      "Iteration 610, loss = 0.09047380\n",
      "Iteration 611, loss = 0.09041898\n",
      "Iteration 612, loss = 0.09036435\n",
      "Iteration 613, loss = 0.09030990\n",
      "Iteration 614, loss = 0.09025562\n",
      "Iteration 615, loss = 0.09020152\n",
      "Iteration 616, loss = 0.09014760\n",
      "Iteration 617, loss = 0.09009384\n",
      "Iteration 618, loss = 0.09004026\n",
      "Iteration 619, loss = 0.08998684\n",
      "Iteration 620, loss = 0.08993359\n",
      "Iteration 621, loss = 0.08988050\n",
      "Iteration 622, loss = 0.08982758\n",
      "Iteration 623, loss = 0.08977484\n",
      "Iteration 624, loss = 0.08972247\n",
      "Iteration 625, loss = 0.08967047\n",
      "Iteration 626, loss = 0.08961864\n",
      "Iteration 627, loss = 0.08956697\n",
      "Iteration 628, loss = 0.08951546\n",
      "Iteration 629, loss = 0.08946411\n",
      "Iteration 630, loss = 0.08941292\n",
      "Iteration 631, loss = 0.08936196\n",
      "Iteration 632, loss = 0.08931121\n",
      "Iteration 633, loss = 0.08926061\n",
      "Iteration 634, loss = 0.08921017\n",
      "Iteration 635, loss = 0.08915988\n",
      "Iteration 636, loss = 0.08910975\n",
      "Iteration 637, loss = 0.08905977\n",
      "Iteration 638, loss = 0.08900996\n",
      "Iteration 639, loss = 0.08896031\n",
      "Iteration 640, loss = 0.08891080\n",
      "Iteration 641, loss = 0.08886143\n",
      "Iteration 642, loss = 0.08881222\n",
      "Iteration 643, loss = 0.08876315\n",
      "Iteration 644, loss = 0.08871422\n",
      "Iteration 645, loss = 0.08866543\n",
      "Iteration 646, loss = 0.08861678\n",
      "Iteration 647, loss = 0.08856828\n",
      "Iteration 648, loss = 0.08851990\n",
      "Iteration 649, loss = 0.08847165\n",
      "Iteration 650, loss = 0.08842355\n",
      "Iteration 651, loss = 0.08837557\n",
      "Iteration 652, loss = 0.08832773\n",
      "Iteration 653, loss = 0.08828001\n",
      "Iteration 654, loss = 0.08823241\n",
      "Iteration 655, loss = 0.08818495\n",
      "Iteration 656, loss = 0.08813761\n",
      "Iteration 657, loss = 0.08809039\n",
      "Iteration 658, loss = 0.08804329\n",
      "Iteration 659, loss = 0.08799631\n",
      "Iteration 660, loss = 0.08794945\n",
      "Iteration 661, loss = 0.08790288\n",
      "Iteration 662, loss = 0.08785644\n",
      "Iteration 663, loss = 0.08781012\n",
      "Iteration 664, loss = 0.08776393\n",
      "Iteration 665, loss = 0.08771788\n",
      "Iteration 666, loss = 0.08767197\n",
      "Iteration 667, loss = 0.08762620\n",
      "Iteration 668, loss = 0.08758054\n",
      "Iteration 669, loss = 0.08753499\n",
      "Iteration 670, loss = 0.08748958\n",
      "Iteration 671, loss = 0.08744428\n",
      "Iteration 672, loss = 0.08739910\n",
      "Iteration 673, loss = 0.08735403\n",
      "Iteration 674, loss = 0.08730909\n",
      "Iteration 675, loss = 0.08726426\n",
      "Iteration 676, loss = 0.08721954\n",
      "Iteration 677, loss = 0.08717496\n",
      "Iteration 678, loss = 0.08713054\n",
      "Iteration 679, loss = 0.08708624\n",
      "Iteration 680, loss = 0.08704200\n",
      "Iteration 681, loss = 0.08699781\n",
      "Iteration 682, loss = 0.08695372\n",
      "Iteration 683, loss = 0.08690973\n",
      "Iteration 684, loss = 0.08686585\n",
      "Iteration 685, loss = 0.08682206\n",
      "Iteration 686, loss = 0.08677835\n",
      "Iteration 687, loss = 0.08673478\n",
      "Iteration 688, loss = 0.08669129\n",
      "Iteration 689, loss = 0.08664790\n",
      "Iteration 690, loss = 0.08660479\n",
      "Iteration 691, loss = 0.08656185\n",
      "Iteration 692, loss = 0.08651901\n",
      "Iteration 693, loss = 0.08647631\n",
      "Iteration 694, loss = 0.08643371\n",
      "Iteration 695, loss = 0.08639120\n",
      "Iteration 696, loss = 0.08634879\n",
      "Iteration 697, loss = 0.08630646\n",
      "Iteration 698, loss = 0.08626427\n",
      "Iteration 699, loss = 0.08622218\n",
      "Iteration 700, loss = 0.08618017\n",
      "Iteration 701, loss = 0.08613827\n",
      "Iteration 702, loss = 0.08609646\n",
      "Iteration 703, loss = 0.08605477\n",
      "Iteration 704, loss = 0.08601316\n",
      "Iteration 705, loss = 0.08597164\n",
      "Iteration 706, loss = 0.08593021\n",
      "Iteration 707, loss = 0.08588958\n",
      "Iteration 708, loss = 0.08584947\n",
      "Iteration 709, loss = 0.08580949\n",
      "Iteration 710, loss = 0.08576964\n",
      "Iteration 711, loss = 0.08572994\n",
      "Iteration 712, loss = 0.08569037\n",
      "Iteration 713, loss = 0.08565091\n",
      "Iteration 714, loss = 0.08561158\n",
      "Iteration 715, loss = 0.08557238\n",
      "Iteration 716, loss = 0.08553329\n",
      "Iteration 717, loss = 0.08549429\n",
      "Iteration 718, loss = 0.08545539\n",
      "Iteration 719, loss = 0.08541662\n",
      "Iteration 720, loss = 0.08537825\n",
      "Iteration 721, loss = 0.08534006\n",
      "Iteration 722, loss = 0.08530200\n",
      "Iteration 723, loss = 0.08526406\n",
      "Iteration 724, loss = 0.08522624\n",
      "Iteration 725, loss = 0.08518885\n",
      "Iteration 726, loss = 0.08515162\n",
      "Iteration 727, loss = 0.08511449\n",
      "Iteration 728, loss = 0.08507750\n",
      "Iteration 729, loss = 0.08504059\n",
      "Iteration 730, loss = 0.08500380\n",
      "Iteration 731, loss = 0.08496713\n",
      "Iteration 732, loss = 0.08493057\n",
      "Iteration 733, loss = 0.08489410\n",
      "Iteration 734, loss = 0.08485773\n",
      "Iteration 735, loss = 0.08482148\n",
      "Iteration 736, loss = 0.08478532\n",
      "Iteration 737, loss = 0.08474923\n",
      "Iteration 738, loss = 0.08471325\n",
      "Iteration 739, loss = 0.08467736\n",
      "Iteration 740, loss = 0.08464155\n",
      "Iteration 741, loss = 0.08460583\n",
      "Iteration 742, loss = 0.08457018\n",
      "Iteration 743, loss = 0.08453483\n",
      "Iteration 744, loss = 0.08449957\n",
      "Iteration 745, loss = 0.08446441\n",
      "Iteration 746, loss = 0.08442933\n",
      "Iteration 747, loss = 0.08439433\n",
      "Iteration 748, loss = 0.08435941\n",
      "Iteration 749, loss = 0.08432459\n",
      "Iteration 750, loss = 0.08428984\n",
      "Iteration 751, loss = 0.08425517\n",
      "Iteration 752, loss = 0.08422057\n",
      "Iteration 753, loss = 0.08418614\n",
      "Iteration 754, loss = 0.08415182\n",
      "Iteration 755, loss = 0.08411761\n",
      "Iteration 756, loss = 0.08408347\n",
      "Iteration 757, loss = 0.08404938\n",
      "Iteration 758, loss = 0.08401538\n",
      "Iteration 759, loss = 0.08398145\n",
      "Iteration 760, loss = 0.08394760\n",
      "Iteration 761, loss = 0.08391381\n",
      "Iteration 762, loss = 0.08388010\n",
      "Iteration 763, loss = 0.08384646\n",
      "Iteration 764, loss = 0.08381289\n",
      "Iteration 765, loss = 0.08377939\n",
      "Iteration 766, loss = 0.08374599\n",
      "Iteration 767, loss = 0.08371265\n",
      "Iteration 768, loss = 0.08367936\n",
      "Iteration 769, loss = 0.08364615\n",
      "Iteration 770, loss = 0.08361303\n",
      "Iteration 771, loss = 0.08357997\n",
      "Iteration 772, loss = 0.08354697\n",
      "Iteration 773, loss = 0.08351404\n",
      "Iteration 774, loss = 0.08348118\n",
      "Iteration 775, loss = 0.08344838\n",
      "Iteration 776, loss = 0.08341565\n",
      "Iteration 777, loss = 0.08338298\n",
      "Iteration 778, loss = 0.08335042\n",
      "Iteration 779, loss = 0.08331789\n",
      "Iteration 780, loss = 0.08328541\n",
      "Iteration 781, loss = 0.08325301\n",
      "Iteration 782, loss = 0.08322069\n",
      "Iteration 783, loss = 0.08318842\n",
      "Iteration 784, loss = 0.08315621\n",
      "Iteration 785, loss = 0.08312407\n",
      "Iteration 786, loss = 0.08309198\n",
      "Iteration 787, loss = 0.08305995\n",
      "Iteration 788, loss = 0.08302798\n",
      "Iteration 789, loss = 0.08299607\n",
      "Iteration 790, loss = 0.08296457\n",
      "Iteration 791, loss = 0.08293350\n",
      "Iteration 792, loss = 0.08290252\n",
      "Iteration 793, loss = 0.08287159\n",
      "Iteration 794, loss = 0.08284076\n",
      "Iteration 795, loss = 0.08281000\n",
      "Iteration 796, loss = 0.08277932\n",
      "Iteration 797, loss = 0.08274873\n",
      "Iteration 798, loss = 0.08271820\n",
      "Iteration 799, loss = 0.08268775\n",
      "Iteration 800, loss = 0.08265736\n",
      "Iteration 801, loss = 0.08262704\n",
      "Iteration 802, loss = 0.08259678\n",
      "Iteration 803, loss = 0.08256659\n",
      "Iteration 804, loss = 0.08253648\n",
      "Iteration 805, loss = 0.08250643\n",
      "Iteration 806, loss = 0.08247647\n",
      "Iteration 807, loss = 0.08244654\n",
      "Iteration 808, loss = 0.08241667\n",
      "Iteration 809, loss = 0.08238688\n",
      "Iteration 810, loss = 0.08235716\n",
      "Iteration 811, loss = 0.08232749\n",
      "Iteration 812, loss = 0.08229788\n",
      "Iteration 813, loss = 0.08226833\n",
      "Iteration 814, loss = 0.08223883\n",
      "Iteration 815, loss = 0.08220940\n",
      "Iteration 816, loss = 0.08218002\n",
      "Iteration 817, loss = 0.08215069\n",
      "Iteration 818, loss = 0.08212143\n",
      "Iteration 819, loss = 0.08209222\n",
      "Iteration 820, loss = 0.08206306\n",
      "Iteration 821, loss = 0.08203395\n",
      "Iteration 822, loss = 0.08200492\n",
      "Iteration 823, loss = 0.08197593\n",
      "Iteration 824, loss = 0.08194699\n",
      "Iteration 825, loss = 0.08191811\n",
      "Iteration 826, loss = 0.08188929\n",
      "Iteration 827, loss = 0.08186051\n",
      "Iteration 828, loss = 0.08183178\n",
      "Iteration 829, loss = 0.08180311\n",
      "Iteration 830, loss = 0.08177448\n",
      "Iteration 831, loss = 0.08174591\n",
      "Iteration 832, loss = 0.08171738\n",
      "Iteration 833, loss = 0.08168891\n",
      "Iteration 834, loss = 0.08166047\n",
      "Iteration 835, loss = 0.08163211\n",
      "Iteration 836, loss = 0.08160378\n",
      "Iteration 837, loss = 0.08157550\n",
      "Iteration 838, loss = 0.08154727\n",
      "Iteration 839, loss = 0.08151908\n",
      "Iteration 840, loss = 0.08149095\n",
      "Iteration 841, loss = 0.08146286\n",
      "Iteration 842, loss = 0.08143482\n",
      "Iteration 843, loss = 0.08140681\n",
      "Iteration 844, loss = 0.08137886\n",
      "Iteration 845, loss = 0.08135095\n",
      "Iteration 846, loss = 0.08132308\n",
      "Iteration 847, loss = 0.08129528\n",
      "Iteration 848, loss = 0.08126750\n",
      "Iteration 849, loss = 0.08123977\n",
      "Iteration 850, loss = 0.08121210\n",
      "Iteration 851, loss = 0.08118446\n",
      "Iteration 852, loss = 0.08115687\n",
      "Iteration 853, loss = 0.08112945\n",
      "Iteration 854, loss = 0.08110247\n",
      "Iteration 855, loss = 0.08107551\n",
      "Iteration 856, loss = 0.08104856\n",
      "Iteration 857, loss = 0.08102162\n",
      "Iteration 858, loss = 0.08099469\n",
      "Iteration 859, loss = 0.08096813\n",
      "Iteration 860, loss = 0.08094212\n",
      "Iteration 861, loss = 0.08091617\n",
      "Iteration 862, loss = 0.08089028\n",
      "Iteration 863, loss = 0.08086446\n",
      "Iteration 864, loss = 0.08083871\n",
      "Iteration 865, loss = 0.08081302\n",
      "Iteration 866, loss = 0.08078740\n",
      "Iteration 867, loss = 0.08076185\n",
      "Iteration 868, loss = 0.08073635\n",
      "Iteration 869, loss = 0.08071092\n",
      "Iteration 870, loss = 0.08068556\n",
      "Iteration 871, loss = 0.08066027\n",
      "Iteration 872, loss = 0.08063509\n",
      "Iteration 873, loss = 0.08061026\n",
      "Iteration 874, loss = 0.08058551\n",
      "Iteration 875, loss = 0.08056084\n",
      "Iteration 876, loss = 0.08053622\n",
      "Iteration 877, loss = 0.08051170\n",
      "Iteration 878, loss = 0.08048735\n",
      "Iteration 879, loss = 0.08046298\n",
      "Iteration 880, loss = 0.08043862\n",
      "Iteration 881, loss = 0.08041433\n",
      "Iteration 882, loss = 0.08039016\n",
      "Iteration 883, loss = 0.08036603\n",
      "Iteration 884, loss = 0.08034196\n",
      "Iteration 885, loss = 0.08031794\n",
      "Iteration 886, loss = 0.08029398\n",
      "Iteration 887, loss = 0.08027007\n",
      "Iteration 888, loss = 0.08024620\n",
      "Iteration 889, loss = 0.08022239\n",
      "Iteration 890, loss = 0.08019878\n",
      "Iteration 891, loss = 0.08017533\n",
      "Iteration 892, loss = 0.08015186\n",
      "Iteration 893, loss = 0.08012836\n",
      "Iteration 894, loss = 0.08010485\n",
      "Iteration 895, loss = 0.08008133\n",
      "Iteration 896, loss = 0.08005789\n",
      "Iteration 897, loss = 0.08003470\n",
      "Iteration 898, loss = 0.08001158\n",
      "Iteration 899, loss = 0.07998850\n",
      "Iteration 900, loss = 0.07996547\n",
      "Iteration 901, loss = 0.07994251\n",
      "Iteration 902, loss = 0.07991958\n",
      "Iteration 903, loss = 0.07989670\n",
      "Iteration 904, loss = 0.07987385\n",
      "Iteration 905, loss = 0.07985104\n",
      "Iteration 906, loss = 0.07982833\n",
      "Iteration 907, loss = 0.07980570\n",
      "Iteration 908, loss = 0.07978305\n",
      "Iteration 909, loss = 0.07976044\n",
      "Iteration 910, loss = 0.07973793\n",
      "Iteration 911, loss = 0.07971545\n",
      "Iteration 912, loss = 0.07969300\n",
      "Iteration 913, loss = 0.07967059\n",
      "Iteration 914, loss = 0.07964829\n",
      "Iteration 915, loss = 0.07962599\n",
      "Iteration 916, loss = 0.07960368\n",
      "Iteration 917, loss = 0.07958149\n",
      "Iteration 918, loss = 0.07955935\n",
      "Iteration 919, loss = 0.07953723\n",
      "Iteration 920, loss = 0.07951514\n",
      "Iteration 921, loss = 0.07949306\n",
      "Iteration 922, loss = 0.07947102\n",
      "Iteration 923, loss = 0.07944906\n",
      "Iteration 924, loss = 0.07942716\n",
      "Iteration 925, loss = 0.07940526\n",
      "Iteration 926, loss = 0.07938338\n",
      "Iteration 927, loss = 0.07936160\n",
      "Iteration 928, loss = 0.07933983\n",
      "Iteration 929, loss = 0.07931808\n",
      "Iteration 930, loss = 0.07929638\n",
      "Iteration 931, loss = 0.07927473\n",
      "Iteration 932, loss = 0.07925311\n",
      "Iteration 933, loss = 0.07923153\n",
      "Iteration 934, loss = 0.07920997\n",
      "Iteration 935, loss = 0.07918849\n",
      "Iteration 936, loss = 0.07916702\n",
      "Iteration 937, loss = 0.07914555\n",
      "Iteration 938, loss = 0.07912416\n",
      "Iteration 939, loss = 0.07910278\n",
      "Iteration 940, loss = 0.07908146\n",
      "Iteration 941, loss = 0.07906015\n",
      "Iteration 942, loss = 0.07903891\n",
      "Iteration 943, loss = 0.07901771\n",
      "Iteration 944, loss = 0.07899657\n",
      "Iteration 945, loss = 0.07897546\n",
      "Iteration 946, loss = 0.07895436\n",
      "Iteration 947, loss = 0.07893330\n",
      "Iteration 948, loss = 0.07891228\n",
      "Iteration 949, loss = 0.07889130\n",
      "Iteration 950, loss = 0.07887035\n",
      "Iteration 951, loss = 0.07884941\n",
      "Iteration 952, loss = 0.07882858\n",
      "Iteration 953, loss = 0.07880773\n",
      "Iteration 954, loss = 0.07878687\n",
      "Iteration 955, loss = 0.07876609\n",
      "Iteration 956, loss = 0.07874535\n",
      "Iteration 957, loss = 0.07872462\n",
      "Iteration 958, loss = 0.07870389\n",
      "Iteration 959, loss = 0.07868323\n",
      "Iteration 960, loss = 0.07866260\n",
      "Iteration 961, loss = 0.07864197\n",
      "Iteration 962, loss = 0.07862140\n",
      "Iteration 963, loss = 0.07860087\n",
      "Iteration 964, loss = 0.07858033\n",
      "Iteration 965, loss = 0.07855981\n",
      "Iteration 966, loss = 0.07853934\n",
      "Iteration 967, loss = 0.07851894\n",
      "Iteration 968, loss = 0.07849853\n",
      "Iteration 969, loss = 0.07847813\n",
      "Iteration 970, loss = 0.07845783\n",
      "Iteration 971, loss = 0.07843753\n",
      "Iteration 972, loss = 0.07841721\n",
      "Iteration 973, loss = 0.07839689\n",
      "Iteration 974, loss = 0.07837667\n",
      "Iteration 975, loss = 0.07835644\n",
      "Iteration 976, loss = 0.07833629\n",
      "Iteration 977, loss = 0.07831613\n",
      "Iteration 978, loss = 0.07829595\n",
      "Iteration 979, loss = 0.07827586\n",
      "Iteration 980, loss = 0.07825579\n",
      "Iteration 981, loss = 0.07823572\n",
      "Iteration 982, loss = 0.07821564\n",
      "Iteration 983, loss = 0.07819566\n",
      "Iteration 984, loss = 0.07817569\n",
      "Iteration 985, loss = 0.07815570\n",
      "Iteration 986, loss = 0.07813570\n",
      "Iteration 987, loss = 0.07811577\n",
      "Iteration 988, loss = 0.07809589\n",
      "Iteration 989, loss = 0.07807600\n",
      "Iteration 990, loss = 0.07805610\n",
      "Iteration 991, loss = 0.07803628\n",
      "Iteration 992, loss = 0.07801649\n",
      "Iteration 993, loss = 0.07799668\n",
      "Iteration 994, loss = 0.07797687\n",
      "Iteration 995, loss = 0.07795717\n",
      "Iteration 996, loss = 0.07793748\n",
      "Iteration 997, loss = 0.07791777\n",
      "Iteration 998, loss = 0.07789805\n",
      "Iteration 999, loss = 0.07787835\n",
      "Iteration 1000, loss = 0.07785876\n",
      "Iteration 1001, loss = 0.07783915\n",
      "Iteration 1002, loss = 0.07781953\n",
      "Iteration 1003, loss = 0.07779995\n",
      "Iteration 1004, loss = 0.07778042\n",
      "Iteration 1005, loss = 0.07776089\n",
      "Iteration 1006, loss = 0.07774137\n",
      "Iteration 1007, loss = 0.07772189\n",
      "Iteration 1008, loss = 0.07770246\n",
      "Iteration 1009, loss = 0.07768302\n",
      "Iteration 1010, loss = 0.07766357\n",
      "Iteration 1011, loss = 0.07764417\n",
      "Iteration 1012, loss = 0.07762481\n",
      "Iteration 1013, loss = 0.07760544\n",
      "Iteration 1014, loss = 0.07758612\n",
      "Iteration 1015, loss = 0.07756683\n",
      "Iteration 1016, loss = 0.07754758\n",
      "Iteration 1017, loss = 0.07752838\n",
      "Iteration 1018, loss = 0.07750917\n",
      "Iteration 1019, loss = 0.07748992\n",
      "Iteration 1020, loss = 0.07747068\n",
      "Iteration 1021, loss = 0.07745155\n",
      "Iteration 1022, loss = 0.07743239\n",
      "Iteration 1023, loss = 0.07741326\n",
      "Iteration 1024, loss = 0.07739414\n",
      "Iteration 1025, loss = 0.07737501\n",
      "Iteration 1026, loss = 0.07735591\n",
      "Iteration 1027, loss = 0.07733687\n",
      "Iteration 1028, loss = 0.07731780\n",
      "Iteration 1029, loss = 0.07729873\n",
      "Iteration 1030, loss = 0.07727970\n",
      "Iteration 1031, loss = 0.07726068\n",
      "Iteration 1032, loss = 0.07724167\n",
      "Iteration 1033, loss = 0.07722270\n",
      "Iteration 1034, loss = 0.07720374\n",
      "Iteration 1035, loss = 0.07718483\n",
      "Iteration 1036, loss = 0.07716589\n",
      "Iteration 1037, loss = 0.07714694\n",
      "Iteration 1038, loss = 0.07712808\n",
      "Iteration 1039, loss = 0.07710921\n",
      "Iteration 1040, loss = 0.07709036\n",
      "Iteration 1041, loss = 0.07707151\n",
      "Iteration 1042, loss = 0.07705270\n",
      "Iteration 1043, loss = 0.07703389\n",
      "Iteration 1044, loss = 0.07701506\n",
      "Iteration 1045, loss = 0.07699631\n",
      "Iteration 1046, loss = 0.07697755\n",
      "Iteration 1047, loss = 0.07695877\n",
      "Iteration 1048, loss = 0.07694005\n",
      "Iteration 1049, loss = 0.07692138\n",
      "Iteration 1050, loss = 0.07690267\n",
      "Iteration 1051, loss = 0.07688417\n",
      "Iteration 1052, loss = 0.07686615\n",
      "Iteration 1053, loss = 0.07684816\n",
      "Iteration 1054, loss = 0.07683017\n",
      "Iteration 1055, loss = 0.07681221\n",
      "Iteration 1056, loss = 0.07679432\n",
      "Iteration 1057, loss = 0.07677625\n",
      "Iteration 1058, loss = 0.07675821\n",
      "Iteration 1059, loss = 0.07674015\n",
      "Iteration 1060, loss = 0.07672209\n",
      "Iteration 1061, loss = 0.07670406\n",
      "Iteration 1062, loss = 0.07668600\n",
      "Iteration 1063, loss = 0.07666800\n",
      "Iteration 1064, loss = 0.07664998\n",
      "Iteration 1065, loss = 0.07663193\n",
      "Iteration 1066, loss = 0.07661399\n",
      "Iteration 1067, loss = 0.07659603\n",
      "Iteration 1068, loss = 0.07657805\n",
      "Iteration 1069, loss = 0.07656007\n",
      "Iteration 1070, loss = 0.07654209\n",
      "Iteration 1071, loss = 0.07652417\n",
      "Iteration 1072, loss = 0.07650630\n",
      "Iteration 1073, loss = 0.07648837\n",
      "Iteration 1074, loss = 0.07647043\n",
      "Iteration 1075, loss = 0.07645262\n",
      "Iteration 1076, loss = 0.07643481\n",
      "Iteration 1077, loss = 0.07641699\n",
      "Iteration 1078, loss = 0.07639916\n",
      "Iteration 1079, loss = 0.07638133\n",
      "Iteration 1080, loss = 0.07636348\n",
      "Iteration 1081, loss = 0.07634568\n",
      "Iteration 1082, loss = 0.07632798\n",
      "Iteration 1083, loss = 0.07631021\n",
      "Iteration 1084, loss = 0.07629244\n",
      "Iteration 1085, loss = 0.07627474\n",
      "Iteration 1086, loss = 0.07625704\n",
      "Iteration 1087, loss = 0.07623933\n",
      "Iteration 1088, loss = 0.07622165\n",
      "Iteration 1089, loss = 0.07620398\n",
      "Iteration 1090, loss = 0.07618633\n",
      "Iteration 1091, loss = 0.07616869\n",
      "Iteration 1092, loss = 0.07615105\n",
      "Iteration 1093, loss = 0.07613349\n",
      "Iteration 1094, loss = 0.07611589\n",
      "Iteration 1095, loss = 0.07609824\n",
      "Iteration 1096, loss = 0.07608072\n",
      "Iteration 1097, loss = 0.07606319\n",
      "Iteration 1098, loss = 0.07604566\n",
      "Iteration 1099, loss = 0.07602813\n",
      "Iteration 1100, loss = 0.07601058\n",
      "Iteration 1101, loss = 0.07599303\n",
      "Iteration 1102, loss = 0.07597554\n",
      "Iteration 1103, loss = 0.07595808\n",
      "Iteration 1104, loss = 0.07594055\n",
      "Iteration 1105, loss = 0.07592300\n",
      "Iteration 1106, loss = 0.07590556\n",
      "Iteration 1107, loss = 0.07588810\n",
      "Iteration 1108, loss = 0.07587063\n",
      "Iteration 1109, loss = 0.07585319\n",
      "Iteration 1110, loss = 0.07583576\n",
      "Iteration 1111, loss = 0.07581836\n",
      "Iteration 1112, loss = 0.07580098\n",
      "Iteration 1113, loss = 0.07578375\n",
      "Iteration 1114, loss = 0.07576697\n",
      "Iteration 1115, loss = 0.07575029\n",
      "Iteration 1116, loss = 0.07573359\n",
      "Iteration 1117, loss = 0.07571686\n",
      "Iteration 1118, loss = 0.07570025\n",
      "Iteration 1119, loss = 0.07568365\n",
      "Iteration 1120, loss = 0.07566706\n",
      "Iteration 1121, loss = 0.07565046\n",
      "Iteration 1122, loss = 0.07563385\n",
      "Iteration 1123, loss = 0.07561722\n",
      "Iteration 1124, loss = 0.07560061\n",
      "Iteration 1125, loss = 0.07558417\n",
      "Iteration 1126, loss = 0.07556768\n",
      "Iteration 1127, loss = 0.07555114\n",
      "Iteration 1128, loss = 0.07553455\n",
      "Iteration 1129, loss = 0.07551807\n",
      "Iteration 1130, loss = 0.07550165\n",
      "Iteration 1131, loss = 0.07548522\n",
      "Iteration 1132, loss = 0.07546879\n",
      "Iteration 1133, loss = 0.07545237\n",
      "Iteration 1134, loss = 0.07543594\n",
      "Iteration 1135, loss = 0.07541951\n",
      "Iteration 1136, loss = 0.07540311\n",
      "Iteration 1137, loss = 0.07538678\n",
      "Iteration 1138, loss = 0.07537047\n",
      "Iteration 1139, loss = 0.07535412\n",
      "Iteration 1140, loss = 0.07533774\n",
      "Iteration 1141, loss = 0.07532143\n",
      "Iteration 1142, loss = 0.07530512\n",
      "Iteration 1143, loss = 0.07528881\n",
      "Iteration 1144, loss = 0.07527250\n",
      "Iteration 1145, loss = 0.07525620\n",
      "Iteration 1146, loss = 0.07524001\n",
      "Iteration 1147, loss = 0.07522377\n",
      "Iteration 1148, loss = 0.07520746\n",
      "Iteration 1149, loss = 0.07519123\n",
      "Iteration 1150, loss = 0.07517507\n",
      "Iteration 1151, loss = 0.07515889\n",
      "Iteration 1152, loss = 0.07514266\n",
      "Iteration 1153, loss = 0.07512645\n",
      "Iteration 1154, loss = 0.07511025\n",
      "Iteration 1155, loss = 0.07509405\n",
      "Iteration 1156, loss = 0.07507784\n",
      "Iteration 1157, loss = 0.07506175\n",
      "Iteration 1158, loss = 0.07504562\n",
      "Iteration 1159, loss = 0.07502942\n",
      "Iteration 1160, loss = 0.07501324\n",
      "Iteration 1161, loss = 0.07499716\n",
      "Iteration 1162, loss = 0.07498105\n",
      "Iteration 1163, loss = 0.07496496\n",
      "Iteration 1164, loss = 0.07494887\n",
      "Iteration 1165, loss = 0.07493279\n",
      "Iteration 1166, loss = 0.07491669\n",
      "Iteration 1167, loss = 0.07490067\n",
      "Iteration 1168, loss = 0.07488464\n",
      "Iteration 1169, loss = 0.07486856\n",
      "Iteration 1170, loss = 0.07485255\n",
      "Iteration 1171, loss = 0.07483654\n",
      "Iteration 1172, loss = 0.07482053\n",
      "Iteration 1173, loss = 0.07480457\n",
      "Iteration 1174, loss = 0.07478857\n",
      "Iteration 1175, loss = 0.07477261\n",
      "Iteration 1176, loss = 0.07475668\n",
      "Iteration 1177, loss = 0.07474072\n",
      "Iteration 1178, loss = 0.07472492\n",
      "Iteration 1179, loss = 0.07470993\n",
      "Iteration 1180, loss = 0.07469476\n",
      "Iteration 1181, loss = 0.07467946\n",
      "Iteration 1182, loss = 0.07466401\n",
      "Iteration 1183, loss = 0.07464843\n",
      "Iteration 1184, loss = 0.07463273\n",
      "Iteration 1185, loss = 0.07461705\n",
      "Iteration 1186, loss = 0.07460134\n",
      "Iteration 1187, loss = 0.07458614\n",
      "Iteration 1188, loss = 0.07457094\n",
      "Iteration 1189, loss = 0.07455569\n",
      "Iteration 1190, loss = 0.07454039\n",
      "Iteration 1191, loss = 0.07452503\n",
      "Iteration 1192, loss = 0.07450968\n",
      "Iteration 1193, loss = 0.07449428\n",
      "Iteration 1194, loss = 0.07447892\n",
      "Iteration 1195, loss = 0.07446391\n",
      "Iteration 1196, loss = 0.07444871\n",
      "Iteration 1197, loss = 0.07443336\n",
      "Iteration 1198, loss = 0.07441829\n",
      "Iteration 1199, loss = 0.07440325\n",
      "Iteration 1200, loss = 0.07438813\n",
      "Iteration 1201, loss = 0.07437295\n",
      "Iteration 1202, loss = 0.07435774\n",
      "Iteration 1203, loss = 0.07434283\n",
      "Iteration 1204, loss = 0.07432786\n",
      "Iteration 1205, loss = 0.07431278\n",
      "Iteration 1206, loss = 0.07429762\n",
      "Iteration 1207, loss = 0.07428254\n",
      "Iteration 1208, loss = 0.07426765\n",
      "Iteration 1209, loss = 0.07425267\n",
      "Iteration 1210, loss = 0.07423769\n",
      "Iteration 1211, loss = 0.07422272\n",
      "Iteration 1212, loss = 0.07420789\n",
      "Iteration 1213, loss = 0.07419301\n",
      "Iteration 1214, loss = 0.07417817\n",
      "Iteration 1215, loss = 0.07416340\n",
      "Iteration 1216, loss = 0.07414860\n",
      "Iteration 1217, loss = 0.07413384\n",
      "Iteration 1218, loss = 0.07411909\n",
      "Iteration 1219, loss = 0.07410433\n",
      "Iteration 1220, loss = 0.07408958\n",
      "Iteration 1221, loss = 0.07407485\n",
      "Iteration 1222, loss = 0.07406013\n",
      "Iteration 1223, loss = 0.07404552\n",
      "Iteration 1224, loss = 0.07403086\n",
      "Iteration 1225, loss = 0.07401612\n",
      "Iteration 1226, loss = 0.07400154\n",
      "Iteration 1227, loss = 0.07398694\n",
      "Iteration 1228, loss = 0.07397225\n",
      "Iteration 1229, loss = 0.07395762\n",
      "Iteration 1230, loss = 0.07394299\n",
      "Iteration 1231, loss = 0.07392835\n",
      "Iteration 1232, loss = 0.07391390\n",
      "Iteration 1233, loss = 0.07389938\n",
      "Iteration 1234, loss = 0.07388477\n",
      "Iteration 1235, loss = 0.07387018\n",
      "Iteration 1236, loss = 0.07385568\n",
      "Iteration 1237, loss = 0.07384111\n",
      "Iteration 1238, loss = 0.07382664\n",
      "Iteration 1239, loss = 0.07381217\n",
      "Iteration 1240, loss = 0.07379761\n",
      "Iteration 1241, loss = 0.07378314\n",
      "Iteration 1242, loss = 0.07376870\n",
      "Iteration 1243, loss = 0.07375418\n",
      "Iteration 1244, loss = 0.07373972\n",
      "Iteration 1245, loss = 0.07372532\n",
      "Iteration 1246, loss = 0.07371106\n",
      "Iteration 1247, loss = 0.07369715\n",
      "Iteration 1248, loss = 0.07368325\n",
      "Iteration 1249, loss = 0.07366925\n",
      "Iteration 1250, loss = 0.07365533\n",
      "Iteration 1251, loss = 0.07364144\n",
      "Iteration 1252, loss = 0.07362746\n",
      "Iteration 1253, loss = 0.07361347\n",
      "Iteration 1254, loss = 0.07359958\n",
      "Iteration 1255, loss = 0.07358560\n",
      "Iteration 1256, loss = 0.07357164\n",
      "Iteration 1257, loss = 0.07355779\n",
      "Iteration 1258, loss = 0.07354387\n",
      "Iteration 1259, loss = 0.07352999\n",
      "Iteration 1260, loss = 0.07351609\n",
      "Iteration 1261, loss = 0.07350219\n",
      "Iteration 1262, loss = 0.07348831\n",
      "Iteration 1263, loss = 0.07347447\n",
      "Iteration 1264, loss = 0.07346061\n",
      "Iteration 1265, loss = 0.07344685\n",
      "Iteration 1266, loss = 0.07343304\n",
      "Iteration 1267, loss = 0.07341913\n",
      "Iteration 1268, loss = 0.07340542\n",
      "Iteration 1269, loss = 0.07339166\n",
      "Iteration 1270, loss = 0.07337780\n",
      "Iteration 1271, loss = 0.07336399\n",
      "Iteration 1272, loss = 0.07335029\n",
      "Iteration 1273, loss = 0.07333649\n",
      "Iteration 1274, loss = 0.07332269\n",
      "Iteration 1275, loss = 0.07330895\n",
      "Iteration 1276, loss = 0.07329510\n",
      "Iteration 1277, loss = 0.07328107\n",
      "Iteration 1278, loss = 0.07326699\n",
      "Iteration 1279, loss = 0.07325304\n",
      "Iteration 1280, loss = 0.07323906\n",
      "Iteration 1281, loss = 0.07322493\n",
      "Iteration 1282, loss = 0.07321091\n",
      "Iteration 1283, loss = 0.07319684\n",
      "Iteration 1284, loss = 0.07318273\n",
      "Iteration 1285, loss = 0.07316853\n",
      "Iteration 1286, loss = 0.07315463\n",
      "Iteration 1287, loss = 0.07314056\n",
      "Iteration 1288, loss = 0.07312640\n",
      "Iteration 1289, loss = 0.07311213\n",
      "Iteration 1290, loss = 0.07309811\n",
      "Iteration 1291, loss = 0.07308410\n",
      "Iteration 1292, loss = 0.07307000\n",
      "Iteration 1293, loss = 0.07305586\n",
      "Iteration 1294, loss = 0.07304167\n",
      "Iteration 1295, loss = 0.07302742\n",
      "Iteration 1296, loss = 0.07301342\n",
      "Iteration 1297, loss = 0.07299930\n",
      "Iteration 1298, loss = 0.07298528\n",
      "Iteration 1299, loss = 0.07297125\n",
      "Iteration 1300, loss = 0.07295716\n",
      "Iteration 1301, loss = 0.07294301\n",
      "Iteration 1302, loss = 0.07292908\n",
      "Iteration 1303, loss = 0.07291511\n",
      "Iteration 1304, loss = 0.07290097\n",
      "Iteration 1305, loss = 0.07288688\n",
      "Iteration 1306, loss = 0.07287295\n",
      "Iteration 1307, loss = 0.07285895\n",
      "Iteration 1308, loss = 0.07284488\n",
      "Iteration 1309, loss = 0.07283098\n",
      "Iteration 1310, loss = 0.07281704\n",
      "Iteration 1311, loss = 0.07280299\n",
      "Iteration 1312, loss = 0.07278911\n",
      "Iteration 1313, loss = 0.07277524\n",
      "Iteration 1314, loss = 0.07276131\n",
      "Iteration 1315, loss = 0.07274733\n",
      "Iteration 1316, loss = 0.07273332\n",
      "Iteration 1317, loss = 0.07271950\n",
      "Iteration 1318, loss = 0.07270568\n",
      "Iteration 1319, loss = 0.07269173\n",
      "Iteration 1320, loss = 0.07267774\n",
      "Iteration 1321, loss = 0.07266392\n",
      "Iteration 1322, loss = 0.07265002\n",
      "Iteration 1323, loss = 0.07263611\n",
      "Iteration 1324, loss = 0.07262241\n",
      "Iteration 1325, loss = 0.07260857\n",
      "Iteration 1326, loss = 0.07259462\n",
      "Iteration 1327, loss = 0.07258084\n",
      "Iteration 1328, loss = 0.07256710\n",
      "Iteration 1329, loss = 0.07255332\n",
      "Iteration 1330, loss = 0.07253947\n",
      "Iteration 1331, loss = 0.07252560\n",
      "Iteration 1332, loss = 0.07251179\n",
      "Iteration 1333, loss = 0.07249807\n",
      "Iteration 1334, loss = 0.07248420\n",
      "Iteration 1335, loss = 0.07247055\n",
      "Iteration 1336, loss = 0.07245691\n",
      "Iteration 1337, loss = 0.07244315\n",
      "Iteration 1338, loss = 0.07242933\n",
      "Iteration 1339, loss = 0.07241547\n",
      "Iteration 1340, loss = 0.07240175\n",
      "Iteration 1341, loss = 0.07238805\n",
      "Iteration 1342, loss = 0.07237419\n",
      "Iteration 1343, loss = 0.07236046\n",
      "Iteration 1344, loss = 0.07234679\n",
      "Iteration 1345, loss = 0.07233308\n",
      "Iteration 1346, loss = 0.07231930\n",
      "Iteration 1347, loss = 0.07230548\n",
      "Iteration 1348, loss = 0.07229183\n",
      "Iteration 1349, loss = 0.07227818\n",
      "Iteration 1350, loss = 0.07226436\n",
      "Iteration 1351, loss = 0.07225071\n",
      "Iteration 1352, loss = 0.07223710\n",
      "Iteration 1353, loss = 0.07222343\n",
      "Iteration 1354, loss = 0.07220973\n",
      "Iteration 1355, loss = 0.07219597\n",
      "Iteration 1356, loss = 0.07218226\n",
      "Iteration 1357, loss = 0.07216868\n",
      "Iteration 1358, loss = 0.07215498\n",
      "Iteration 1359, loss = 0.07214135\n",
      "Iteration 1360, loss = 0.07212767\n",
      "Iteration 1361, loss = 0.07211409\n",
      "Iteration 1362, loss = 0.07210052\n",
      "Iteration 1363, loss = 0.07208686\n",
      "Iteration 1364, loss = 0.07207330\n",
      "Iteration 1365, loss = 0.07205970\n",
      "Iteration 1366, loss = 0.07204612\n",
      "Iteration 1367, loss = 0.07203250\n",
      "Iteration 1368, loss = 0.07201938\n",
      "Iteration 1369, loss = 0.07200664\n",
      "Iteration 1370, loss = 0.07199368\n",
      "Iteration 1371, loss = 0.07198076\n",
      "Iteration 1372, loss = 0.07196791\n",
      "Iteration 1373, loss = 0.07195493\n",
      "Iteration 1374, loss = 0.07194185\n",
      "Iteration 1375, loss = 0.07192875\n",
      "Iteration 1376, loss = 0.07191602\n",
      "Iteration 1377, loss = 0.07190362\n",
      "Iteration 1378, loss = 0.07189104\n",
      "Iteration 1379, loss = 0.07187873\n",
      "Iteration 1380, loss = 0.07186643\n",
      "Iteration 1381, loss = 0.07185404\n",
      "Iteration 1382, loss = 0.07184161\n",
      "Iteration 1383, loss = 0.07182919\n",
      "Iteration 1384, loss = 0.07181675\n",
      "Iteration 1385, loss = 0.07180430\n",
      "Iteration 1386, loss = 0.07179207\n",
      "Iteration 1387, loss = 0.07177979\n",
      "Iteration 1388, loss = 0.07176761\n",
      "Iteration 1389, loss = 0.07175546\n",
      "Iteration 1390, loss = 0.07174340\n",
      "Iteration 1391, loss = 0.07173124\n",
      "Iteration 1392, loss = 0.07171902\n",
      "Iteration 1393, loss = 0.07170674\n",
      "Iteration 1394, loss = 0.07169462\n",
      "Iteration 1395, loss = 0.07168250\n",
      "Iteration 1396, loss = 0.07167052\n",
      "Iteration 1397, loss = 0.07165852\n",
      "Iteration 1398, loss = 0.07164629\n",
      "Iteration 1399, loss = 0.07163423\n",
      "Iteration 1400, loss = 0.07162213\n",
      "Iteration 1401, loss = 0.07161004\n",
      "Iteration 1402, loss = 0.07159798\n",
      "Iteration 1403, loss = 0.07158590\n",
      "Iteration 1404, loss = 0.07157399\n",
      "Iteration 1405, loss = 0.07156204\n",
      "Iteration 1406, loss = 0.07155022\n",
      "Iteration 1407, loss = 0.07153814\n",
      "Iteration 1408, loss = 0.07152600\n",
      "Iteration 1409, loss = 0.07151412\n",
      "Iteration 1410, loss = 0.07150225\n",
      "Iteration 1411, loss = 0.07149036\n",
      "Iteration 1412, loss = 0.07147842\n",
      "Iteration 1413, loss = 0.07146648\n",
      "Iteration 1414, loss = 0.07145462\n",
      "Iteration 1415, loss = 0.07144269\n",
      "Iteration 1416, loss = 0.07143080\n",
      "Iteration 1417, loss = 0.07141903\n",
      "Iteration 1418, loss = 0.07140737\n",
      "Iteration 1419, loss = 0.07139568\n",
      "Iteration 1420, loss = 0.07138388\n",
      "Iteration 1421, loss = 0.07137200\n",
      "Iteration 1422, loss = 0.07136043\n",
      "Iteration 1423, loss = 0.07134869\n",
      "Iteration 1424, loss = 0.07133684\n",
      "Iteration 1425, loss = 0.07132513\n",
      "Iteration 1426, loss = 0.07131341\n",
      "Iteration 1427, loss = 0.07130168\n",
      "Iteration 1428, loss = 0.07128992\n",
      "Iteration 1429, loss = 0.07127814\n",
      "Iteration 1430, loss = 0.07126673\n",
      "Iteration 1431, loss = 0.07125510\n",
      "Iteration 1432, loss = 0.07124337\n",
      "Iteration 1433, loss = 0.07123172\n",
      "Iteration 1434, loss = 0.07122021\n",
      "Iteration 1435, loss = 0.07120865\n",
      "Iteration 1436, loss = 0.07119707\n",
      "Iteration 1437, loss = 0.07118546\n",
      "Iteration 1438, loss = 0.07117384\n",
      "Iteration 1439, loss = 0.07116231\n",
      "Iteration 1440, loss = 0.07115071\n",
      "Iteration 1441, loss = 0.07113916\n",
      "Iteration 1442, loss = 0.07112765\n",
      "Iteration 1443, loss = 0.07111610\n",
      "Iteration 1444, loss = 0.07110469\n",
      "Iteration 1445, loss = 0.07109322\n",
      "Iteration 1446, loss = 0.07108171\n",
      "Iteration 1447, loss = 0.07107007\n",
      "Iteration 1448, loss = 0.07105853\n",
      "Iteration 1449, loss = 0.07104729\n",
      "Iteration 1450, loss = 0.07103587\n",
      "Iteration 1451, loss = 0.07102437\n",
      "Iteration 1452, loss = 0.07101293\n",
      "Iteration 1453, loss = 0.07100158\n",
      "Iteration 1454, loss = 0.07099021\n",
      "Iteration 1455, loss = 0.07097882\n",
      "Iteration 1456, loss = 0.07096744\n",
      "Iteration 1457, loss = 0.07095600\n",
      "Iteration 1458, loss = 0.07094460\n",
      "Iteration 1459, loss = 0.07093351\n",
      "Iteration 1460, loss = 0.07092207\n",
      "Iteration 1461, loss = 0.07091062\n",
      "Iteration 1462, loss = 0.07089936\n",
      "Iteration 1463, loss = 0.07088807\n",
      "Iteration 1464, loss = 0.07087686\n",
      "Iteration 1465, loss = 0.07086557\n",
      "Iteration 1466, loss = 0.07085417\n",
      "Iteration 1467, loss = 0.07084287\n",
      "Iteration 1468, loss = 0.07083159\n",
      "Iteration 1469, loss = 0.07082033\n",
      "Iteration 1470, loss = 0.07081097\n",
      "Iteration 1471, loss = 0.07079924\n",
      "Iteration 1472, loss = 0.07078895\n",
      "Iteration 1473, loss = 0.07077877\n",
      "Iteration 1474, loss = 0.07076849\n",
      "Iteration 1475, loss = 0.07075814\n",
      "Iteration 1476, loss = 0.07074776\n",
      "Iteration 1477, loss = 0.07073718\n",
      "Iteration 1478, loss = 0.07072653\n",
      "Iteration 1479, loss = 0.07071594\n",
      "Iteration 1480, loss = 0.07070520\n",
      "Iteration 1481, loss = 0.07069441\n",
      "Iteration 1482, loss = 0.07068358\n",
      "Iteration 1483, loss = 0.07067287\n",
      "Iteration 1484, loss = 0.07066203\n",
      "Iteration 1485, loss = 0.07065113\n",
      "Iteration 1486, loss = 0.07064257\n",
      "Iteration 1487, loss = 0.07063192\n",
      "Iteration 1488, loss = 0.07061974\n",
      "Iteration 1489, loss = 0.07060950\n",
      "Iteration 1490, loss = 0.07059917\n",
      "Iteration 1491, loss = 0.07058874\n",
      "Iteration 1492, loss = 0.07057838\n",
      "Iteration 1493, loss = 0.07056774\n",
      "Iteration 1494, loss = 0.07055720\n",
      "Iteration 1495, loss = 0.07054667\n",
      "Iteration 1496, loss = 0.07053603\n",
      "Iteration 1497, loss = 0.07052537\n",
      "Iteration 1498, loss = 0.07051604\n",
      "Iteration 1499, loss = 0.07050489\n",
      "Iteration 1500, loss = 0.07049436\n",
      "Iteration 1501, loss = 0.07048425\n",
      "Iteration 1502, loss = 0.07047402\n",
      "Iteration 1503, loss = 0.07046371\n",
      "Iteration 1504, loss = 0.07045334\n",
      "Iteration 1505, loss = 0.07044290\n",
      "Iteration 1506, loss = 0.07043240\n",
      "Iteration 1507, loss = 0.07042197\n",
      "Iteration 1508, loss = 0.07041141\n",
      "Iteration 1509, loss = 0.07040088\n",
      "Iteration 1510, loss = 0.07039021\n",
      "Iteration 1511, loss = 0.07037955\n",
      "Iteration 1512, loss = 0.07036900\n",
      "Iteration 1513, loss = 0.07035858\n",
      "Iteration 1514, loss = 0.07034820\n",
      "Iteration 1515, loss = 0.07033775\n",
      "Iteration 1516, loss = 0.07032772\n",
      "Iteration 1517, loss = 0.07031706\n",
      "Iteration 1518, loss = 0.07030700\n",
      "Iteration 1519, loss = 0.07029660\n",
      "Iteration 1520, loss = 0.07028636\n",
      "Iteration 1521, loss = 0.07027600\n",
      "Iteration 1522, loss = 0.07026556\n",
      "Iteration 1523, loss = 0.07025517\n",
      "Iteration 1524, loss = 0.07024467\n",
      "Iteration 1525, loss = 0.07023533\n",
      "Iteration 1526, loss = 0.07022408\n",
      "Iteration 1527, loss = 0.07021415\n",
      "Iteration 1528, loss = 0.07020420\n",
      "Iteration 1529, loss = 0.07019459\n",
      "Iteration 1530, loss = 0.07018497\n",
      "Iteration 1531, loss = 0.07017494\n",
      "Iteration 1532, loss = 0.07016474\n",
      "Iteration 1533, loss = 0.07015437\n",
      "Iteration 1534, loss = 0.07014425\n",
      "Iteration 1535, loss = 0.07013403\n",
      "Iteration 1536, loss = 0.07012393\n",
      "Iteration 1537, loss = 0.07011374\n",
      "Iteration 1538, loss = 0.07010349\n",
      "Iteration 1539, loss = 0.07009318\n",
      "Iteration 1540, loss = 0.07008277\n",
      "Iteration 1541, loss = 0.07007449\n",
      "Iteration 1542, loss = 0.07006424\n",
      "Iteration 1543, loss = 0.07005297\n",
      "Iteration 1544, loss = 0.07004332\n",
      "Iteration 1545, loss = 0.07003356\n",
      "Iteration 1546, loss = 0.07002369\n",
      "Iteration 1547, loss = 0.07001373\n",
      "Iteration 1548, loss = 0.07000380\n",
      "Iteration 1549, loss = 0.06999400\n",
      "Iteration 1550, loss = 0.06998418\n",
      "Iteration 1551, loss = 0.06997400\n",
      "Iteration 1552, loss = 0.06996373\n",
      "Iteration 1553, loss = 0.06995372\n",
      "Iteration 1554, loss = 0.06994502\n",
      "Iteration 1555, loss = 0.06993442\n",
      "Iteration 1556, loss = 0.06992465\n",
      "Iteration 1557, loss = 0.06991516\n",
      "Iteration 1558, loss = 0.06990557\n",
      "Iteration 1559, loss = 0.06989591\n",
      "Iteration 1560, loss = 0.06988610\n",
      "Iteration 1561, loss = 0.06987640\n",
      "Iteration 1562, loss = 0.06986654\n",
      "Iteration 1563, loss = 0.06985663\n",
      "Iteration 1564, loss = 0.06984672\n",
      "Iteration 1565, loss = 0.06983682\n",
      "Iteration 1566, loss = 0.06982685\n",
      "Iteration 1567, loss = 0.06981680\n",
      "Iteration 1568, loss = 0.06980669\n",
      "Iteration 1569, loss = 0.06979824\n",
      "Iteration 1570, loss = 0.06978813\n",
      "Iteration 1571, loss = 0.06977758\n",
      "Iteration 1572, loss = 0.06976816\n",
      "Iteration 1573, loss = 0.06975870\n",
      "Iteration 1574, loss = 0.06974916\n",
      "Iteration 1575, loss = 0.06973951\n",
      "Iteration 1576, loss = 0.06972977\n",
      "Iteration 1577, loss = 0.06971997\n",
      "Iteration 1578, loss = 0.06971006\n",
      "Iteration 1579, loss = 0.06970019\n",
      "Iteration 1580, loss = 0.06969018\n",
      "Iteration 1581, loss = 0.06968036\n",
      "Iteration 1582, loss = 0.06967028\n",
      "Iteration 1583, loss = 0.06966190\n",
      "Iteration 1584, loss = 0.06965170\n",
      "Iteration 1585, loss = 0.06964149\n",
      "Iteration 1586, loss = 0.06963205\n",
      "Iteration 1587, loss = 0.06962266\n",
      "Iteration 1588, loss = 0.06961325\n",
      "Iteration 1589, loss = 0.06960364\n",
      "Iteration 1590, loss = 0.06959405\n",
      "Iteration 1591, loss = 0.06958448\n",
      "Iteration 1592, loss = 0.06957479\n",
      "Iteration 1593, loss = 0.06956505\n",
      "Iteration 1594, loss = 0.06955524\n",
      "Iteration 1595, loss = 0.06954534\n",
      "Iteration 1596, loss = 0.06953548\n",
      "Iteration 1597, loss = 0.06952548\n",
      "Iteration 1598, loss = 0.06951779\n",
      "Iteration 1599, loss = 0.06950798\n",
      "Iteration 1600, loss = 0.06949707\n",
      "Iteration 1601, loss = 0.06948752\n",
      "Iteration 1602, loss = 0.06947832\n",
      "Iteration 1603, loss = 0.06946900\n",
      "Iteration 1604, loss = 0.06945955\n",
      "Iteration 1605, loss = 0.06945001\n",
      "Iteration 1606, loss = 0.06944036\n",
      "Iteration 1607, loss = 0.06943063\n",
      "Iteration 1608, loss = 0.06942086\n",
      "Iteration 1609, loss = 0.06941110\n",
      "Iteration 1610, loss = 0.06940118\n",
      "Iteration 1611, loss = 0.06939126\n",
      "Iteration 1612, loss = 0.06938253\n",
      "Iteration 1613, loss = 0.06937218\n",
      "Iteration 1614, loss = 0.06936269\n",
      "Iteration 1615, loss = 0.06935347\n",
      "Iteration 1616, loss = 0.06934419\n",
      "Iteration 1617, loss = 0.06933487\n",
      "Iteration 1618, loss = 0.06932539\n",
      "Iteration 1619, loss = 0.06931583\n",
      "Iteration 1620, loss = 0.06930614\n",
      "Iteration 1621, loss = 0.06929668\n",
      "Iteration 1622, loss = 0.06928690\n",
      "Iteration 1623, loss = 0.06927727\n",
      "Iteration 1624, loss = 0.06926749\n",
      "Iteration 1625, loss = 0.06925774\n",
      "Iteration 1626, loss = 0.06924794\n",
      "Iteration 1627, loss = 0.06923883\n",
      "Iteration 1628, loss = 0.06922867\n",
      "Iteration 1629, loss = 0.06921951\n",
      "Iteration 1630, loss = 0.06921027\n",
      "Iteration 1631, loss = 0.06920126\n",
      "Iteration 1632, loss = 0.06919196\n",
      "Iteration 1633, loss = 0.06918231\n",
      "Iteration 1634, loss = 0.06917287\n",
      "Iteration 1635, loss = 0.06916338\n",
      "Iteration 1636, loss = 0.06915386\n",
      "Iteration 1637, loss = 0.06914414\n",
      "Iteration 1638, loss = 0.06913449\n",
      "Iteration 1639, loss = 0.06912477\n",
      "Iteration 1640, loss = 0.06911499\n",
      "Iteration 1641, loss = 0.06910524\n",
      "Iteration 1642, loss = 0.06909638\n",
      "Iteration 1643, loss = 0.06908633\n",
      "Iteration 1644, loss = 0.06907718\n",
      "Iteration 1645, loss = 0.06906812\n",
      "Iteration 1646, loss = 0.06905889\n",
      "Iteration 1647, loss = 0.06904957\n",
      "Iteration 1648, loss = 0.06904021\n",
      "Iteration 1649, loss = 0.06903105\n",
      "Iteration 1650, loss = 0.06902150\n",
      "Iteration 1651, loss = 0.06901186\n",
      "Iteration 1652, loss = 0.06900233\n",
      "Iteration 1653, loss = 0.06899277\n",
      "Iteration 1654, loss = 0.06898315\n",
      "Iteration 1655, loss = 0.06897359\n",
      "Iteration 1656, loss = 0.06896445\n",
      "Iteration 1657, loss = 0.06895597\n",
      "Iteration 1658, loss = 0.06894663\n",
      "Iteration 1659, loss = 0.06893782\n",
      "Iteration 1660, loss = 0.06892914\n",
      "Iteration 1661, loss = 0.06892087\n",
      "Iteration 1662, loss = 0.06891197\n",
      "Iteration 1663, loss = 0.06890350\n",
      "Iteration 1664, loss = 0.06889494\n",
      "Iteration 1665, loss = 0.06888623\n",
      "Iteration 1666, loss = 0.06887751\n",
      "Iteration 1667, loss = 0.06886871\n",
      "Iteration 1668, loss = 0.06886015\n",
      "Iteration 1669, loss = 0.06885133\n",
      "Iteration 1670, loss = 0.06884229\n",
      "Iteration 1671, loss = 0.06883403\n",
      "Iteration 1672, loss = 0.06882506\n",
      "Iteration 1673, loss = 0.06881654\n",
      "Iteration 1674, loss = 0.06880794\n",
      "Iteration 1675, loss = 0.06879925\n",
      "Iteration 1676, loss = 0.06879062\n",
      "Iteration 1677, loss = 0.06878177\n",
      "Iteration 1678, loss = 0.06877447\n",
      "Iteration 1679, loss = 0.06876480\n",
      "Iteration 1680, loss = 0.06875644\n",
      "Iteration 1681, loss = 0.06874792\n",
      "Iteration 1682, loss = 0.06873939\n",
      "Iteration 1683, loss = 0.06873070\n",
      "Iteration 1684, loss = 0.06872198\n",
      "Iteration 1685, loss = 0.06871331\n",
      "Iteration 1686, loss = 0.06870486\n",
      "Iteration 1687, loss = 0.06869645\n",
      "Iteration 1688, loss = 0.06868809\n",
      "Iteration 1689, loss = 0.06867962\n",
      "Iteration 1690, loss = 0.06867110\n",
      "Iteration 1691, loss = 0.06866255\n",
      "Iteration 1692, loss = 0.06865394\n",
      "Iteration 1693, loss = 0.06864507\n",
      "Iteration 1694, loss = 0.06863709\n",
      "Iteration 1695, loss = 0.06862804\n",
      "Iteration 1696, loss = 0.06861962\n",
      "Iteration 1697, loss = 0.06861118\n",
      "Iteration 1698, loss = 0.06860269\n",
      "Iteration 1699, loss = 0.06859417\n",
      "Iteration 1700, loss = 0.06858558\n",
      "Iteration 1701, loss = 0.06857779\n",
      "Iteration 1702, loss = 0.06856865\n",
      "Iteration 1703, loss = 0.06856021\n",
      "Iteration 1704, loss = 0.06855186\n",
      "Iteration 1705, loss = 0.06854338\n",
      "Iteration 1706, loss = 0.06853493\n",
      "Iteration 1707, loss = 0.06852642\n",
      "Iteration 1708, loss = 0.06851789\n",
      "Iteration 1709, loss = 0.06850964\n",
      "Iteration 1710, loss = 0.06850106\n",
      "Iteration 1711, loss = 0.06849268\n",
      "Iteration 1712, loss = 0.06848423\n",
      "Iteration 1713, loss = 0.06847568\n",
      "Iteration 1714, loss = 0.06846755\n",
      "Iteration 1715, loss = 0.06845907\n",
      "Iteration 1716, loss = 0.06845011\n",
      "Iteration 1717, loss = 0.06844210\n",
      "Iteration 1718, loss = 0.06843360\n",
      "Iteration 1719, loss = 0.06842542\n",
      "Iteration 1720, loss = 0.06841713\n",
      "Iteration 1721, loss = 0.06840877\n",
      "Iteration 1722, loss = 0.06840031\n",
      "Iteration 1723, loss = 0.06839176\n",
      "Iteration 1724, loss = 0.06838320\n",
      "Iteration 1725, loss = 0.06837512\n",
      "Iteration 1726, loss = 0.06836629\n",
      "Iteration 1727, loss = 0.06835791\n",
      "Iteration 1728, loss = 0.06834950\n",
      "Iteration 1729, loss = 0.06834121\n",
      "Iteration 1730, loss = 0.06833275\n",
      "Iteration 1731, loss = 0.06832433\n",
      "Iteration 1732, loss = 0.06831583\n",
      "Iteration 1733, loss = 0.06830792\n",
      "Iteration 1734, loss = 0.06829909\n",
      "Iteration 1735, loss = 0.06829086\n",
      "Iteration 1736, loss = 0.06828257\n",
      "Iteration 1737, loss = 0.06827422\n",
      "Iteration 1738, loss = 0.06826583\n",
      "Iteration 1739, loss = 0.06825749\n",
      "Iteration 1740, loss = 0.06824890\n",
      "Iteration 1741, loss = 0.06824108\n",
      "Iteration 1742, loss = 0.06823228\n",
      "Iteration 1743, loss = 0.06822408\n",
      "Iteration 1744, loss = 0.06821603\n",
      "Iteration 1745, loss = 0.06820753\n",
      "Iteration 1746, loss = 0.06819928\n",
      "Iteration 1747, loss = 0.06819091\n",
      "Iteration 1748, loss = 0.06818255\n",
      "Iteration 1749, loss = 0.06817476\n",
      "Iteration 1750, loss = 0.06816604\n",
      "Iteration 1751, loss = 0.06815784\n",
      "Iteration 1752, loss = 0.06814953\n",
      "Iteration 1753, loss = 0.06814118\n",
      "Iteration 1754, loss = 0.06813280\n",
      "Iteration 1755, loss = 0.06812461\n",
      "Iteration 1756, loss = 0.06811621\n",
      "Iteration 1757, loss = 0.06810782\n",
      "Iteration 1758, loss = 0.06809945\n",
      "Iteration 1759, loss = 0.06809136\n",
      "Iteration 1760, loss = 0.06808319\n",
      "Iteration 1761, loss = 0.06807490\n",
      "Iteration 1762, loss = 0.06806657\n",
      "Iteration 1763, loss = 0.06805818\n",
      "Iteration 1764, loss = 0.06804980\n",
      "Iteration 1765, loss = 0.06804139\n",
      "Iteration 1766, loss = 0.06803453\n",
      "Iteration 1767, loss = 0.06802485\n",
      "Iteration 1768, loss = 0.06801677\n",
      "Iteration 1769, loss = 0.06800859\n",
      "Iteration 1770, loss = 0.06800033\n",
      "Iteration 1771, loss = 0.06799213\n",
      "Iteration 1772, loss = 0.06798369\n",
      "Iteration 1773, loss = 0.06797619\n",
      "Iteration 1774, loss = 0.06796737\n",
      "Iteration 1775, loss = 0.06795946\n",
      "Iteration 1776, loss = 0.06795126\n",
      "Iteration 1777, loss = 0.06794309\n",
      "Iteration 1778, loss = 0.06793489\n",
      "Iteration 1779, loss = 0.06792660\n",
      "Iteration 1780, loss = 0.06791836\n",
      "Iteration 1781, loss = 0.06790989\n",
      "Iteration 1782, loss = 0.06790218\n",
      "Iteration 1783, loss = 0.06789347\n",
      "Iteration 1784, loss = 0.06788550\n",
      "Iteration 1785, loss = 0.06787733\n",
      "Iteration 1786, loss = 0.06786920\n",
      "Iteration 1787, loss = 0.06786096\n",
      "Iteration 1788, loss = 0.06785265\n",
      "Iteration 1789, loss = 0.06784437\n",
      "Iteration 1790, loss = 0.06783697\n",
      "Iteration 1791, loss = 0.06782805\n",
      "Iteration 1792, loss = 0.06782004\n",
      "Iteration 1793, loss = 0.06781200\n",
      "Iteration 1794, loss = 0.06780387\n",
      "Iteration 1795, loss = 0.06779569\n",
      "Iteration 1796, loss = 0.06778743\n",
      "Iteration 1797, loss = 0.06777912\n",
      "Iteration 1798, loss = 0.06777128\n",
      "Iteration 1799, loss = 0.06776290\n",
      "Iteration 1800, loss = 0.06775496\n",
      "Iteration 1801, loss = 0.06774690\n",
      "Iteration 1802, loss = 0.06773873\n",
      "Iteration 1803, loss = 0.06773083\n",
      "Iteration 1804, loss = 0.06772248\n",
      "Iteration 1805, loss = 0.06771425\n",
      "Iteration 1806, loss = 0.06770603\n",
      "Iteration 1807, loss = 0.06769906\n",
      "Iteration 1808, loss = 0.06768991\n",
      "Iteration 1809, loss = 0.06768196\n",
      "Iteration 1810, loss = 0.06767384\n",
      "Iteration 1811, loss = 0.06766567\n",
      "Iteration 1812, loss = 0.06765743\n",
      "Iteration 1813, loss = 0.06764915\n",
      "Iteration 1814, loss = 0.06764101\n",
      "Iteration 1815, loss = 0.06763311\n",
      "Iteration 1816, loss = 0.06762524\n",
      "Iteration 1817, loss = 0.06761733\n",
      "Iteration 1818, loss = 0.06760930\n",
      "Iteration 1819, loss = 0.06760117\n",
      "Iteration 1820, loss = 0.06759295\n",
      "Iteration 1821, loss = 0.06758480\n",
      "Iteration 1822, loss = 0.06757654\n",
      "Iteration 1823, loss = 0.06756829\n",
      "Iteration 1824, loss = 0.06756113\n",
      "Iteration 1825, loss = 0.06755221\n",
      "Iteration 1826, loss = 0.06754413\n",
      "Iteration 1827, loss = 0.06753616\n",
      "Iteration 1828, loss = 0.06752807\n",
      "Iteration 1829, loss = 0.06752002\n",
      "Iteration 1830, loss = 0.06751186\n",
      "Iteration 1831, loss = 0.06750435\n",
      "Iteration 1832, loss = 0.06749589\n",
      "Iteration 1833, loss = 0.06748802\n",
      "Iteration 1834, loss = 0.06748010\n",
      "Iteration 1835, loss = 0.06747211\n",
      "Iteration 1836, loss = 0.06746408\n",
      "Iteration 1837, loss = 0.06745597\n",
      "Iteration 1838, loss = 0.06744778\n",
      "Iteration 1839, loss = 0.06743979\n",
      "Iteration 1840, loss = 0.06743140\n",
      "Iteration 1841, loss = 0.06742521\n",
      "Iteration 1842, loss = 0.06741578\n",
      "Iteration 1843, loss = 0.06740818\n",
      "Iteration 1844, loss = 0.06740068\n",
      "Iteration 1845, loss = 0.06739304\n",
      "Iteration 1846, loss = 0.06738529\n",
      "Iteration 1847, loss = 0.06737742\n",
      "Iteration 1848, loss = 0.06736947\n",
      "Iteration 1849, loss = 0.06736141\n",
      "Iteration 1850, loss = 0.06735379\n",
      "Iteration 1851, loss = 0.06734578\n",
      "Iteration 1852, loss = 0.06733735\n",
      "Iteration 1853, loss = 0.06732928\n",
      "Iteration 1854, loss = 0.06732123\n",
      "Iteration 1855, loss = 0.06731317\n",
      "Iteration 1856, loss = 0.06730503\n",
      "Iteration 1857, loss = 0.06729680\n",
      "Iteration 1858, loss = 0.06728850\n",
      "Iteration 1859, loss = 0.06728015\n",
      "Iteration 1860, loss = 0.06727402\n",
      "Iteration 1861, loss = 0.06726532\n",
      "Iteration 1862, loss = 0.06725628\n",
      "Iteration 1863, loss = 0.06724859\n",
      "Iteration 1864, loss = 0.06724130\n",
      "Iteration 1865, loss = 0.06723370\n",
      "Iteration 1866, loss = 0.06722560\n",
      "Iteration 1867, loss = 0.06721768\n",
      "Iteration 1868, loss = 0.06720989\n",
      "Iteration 1869, loss = 0.06720202\n",
      "Iteration 1870, loss = 0.06719404\n",
      "Iteration 1871, loss = 0.06718599\n",
      "Iteration 1872, loss = 0.06717788\n",
      "Iteration 1873, loss = 0.06716970\n",
      "Iteration 1874, loss = 0.06716144\n",
      "Iteration 1875, loss = 0.06715316\n",
      "Iteration 1876, loss = 0.06714584\n",
      "Iteration 1877, loss = 0.06713730\n",
      "Iteration 1878, loss = 0.06712954\n",
      "Iteration 1879, loss = 0.06712131\n",
      "Iteration 1880, loss = 0.06711355\n",
      "Iteration 1881, loss = 0.06710613\n",
      "Iteration 1882, loss = 0.06709825\n",
      "Iteration 1883, loss = 0.06709067\n",
      "Iteration 1884, loss = 0.06708297\n",
      "Iteration 1885, loss = 0.06707516\n",
      "Iteration 1886, loss = 0.06706727\n",
      "Iteration 1887, loss = 0.06705929\n",
      "Iteration 1888, loss = 0.06705123\n",
      "Iteration 1889, loss = 0.06704312\n",
      "Iteration 1890, loss = 0.06703494\n",
      "Iteration 1891, loss = 0.06702734\n",
      "Iteration 1892, loss = 0.06701978\n",
      "Iteration 1893, loss = 0.06701136\n",
      "Iteration 1894, loss = 0.06700345\n",
      "Iteration 1895, loss = 0.06699581\n",
      "Iteration 1896, loss = 0.06698805\n",
      "Iteration 1897, loss = 0.06698019\n",
      "Iteration 1898, loss = 0.06697225\n",
      "Iteration 1899, loss = 0.06696422\n",
      "Iteration 1900, loss = 0.06695760\n",
      "Iteration 1901, loss = 0.06694849\n",
      "Iteration 1902, loss = 0.06694072\n",
      "Iteration 1903, loss = 0.06693282\n",
      "Iteration 1904, loss = 0.06692544\n",
      "Iteration 1905, loss = 0.06691764\n",
      "Iteration 1906, loss = 0.06690939\n",
      "Iteration 1907, loss = 0.06690146\n",
      "Iteration 1908, loss = 0.06689366\n",
      "Iteration 1909, loss = 0.06688619\n",
      "Iteration 1910, loss = 0.06687864\n",
      "Iteration 1911, loss = 0.06687095\n",
      "Iteration 1912, loss = 0.06686315\n",
      "Iteration 1913, loss = 0.06685527\n",
      "Iteration 1914, loss = 0.06684733\n",
      "Iteration 1915, loss = 0.06683930\n",
      "Iteration 1916, loss = 0.06683119\n",
      "Iteration 1917, loss = 0.06682310\n",
      "Iteration 1918, loss = 0.06681639\n",
      "Iteration 1919, loss = 0.06680754\n",
      "Iteration 1920, loss = 0.06679995\n",
      "Iteration 1921, loss = 0.06679226\n",
      "Iteration 1922, loss = 0.06678447\n",
      "Iteration 1923, loss = 0.06677659\n",
      "Iteration 1924, loss = 0.06676863\n",
      "Iteration 1925, loss = 0.06676100\n",
      "Iteration 1926, loss = 0.06675443\n",
      "Iteration 1927, loss = 0.06674531\n",
      "Iteration 1928, loss = 0.06673782\n",
      "Iteration 1929, loss = 0.06673019\n",
      "Iteration 1930, loss = 0.06672247\n",
      "Iteration 1931, loss = 0.06671468\n",
      "Iteration 1932, loss = 0.06670678\n",
      "Iteration 1933, loss = 0.06669880\n",
      "Iteration 1934, loss = 0.06669102\n",
      "Iteration 1935, loss = 0.06668411\n",
      "Iteration 1936, loss = 0.06667547\n",
      "Iteration 1937, loss = 0.06666799\n",
      "Iteration 1938, loss = 0.06666039\n",
      "Iteration 1939, loss = 0.06665267\n",
      "Iteration 1940, loss = 0.06664488\n",
      "Iteration 1941, loss = 0.06663700\n",
      "Iteration 1942, loss = 0.06662904\n",
      "Iteration 1943, loss = 0.06662120\n",
      "Iteration 1944, loss = 0.06661474\n",
      "Iteration 1945, loss = 0.06660572\n",
      "Iteration 1946, loss = 0.06659824\n",
      "Iteration 1947, loss = 0.06659066\n",
      "Iteration 1948, loss = 0.06658294\n",
      "Iteration 1949, loss = 0.06657518\n",
      "Iteration 1950, loss = 0.06656734\n",
      "Iteration 1951, loss = 0.06655938\n",
      "Iteration 1952, loss = 0.06655189\n",
      "Iteration 1953, loss = 0.06654412\n",
      "Iteration 1954, loss = 0.06653658\n",
      "Iteration 1955, loss = 0.06652907\n",
      "Iteration 1956, loss = 0.06652143\n",
      "Iteration 1957, loss = 0.06651373\n",
      "Iteration 1958, loss = 0.06650595\n",
      "Iteration 1959, loss = 0.06649806\n",
      "Iteration 1960, loss = 0.06649009\n",
      "Iteration 1961, loss = 0.06648239\n",
      "Iteration 1962, loss = 0.06647545\n",
      "Iteration 1963, loss = 0.06646682\n",
      "Iteration 1964, loss = 0.06645934\n",
      "Iteration 1965, loss = 0.06645177\n",
      "Iteration 1966, loss = 0.06644409\n",
      "Iteration 1967, loss = 0.06643634\n",
      "Iteration 1968, loss = 0.06642852\n",
      "Iteration 1969, loss = 0.06642057\n",
      "Iteration 1970, loss = 0.06641362\n",
      "Iteration 1971, loss = 0.06640554\n",
      "Iteration 1972, loss = 0.06639782\n",
      "Iteration 1973, loss = 0.06639036\n",
      "Iteration 1974, loss = 0.06638278\n",
      "Iteration 1975, loss = 0.06637507\n",
      "Iteration 1976, loss = 0.06636732\n",
      "Iteration 1977, loss = 0.06635948\n",
      "Iteration 1978, loss = 0.06635154\n",
      "Iteration 1979, loss = 0.06634407\n",
      "Iteration 1980, loss = 0.06633768\n",
      "Iteration 1981, loss = 0.06632839\n",
      "Iteration 1982, loss = 0.06632097\n",
      "Iteration 1983, loss = 0.06631340\n",
      "Iteration 1984, loss = 0.06630574\n",
      "Iteration 1985, loss = 0.06629802\n",
      "Iteration 1986, loss = 0.06629020\n",
      "Iteration 1987, loss = 0.06628241\n",
      "Iteration 1988, loss = 0.06627552\n",
      "Iteration 1989, loss = 0.06626717\n",
      "Iteration 1990, loss = 0.06625974\n",
      "Iteration 1991, loss = 0.06625221\n",
      "Iteration 1992, loss = 0.06624468\n",
      "Iteration 1993, loss = 0.06623706\n",
      "Iteration 1994, loss = 0.06622932\n",
      "Iteration 1995, loss = 0.06622148\n",
      "Iteration 1996, loss = 0.06621385\n",
      "Iteration 1997, loss = 0.06620588\n",
      "Iteration 1998, loss = 0.06619973\n",
      "Iteration 1999, loss = 0.06619081\n",
      "Iteration 2000, loss = 0.06618340\n",
      "Iteration 2001, loss = 0.06617585\n",
      "Iteration 2002, loss = 0.06616825\n",
      "Iteration 2003, loss = 0.06616054\n",
      "Iteration 2004, loss = 0.06615272\n",
      "Iteration 2005, loss = 0.06614482\n",
      "Iteration 2006, loss = 0.06613798\n",
      "Iteration 2007, loss = 0.06612984\n",
      "Iteration 2008, loss = 0.06612228\n",
      "Iteration 2009, loss = 0.06611488\n",
      "Iteration 2010, loss = 0.06610735\n",
      "Iteration 2011, loss = 0.06609972\n",
      "Iteration 2012, loss = 0.06609201\n",
      "Iteration 2013, loss = 0.06608421\n",
      "Iteration 2014, loss = 0.06607654\n",
      "Iteration 2015, loss = 0.06606862\n",
      "Iteration 2016, loss = 0.06606199\n",
      "Iteration 2017, loss = 0.06605368\n",
      "Iteration 2018, loss = 0.06604630\n",
      "Iteration 2019, loss = 0.06603879\n",
      "Iteration 2020, loss = 0.06603117\n",
      "Iteration 2021, loss = 0.06602349\n",
      "Iteration 2022, loss = 0.06601571\n",
      "Iteration 2023, loss = 0.06600785\n",
      "Iteration 2024, loss = 0.06600041\n",
      "Iteration 2025, loss = 0.06599441\n",
      "Iteration 2026, loss = 0.06598494\n",
      "Iteration 2027, loss = 0.06597762\n",
      "Iteration 2028, loss = 0.06597021\n",
      "Iteration 2029, loss = 0.06596262\n",
      "Iteration 2030, loss = 0.06595495\n",
      "Iteration 2031, loss = 0.06594725\n",
      "Iteration 2032, loss = 0.06593986\n",
      "Iteration 2033, loss = 0.06593199\n",
      "Iteration 2034, loss = 0.06592570\n",
      "Iteration 2035, loss = 0.06591690\n",
      "Iteration 2036, loss = 0.06590958\n",
      "Iteration 2037, loss = 0.06590212\n",
      "Iteration 2038, loss = 0.06589456\n",
      "Iteration 2039, loss = 0.06588691\n",
      "Iteration 2040, loss = 0.06587918\n",
      "Iteration 2041, loss = 0.06587137\n",
      "Iteration 2042, loss = 0.06586367\n",
      "Iteration 2043, loss = 0.06585679\n",
      "Iteration 2044, loss = 0.06584876\n",
      "Iteration 2045, loss = 0.06584123\n",
      "Iteration 2046, loss = 0.06583384\n",
      "Iteration 2047, loss = 0.06582632\n",
      "Iteration 2048, loss = 0.06581869\n",
      "Iteration 2049, loss = 0.06581123\n",
      "Iteration 2050, loss = 0.06580341\n",
      "Iteration 2051, loss = 0.06579572\n",
      "Iteration 2052, loss = 0.06578794\n",
      "Iteration 2053, loss = 0.06578233\n",
      "Iteration 2054, loss = 0.06577290\n",
      "Iteration 2055, loss = 0.06576555\n",
      "Iteration 2056, loss = 0.06575807\n",
      "Iteration 2057, loss = 0.06575050\n",
      "Iteration 2058, loss = 0.06574315\n",
      "Iteration 2059, loss = 0.06573530\n",
      "Iteration 2060, loss = 0.06572764\n",
      "Iteration 2061, loss = 0.06572064\n",
      "Iteration 2062, loss = 0.06571280\n",
      "Iteration 2063, loss = 0.06570554\n",
      "Iteration 2064, loss = 0.06569814\n",
      "Iteration 2065, loss = 0.06569065\n",
      "Iteration 2066, loss = 0.06568308\n",
      "Iteration 2067, loss = 0.06567540\n",
      "Iteration 2068, loss = 0.06566812\n",
      "Iteration 2069, loss = 0.06566035\n",
      "Iteration 2070, loss = 0.06565247\n",
      "Iteration 2071, loss = 0.06564495\n",
      "Iteration 2072, loss = 0.06563760\n",
      "Iteration 2073, loss = 0.06563026\n",
      "Iteration 2074, loss = 0.06562282\n",
      "Iteration 2075, loss = 0.06561528\n",
      "Iteration 2076, loss = 0.06560761\n",
      "Iteration 2077, loss = 0.06560025\n",
      "Iteration 2078, loss = 0.06559255\n",
      "Iteration 2079, loss = 0.06558477\n",
      "Iteration 2080, loss = 0.06557718\n",
      "Iteration 2081, loss = 0.06557154\n",
      "Iteration 2082, loss = 0.06556221\n",
      "Iteration 2083, loss = 0.06555482\n",
      "Iteration 2084, loss = 0.06554734\n",
      "Iteration 2085, loss = 0.06553975\n",
      "Iteration 2086, loss = 0.06553249\n",
      "Iteration 2087, loss = 0.06552484\n",
      "Iteration 2088, loss = 0.06551703\n",
      "Iteration 2089, loss = 0.06551014\n",
      "Iteration 2090, loss = 0.06550231\n",
      "Iteration 2091, loss = 0.06549501\n",
      "Iteration 2092, loss = 0.06548761\n",
      "Iteration 2093, loss = 0.06548009\n",
      "Iteration 2094, loss = 0.06547246\n",
      "Iteration 2095, loss = 0.06546532\n",
      "Iteration 2096, loss = 0.06545767\n",
      "Iteration 2097, loss = 0.06544974\n",
      "Iteration 2098, loss = 0.06544216\n",
      "Iteration 2099, loss = 0.06543457\n",
      "Iteration 2100, loss = 0.06542879\n",
      "Iteration 2101, loss = 0.06541950\n",
      "Iteration 2102, loss = 0.06541203\n",
      "Iteration 2103, loss = 0.06540443\n",
      "Iteration 2104, loss = 0.06539751\n",
      "Iteration 2105, loss = 0.06539027\n",
      "Iteration 2106, loss = 0.06538251\n",
      "Iteration 2107, loss = 0.06537511\n",
      "Iteration 2108, loss = 0.06536845\n",
      "Iteration 2109, loss = 0.06536027\n",
      "Iteration 2110, loss = 0.06535310\n",
      "Iteration 2111, loss = 0.06534586\n",
      "Iteration 2112, loss = 0.06533871\n",
      "Iteration 2113, loss = 0.06533122\n",
      "Iteration 2114, loss = 0.06532382\n",
      "Iteration 2115, loss = 0.06531656\n",
      "Iteration 2116, loss = 0.06530896\n",
      "Iteration 2117, loss = 0.06530144\n",
      "Iteration 2118, loss = 0.06529381\n",
      "Iteration 2119, loss = 0.06528793\n",
      "Iteration 2120, loss = 0.06527893\n",
      "Iteration 2121, loss = 0.06527193\n",
      "Iteration 2122, loss = 0.06526474\n",
      "Iteration 2123, loss = 0.06525733\n",
      "Iteration 2124, loss = 0.06524976\n",
      "Iteration 2125, loss = 0.06524262\n",
      "Iteration 2126, loss = 0.06523498\n",
      "Iteration 2127, loss = 0.06522776\n",
      "Iteration 2128, loss = 0.06522076\n",
      "Iteration 2129, loss = 0.06521375\n",
      "Iteration 2130, loss = 0.06520657\n",
      "Iteration 2131, loss = 0.06519924\n",
      "Iteration 2132, loss = 0.06519181\n",
      "Iteration 2133, loss = 0.06518430\n",
      "Iteration 2134, loss = 0.06517689\n",
      "Iteration 2135, loss = 0.06516944\n",
      "Iteration 2136, loss = 0.06516192\n",
      "Iteration 2137, loss = 0.06515431\n",
      "Iteration 2138, loss = 0.06514727\n",
      "Iteration 2139, loss = 0.06513990\n",
      "Iteration 2140, loss = 0.06513273\n",
      "Iteration 2141, loss = 0.06512548\n",
      "Iteration 2142, loss = 0.06511809\n",
      "Iteration 2143, loss = 0.06511066\n",
      "Iteration 2144, loss = 0.06510326\n",
      "Iteration 2145, loss = 0.06509576\n",
      "Iteration 2146, loss = 0.06508831\n",
      "Iteration 2147, loss = 0.06508108\n",
      "Iteration 2148, loss = 0.06507389\n",
      "Iteration 2149, loss = 0.06506684\n",
      "Iteration 2150, loss = 0.06505958\n",
      "Iteration 2151, loss = 0.06505209\n",
      "Iteration 2152, loss = 0.06504520\n",
      "Iteration 2153, loss = 0.06503774\n",
      "Iteration 2154, loss = 0.06503024\n",
      "Iteration 2155, loss = 0.06502296\n",
      "Iteration 2156, loss = 0.06501557\n",
      "Iteration 2157, loss = 0.06500816\n",
      "Iteration 2158, loss = 0.06500096\n",
      "Iteration 2159, loss = 0.06499372\n",
      "Iteration 2160, loss = 0.06498636\n",
      "Iteration 2161, loss = 0.06497900\n",
      "Iteration 2162, loss = 0.06497236\n",
      "Iteration 2163, loss = 0.06496495\n",
      "Iteration 2164, loss = 0.06495695\n",
      "Iteration 2165, loss = 0.06494983\n",
      "Iteration 2166, loss = 0.06494258\n",
      "Iteration 2167, loss = 0.06493596\n",
      "Iteration 2168, loss = 0.06492830\n",
      "Iteration 2169, loss = 0.06492120\n",
      "Iteration 2170, loss = 0.06491403\n",
      "Iteration 2171, loss = 0.06490674\n",
      "Iteration 2172, loss = 0.06489932\n",
      "Iteration 2173, loss = 0.06489187\n",
      "Iteration 2174, loss = 0.06488432\n",
      "Iteration 2175, loss = 0.06487666\n",
      "Iteration 2176, loss = 0.06486939\n",
      "Iteration 2177, loss = 0.06486350\n",
      "Iteration 2178, loss = 0.06485458\n",
      "Iteration 2179, loss = 0.06484775\n",
      "Iteration 2180, loss = 0.06484075\n",
      "Iteration 2181, loss = 0.06483365\n",
      "Iteration 2182, loss = 0.06482647\n",
      "Iteration 2183, loss = 0.06481918\n",
      "Iteration 2184, loss = 0.06481178\n",
      "Iteration 2185, loss = 0.06480426\n",
      "Iteration 2186, loss = 0.06479672\n",
      "Iteration 2187, loss = 0.06478955\n",
      "Iteration 2188, loss = 0.06478232\n",
      "Iteration 2189, loss = 0.06477544\n",
      "Iteration 2190, loss = 0.06476833\n",
      "Iteration 2191, loss = 0.06476063\n",
      "Iteration 2192, loss = 0.06475344\n",
      "Iteration 2193, loss = 0.06474615\n",
      "Iteration 2194, loss = 0.06473870\n",
      "Iteration 2195, loss = 0.06473117\n",
      "Iteration 2196, loss = 0.06472414\n",
      "Iteration 2197, loss = 0.06471732\n",
      "Iteration 2198, loss = 0.06470941\n",
      "Iteration 2199, loss = 0.06470246\n",
      "Iteration 2200, loss = 0.06469543\n",
      "Iteration 2201, loss = 0.06468822\n",
      "Iteration 2202, loss = 0.06468097\n",
      "Iteration 2203, loss = 0.06467362\n",
      "Iteration 2204, loss = 0.06466613\n",
      "Iteration 2205, loss = 0.06465858\n",
      "Iteration 2206, loss = 0.06465158\n",
      "Iteration 2207, loss = 0.06464425\n",
      "Iteration 2208, loss = 0.06463696\n",
      "Iteration 2209, loss = 0.06462993\n",
      "Iteration 2210, loss = 0.06462276\n",
      "Iteration 2211, loss = 0.06461548\n",
      "Iteration 2212, loss = 0.06460806\n",
      "Iteration 2213, loss = 0.06460050\n",
      "Iteration 2214, loss = 0.06459318\n",
      "Iteration 2215, loss = 0.06458553\n",
      "Iteration 2216, loss = 0.06457823\n",
      "Iteration 2217, loss = 0.06457199\n",
      "Iteration 2218, loss = 0.06456386\n",
      "Iteration 2219, loss = 0.06455671\n",
      "Iteration 2220, loss = 0.06454950\n",
      "Iteration 2221, loss = 0.06454215\n",
      "Iteration 2222, loss = 0.06453467\n",
      "Iteration 2223, loss = 0.06452709\n",
      "Iteration 2224, loss = 0.06451997\n",
      "Iteration 2225, loss = 0.06451234\n",
      "Iteration 2226, loss = 0.06450424\n",
      "Iteration 2227, loss = 0.06449686\n",
      "Iteration 2228, loss = 0.06448954\n",
      "Iteration 2229, loss = 0.06448200\n",
      "Iteration 2230, loss = 0.06447435\n",
      "Iteration 2231, loss = 0.06446668\n",
      "Iteration 2232, loss = 0.06445899\n",
      "Iteration 2233, loss = 0.06445121\n",
      "Iteration 2234, loss = 0.06444332\n",
      "Iteration 2235, loss = 0.06443543\n",
      "Iteration 2236, loss = 0.06442753\n",
      "Iteration 2237, loss = 0.06442029\n",
      "Iteration 2238, loss = 0.06441203\n",
      "Iteration 2239, loss = 0.06440441\n",
      "Iteration 2240, loss = 0.06439697\n",
      "Iteration 2241, loss = 0.06438907\n",
      "Iteration 2242, loss = 0.06438132\n",
      "Iteration 2243, loss = 0.06437341\n",
      "Iteration 2244, loss = 0.06436574\n",
      "Iteration 2245, loss = 0.06435753\n",
      "Iteration 2246, loss = 0.06434956\n",
      "Iteration 2247, loss = 0.06434300\n",
      "Iteration 2248, loss = 0.06433404\n",
      "Iteration 2249, loss = 0.06432637\n",
      "Iteration 2250, loss = 0.06431875\n",
      "Iteration 2251, loss = 0.06431107\n",
      "Iteration 2252, loss = 0.06430322\n",
      "Iteration 2253, loss = 0.06429524\n",
      "Iteration 2254, loss = 0.06428773\n",
      "Iteration 2255, loss = 0.06427956\n",
      "Iteration 2256, loss = 0.06427140\n",
      "Iteration 2257, loss = 0.06426470\n",
      "Iteration 2258, loss = 0.06425612\n",
      "Iteration 2259, loss = 0.06424844\n",
      "Iteration 2260, loss = 0.06424070\n",
      "Iteration 2261, loss = 0.06423280\n",
      "Iteration 2262, loss = 0.06422484\n",
      "Iteration 2263, loss = 0.06421682\n",
      "Iteration 2264, loss = 0.06420963\n",
      "Iteration 2265, loss = 0.06420164\n",
      "Iteration 2266, loss = 0.06419300\n",
      "Iteration 2267, loss = 0.06418562\n",
      "Iteration 2268, loss = 0.06417801\n",
      "Iteration 2269, loss = 0.06417059\n",
      "Iteration 2270, loss = 0.06416298\n",
      "Iteration 2271, loss = 0.06415525\n",
      "Iteration 2272, loss = 0.06414739\n",
      "Iteration 2273, loss = 0.06413938\n",
      "Iteration 2274, loss = 0.06413128\n",
      "Iteration 2275, loss = 0.06412316\n",
      "Iteration 2276, loss = 0.06411497\n",
      "Iteration 2277, loss = 0.06410673\n",
      "Iteration 2278, loss = 0.06410045\n",
      "Iteration 2279, loss = 0.06409138\n",
      "Iteration 2280, loss = 0.06408318\n",
      "Iteration 2281, loss = 0.06407560\n",
      "Iteration 2282, loss = 0.06406785\n",
      "Iteration 2283, loss = 0.06405988\n",
      "Iteration 2284, loss = 0.06405209\n",
      "Iteration 2285, loss = 0.06404404\n",
      "Iteration 2286, loss = 0.06403606\n",
      "Iteration 2287, loss = 0.06402826\n",
      "Iteration 2288, loss = 0.06402039\n",
      "Iteration 2289, loss = 0.06401309\n",
      "Iteration 2290, loss = 0.06400515\n",
      "Iteration 2291, loss = 0.06399753\n",
      "Iteration 2292, loss = 0.06398988\n",
      "Iteration 2293, loss = 0.06398210\n",
      "Iteration 2294, loss = 0.06397427\n",
      "Iteration 2295, loss = 0.06396623\n",
      "Iteration 2296, loss = 0.06395822\n",
      "Iteration 2297, loss = 0.06395011\n",
      "Iteration 2298, loss = 0.06394191\n",
      "Iteration 2299, loss = 0.06393594\n",
      "Iteration 2300, loss = 0.06392671\n",
      "Iteration 2301, loss = 0.06391906\n",
      "Iteration 2302, loss = 0.06391065\n",
      "Iteration 2303, loss = 0.06390304\n",
      "Iteration 2304, loss = 0.06389549\n",
      "Iteration 2305, loss = 0.06388766\n",
      "Iteration 2306, loss = 0.06387974\n",
      "Iteration 2307, loss = 0.06387180\n",
      "Iteration 2308, loss = 0.06386552\n",
      "Iteration 2309, loss = 0.06385621\n",
      "Iteration 2310, loss = 0.06384907\n",
      "Iteration 2311, loss = 0.06384180\n",
      "Iteration 2312, loss = 0.06383449\n",
      "Iteration 2313, loss = 0.06382712\n",
      "Iteration 2314, loss = 0.06382090\n",
      "Iteration 2315, loss = 0.06381376\n",
      "Iteration 2316, loss = 0.06380593\n",
      "Iteration 2317, loss = 0.06379807\n",
      "Iteration 2318, loss = 0.06379093\n",
      "Iteration 2319, loss = 0.06378513\n",
      "Iteration 2320, loss = 0.06377687\n",
      "Iteration 2321, loss = 0.06376995\n",
      "Iteration 2322, loss = 0.06376288\n",
      "Iteration 2323, loss = 0.06375568\n",
      "Iteration 2324, loss = 0.06374833\n",
      "Iteration 2325, loss = 0.06374085\n",
      "Iteration 2326, loss = 0.06373341\n",
      "Iteration 2327, loss = 0.06372617\n",
      "Iteration 2328, loss = 0.06371893\n",
      "Iteration 2329, loss = 0.06371266\n",
      "Iteration 2330, loss = 0.06370470\n",
      "Iteration 2331, loss = 0.06369773\n",
      "Iteration 2332, loss = 0.06369059\n",
      "Iteration 2333, loss = 0.06368334\n",
      "Iteration 2334, loss = 0.06367604\n",
      "Iteration 2335, loss = 0.06366879\n",
      "Iteration 2336, loss = 0.06366155\n",
      "Iteration 2337, loss = 0.06365442\n",
      "Iteration 2338, loss = 0.06364703\n",
      "Iteration 2339, loss = 0.06363969\n",
      "Iteration 2340, loss = 0.06363417\n",
      "Iteration 2341, loss = 0.06362535\n",
      "Iteration 2342, loss = 0.06361827\n",
      "Iteration 2343, loss = 0.06361164\n",
      "Iteration 2344, loss = 0.06360426\n",
      "Iteration 2345, loss = 0.06359696\n",
      "Iteration 2346, loss = 0.06358992\n",
      "Iteration 2347, loss = 0.06358273\n",
      "Iteration 2348, loss = 0.06357544\n",
      "Iteration 2349, loss = 0.06356802\n",
      "Iteration 2350, loss = 0.06356221\n",
      "Iteration 2351, loss = 0.06355347\n",
      "Iteration 2352, loss = 0.06354630\n",
      "Iteration 2353, loss = 0.06353992\n",
      "Iteration 2354, loss = 0.06353280\n",
      "Iteration 2355, loss = 0.06352490\n",
      "Iteration 2356, loss = 0.06351784\n",
      "Iteration 2357, loss = 0.06351064\n",
      "Iteration 2358, loss = 0.06350332\n",
      "Iteration 2359, loss = 0.06349592\n",
      "Iteration 2360, loss = 0.06348848\n",
      "Iteration 2361, loss = 0.06348320\n",
      "Iteration 2362, loss = 0.06347430\n",
      "Iteration 2363, loss = 0.06346723\n",
      "Iteration 2364, loss = 0.06346007\n",
      "Iteration 2365, loss = 0.06345305\n",
      "Iteration 2366, loss = 0.06344590\n",
      "Iteration 2367, loss = 0.06343867\n",
      "Iteration 2368, loss = 0.06343129\n",
      "Iteration 2369, loss = 0.06342404\n",
      "Iteration 2370, loss = 0.06341659\n",
      "Iteration 2371, loss = 0.06341065\n",
      "Iteration 2372, loss = 0.06340235\n",
      "Iteration 2373, loss = 0.06339545\n",
      "Iteration 2374, loss = 0.06338834\n",
      "Iteration 2375, loss = 0.06338127\n",
      "Iteration 2376, loss = 0.06337399\n",
      "Iteration 2377, loss = 0.06336686\n",
      "Iteration 2378, loss = 0.06335985\n",
      "Iteration 2379, loss = 0.06335215\n",
      "Iteration 2380, loss = 0.06334487\n",
      "Iteration 2381, loss = 0.06333756\n",
      "Iteration 2382, loss = 0.06333176\n",
      "Iteration 2383, loss = 0.06332329\n",
      "Iteration 2384, loss = 0.06331632\n",
      "Iteration 2385, loss = 0.06330922\n",
      "Iteration 2386, loss = 0.06330194\n",
      "Iteration 2387, loss = 0.06329508\n",
      "Iteration 2388, loss = 0.06328746\n",
      "Iteration 2389, loss = 0.06328043\n",
      "Iteration 2390, loss = 0.06327326\n",
      "Iteration 2391, loss = 0.06326606\n",
      "Iteration 2392, loss = 0.06325878\n",
      "Iteration 2393, loss = 0.06325179\n",
      "Iteration 2394, loss = 0.06324474\n",
      "Iteration 2395, loss = 0.06323755\n",
      "Iteration 2396, loss = 0.06323059\n",
      "Iteration 2397, loss = 0.06322347\n",
      "Iteration 2398, loss = 0.06321694\n",
      "Iteration 2399, loss = 0.06320967\n",
      "Iteration 2400, loss = 0.06320150\n",
      "Iteration 2401, loss = 0.06319455\n",
      "Iteration 2402, loss = 0.06318765\n",
      "Iteration 2403, loss = 0.06318055\n",
      "Iteration 2404, loss = 0.06317528\n",
      "Iteration 2405, loss = 0.06316657\n",
      "Iteration 2406, loss = 0.06315963\n",
      "Iteration 2407, loss = 0.06315252\n",
      "Iteration 2408, loss = 0.06314535\n",
      "Iteration 2409, loss = 0.06313807\n",
      "Iteration 2410, loss = 0.06313063\n",
      "Iteration 2411, loss = 0.06312304\n",
      "Iteration 2412, loss = 0.06311658\n",
      "Iteration 2413, loss = 0.06310951\n",
      "Iteration 2414, loss = 0.06310329\n",
      "Iteration 2415, loss = 0.06309459\n",
      "Iteration 2416, loss = 0.06308781\n",
      "Iteration 2417, loss = 0.06308100\n",
      "Iteration 2418, loss = 0.06307432\n",
      "Iteration 2419, loss = 0.06306740\n",
      "Iteration 2420, loss = 0.06306016\n",
      "Iteration 2421, loss = 0.06305271\n",
      "Iteration 2422, loss = 0.06304527\n",
      "Iteration 2423, loss = 0.06303782\n",
      "Iteration 2424, loss = 0.06303056\n",
      "Iteration 2425, loss = 0.06302503\n",
      "Iteration 2426, loss = 0.06301713\n",
      "Iteration 2427, loss = 0.06300949\n",
      "Iteration 2428, loss = 0.06300271\n",
      "Iteration 2429, loss = 0.06299599\n",
      "Iteration 2430, loss = 0.06298920\n",
      "Iteration 2431, loss = 0.06298217\n",
      "Iteration 2432, loss = 0.06297495\n",
      "Iteration 2433, loss = 0.06296771\n",
      "Iteration 2434, loss = 0.06296033\n",
      "Iteration 2435, loss = 0.06295311\n",
      "Iteration 2436, loss = 0.06294690\n",
      "Iteration 2437, loss = 0.06293841\n",
      "Iteration 2438, loss = 0.06293121\n",
      "Iteration 2439, loss = 0.06292458\n",
      "Iteration 2440, loss = 0.06291764\n",
      "Iteration 2441, loss = 0.06291061\n",
      "Iteration 2442, loss = 0.06290354\n",
      "Iteration 2443, loss = 0.06289639\n",
      "Iteration 2444, loss = 0.06288905\n",
      "Iteration 2445, loss = 0.06288160\n",
      "Iteration 2446, loss = 0.06287481\n",
      "Iteration 2447, loss = 0.06286890\n",
      "Iteration 2448, loss = 0.06286053\n",
      "Iteration 2449, loss = 0.06285382\n",
      "Iteration 2450, loss = 0.06284689\n",
      "Iteration 2451, loss = 0.06283991\n",
      "Iteration 2452, loss = 0.06283302\n",
      "Iteration 2453, loss = 0.06282581\n",
      "Iteration 2454, loss = 0.06281836\n",
      "Iteration 2455, loss = 0.06281130\n",
      "Iteration 2456, loss = 0.06280393\n",
      "Iteration 2457, loss = 0.06279690\n",
      "Iteration 2458, loss = 0.06279136\n",
      "Iteration 2459, loss = 0.06278311\n",
      "Iteration 2460, loss = 0.06277629\n",
      "Iteration 2461, loss = 0.06276926\n",
      "Iteration 2462, loss = 0.06276223\n",
      "Iteration 2463, loss = 0.06275504\n",
      "Iteration 2464, loss = 0.06274793\n",
      "Iteration 2465, loss = 0.06274063\n",
      "Iteration 2466, loss = 0.06273362\n",
      "Iteration 2467, loss = 0.06272660\n",
      "Iteration 2468, loss = 0.06271954\n",
      "Iteration 2469, loss = 0.06271373\n",
      "Iteration 2470, loss = 0.06270551\n",
      "Iteration 2471, loss = 0.06269857\n",
      "Iteration 2472, loss = 0.06269144\n",
      "Iteration 2473, loss = 0.06268456\n",
      "Iteration 2474, loss = 0.06267736\n",
      "Iteration 2475, loss = 0.06267042\n",
      "Iteration 2476, loss = 0.06266361\n",
      "Iteration 2477, loss = 0.06265658\n",
      "Iteration 2478, loss = 0.06264932\n",
      "Iteration 2479, loss = 0.06264202\n",
      "Iteration 2480, loss = 0.06263582\n",
      "Iteration 2481, loss = 0.06262782\n",
      "Iteration 2482, loss = 0.06262114\n",
      "Iteration 2483, loss = 0.06261422\n",
      "Iteration 2484, loss = 0.06260738\n",
      "Iteration 2485, loss = 0.06260056\n",
      "Iteration 2486, loss = 0.06259362\n",
      "Iteration 2487, loss = 0.06258652\n",
      "Iteration 2488, loss = 0.06257924\n",
      "Iteration 2489, loss = 0.06257201\n",
      "Iteration 2490, loss = 0.06256482\n",
      "Iteration 2491, loss = 0.06255795\n",
      "Iteration 2492, loss = 0.06255083\n",
      "Iteration 2493, loss = 0.06254418\n",
      "Iteration 2494, loss = 0.06253753\n",
      "Iteration 2495, loss = 0.06253067\n",
      "Iteration 2496, loss = 0.06252373\n",
      "Iteration 2497, loss = 0.06251662\n",
      "Iteration 2498, loss = 0.06250931\n",
      "Iteration 2499, loss = 0.06250193\n",
      "Iteration 2500, loss = 0.06249488\n",
      "Iteration 2501, loss = 0.06248763\n",
      "Iteration 2502, loss = 0.06248039\n",
      "Iteration 2503, loss = 0.06247530\n",
      "Iteration 2504, loss = 0.06246671\n",
      "Iteration 2505, loss = 0.06245986\n",
      "Iteration 2506, loss = 0.06245299\n",
      "Iteration 2507, loss = 0.06244590\n",
      "Iteration 2508, loss = 0.06243875\n",
      "Iteration 2509, loss = 0.06243211\n",
      "Iteration 2510, loss = 0.06242488\n",
      "Iteration 2511, loss = 0.06241768\n",
      "Iteration 2512, loss = 0.06241074\n",
      "Iteration 2513, loss = 0.06240387\n",
      "Iteration 2514, loss = 0.06239701\n",
      "Iteration 2515, loss = 0.06239014\n",
      "Iteration 2516, loss = 0.06238311\n",
      "Iteration 2517, loss = 0.06237598\n",
      "Iteration 2518, loss = 0.06236925\n",
      "Iteration 2519, loss = 0.06236197\n",
      "Iteration 2520, loss = 0.06235459\n",
      "Iteration 2521, loss = 0.06234763\n",
      "Iteration 2522, loss = 0.06234039\n",
      "Iteration 2523, loss = 0.06233303\n",
      "Iteration 2524, loss = 0.06232558\n",
      "Iteration 2525, loss = 0.06231810\n",
      "Iteration 2526, loss = 0.06231294\n",
      "Iteration 2527, loss = 0.06230378\n",
      "Iteration 2528, loss = 0.06229654\n",
      "Iteration 2529, loss = 0.06228961\n",
      "Iteration 2530, loss = 0.06228263\n",
      "Iteration 2531, loss = 0.06227530\n",
      "Iteration 2532, loss = 0.06226824\n",
      "Iteration 2533, loss = 0.06226094\n",
      "Iteration 2534, loss = 0.06225364\n",
      "Iteration 2535, loss = 0.06224630\n",
      "Iteration 2536, loss = 0.06223946\n",
      "Iteration 2537, loss = 0.06223204\n",
      "Iteration 2538, loss = 0.06222467\n",
      "Iteration 2539, loss = 0.06221834\n",
      "Iteration 2540, loss = 0.06221047\n",
      "Iteration 2541, loss = 0.06220358\n",
      "Iteration 2542, loss = 0.06219647\n",
      "Iteration 2543, loss = 0.06218912\n",
      "Iteration 2544, loss = 0.06218182\n",
      "Iteration 2545, loss = 0.06217507\n",
      "Iteration 2546, loss = 0.06216786\n",
      "Iteration 2547, loss = 0.06216057\n",
      "Iteration 2548, loss = 0.06215342\n",
      "Iteration 2549, loss = 0.06214609\n",
      "Iteration 2550, loss = 0.06213874\n",
      "Iteration 2551, loss = 0.06213145\n",
      "Iteration 2552, loss = 0.06212411\n",
      "Iteration 2553, loss = 0.06211658\n",
      "Iteration 2554, loss = 0.06210957\n",
      "Iteration 2555, loss = 0.06210226\n",
      "Iteration 2556, loss = 0.06209498\n",
      "Iteration 2557, loss = 0.06208785\n",
      "Iteration 2558, loss = 0.06208090\n",
      "Iteration 2559, loss = 0.06207377\n",
      "Iteration 2560, loss = 0.06206628\n",
      "Iteration 2561, loss = 0.06205888\n",
      "Iteration 2562, loss = 0.06205149\n",
      "Iteration 2563, loss = 0.06204451\n",
      "Iteration 2564, loss = 0.06203760\n",
      "Iteration 2565, loss = 0.06202998\n",
      "Iteration 2566, loss = 0.06202310\n",
      "Iteration 2567, loss = 0.06201604\n",
      "Iteration 2568, loss = 0.06200886\n",
      "Iteration 2569, loss = 0.06200179\n",
      "Iteration 2570, loss = 0.06199455\n",
      "Iteration 2571, loss = 0.06198731\n",
      "Iteration 2572, loss = 0.06198044\n",
      "Iteration 2573, loss = 0.06197336\n",
      "Iteration 2574, loss = 0.06196615\n",
      "Iteration 2575, loss = 0.06195898\n",
      "Iteration 2576, loss = 0.06195189\n",
      "Iteration 2577, loss = 0.06194466\n",
      "Iteration 2578, loss = 0.06193737\n",
      "Iteration 2579, loss = 0.06193003\n",
      "Iteration 2580, loss = 0.06192280\n",
      "Iteration 2581, loss = 0.06191588\n",
      "Iteration 2582, loss = 0.06190849\n",
      "Iteration 2583, loss = 0.06190136\n",
      "Iteration 2584, loss = 0.06189443\n",
      "Iteration 2585, loss = 0.06188730\n",
      "Iteration 2586, loss = 0.06188001\n",
      "Iteration 2587, loss = 0.06187281\n",
      "Iteration 2588, loss = 0.06186552\n",
      "Iteration 2589, loss = 0.06185798\n",
      "Iteration 2590, loss = 0.06185117\n",
      "Iteration 2591, loss = 0.06184384\n",
      "Iteration 2592, loss = 0.06183675\n",
      "Iteration 2593, loss = 0.06182978\n",
      "Iteration 2594, loss = 0.06182278\n",
      "Iteration 2595, loss = 0.06181563\n",
      "Iteration 2596, loss = 0.06180828\n",
      "Iteration 2597, loss = 0.06180084\n",
      "Iteration 2598, loss = 0.06179447\n",
      "Iteration 2599, loss = 0.06178650\n",
      "Iteration 2600, loss = 0.06177931\n",
      "Iteration 2601, loss = 0.06177239\n",
      "Iteration 2602, loss = 0.06176546\n",
      "Iteration 2603, loss = 0.06175841\n",
      "Iteration 2604, loss = 0.06175140\n",
      "Iteration 2605, loss = 0.06174439\n",
      "Iteration 2606, loss = 0.06173707\n",
      "Iteration 2607, loss = 0.06172976\n",
      "Iteration 2608, loss = 0.06172266\n",
      "Iteration 2609, loss = 0.06171521\n",
      "Iteration 2610, loss = 0.06170814\n",
      "Iteration 2611, loss = 0.06170115\n",
      "Iteration 2612, loss = 0.06169404\n",
      "Iteration 2613, loss = 0.06168686\n",
      "Iteration 2614, loss = 0.06167944\n",
      "Iteration 2615, loss = 0.06167214\n",
      "Iteration 2616, loss = 0.06166474\n",
      "Iteration 2617, loss = 0.06165746\n",
      "Iteration 2618, loss = 0.06165013\n",
      "Iteration 2619, loss = 0.06164291\n",
      "Iteration 2620, loss = 0.06163572\n",
      "Iteration 2621, loss = 0.06162846\n",
      "Iteration 2622, loss = 0.06162173\n",
      "Iteration 2623, loss = 0.06161454\n",
      "Iteration 2624, loss = 0.06160681\n",
      "Iteration 2625, loss = 0.06159935\n",
      "Iteration 2626, loss = 0.06159280\n",
      "Iteration 2627, loss = 0.06158542\n",
      "Iteration 2628, loss = 0.06157807\n",
      "Iteration 2629, loss = 0.06157101\n",
      "Iteration 2630, loss = 0.06156377\n",
      "Iteration 2631, loss = 0.06155639\n",
      "Iteration 2632, loss = 0.06154902\n",
      "Iteration 2633, loss = 0.06154161\n",
      "Iteration 2634, loss = 0.06153422\n",
      "Iteration 2635, loss = 0.06152674\n",
      "Iteration 2636, loss = 0.06151959\n",
      "Iteration 2637, loss = 0.06151226\n",
      "Iteration 2638, loss = 0.06150530\n",
      "Iteration 2639, loss = 0.06149845\n",
      "Iteration 2640, loss = 0.06149122\n",
      "Iteration 2641, loss = 0.06148408\n",
      "Iteration 2642, loss = 0.06147679\n",
      "Iteration 2643, loss = 0.06146953\n",
      "Iteration 2644, loss = 0.06146273\n",
      "Iteration 2645, loss = 0.06145517\n",
      "Iteration 2646, loss = 0.06144807\n",
      "Iteration 2647, loss = 0.06144104\n",
      "Iteration 2648, loss = 0.06143422\n",
      "Iteration 2649, loss = 0.06142699\n",
      "Iteration 2650, loss = 0.06141934\n",
      "Iteration 2651, loss = 0.06141198\n",
      "Iteration 2652, loss = 0.06140459\n",
      "Iteration 2653, loss = 0.06139727\n",
      "Iteration 2654, loss = 0.06139083\n",
      "Iteration 2655, loss = 0.06138364\n",
      "Iteration 2656, loss = 0.06137554\n",
      "Iteration 2657, loss = 0.06136847\n",
      "Iteration 2658, loss = 0.06136121\n",
      "Iteration 2659, loss = 0.06135375\n",
      "Iteration 2660, loss = 0.06134659\n",
      "Iteration 2661, loss = 0.06133938\n",
      "Iteration 2662, loss = 0.06133250\n",
      "Iteration 2663, loss = 0.06132472\n",
      "Iteration 2664, loss = 0.06131752\n",
      "Iteration 2665, loss = 0.06131064\n",
      "Iteration 2666, loss = 0.06130355\n",
      "Iteration 2667, loss = 0.06129633\n",
      "Iteration 2668, loss = 0.06128893\n",
      "Iteration 2669, loss = 0.06128158\n",
      "Iteration 2670, loss = 0.06127406\n",
      "Iteration 2671, loss = 0.06126639\n",
      "Iteration 2672, loss = 0.06125988\n",
      "Iteration 2673, loss = 0.06125282\n",
      "Iteration 2674, loss = 0.06124503\n",
      "Iteration 2675, loss = 0.06123791\n",
      "Iteration 2676, loss = 0.06123056\n",
      "Iteration 2677, loss = 0.06122373\n",
      "Iteration 2678, loss = 0.06121605\n",
      "Iteration 2679, loss = 0.06120889\n",
      "Iteration 2680, loss = 0.06120193\n",
      "Iteration 2681, loss = 0.06119428\n",
      "Iteration 2682, loss = 0.06118751\n",
      "Iteration 2683, loss = 0.06118057\n",
      "Iteration 2684, loss = 0.06117337\n",
      "Iteration 2685, loss = 0.06116626\n",
      "Iteration 2686, loss = 0.06115898\n",
      "Iteration 2687, loss = 0.06115168\n",
      "Iteration 2688, loss = 0.06114436\n",
      "Iteration 2689, loss = 0.06113686\n",
      "Iteration 2690, loss = 0.06112961\n",
      "Iteration 2691, loss = 0.06112231\n",
      "Iteration 2692, loss = 0.06111509\n",
      "Iteration 2693, loss = 0.06110788\n",
      "Iteration 2694, loss = 0.06110088\n",
      "Iteration 2695, loss = 0.06109379\n",
      "Iteration 2696, loss = 0.06108653\n",
      "Iteration 2697, loss = 0.06107910\n",
      "Iteration 2698, loss = 0.06107150\n",
      "Iteration 2699, loss = 0.06106481\n",
      "Iteration 2700, loss = 0.06105770\n",
      "Iteration 2701, loss = 0.06104999\n",
      "Iteration 2702, loss = 0.06104281\n",
      "Iteration 2703, loss = 0.06103558\n",
      "Iteration 2704, loss = 0.06102843\n",
      "Iteration 2705, loss = 0.06102106\n",
      "Iteration 2706, loss = 0.06101350\n",
      "Iteration 2707, loss = 0.06100611\n",
      "Iteration 2708, loss = 0.06099966\n",
      "Iteration 2709, loss = 0.06099205\n",
      "Iteration 2710, loss = 0.06098453\n",
      "Iteration 2711, loss = 0.06097754\n",
      "Iteration 2712, loss = 0.06097046\n",
      "Iteration 2713, loss = 0.06096315\n",
      "Iteration 2714, loss = 0.06095589\n",
      "Iteration 2715, loss = 0.06094820\n",
      "Iteration 2716, loss = 0.06094081\n",
      "Iteration 2717, loss = 0.06093432\n",
      "Iteration 2718, loss = 0.06092681\n",
      "Iteration 2719, loss = 0.06091909\n",
      "Iteration 2720, loss = 0.06091223\n",
      "Iteration 2721, loss = 0.06090514\n",
      "Iteration 2722, loss = 0.06089819\n",
      "Iteration 2723, loss = 0.06089078\n",
      "Iteration 2724, loss = 0.06088328\n",
      "Iteration 2725, loss = 0.06087569\n",
      "Iteration 2726, loss = 0.06086863\n",
      "Iteration 2727, loss = 0.06086139\n",
      "Iteration 2728, loss = 0.06085423\n",
      "Iteration 2729, loss = 0.06084710\n",
      "Iteration 2730, loss = 0.06083983\n",
      "Iteration 2731, loss = 0.06083306\n",
      "Iteration 2732, loss = 0.06082527\n",
      "Iteration 2733, loss = 0.06081804\n",
      "Iteration 2734, loss = 0.06081061\n",
      "Iteration 2735, loss = 0.06080326\n",
      "Iteration 2736, loss = 0.06079576\n",
      "Iteration 2737, loss = 0.06078873\n",
      "Iteration 2738, loss = 0.06078146\n",
      "Iteration 2739, loss = 0.06077453\n",
      "Iteration 2740, loss = 0.06076755\n",
      "Iteration 2741, loss = 0.06076024\n",
      "Iteration 2742, loss = 0.06075295\n",
      "Iteration 2743, loss = 0.06074551\n",
      "Iteration 2744, loss = 0.06073855\n",
      "Iteration 2745, loss = 0.06073099\n",
      "Iteration 2746, loss = 0.06072384\n",
      "Iteration 2747, loss = 0.06071686\n",
      "Iteration 2748, loss = 0.06070972\n",
      "Iteration 2749, loss = 0.06070253\n",
      "Iteration 2750, loss = 0.06069523\n",
      "Iteration 2751, loss = 0.06068766\n",
      "Iteration 2752, loss = 0.06068028\n",
      "Iteration 2753, loss = 0.06067269\n",
      "Iteration 2754, loss = 0.06066630\n",
      "Iteration 2755, loss = 0.06065901\n",
      "Iteration 2756, loss = 0.06065102\n",
      "Iteration 2757, loss = 0.06064395\n",
      "Iteration 2758, loss = 0.06063680\n",
      "Iteration 2759, loss = 0.06062940\n",
      "Iteration 2760, loss = 0.06062204\n",
      "Iteration 2761, loss = 0.06061460\n",
      "Iteration 2762, loss = 0.06060750\n",
      "Iteration 2763, loss = 0.06059994\n",
      "Iteration 2764, loss = 0.06059309\n",
      "Iteration 2765, loss = 0.06058609\n",
      "Iteration 2766, loss = 0.06057885\n",
      "Iteration 2767, loss = 0.06057179\n",
      "Iteration 2768, loss = 0.06056440\n",
      "Iteration 2769, loss = 0.06055697\n",
      "Iteration 2770, loss = 0.06054963\n",
      "Iteration 2771, loss = 0.06054207\n",
      "Iteration 2772, loss = 0.06053500\n",
      "Iteration 2773, loss = 0.06052763\n",
      "Iteration 2774, loss = 0.06052012\n",
      "Iteration 2775, loss = 0.06051320\n",
      "Iteration 2776, loss = 0.06050599\n",
      "Iteration 2777, loss = 0.06049863\n",
      "Iteration 2778, loss = 0.06049126\n",
      "Iteration 2779, loss = 0.06048396\n",
      "Iteration 2780, loss = 0.06047644\n",
      "Iteration 2781, loss = 0.06047005\n",
      "Iteration 2782, loss = 0.06046267\n",
      "Iteration 2783, loss = 0.06045462\n",
      "Iteration 2784, loss = 0.06044763\n",
      "Iteration 2785, loss = 0.06044046\n",
      "Iteration 2786, loss = 0.06043331\n",
      "Iteration 2787, loss = 0.06042595\n",
      "Iteration 2788, loss = 0.06041843\n",
      "Iteration 2789, loss = 0.06041075\n",
      "Iteration 2790, loss = 0.06040459\n",
      "Iteration 2791, loss = 0.06039732\n",
      "Iteration 2792, loss = 0.06038920\n",
      "Iteration 2793, loss = 0.06038220\n",
      "Iteration 2794, loss = 0.06037484\n",
      "Iteration 2795, loss = 0.06036754\n",
      "Iteration 2796, loss = 0.06036091\n",
      "Iteration 2797, loss = 0.06035317\n",
      "Iteration 2798, loss = 0.06034558\n",
      "Iteration 2799, loss = 0.06033879\n",
      "Iteration 2800, loss = 0.06033133\n",
      "Iteration 2801, loss = 0.06032392\n",
      "Iteration 2802, loss = 0.06031691\n",
      "Iteration 2803, loss = 0.06030986\n",
      "Iteration 2804, loss = 0.06030254\n",
      "Iteration 2805, loss = 0.06029546\n",
      "Iteration 2806, loss = 0.06028797\n",
      "Iteration 2807, loss = 0.06028025\n",
      "Iteration 2808, loss = 0.06027316\n",
      "Iteration 2809, loss = 0.06026548\n",
      "Iteration 2810, loss = 0.06025859\n",
      "Iteration 2811, loss = 0.06025162\n",
      "Iteration 2812, loss = 0.06024460\n",
      "Iteration 2813, loss = 0.06023714\n",
      "Iteration 2814, loss = 0.06022977\n",
      "Iteration 2815, loss = 0.06022246\n",
      "Iteration 2816, loss = 0.06021485\n",
      "Iteration 2817, loss = 0.06020741\n",
      "Iteration 2818, loss = 0.06020114\n",
      "Iteration 2819, loss = 0.06019365\n",
      "Iteration 2820, loss = 0.06018531\n",
      "Iteration 2821, loss = 0.06017808\n",
      "Iteration 2822, loss = 0.06017086\n",
      "Iteration 2823, loss = 0.06016359\n",
      "Iteration 2824, loss = 0.06015619\n",
      "Iteration 2825, loss = 0.06014874\n",
      "Iteration 2826, loss = 0.06014183\n",
      "Iteration 2827, loss = 0.06013408\n",
      "Iteration 2828, loss = 0.06012692\n",
      "Iteration 2829, loss = 0.06011986\n",
      "Iteration 2830, loss = 0.06011291\n",
      "Iteration 2831, loss = 0.06010570\n",
      "Iteration 2832, loss = 0.06009816\n",
      "Iteration 2833, loss = 0.06009084\n",
      "Iteration 2834, loss = 0.06008327\n",
      "Iteration 2835, loss = 0.06007596\n",
      "Iteration 2836, loss = 0.06006888\n",
      "Iteration 2837, loss = 0.06006104\n",
      "Iteration 2838, loss = 0.06005372\n",
      "Iteration 2839, loss = 0.06004679\n",
      "Iteration 2840, loss = 0.06003952\n",
      "Iteration 2841, loss = 0.06003191\n",
      "Iteration 2842, loss = 0.06002443\n",
      "Iteration 2843, loss = 0.06001696\n",
      "Iteration 2844, loss = 0.06000937\n",
      "Iteration 2845, loss = 0.06000319\n",
      "Iteration 2846, loss = 0.05999560\n",
      "Iteration 2847, loss = 0.05998763\n",
      "Iteration 2848, loss = 0.05998061\n",
      "Iteration 2849, loss = 0.05997324\n",
      "Iteration 2850, loss = 0.05996590\n",
      "Iteration 2851, loss = 0.05995850\n",
      "Iteration 2852, loss = 0.05995104\n",
      "Iteration 2853, loss = 0.05994351\n",
      "Iteration 2854, loss = 0.05993716\n",
      "Iteration 2855, loss = 0.05992938\n",
      "Iteration 2856, loss = 0.05992136\n",
      "Iteration 2857, loss = 0.05991427\n",
      "Iteration 2858, loss = 0.05990740\n",
      "Iteration 2859, loss = 0.05989984\n",
      "Iteration 2860, loss = 0.05989224\n",
      "Iteration 2861, loss = 0.05988497\n",
      "Iteration 2862, loss = 0.05987740\n",
      "Iteration 2863, loss = 0.05987037\n",
      "Iteration 2864, loss = 0.05986331\n",
      "Iteration 2865, loss = 0.05985535\n",
      "Iteration 2866, loss = 0.05984843\n",
      "Iteration 2867, loss = 0.05984114\n",
      "Iteration 2868, loss = 0.05983380\n",
      "Iteration 2869, loss = 0.05982650\n",
      "Iteration 2870, loss = 0.05981891\n",
      "Iteration 2871, loss = 0.05981137\n",
      "Iteration 2872, loss = 0.05980364\n",
      "Iteration 2873, loss = 0.05979657\n",
      "Iteration 2874, loss = 0.05978914\n",
      "Iteration 2875, loss = 0.05978196\n",
      "Iteration 2876, loss = 0.05977440\n",
      "Iteration 2877, loss = 0.05976728\n",
      "Iteration 2878, loss = 0.05975988\n",
      "Iteration 2879, loss = 0.05975232\n",
      "Iteration 2880, loss = 0.05974470\n",
      "Iteration 2881, loss = 0.05973742\n",
      "Iteration 2882, loss = 0.05973028\n",
      "Iteration 2883, loss = 0.05972289\n",
      "Iteration 2884, loss = 0.05971533\n",
      "Iteration 2885, loss = 0.05970829\n",
      "Iteration 2886, loss = 0.05970058\n",
      "Iteration 2887, loss = 0.05969322\n",
      "Iteration 2888, loss = 0.05968557\n",
      "Iteration 2889, loss = 0.05967860\n",
      "Iteration 2890, loss = 0.05967109\n",
      "Iteration 2891, loss = 0.05966370\n",
      "Iteration 2892, loss = 0.05965640\n",
      "Iteration 2893, loss = 0.05964892\n",
      "Iteration 2894, loss = 0.05964169\n",
      "Iteration 2895, loss = 0.05963421\n",
      "Iteration 2896, loss = 0.05962680\n",
      "Iteration 2897, loss = 0.05961902\n",
      "Iteration 2898, loss = 0.05961171\n",
      "Iteration 2899, loss = 0.05960428\n",
      "Iteration 2900, loss = 0.05959697\n",
      "Iteration 2901, loss = 0.05958948\n",
      "Iteration 2902, loss = 0.05958189\n",
      "Iteration 2903, loss = 0.05957428\n",
      "Iteration 2904, loss = 0.05956704\n",
      "Iteration 2905, loss = 0.05955967\n",
      "Iteration 2906, loss = 0.05955224\n",
      "Iteration 2907, loss = 0.05954456\n",
      "Iteration 2908, loss = 0.05953782\n",
      "Iteration 2909, loss = 0.05952974\n",
      "Iteration 2910, loss = 0.05952221\n",
      "Iteration 2911, loss = 0.05951481\n",
      "Iteration 2912, loss = 0.05950777\n",
      "Iteration 2913, loss = 0.05950044\n",
      "Iteration 2914, loss = 0.05949316\n",
      "Iteration 2915, loss = 0.05948563\n",
      "Iteration 2916, loss = 0.05947834\n",
      "Iteration 2917, loss = 0.05947042\n",
      "Iteration 2918, loss = 0.05946356\n",
      "Iteration 2919, loss = 0.05945596\n",
      "Iteration 2920, loss = 0.05944822\n",
      "Iteration 2921, loss = 0.05944089\n",
      "Iteration 2922, loss = 0.05943355\n",
      "Iteration 2923, loss = 0.05942631\n",
      "Iteration 2924, loss = 0.05941886\n",
      "Iteration 2925, loss = 0.05941112\n",
      "Iteration 2926, loss = 0.05940335\n",
      "Iteration 2927, loss = 0.05939563\n",
      "Iteration 2928, loss = 0.05938968\n",
      "Iteration 2929, loss = 0.05938202\n",
      "Iteration 2930, loss = 0.05937316\n",
      "Iteration 2931, loss = 0.05936588\n",
      "Iteration 2932, loss = 0.05935855\n",
      "Iteration 2933, loss = 0.05935108\n",
      "Iteration 2934, loss = 0.05934378\n",
      "Iteration 2935, loss = 0.05933577\n",
      "Iteration 2936, loss = 0.05932887\n",
      "Iteration 2937, loss = 0.05932126\n",
      "Iteration 2938, loss = 0.05931401\n",
      "Iteration 2939, loss = 0.05930673\n",
      "Iteration 2940, loss = 0.05929975\n",
      "Iteration 2941, loss = 0.05929250\n",
      "Iteration 2942, loss = 0.05928488\n",
      "Iteration 2943, loss = 0.05927698\n",
      "Iteration 2944, loss = 0.05926948\n",
      "Iteration 2945, loss = 0.05926171\n",
      "Iteration 2946, loss = 0.05925412\n",
      "Iteration 2947, loss = 0.05924644\n",
      "Iteration 2948, loss = 0.05923920\n",
      "Iteration 2949, loss = 0.05923185\n",
      "Iteration 2950, loss = 0.05922460\n",
      "Iteration 2951, loss = 0.05921723\n",
      "Iteration 2952, loss = 0.05920941\n",
      "Iteration 2953, loss = 0.05920174\n",
      "Iteration 2954, loss = 0.05919414\n",
      "Iteration 2955, loss = 0.05918696\n",
      "Iteration 2956, loss = 0.05917896\n",
      "Iteration 2957, loss = 0.05917160\n",
      "Iteration 2958, loss = 0.05916451\n",
      "Iteration 2959, loss = 0.05915708\n",
      "Iteration 2960, loss = 0.05914946\n",
      "Iteration 2961, loss = 0.05914213\n",
      "Iteration 2962, loss = 0.05913454\n",
      "Iteration 2963, loss = 0.05912662\n",
      "Iteration 2964, loss = 0.05911881\n",
      "Iteration 2965, loss = 0.05911083\n",
      "Iteration 2966, loss = 0.05910352\n",
      "Iteration 2967, loss = 0.05909644\n",
      "Iteration 2968, loss = 0.05908930\n",
      "Iteration 2969, loss = 0.05908172\n",
      "Iteration 2970, loss = 0.05907375\n",
      "Iteration 2971, loss = 0.05906604\n",
      "Iteration 2972, loss = 0.05905811\n",
      "Iteration 2973, loss = 0.05905079\n",
      "Iteration 2974, loss = 0.05904288\n",
      "Iteration 2975, loss = 0.05903579\n",
      "Iteration 2976, loss = 0.05902871\n",
      "Iteration 2977, loss = 0.05902114\n",
      "Iteration 2978, loss = 0.05901371\n",
      "Iteration 2979, loss = 0.05900612\n",
      "Iteration 2980, loss = 0.05899823\n",
      "Iteration 2981, loss = 0.05899032\n",
      "Iteration 2982, loss = 0.05898254\n",
      "Iteration 2983, loss = 0.05897565\n",
      "Iteration 2984, loss = 0.05896805\n",
      "Iteration 2985, loss = 0.05895963\n",
      "Iteration 2986, loss = 0.05895215\n",
      "Iteration 2987, loss = 0.05894487\n",
      "Iteration 2988, loss = 0.05893750\n",
      "Iteration 2989, loss = 0.05892964\n",
      "Iteration 2990, loss = 0.05892157\n",
      "Iteration 2991, loss = 0.05891392\n",
      "Iteration 2992, loss = 0.05890639\n",
      "Iteration 2993, loss = 0.05889909\n",
      "Iteration 2994, loss = 0.05889098\n",
      "Iteration 2995, loss = 0.05888417\n",
      "Iteration 2996, loss = 0.05887612\n",
      "Iteration 2997, loss = 0.05886866\n",
      "Iteration 2998, loss = 0.05886080\n",
      "Iteration 2999, loss = 0.05885298\n",
      "Iteration 3000, loss = 0.05884563\n",
      "Iteration 3001, loss = 0.05883829\n",
      "Iteration 3002, loss = 0.05883091\n",
      "Iteration 3003, loss = 0.05882312\n",
      "Iteration 3004, loss = 0.05881491\n",
      "Iteration 3005, loss = 0.05880854\n",
      "Iteration 3006, loss = 0.05880051\n",
      "Iteration 3007, loss = 0.05879310\n",
      "Iteration 3008, loss = 0.05878586\n",
      "Iteration 3009, loss = 0.05877834\n",
      "Iteration 3010, loss = 0.05877014\n",
      "Iteration 3011, loss = 0.05876286\n",
      "Iteration 3012, loss = 0.05875534\n",
      "Iteration 3013, loss = 0.05874745\n",
      "Iteration 3014, loss = 0.05873978\n",
      "Iteration 3015, loss = 0.05873269\n",
      "Iteration 3016, loss = 0.05872481\n",
      "Iteration 3017, loss = 0.05871662\n",
      "Iteration 3018, loss = 0.05870886\n",
      "Iteration 3019, loss = 0.05870166\n",
      "Iteration 3020, loss = 0.05869413\n",
      "Iteration 3021, loss = 0.05868588\n",
      "Iteration 3022, loss = 0.05867830\n",
      "Iteration 3023, loss = 0.05867039\n",
      "Iteration 3024, loss = 0.05866380\n",
      "Iteration 3025, loss = 0.05865597\n",
      "Iteration 3026, loss = 0.05864793\n",
      "Iteration 3027, loss = 0.05864028\n",
      "Iteration 3028, loss = 0.05863265\n",
      "Iteration 3029, loss = 0.05862469\n",
      "Iteration 3030, loss = 0.05861785\n",
      "Iteration 3031, loss = 0.05861023\n",
      "Iteration 3032, loss = 0.05860171\n",
      "Iteration 3033, loss = 0.05859343\n",
      "Iteration 3034, loss = 0.05858578\n",
      "Iteration 3035, loss = 0.05857826\n",
      "Iteration 3036, loss = 0.05857074\n",
      "Iteration 3037, loss = 0.05856322\n",
      "Iteration 3038, loss = 0.05855535\n",
      "Iteration 3039, loss = 0.05854738\n",
      "Iteration 3040, loss = 0.05854023\n",
      "Iteration 3041, loss = 0.05853226\n",
      "Iteration 3042, loss = 0.05852450\n",
      "Iteration 3043, loss = 0.05851639\n",
      "Iteration 3044, loss = 0.05850912\n",
      "Iteration 3045, loss = 0.05850225\n",
      "Iteration 3046, loss = 0.05849494\n",
      "Iteration 3047, loss = 0.05848716\n",
      "Iteration 3048, loss = 0.05847897\n",
      "Iteration 3049, loss = 0.05847094\n",
      "Iteration 3050, loss = 0.05846280\n",
      "Iteration 3051, loss = 0.05845519\n",
      "Iteration 3052, loss = 0.05844815\n",
      "Iteration 3053, loss = 0.05843987\n",
      "Iteration 3054, loss = 0.05843151\n",
      "Iteration 3055, loss = 0.05842430\n",
      "Iteration 3056, loss = 0.05841659\n",
      "Iteration 3057, loss = 0.05840906\n",
      "Iteration 3058, loss = 0.05840125\n",
      "Iteration 3059, loss = 0.05839300\n",
      "Iteration 3060, loss = 0.05838443\n",
      "Iteration 3061, loss = 0.05837700\n",
      "Iteration 3062, loss = 0.05836949\n",
      "Iteration 3063, loss = 0.05836120\n",
      "Iteration 3064, loss = 0.05835403\n",
      "Iteration 3065, loss = 0.05834559\n",
      "Iteration 3066, loss = 0.05833797\n",
      "Iteration 3067, loss = 0.05833043\n",
      "Iteration 3068, loss = 0.05832233\n",
      "Iteration 3069, loss = 0.05831477\n",
      "Iteration 3070, loss = 0.05830753\n",
      "Iteration 3071, loss = 0.05829977\n",
      "Iteration 3072, loss = 0.05829153\n",
      "Iteration 3073, loss = 0.05828333\n",
      "Iteration 3074, loss = 0.05827656\n",
      "Iteration 3075, loss = 0.05826846\n",
      "Iteration 3076, loss = 0.05826041\n",
      "Iteration 3077, loss = 0.05825287\n",
      "Iteration 3078, loss = 0.05824557\n",
      "Iteration 3079, loss = 0.05823788\n",
      "Iteration 3080, loss = 0.05823025\n",
      "Iteration 3081, loss = 0.05822282\n",
      "Iteration 3082, loss = 0.05821529\n",
      "Iteration 3083, loss = 0.05820723\n",
      "Iteration 3084, loss = 0.05819993\n",
      "Iteration 3085, loss = 0.05819238\n",
      "Iteration 3086, loss = 0.05818464\n",
      "Iteration 3087, loss = 0.05817714\n",
      "Iteration 3088, loss = 0.05816938\n",
      "Iteration 3089, loss = 0.05816202\n",
      "Iteration 3090, loss = 0.05815459\n",
      "Iteration 3091, loss = 0.05814678\n",
      "Iteration 3092, loss = 0.05813869\n",
      "Iteration 3093, loss = 0.05813185\n",
      "Iteration 3094, loss = 0.05812405\n",
      "Iteration 3095, loss = 0.05811579\n",
      "Iteration 3096, loss = 0.05810856\n",
      "Iteration 3097, loss = 0.05810164\n",
      "Iteration 3098, loss = 0.05809405\n",
      "Iteration 3099, loss = 0.05808637\n",
      "Iteration 3100, loss = 0.05807825\n",
      "Iteration 3101, loss = 0.05806964\n",
      "Iteration 3102, loss = 0.05806395\n",
      "Iteration 3103, loss = 0.05805692\n",
      "Iteration 3104, loss = 0.05804808\n",
      "Iteration 3105, loss = 0.05804040\n",
      "Iteration 3106, loss = 0.05803274\n",
      "Iteration 3107, loss = 0.05802496\n",
      "Iteration 3108, loss = 0.05801731\n",
      "Iteration 3109, loss = 0.05800909\n",
      "Iteration 3110, loss = 0.05800220\n",
      "Iteration 3111, loss = 0.05799388\n",
      "Iteration 3112, loss = 0.05798669\n",
      "Iteration 3113, loss = 0.05797979\n",
      "Iteration 3114, loss = 0.05797238\n",
      "Iteration 3115, loss = 0.05796446\n",
      "Iteration 3116, loss = 0.05795605\n",
      "Iteration 3117, loss = 0.05794841\n",
      "Iteration 3118, loss = 0.05794055\n",
      "Iteration 3119, loss = 0.05793411\n",
      "Iteration 3120, loss = 0.05792633\n",
      "Iteration 3121, loss = 0.05791724\n",
      "Iteration 3122, loss = 0.05790932\n",
      "Iteration 3123, loss = 0.05790216\n",
      "Iteration 3124, loss = 0.05789503\n",
      "Iteration 3125, loss = 0.05788695\n",
      "Iteration 3126, loss = 0.05788042\n",
      "Iteration 3127, loss = 0.05787222\n",
      "Iteration 3128, loss = 0.05786357\n",
      "Iteration 3129, loss = 0.05785685\n",
      "Iteration 3130, loss = 0.05784964\n",
      "Iteration 3131, loss = 0.05784181\n",
      "Iteration 3132, loss = 0.05783418\n",
      "Iteration 3133, loss = 0.05782584\n",
      "Iteration 3134, loss = 0.05781766\n",
      "Iteration 3135, loss = 0.05780943\n",
      "Iteration 3136, loss = 0.05780194\n",
      "Iteration 3137, loss = 0.05779462\n",
      "Iteration 3138, loss = 0.05778697\n",
      "Iteration 3139, loss = 0.05777883\n",
      "Iteration 3140, loss = 0.05777102\n",
      "Iteration 3141, loss = 0.05776296\n",
      "Iteration 3142, loss = 0.05775653\n",
      "Iteration 3143, loss = 0.05774883\n",
      "Iteration 3144, loss = 0.05773988\n",
      "Iteration 3145, loss = 0.05773224\n",
      "Iteration 3146, loss = 0.05772504\n",
      "Iteration 3147, loss = 0.05771717\n",
      "Iteration 3148, loss = 0.05770891\n",
      "Iteration 3149, loss = 0.05770213\n",
      "Iteration 3150, loss = 0.05769378\n",
      "Iteration 3151, loss = 0.05768652\n",
      "Iteration 3152, loss = 0.05767922\n",
      "Iteration 3153, loss = 0.05767195\n",
      "Iteration 3154, loss = 0.05766381\n",
      "Iteration 3155, loss = 0.05765575\n",
      "Iteration 3156, loss = 0.05764768\n",
      "Iteration 3157, loss = 0.05763960\n",
      "Iteration 3158, loss = 0.05763181\n",
      "Iteration 3159, loss = 0.05762405\n",
      "Iteration 3160, loss = 0.05761680\n",
      "Iteration 3161, loss = 0.05760949\n",
      "Iteration 3162, loss = 0.05760185\n",
      "Iteration 3163, loss = 0.05759396\n",
      "Iteration 3164, loss = 0.05758591\n",
      "Iteration 3165, loss = 0.05757847\n",
      "Iteration 3166, loss = 0.05757067\n",
      "Iteration 3167, loss = 0.05756242\n",
      "Iteration 3168, loss = 0.05755664\n",
      "Iteration 3169, loss = 0.05754825\n",
      "Iteration 3170, loss = 0.05754013\n",
      "Iteration 3171, loss = 0.05753323\n",
      "Iteration 3172, loss = 0.05752624\n",
      "Iteration 3173, loss = 0.05751880\n",
      "Iteration 3174, loss = 0.05751080\n",
      "Iteration 3175, loss = 0.05750258\n",
      "Iteration 3176, loss = 0.05749425\n",
      "Iteration 3177, loss = 0.05748639\n",
      "Iteration 3178, loss = 0.05747836\n",
      "Iteration 3179, loss = 0.05746979\n",
      "Iteration 3180, loss = 0.05746281\n",
      "Iteration 3181, loss = 0.05745515\n",
      "Iteration 3182, loss = 0.05744717\n",
      "Iteration 3183, loss = 0.05743904\n",
      "Iteration 3184, loss = 0.05743178\n",
      "Iteration 3185, loss = 0.05742394\n",
      "Iteration 3186, loss = 0.05741645\n",
      "Iteration 3187, loss = 0.05740914\n",
      "Iteration 3188, loss = 0.05740171\n",
      "Iteration 3189, loss = 0.05739373\n",
      "Iteration 3190, loss = 0.05738590\n",
      "Iteration 3191, loss = 0.05737770\n",
      "Iteration 3192, loss = 0.05736922\n",
      "Iteration 3193, loss = 0.05736164\n",
      "Iteration 3194, loss = 0.05735505\n",
      "Iteration 3195, loss = 0.05734726\n",
      "Iteration 3196, loss = 0.05733998\n",
      "Iteration 3197, loss = 0.05733237\n",
      "Iteration 3198, loss = 0.05732420\n",
      "Iteration 3199, loss = 0.05731592\n",
      "Iteration 3200, loss = 0.05730897\n",
      "Iteration 3201, loss = 0.05730082\n",
      "Iteration 3202, loss = 0.05729191\n",
      "Iteration 3203, loss = 0.05728468\n",
      "Iteration 3204, loss = 0.05727694\n",
      "Iteration 3205, loss = 0.05727022\n",
      "Iteration 3206, loss = 0.05726220\n",
      "Iteration 3207, loss = 0.05725305\n",
      "Iteration 3208, loss = 0.05724466\n",
      "Iteration 3209, loss = 0.05723747\n",
      "Iteration 3210, loss = 0.05723017\n",
      "Iteration 3211, loss = 0.05722169\n",
      "Iteration 3212, loss = 0.05721366\n",
      "Iteration 3213, loss = 0.05720580\n",
      "Iteration 3214, loss = 0.05719819\n",
      "Iteration 3215, loss = 0.05719024\n",
      "Iteration 3216, loss = 0.05718244\n",
      "Iteration 3217, loss = 0.05717480\n",
      "Iteration 3218, loss = 0.05716681\n",
      "Iteration 3219, loss = 0.05715908\n",
      "Iteration 3220, loss = 0.05715120\n",
      "Iteration 3221, loss = 0.05714346\n",
      "Iteration 3222, loss = 0.05713565\n",
      "Iteration 3223, loss = 0.05712775\n",
      "Iteration 3224, loss = 0.05712028\n",
      "Iteration 3225, loss = 0.05711222\n",
      "Iteration 3226, loss = 0.05710496\n",
      "Iteration 3227, loss = 0.05709666\n",
      "Iteration 3228, loss = 0.05708963\n",
      "Iteration 3229, loss = 0.05708240\n",
      "Iteration 3230, loss = 0.05707538\n",
      "Iteration 3231, loss = 0.05706693\n",
      "Iteration 3232, loss = 0.05705862\n",
      "Iteration 3233, loss = 0.05705065\n",
      "Iteration 3234, loss = 0.05704196\n",
      "Iteration 3235, loss = 0.05703346\n",
      "Iteration 3236, loss = 0.05702785\n",
      "Iteration 3237, loss = 0.05702072\n",
      "Iteration 3238, loss = 0.05701162\n",
      "Iteration 3239, loss = 0.05700340\n",
      "Iteration 3240, loss = 0.05699601\n",
      "Iteration 3241, loss = 0.05698839\n",
      "Iteration 3242, loss = 0.05698066\n",
      "Iteration 3243, loss = 0.05697270\n",
      "Iteration 3244, loss = 0.05696465\n",
      "Iteration 3245, loss = 0.05695625\n",
      "Iteration 3246, loss = 0.05694781\n",
      "Iteration 3247, loss = 0.05693932\n",
      "Iteration 3248, loss = 0.05693447\n",
      "Iteration 3249, loss = 0.05692702\n",
      "Iteration 3250, loss = 0.05691827\n",
      "Iteration 3251, loss = 0.05690938\n",
      "Iteration 3252, loss = 0.05690173\n",
      "Iteration 3253, loss = 0.05689494\n",
      "Iteration 3254, loss = 0.05688777\n",
      "Iteration 3255, loss = 0.05687992\n",
      "Iteration 3256, loss = 0.05687162\n",
      "Iteration 3257, loss = 0.05686380\n",
      "Iteration 3258, loss = 0.05685528\n",
      "Iteration 3259, loss = 0.05684860\n",
      "Iteration 3260, loss = 0.05684131\n",
      "Iteration 3261, loss = 0.05683194\n",
      "Iteration 3262, loss = 0.05682525\n",
      "Iteration 3263, loss = 0.05681816\n",
      "Iteration 3264, loss = 0.05681081\n",
      "Iteration 3265, loss = 0.05680347\n",
      "Iteration 3266, loss = 0.05679611\n",
      "Iteration 3267, loss = 0.05678813\n",
      "Iteration 3268, loss = 0.05677998\n",
      "Iteration 3269, loss = 0.05677149\n",
      "Iteration 3270, loss = 0.05676323\n",
      "Iteration 3271, loss = 0.05675601\n",
      "Iteration 3272, loss = 0.05674942\n",
      "Iteration 3273, loss = 0.05674046\n",
      "Iteration 3274, loss = 0.05673279\n",
      "Iteration 3275, loss = 0.05672631\n",
      "Iteration 3276, loss = 0.05671922\n",
      "Iteration 3277, loss = 0.05671174\n",
      "Iteration 3278, loss = 0.05670414\n",
      "Iteration 3279, loss = 0.05669591\n",
      "Iteration 3280, loss = 0.05668752\n",
      "Iteration 3281, loss = 0.05667927\n",
      "Iteration 3282, loss = 0.05667026\n",
      "Iteration 3283, loss = 0.05666237\n",
      "Iteration 3284, loss = 0.05665639\n",
      "Iteration 3285, loss = 0.05664930\n",
      "Iteration 3286, loss = 0.05664128\n",
      "Iteration 3287, loss = 0.05663206\n",
      "Iteration 3288, loss = 0.05662428\n",
      "Iteration 3289, loss = 0.05661850\n",
      "Iteration 3290, loss = 0.05661108\n",
      "Iteration 3291, loss = 0.05660290\n",
      "Iteration 3292, loss = 0.05659624\n",
      "Iteration 3293, loss = 0.05658895\n",
      "Iteration 3294, loss = 0.05658098\n",
      "Iteration 3295, loss = 0.05657229\n",
      "Iteration 3296, loss = 0.05656415\n",
      "Iteration 3297, loss = 0.05655699\n",
      "Iteration 3298, loss = 0.05654826\n",
      "Iteration 3299, loss = 0.05654102\n",
      "Iteration 3300, loss = 0.05653457\n",
      "Iteration 3301, loss = 0.05652697\n",
      "Iteration 3302, loss = 0.05651822\n",
      "Iteration 3303, loss = 0.05650960\n",
      "Iteration 3304, loss = 0.05650238\n",
      "Iteration 3305, loss = 0.05649689\n",
      "Iteration 3306, loss = 0.05648930\n",
      "Iteration 3307, loss = 0.05647993\n",
      "Iteration 3308, loss = 0.05647263\n",
      "Iteration 3309, loss = 0.05646448\n",
      "Iteration 3310, loss = 0.05645645\n",
      "Iteration 3311, loss = 0.05644822\n",
      "Iteration 3312, loss = 0.05644148\n",
      "Iteration 3313, loss = 0.05643351\n",
      "Iteration 3314, loss = 0.05642528\n",
      "Iteration 3315, loss = 0.05641820\n",
      "Iteration 3316, loss = 0.05641104\n",
      "Iteration 3317, loss = 0.05640365\n",
      "Iteration 3318, loss = 0.05639555\n",
      "Iteration 3319, loss = 0.05638753\n",
      "Iteration 3320, loss = 0.05637904\n",
      "Iteration 3321, loss = 0.05637359\n",
      "Iteration 3322, loss = 0.05636626\n",
      "Iteration 3323, loss = 0.05635671\n",
      "Iteration 3324, loss = 0.05634928\n",
      "Iteration 3325, loss = 0.05634315\n",
      "Iteration 3326, loss = 0.05633630\n",
      "Iteration 3327, loss = 0.05632873\n",
      "Iteration 3328, loss = 0.05632107\n",
      "Iteration 3329, loss = 0.05631375\n",
      "Iteration 3330, loss = 0.05630587\n",
      "Iteration 3331, loss = 0.05629758\n",
      "Iteration 3332, loss = 0.05628860\n",
      "Iteration 3333, loss = 0.05628138\n",
      "Iteration 3334, loss = 0.05627462\n",
      "Iteration 3335, loss = 0.05626614\n",
      "Iteration 3336, loss = 0.05625915\n",
      "Iteration 3337, loss = 0.05625178\n",
      "Iteration 3338, loss = 0.05624517\n",
      "Iteration 3339, loss = 0.05623824\n",
      "Iteration 3340, loss = 0.05623059\n",
      "Iteration 3341, loss = 0.05622275\n",
      "Iteration 3342, loss = 0.05621467\n",
      "Iteration 3343, loss = 0.05620590\n",
      "Iteration 3344, loss = 0.05619791\n",
      "Iteration 3345, loss = 0.05619092\n",
      "Iteration 3346, loss = 0.05618326\n",
      "Iteration 3347, loss = 0.05617627\n",
      "Iteration 3348, loss = 0.05616853\n",
      "Iteration 3349, loss = 0.05616025\n",
      "Iteration 3350, loss = 0.05615230\n",
      "Iteration 3351, loss = 0.05614599\n",
      "Iteration 3352, loss = 0.05613769\n",
      "Iteration 3353, loss = 0.05612966\n",
      "Iteration 3354, loss = 0.05612289\n",
      "Iteration 3355, loss = 0.05611533\n",
      "Iteration 3356, loss = 0.05610725\n",
      "Iteration 3357, loss = 0.05609959\n",
      "Iteration 3358, loss = 0.05609140\n",
      "Iteration 3359, loss = 0.05608508\n",
      "Iteration 3360, loss = 0.05607796\n",
      "Iteration 3361, loss = 0.05606901\n",
      "Iteration 3362, loss = 0.05606152\n",
      "Iteration 3363, loss = 0.05605379\n",
      "Iteration 3364, loss = 0.05604689\n",
      "Iteration 3365, loss = 0.05603863\n",
      "Iteration 3366, loss = 0.05603194\n",
      "Iteration 3367, loss = 0.05602420\n",
      "Iteration 3368, loss = 0.05601694\n",
      "Iteration 3369, loss = 0.05600964\n",
      "Iteration 3370, loss = 0.05600211\n",
      "Iteration 3371, loss = 0.05599432\n",
      "Iteration 3372, loss = 0.05598609\n",
      "Iteration 3373, loss = 0.05597845\n",
      "Iteration 3374, loss = 0.05597123\n",
      "Iteration 3375, loss = 0.05596343\n",
      "Iteration 3376, loss = 0.05595603\n",
      "Iteration 3377, loss = 0.05594901\n",
      "Iteration 3378, loss = 0.05594142\n",
      "Iteration 3379, loss = 0.05593360\n",
      "Iteration 3380, loss = 0.05592596\n",
      "Iteration 3381, loss = 0.05591861\n",
      "Iteration 3382, loss = 0.05591256\n",
      "Iteration 3383, loss = 0.05590484\n",
      "Iteration 3384, loss = 0.05589496\n",
      "Iteration 3385, loss = 0.05588846\n",
      "Iteration 3386, loss = 0.05588163\n",
      "Iteration 3387, loss = 0.05587365\n",
      "Iteration 3388, loss = 0.05586513\n",
      "Iteration 3389, loss = 0.05585947\n",
      "Iteration 3390, loss = 0.05585179\n",
      "Iteration 3391, loss = 0.05584398\n",
      "Iteration 3392, loss = 0.05583694\n",
      "Iteration 3393, loss = 0.05582899\n",
      "Iteration 3394, loss = 0.05582179\n",
      "Iteration 3395, loss = 0.05581457\n",
      "Iteration 3396, loss = 0.05580593\n",
      "Iteration 3397, loss = 0.05579896\n",
      "Iteration 3398, loss = 0.05579122\n",
      "Iteration 3399, loss = 0.05578408\n",
      "Iteration 3400, loss = 0.05577740\n",
      "Iteration 3401, loss = 0.05576987\n",
      "Iteration 3402, loss = 0.05576217\n",
      "Iteration 3403, loss = 0.05575348\n",
      "Iteration 3404, loss = 0.05574394\n",
      "Iteration 3405, loss = 0.05573913\n",
      "Iteration 3406, loss = 0.05573242\n",
      "Iteration 3407, loss = 0.05572241\n",
      "Iteration 3408, loss = 0.05571459\n",
      "Iteration 3409, loss = 0.05570662\n",
      "Iteration 3410, loss = 0.05569909\n",
      "Iteration 3411, loss = 0.05569216\n",
      "Iteration 3412, loss = 0.05568615\n",
      "Iteration 3413, loss = 0.05567745\n",
      "Iteration 3414, loss = 0.05566886\n",
      "Iteration 3415, loss = 0.05566223\n",
      "Iteration 3416, loss = 0.05565451\n",
      "Iteration 3417, loss = 0.05564673\n",
      "Iteration 3418, loss = 0.05563876\n",
      "Iteration 3419, loss = 0.05563175\n",
      "Iteration 3420, loss = 0.05562503\n",
      "Iteration 3421, loss = 0.05561627\n",
      "Iteration 3422, loss = 0.05560850\n",
      "Iteration 3423, loss = 0.05560207\n",
      "Iteration 3424, loss = 0.05559515\n",
      "Iteration 3425, loss = 0.05558731\n",
      "Iteration 3426, loss = 0.05557968\n",
      "Iteration 3427, loss = 0.05557167\n",
      "Iteration 3428, loss = 0.05556318\n",
      "Iteration 3429, loss = 0.05555502\n",
      "Iteration 3430, loss = 0.05554767\n",
      "Iteration 3431, loss = 0.05554131\n",
      "Iteration 3432, loss = 0.05553422\n",
      "Iteration 3433, loss = 0.05552572\n",
      "Iteration 3434, loss = 0.05551745\n",
      "Iteration 3435, loss = 0.05550999\n",
      "Iteration 3436, loss = 0.05550296\n",
      "Iteration 3437, loss = 0.05549504\n",
      "Iteration 3438, loss = 0.05548804\n",
      "Iteration 3439, loss = 0.05547976\n",
      "Iteration 3440, loss = 0.05547360\n",
      "Iteration 3441, loss = 0.05546586\n",
      "Iteration 3442, loss = 0.05545912\n",
      "Iteration 3443, loss = 0.05545222\n",
      "Iteration 3444, loss = 0.05544480\n",
      "Iteration 3445, loss = 0.05543644\n",
      "Iteration 3446, loss = 0.05542751\n",
      "Iteration 3447, loss = 0.05542009\n",
      "Iteration 3448, loss = 0.05541258\n",
      "Iteration 3449, loss = 0.05540472\n",
      "Iteration 3450, loss = 0.05539712\n",
      "Iteration 3451, loss = 0.05539060\n",
      "Iteration 3452, loss = 0.05538281\n",
      "Iteration 3453, loss = 0.05537376\n",
      "Iteration 3454, loss = 0.05536644\n",
      "Iteration 3455, loss = 0.05535884\n",
      "Iteration 3456, loss = 0.05535114\n",
      "Iteration 3457, loss = 0.05534454\n",
      "Iteration 3458, loss = 0.05533754\n",
      "Iteration 3459, loss = 0.05532972\n",
      "Iteration 3460, loss = 0.05532140\n",
      "Iteration 3461, loss = 0.05531290\n",
      "Iteration 3462, loss = 0.05530737\n",
      "Iteration 3463, loss = 0.05530066\n",
      "Iteration 3464, loss = 0.05529093\n",
      "Iteration 3465, loss = 0.05528487\n",
      "Iteration 3466, loss = 0.05527854\n",
      "Iteration 3467, loss = 0.05527124\n",
      "Iteration 3468, loss = 0.05526366\n",
      "Iteration 3469, loss = 0.05525591\n",
      "Iteration 3470, loss = 0.05524763\n",
      "Iteration 3471, loss = 0.05523933\n",
      "Iteration 3472, loss = 0.05523129\n",
      "Iteration 3473, loss = 0.05522243\n",
      "Iteration 3474, loss = 0.05521759\n",
      "Iteration 3475, loss = 0.05521068\n",
      "Iteration 3476, loss = 0.05520203\n",
      "Iteration 3477, loss = 0.05519236\n",
      "Iteration 3478, loss = 0.05518459\n",
      "Iteration 3479, loss = 0.05517781\n",
      "Iteration 3480, loss = 0.05517050\n",
      "Iteration 3481, loss = 0.05516220\n",
      "Iteration 3482, loss = 0.05515369\n",
      "Iteration 3483, loss = 0.05514606\n",
      "Iteration 3484, loss = 0.05513836\n",
      "Iteration 3485, loss = 0.05513116\n",
      "Iteration 3486, loss = 0.05512314\n",
      "Iteration 3487, loss = 0.05511656\n",
      "Iteration 3488, loss = 0.05510882\n",
      "Iteration 3489, loss = 0.05510010\n",
      "Iteration 3490, loss = 0.05509332\n",
      "Iteration 3491, loss = 0.05508579\n",
      "Iteration 3492, loss = 0.05507813\n",
      "Iteration 3493, loss = 0.05507204\n",
      "Iteration 3494, loss = 0.05506327\n",
      "Iteration 3495, loss = 0.05505529\n",
      "Iteration 3496, loss = 0.05504855\n",
      "Iteration 3497, loss = 0.05504066\n",
      "Iteration 3498, loss = 0.05503410\n",
      "Iteration 3499, loss = 0.05502634\n",
      "Iteration 3500, loss = 0.05501637\n",
      "Iteration 3501, loss = 0.05500967\n",
      "Iteration 3502, loss = 0.05500309\n",
      "Iteration 3503, loss = 0.05499585\n",
      "Iteration 3504, loss = 0.05498851\n",
      "Iteration 3505, loss = 0.05498048\n",
      "Iteration 3506, loss = 0.05497248\n",
      "Iteration 3507, loss = 0.05496365\n",
      "Iteration 3508, loss = 0.05495715\n",
      "Iteration 3509, loss = 0.05494861\n",
      "Iteration 3510, loss = 0.05494063\n",
      "Iteration 3511, loss = 0.05493327\n",
      "Iteration 3512, loss = 0.05492528\n",
      "Iteration 3513, loss = 0.05491718\n",
      "Iteration 3514, loss = 0.05490885\n",
      "Iteration 3515, loss = 0.05490482\n",
      "Iteration 3516, loss = 0.05489645\n",
      "Iteration 3517, loss = 0.05488686\n",
      "Iteration 3518, loss = 0.05488008\n",
      "Iteration 3519, loss = 0.05487248\n",
      "Iteration 3520, loss = 0.05486484\n",
      "Iteration 3521, loss = 0.05485608\n",
      "Iteration 3522, loss = 0.05484988\n",
      "Iteration 3523, loss = 0.05484343\n",
      "Iteration 3524, loss = 0.05483498\n",
      "Iteration 3525, loss = 0.05482625\n",
      "Iteration 3526, loss = 0.05482010\n",
      "Iteration 3527, loss = 0.05481304\n",
      "Iteration 3528, loss = 0.05480475\n",
      "Iteration 3529, loss = 0.05479850\n",
      "Iteration 3530, loss = 0.05479085\n",
      "Iteration 3531, loss = 0.05478290\n",
      "Iteration 3532, loss = 0.05477554\n",
      "Iteration 3533, loss = 0.05476701\n",
      "Iteration 3534, loss = 0.05475812\n",
      "Iteration 3535, loss = 0.05475245\n",
      "Iteration 3536, loss = 0.05474493\n",
      "Iteration 3537, loss = 0.05473659\n",
      "Iteration 3538, loss = 0.05472814\n",
      "Iteration 3539, loss = 0.05472155\n",
      "Iteration 3540, loss = 0.05471423\n",
      "Iteration 3541, loss = 0.05470600\n",
      "Iteration 3542, loss = 0.05469800\n",
      "Iteration 3543, loss = 0.05468959\n",
      "Iteration 3544, loss = 0.05468461\n",
      "Iteration 3545, loss = 0.05467677\n",
      "Iteration 3546, loss = 0.05466699\n",
      "Iteration 3547, loss = 0.05466046\n",
      "Iteration 3548, loss = 0.05465386\n",
      "Iteration 3549, loss = 0.05464652\n",
      "Iteration 3550, loss = 0.05463978\n",
      "Iteration 3551, loss = 0.05463280\n",
      "Iteration 3552, loss = 0.05462400\n",
      "Iteration 3553, loss = 0.05461466\n",
      "Iteration 3554, loss = 0.05460657\n",
      "Iteration 3555, loss = 0.05459963\n",
      "Iteration 3556, loss = 0.05459236\n",
      "Iteration 3557, loss = 0.05458299\n",
      "Iteration 3558, loss = 0.05457613\n",
      "Iteration 3559, loss = 0.05456920\n",
      "Iteration 3560, loss = 0.05456274\n",
      "Iteration 3561, loss = 0.05455522\n",
      "Iteration 3562, loss = 0.05454762\n",
      "Iteration 3563, loss = 0.05453905\n",
      "Iteration 3564, loss = 0.05453022\n",
      "Iteration 3565, loss = 0.05452249\n",
      "Iteration 3566, loss = 0.05451761\n",
      "Iteration 3567, loss = 0.05451037\n",
      "Iteration 3568, loss = 0.05450227\n",
      "Iteration 3569, loss = 0.05449287\n",
      "Iteration 3570, loss = 0.05448555\n",
      "Iteration 3571, loss = 0.05447851\n",
      "Iteration 3572, loss = 0.05447080\n",
      "Iteration 3573, loss = 0.05446378\n",
      "Iteration 3574, loss = 0.05445554\n",
      "Iteration 3575, loss = 0.05444555\n",
      "Iteration 3576, loss = 0.05444003\n",
      "Iteration 3577, loss = 0.05443311\n",
      "Iteration 3578, loss = 0.05442293\n",
      "Iteration 3579, loss = 0.05441586\n",
      "Iteration 3580, loss = 0.05440945\n",
      "Iteration 3581, loss = 0.05440238\n",
      "Iteration 3582, loss = 0.05439522\n",
      "Iteration 3583, loss = 0.05438776\n",
      "Iteration 3584, loss = 0.05437939\n",
      "Iteration 3585, loss = 0.05437077\n",
      "Iteration 3586, loss = 0.05436313\n",
      "Iteration 3587, loss = 0.05435590\n",
      "Iteration 3588, loss = 0.05434862\n",
      "Iteration 3589, loss = 0.05433977\n",
      "Iteration 3590, loss = 0.05433242\n",
      "Iteration 3591, loss = 0.05432566\n",
      "Iteration 3592, loss = 0.05431859\n",
      "Iteration 3593, loss = 0.05431076\n",
      "Iteration 3594, loss = 0.05430319\n",
      "Iteration 3595, loss = 0.05429492\n",
      "Iteration 3596, loss = 0.05428599\n",
      "Iteration 3597, loss = 0.05427862\n",
      "Iteration 3598, loss = 0.05427069\n",
      "Iteration 3599, loss = 0.05426300\n",
      "Iteration 3600, loss = 0.05425614\n",
      "Iteration 3601, loss = 0.05424860\n",
      "Iteration 3602, loss = 0.05424071\n",
      "Iteration 3603, loss = 0.05423281\n",
      "Iteration 3604, loss = 0.05422399\n",
      "Iteration 3605, loss = 0.05421696\n",
      "Iteration 3606, loss = 0.05421041\n",
      "Iteration 3607, loss = 0.05420111\n",
      "Iteration 3608, loss = 0.05419480\n",
      "Iteration 3609, loss = 0.05418783\n",
      "Iteration 3610, loss = 0.05418035\n",
      "Iteration 3611, loss = 0.05417279\n",
      "Iteration 3612, loss = 0.05416524\n",
      "Iteration 3613, loss = 0.05415691\n",
      "Iteration 3614, loss = 0.05415000\n",
      "Iteration 3615, loss = 0.05414259\n",
      "Iteration 3616, loss = 0.05413441\n",
      "Iteration 3617, loss = 0.05412688\n",
      "Iteration 3618, loss = 0.05411925\n",
      "Iteration 3619, loss = 0.05411234\n",
      "Iteration 3620, loss = 0.05410356\n",
      "Iteration 3621, loss = 0.05409456\n",
      "Iteration 3622, loss = 0.05408757\n",
      "Iteration 3623, loss = 0.05408022\n",
      "Iteration 3624, loss = 0.05407295\n",
      "Iteration 3625, loss = 0.05406547\n",
      "Iteration 3626, loss = 0.05405711\n",
      "Iteration 3627, loss = 0.05404877\n",
      "Iteration 3628, loss = 0.05404236\n",
      "Iteration 3629, loss = 0.05403480\n",
      "Iteration 3630, loss = 0.05402820\n",
      "Iteration 3631, loss = 0.05402097\n",
      "Iteration 3632, loss = 0.05401319\n",
      "Iteration 3633, loss = 0.05400494\n",
      "Iteration 3634, loss = 0.05399820\n",
      "Iteration 3635, loss = 0.05398926\n",
      "Iteration 3636, loss = 0.05398208\n",
      "Iteration 3637, loss = 0.05397571\n",
      "Iteration 3638, loss = 0.05396684\n",
      "Iteration 3639, loss = 0.05395966\n",
      "Iteration 3640, loss = 0.05395228\n",
      "Iteration 3641, loss = 0.05394416\n",
      "Iteration 3642, loss = 0.05393692\n",
      "Iteration 3643, loss = 0.05392802\n",
      "Iteration 3644, loss = 0.05392048\n",
      "Iteration 3645, loss = 0.05391308\n",
      "Iteration 3646, loss = 0.05390575\n",
      "Iteration 3647, loss = 0.05389778\n",
      "Iteration 3648, loss = 0.05389007\n",
      "Iteration 3649, loss = 0.05388140\n",
      "Iteration 3650, loss = 0.05387353\n",
      "Iteration 3651, loss = 0.05386613\n",
      "Iteration 3652, loss = 0.05385851\n",
      "Iteration 3653, loss = 0.05385054\n",
      "Iteration 3654, loss = 0.05384349\n",
      "Iteration 3655, loss = 0.05383467\n",
      "Iteration 3656, loss = 0.05382725\n",
      "Iteration 3657, loss = 0.05382255\n",
      "Iteration 3658, loss = 0.05381490\n",
      "Iteration 3659, loss = 0.05380535\n",
      "Iteration 3660, loss = 0.05379771\n",
      "Iteration 3661, loss = 0.05378924\n",
      "Iteration 3662, loss = 0.05378324\n",
      "Iteration 3663, loss = 0.05377554\n",
      "Iteration 3664, loss = 0.05376592\n",
      "Iteration 3665, loss = 0.05375905\n",
      "Iteration 3666, loss = 0.05375237\n",
      "Iteration 3667, loss = 0.05374473\n",
      "Iteration 3668, loss = 0.05373719\n",
      "Iteration 3669, loss = 0.05372920\n",
      "Iteration 3670, loss = 0.05372089\n",
      "Iteration 3671, loss = 0.05371350\n",
      "Iteration 3672, loss = 0.05370690\n",
      "Iteration 3673, loss = 0.05369895\n",
      "Iteration 3674, loss = 0.05368993\n",
      "Iteration 3675, loss = 0.05368215\n",
      "Iteration 3676, loss = 0.05367444\n",
      "Iteration 3677, loss = 0.05366648\n",
      "Iteration 3678, loss = 0.05365959\n",
      "Iteration 3679, loss = 0.05365208\n",
      "Iteration 3680, loss = 0.05364391\n",
      "Iteration 3681, loss = 0.05363814\n",
      "Iteration 3682, loss = 0.05362904\n",
      "Iteration 3683, loss = 0.05362029\n",
      "Iteration 3684, loss = 0.05361354\n",
      "Iteration 3685, loss = 0.05360619\n",
      "Iteration 3686, loss = 0.05359971\n",
      "Iteration 3687, loss = 0.05359149\n",
      "Iteration 3688, loss = 0.05358239\n",
      "Iteration 3689, loss = 0.05357640\n",
      "Iteration 3690, loss = 0.05356915\n",
      "Iteration 3691, loss = 0.05355882\n",
      "Iteration 3692, loss = 0.05355231\n",
      "Iteration 3693, loss = 0.05354502\n",
      "Iteration 3694, loss = 0.05353666\n",
      "Iteration 3695, loss = 0.05352968\n",
      "Iteration 3696, loss = 0.05352047\n",
      "Iteration 3697, loss = 0.05351412\n",
      "Iteration 3698, loss = 0.05350886\n",
      "Iteration 3699, loss = 0.05350151\n",
      "Iteration 3700, loss = 0.05349251\n",
      "Iteration 3701, loss = 0.05348502\n",
      "Iteration 3702, loss = 0.05347675\n",
      "Iteration 3703, loss = 0.05346996\n",
      "Iteration 3704, loss = 0.05346254\n",
      "Iteration 3705, loss = 0.05345359\n",
      "Iteration 3706, loss = 0.05344768\n",
      "Iteration 3707, loss = 0.05343981\n",
      "Iteration 3708, loss = 0.05343021\n",
      "Iteration 3709, loss = 0.05342145\n",
      "Iteration 3710, loss = 0.05341512\n",
      "Iteration 3711, loss = 0.05340689\n",
      "Iteration 3712, loss = 0.05339933\n",
      "Iteration 3713, loss = 0.05339036\n",
      "Iteration 3714, loss = 0.05338263\n",
      "Iteration 3715, loss = 0.05337603\n",
      "Iteration 3716, loss = 0.05336948\n",
      "Iteration 3717, loss = 0.05335918\n",
      "Iteration 3718, loss = 0.05335251\n",
      "Iteration 3719, loss = 0.05334565\n",
      "Iteration 3720, loss = 0.05333843\n",
      "Iteration 3721, loss = 0.05333073\n",
      "Iteration 3722, loss = 0.05332224\n",
      "Iteration 3723, loss = 0.05331464\n",
      "Iteration 3724, loss = 0.05330594\n",
      "Iteration 3725, loss = 0.05329745\n",
      "Iteration 3726, loss = 0.05329181\n",
      "Iteration 3727, loss = 0.05328453\n",
      "Iteration 3728, loss = 0.05327647\n",
      "Iteration 3729, loss = 0.05326889\n",
      "Iteration 3730, loss = 0.05325986\n",
      "Iteration 3731, loss = 0.05325400\n",
      "Iteration 3732, loss = 0.05324520\n",
      "Iteration 3733, loss = 0.05323702\n",
      "Iteration 3734, loss = 0.05323042\n",
      "Iteration 3735, loss = 0.05322205\n",
      "Iteration 3736, loss = 0.05321417\n",
      "Iteration 3737, loss = 0.05320566\n",
      "Iteration 3738, loss = 0.05319741\n",
      "Iteration 3739, loss = 0.05318949\n",
      "Iteration 3740, loss = 0.05318378\n",
      "Iteration 3741, loss = 0.05317497\n",
      "Iteration 3742, loss = 0.05316637\n",
      "Iteration 3743, loss = 0.05315932\n",
      "Iteration 3744, loss = 0.05315262\n",
      "Iteration 3745, loss = 0.05314258\n",
      "Iteration 3746, loss = 0.05313663\n",
      "Iteration 3747, loss = 0.05313124\n",
      "Iteration 3748, loss = 0.05312390\n",
      "Iteration 3749, loss = 0.05311541\n",
      "Iteration 3750, loss = 0.05310832\n",
      "Iteration 3751, loss = 0.05309951\n",
      "Iteration 3752, loss = 0.05308937\n",
      "Iteration 3753, loss = 0.05308478\n",
      "Iteration 3754, loss = 0.05307881\n",
      "Iteration 3755, loss = 0.05306920\n",
      "Iteration 3756, loss = 0.05305948\n",
      "Iteration 3757, loss = 0.05305236\n",
      "Iteration 3758, loss = 0.05304593\n",
      "Iteration 3759, loss = 0.05303856\n",
      "Iteration 3760, loss = 0.05302968\n",
      "Iteration 3761, loss = 0.05302076\n",
      "Iteration 3762, loss = 0.05301137\n",
      "Iteration 3763, loss = 0.05300622\n",
      "Iteration 3764, loss = 0.05299921\n",
      "Iteration 3765, loss = 0.05298868\n",
      "Iteration 3766, loss = 0.05298122\n",
      "Iteration 3767, loss = 0.05297396\n",
      "Iteration 3768, loss = 0.05296748\n",
      "Iteration 3769, loss = 0.05295893\n",
      "Iteration 3770, loss = 0.05295123\n",
      "Iteration 3771, loss = 0.05294320\n",
      "Iteration 3772, loss = 0.05293606\n",
      "Iteration 3773, loss = 0.05292644\n",
      "Iteration 3774, loss = 0.05291998\n",
      "Iteration 3775, loss = 0.05291348\n",
      "Iteration 3776, loss = 0.05290448\n",
      "Iteration 3777, loss = 0.05289530\n",
      "Iteration 3778, loss = 0.05288739\n",
      "Iteration 3779, loss = 0.05288288\n",
      "Iteration 3780, loss = 0.05287443\n",
      "Iteration 3781, loss = 0.05286676\n",
      "Iteration 3782, loss = 0.05285856\n",
      "Iteration 3783, loss = 0.05285146\n",
      "Iteration 3784, loss = 0.05284455\n",
      "Iteration 3785, loss = 0.05283730\n",
      "Iteration 3786, loss = 0.05282797\n",
      "Iteration 3787, loss = 0.05281975\n",
      "Iteration 3788, loss = 0.05281553\n",
      "Iteration 3789, loss = 0.05280779\n",
      "Iteration 3790, loss = 0.05279943\n",
      "Iteration 3791, loss = 0.05279049\n",
      "Iteration 3792, loss = 0.05278298\n",
      "Iteration 3793, loss = 0.05277726\n",
      "Iteration 3794, loss = 0.05277058\n",
      "Iteration 3795, loss = 0.05276123\n",
      "Iteration 3796, loss = 0.05275401\n",
      "Iteration 3797, loss = 0.05274553\n",
      "Iteration 3798, loss = 0.05273932\n",
      "Iteration 3799, loss = 0.05273012\n",
      "Iteration 3800, loss = 0.05272017\n",
      "Iteration 3801, loss = 0.05271388\n",
      "Iteration 3802, loss = 0.05270785\n",
      "Iteration 3803, loss = 0.05269896\n",
      "Iteration 3804, loss = 0.05269340\n",
      "Iteration 3805, loss = 0.05268600\n",
      "Iteration 3806, loss = 0.05267701\n",
      "Iteration 3807, loss = 0.05266804\n",
      "Iteration 3808, loss = 0.05265896\n",
      "Iteration 3809, loss = 0.05265353\n",
      "Iteration 3810, loss = 0.05264591\n",
      "Iteration 3811, loss = 0.05263427\n",
      "Iteration 3812, loss = 0.05262813\n",
      "Iteration 3813, loss = 0.05262233\n",
      "Iteration 3814, loss = 0.05261367\n",
      "Iteration 3815, loss = 0.05260715\n",
      "Iteration 3816, loss = 0.05259741\n",
      "Iteration 3817, loss = 0.05259007\n",
      "Iteration 3818, loss = 0.05258082\n",
      "Iteration 3819, loss = 0.05257467\n",
      "Iteration 3820, loss = 0.05256761\n",
      "Iteration 3821, loss = 0.05255676\n",
      "Iteration 3822, loss = 0.05255289\n",
      "Iteration 3823, loss = 0.05254694\n",
      "Iteration 3824, loss = 0.05253816\n",
      "Iteration 3825, loss = 0.05252976\n",
      "Iteration 3826, loss = 0.05252230\n",
      "Iteration 3827, loss = 0.05251445\n",
      "Iteration 3828, loss = 0.05250550\n",
      "Iteration 3829, loss = 0.05249737\n",
      "Iteration 3830, loss = 0.05249299\n",
      "Iteration 3831, loss = 0.05248429\n",
      "Iteration 3832, loss = 0.05247468\n",
      "Iteration 3833, loss = 0.05246449\n",
      "Iteration 3834, loss = 0.05245815\n",
      "Iteration 3835, loss = 0.05245059\n",
      "Iteration 3836, loss = 0.05244261\n",
      "Iteration 3837, loss = 0.05243382\n",
      "Iteration 3838, loss = 0.05242573\n",
      "Iteration 3839, loss = 0.05241717\n",
      "Iteration 3840, loss = 0.05240907\n",
      "Iteration 3841, loss = 0.05240286\n",
      "Iteration 3842, loss = 0.05239436\n",
      "Iteration 3843, loss = 0.05238732\n",
      "Iteration 3844, loss = 0.05237956\n",
      "Iteration 3845, loss = 0.05237104\n",
      "Iteration 3846, loss = 0.05236424\n",
      "Iteration 3847, loss = 0.05235615\n",
      "Iteration 3848, loss = 0.05234774\n",
      "Iteration 3849, loss = 0.05234047\n",
      "Iteration 3850, loss = 0.05233375\n",
      "Iteration 3851, loss = 0.05232549\n",
      "Iteration 3852, loss = 0.05231929\n",
      "Iteration 3853, loss = 0.05231064\n",
      "Iteration 3854, loss = 0.05230090\n",
      "Iteration 3855, loss = 0.05229511\n",
      "Iteration 3856, loss = 0.05228844\n",
      "Iteration 3857, loss = 0.05228008\n",
      "Iteration 3858, loss = 0.05226994\n",
      "Iteration 3859, loss = 0.05226315\n",
      "Iteration 3860, loss = 0.05225617\n",
      "Iteration 3861, loss = 0.05224838\n",
      "Iteration 3862, loss = 0.05223986\n",
      "Iteration 3863, loss = 0.05223205\n",
      "Iteration 3864, loss = 0.05222438\n",
      "Iteration 3865, loss = 0.05221562\n",
      "Iteration 3866, loss = 0.05220939\n",
      "Iteration 3867, loss = 0.05219962\n",
      "Iteration 3868, loss = 0.05219379\n",
      "Iteration 3869, loss = 0.05218772\n",
      "Iteration 3870, loss = 0.05218081\n",
      "Iteration 3871, loss = 0.05217270\n",
      "Iteration 3872, loss = 0.05216316\n",
      "Iteration 3873, loss = 0.05215427\n",
      "Iteration 3874, loss = 0.05214693\n",
      "Iteration 3875, loss = 0.05214050\n",
      "Iteration 3876, loss = 0.05213180\n",
      "Iteration 3877, loss = 0.05212438\n",
      "Iteration 3878, loss = 0.05211584\n",
      "Iteration 3879, loss = 0.05210706\n",
      "Iteration 3880, loss = 0.05210055\n",
      "Iteration 3881, loss = 0.05209476\n",
      "Iteration 3882, loss = 0.05208652\n",
      "Iteration 3883, loss = 0.05207645\n",
      "Iteration 3884, loss = 0.05207032\n",
      "Iteration 3885, loss = 0.05206268\n",
      "Iteration 3886, loss = 0.05205362\n",
      "Iteration 3887, loss = 0.05204592\n",
      "Iteration 3888, loss = 0.05203711\n",
      "Iteration 3889, loss = 0.05203264\n",
      "Iteration 3890, loss = 0.05202450\n",
      "Iteration 3891, loss = 0.05201713\n",
      "Iteration 3892, loss = 0.05200694\n",
      "Iteration 3893, loss = 0.05199983\n",
      "Iteration 3894, loss = 0.05199355\n",
      "Iteration 3895, loss = 0.05198674\n",
      "Iteration 3896, loss = 0.05197913\n",
      "Iteration 3897, loss = 0.05196941\n",
      "Iteration 3898, loss = 0.05196323\n",
      "Iteration 3899, loss = 0.05195335\n",
      "Iteration 3900, loss = 0.05194365\n",
      "Iteration 3901, loss = 0.05193715\n",
      "Iteration 3902, loss = 0.05193149\n",
      "Iteration 3903, loss = 0.05192483\n",
      "Iteration 3904, loss = 0.05191488\n",
      "Iteration 3905, loss = 0.05190867\n",
      "Iteration 3906, loss = 0.05190240\n",
      "Iteration 3907, loss = 0.05189449\n",
      "Iteration 3908, loss = 0.05188523\n",
      "Iteration 3909, loss = 0.05187946\n",
      "Iteration 3910, loss = 0.05187119\n",
      "Iteration 3911, loss = 0.05185799\n",
      "Iteration 3912, loss = 0.05185288\n",
      "Iteration 3913, loss = 0.05184787\n",
      "Iteration 3914, loss = 0.05184140\n",
      "Iteration 3915, loss = 0.05183045\n",
      "Iteration 3916, loss = 0.05182420\n",
      "Iteration 3917, loss = 0.05181607\n",
      "Iteration 3918, loss = 0.05180772\n",
      "Iteration 3919, loss = 0.05179906\n",
      "Iteration 3920, loss = 0.05179337\n",
      "Iteration 3921, loss = 0.05178478\n",
      "Iteration 3922, loss = 0.05177338\n",
      "Iteration 3923, loss = 0.05176533\n",
      "Iteration 3924, loss = 0.05176033\n",
      "Iteration 3925, loss = 0.05175182\n",
      "Iteration 3926, loss = 0.05174373\n",
      "Iteration 3927, loss = 0.05173526\n",
      "Iteration 3928, loss = 0.05172518\n",
      "Iteration 3929, loss = 0.05171819\n",
      "Iteration 3930, loss = 0.05171215\n",
      "Iteration 3931, loss = 0.05170481\n",
      "Iteration 3932, loss = 0.05169474\n",
      "Iteration 3933, loss = 0.05168666\n",
      "Iteration 3934, loss = 0.05168020\n",
      "Iteration 3935, loss = 0.05167386\n",
      "Iteration 3936, loss = 0.05166556\n",
      "Iteration 3937, loss = 0.05165669\n",
      "Iteration 3938, loss = 0.05164781\n",
      "Iteration 3939, loss = 0.05164038\n",
      "Iteration 3940, loss = 0.05163392\n",
      "Iteration 3941, loss = 0.05162625\n",
      "Iteration 3942, loss = 0.05161569\n",
      "Iteration 3943, loss = 0.05160656\n",
      "Iteration 3944, loss = 0.05159892\n",
      "Iteration 3945, loss = 0.05159109\n",
      "Iteration 3946, loss = 0.05158479\n",
      "Iteration 3947, loss = 0.05157507\n",
      "Iteration 3948, loss = 0.05156645\n",
      "Iteration 3949, loss = 0.05155878\n",
      "Iteration 3950, loss = 0.05155298\n",
      "Iteration 3951, loss = 0.05154388\n",
      "Iteration 3952, loss = 0.05153550\n",
      "Iteration 3953, loss = 0.05152658\n",
      "Iteration 3954, loss = 0.05151889\n",
      "Iteration 3955, loss = 0.05151055\n",
      "Iteration 3956, loss = 0.05150385\n",
      "Iteration 3957, loss = 0.05149624\n",
      "Iteration 3958, loss = 0.05148770\n",
      "Iteration 3959, loss = 0.05148048\n",
      "Iteration 3960, loss = 0.05147464\n",
      "Iteration 3961, loss = 0.05146515\n",
      "Iteration 3962, loss = 0.05145564\n",
      "Iteration 3963, loss = 0.05144819\n",
      "Iteration 3964, loss = 0.05144043\n",
      "Iteration 3965, loss = 0.05143320\n",
      "Iteration 3966, loss = 0.05142421\n",
      "Iteration 3967, loss = 0.05141830\n",
      "Iteration 3968, loss = 0.05140969\n",
      "Iteration 3969, loss = 0.05140112\n",
      "Iteration 3970, loss = 0.05139172\n",
      "Iteration 3971, loss = 0.05138589\n",
      "Iteration 3972, loss = 0.05137900\n",
      "Iteration 3973, loss = 0.05136950\n",
      "Iteration 3974, loss = 0.05136418\n",
      "Iteration 3975, loss = 0.05135614\n",
      "Iteration 3976, loss = 0.05134531\n",
      "Iteration 3977, loss = 0.05133897\n",
      "Iteration 3978, loss = 0.05133058\n",
      "Iteration 3979, loss = 0.05132034\n",
      "Iteration 3980, loss = 0.05131152\n",
      "Iteration 3981, loss = 0.05130435\n",
      "Iteration 3982, loss = 0.05129601\n",
      "Iteration 3983, loss = 0.05128865\n",
      "Iteration 3984, loss = 0.05127887\n",
      "Iteration 3985, loss = 0.05127109\n",
      "Iteration 3986, loss = 0.05126494\n",
      "Iteration 3987, loss = 0.05125788\n",
      "Iteration 3988, loss = 0.05124893\n",
      "Iteration 3989, loss = 0.05124101\n",
      "Iteration 3990, loss = 0.05123524\n",
      "Iteration 3991, loss = 0.05122703\n",
      "Iteration 3992, loss = 0.05121953\n",
      "Iteration 3993, loss = 0.05121156\n",
      "Iteration 3994, loss = 0.05120273\n",
      "Iteration 3995, loss = 0.05119485\n",
      "Iteration 3996, loss = 0.05118718\n",
      "Iteration 3997, loss = 0.05117626\n",
      "Iteration 3998, loss = 0.05116833\n",
      "Iteration 3999, loss = 0.05116113\n",
      "Iteration 4000, loss = 0.05115296\n",
      "Iteration 4001, loss = 0.05114663\n",
      "Iteration 4002, loss = 0.05113903\n",
      "Iteration 4003, loss = 0.05113016\n",
      "Iteration 4004, loss = 0.05112274\n",
      "Iteration 4005, loss = 0.05111666\n",
      "Iteration 4006, loss = 0.05110837\n",
      "Iteration 4007, loss = 0.05110067\n",
      "Iteration 4008, loss = 0.05109192\n",
      "Iteration 4009, loss = 0.05108235\n",
      "Iteration 4010, loss = 0.05107381\n",
      "Iteration 4011, loss = 0.05106576\n",
      "Iteration 4012, loss = 0.05105979\n",
      "Iteration 4013, loss = 0.05105031\n",
      "Iteration 4014, loss = 0.05104213\n",
      "Iteration 4015, loss = 0.05103742\n",
      "Iteration 4016, loss = 0.05102793\n",
      "Iteration 4017, loss = 0.05101708\n",
      "Iteration 4018, loss = 0.05101263\n",
      "Iteration 4019, loss = 0.05100633\n",
      "Iteration 4020, loss = 0.05099331\n",
      "Iteration 4021, loss = 0.05098537\n",
      "Iteration 4022, loss = 0.05098197\n",
      "Iteration 4023, loss = 0.05097257\n",
      "Iteration 4024, loss = 0.05096337\n",
      "Iteration 4025, loss = 0.05095815\n",
      "Iteration 4026, loss = 0.05095015\n",
      "Iteration 4027, loss = 0.05093932\n",
      "Iteration 4028, loss = 0.05093018\n",
      "Iteration 4029, loss = 0.05092613\n",
      "Iteration 4030, loss = 0.05091647\n",
      "Iteration 4031, loss = 0.05090655\n",
      "Iteration 4032, loss = 0.05089850\n",
      "Iteration 4033, loss = 0.05089005\n",
      "Iteration 4034, loss = 0.05088082\n",
      "Iteration 4035, loss = 0.05087528\n",
      "Iteration 4036, loss = 0.05086747\n",
      "Iteration 4037, loss = 0.05086072\n",
      "Iteration 4038, loss = 0.05085377\n",
      "Iteration 4039, loss = 0.05084503\n",
      "Iteration 4040, loss = 0.05083514\n",
      "Iteration 4041, loss = 0.05082851\n",
      "Iteration 4042, loss = 0.05081914\n",
      "Iteration 4043, loss = 0.05081332\n",
      "Iteration 4044, loss = 0.05080503\n",
      "Iteration 4045, loss = 0.05079395\n",
      "Iteration 4046, loss = 0.05078694\n",
      "Iteration 4047, loss = 0.05078048\n",
      "Iteration 4048, loss = 0.05077279\n",
      "Iteration 4049, loss = 0.05076475\n",
      "Iteration 4050, loss = 0.05075709\n",
      "Iteration 4051, loss = 0.05074700\n",
      "Iteration 4052, loss = 0.05073559\n",
      "Iteration 4053, loss = 0.05072736\n",
      "Iteration 4054, loss = 0.05072192\n",
      "Iteration 4055, loss = 0.05071496\n",
      "Iteration 4056, loss = 0.05070511\n",
      "Iteration 4057, loss = 0.05069643\n",
      "Iteration 4058, loss = 0.05068990\n",
      "Iteration 4059, loss = 0.05068349\n",
      "Iteration 4060, loss = 0.05067448\n",
      "Iteration 4061, loss = 0.05066470\n",
      "Iteration 4062, loss = 0.05065531\n",
      "Iteration 4063, loss = 0.05064864\n",
      "Iteration 4064, loss = 0.05064038\n",
      "Iteration 4065, loss = 0.05063196\n",
      "Iteration 4066, loss = 0.05062342\n",
      "Iteration 4067, loss = 0.05061604\n",
      "Iteration 4068, loss = 0.05060768\n",
      "Iteration 4069, loss = 0.05059885\n",
      "Iteration 4070, loss = 0.05059404\n",
      "Iteration 4071, loss = 0.05058388\n",
      "Iteration 4072, loss = 0.05057381\n",
      "Iteration 4073, loss = 0.05056688\n",
      "Iteration 4074, loss = 0.05055771\n",
      "Iteration 4075, loss = 0.05054922\n",
      "Iteration 4076, loss = 0.05054245\n",
      "Iteration 4077, loss = 0.05053469\n",
      "Iteration 4078, loss = 0.05052571\n",
      "Iteration 4079, loss = 0.05051861\n",
      "Iteration 4080, loss = 0.05051135\n",
      "Iteration 4081, loss = 0.05050551\n",
      "Iteration 4082, loss = 0.05049527\n",
      "Iteration 4083, loss = 0.05048512\n",
      "Iteration 4084, loss = 0.05047869\n",
      "Iteration 4085, loss = 0.05047282\n",
      "Iteration 4086, loss = 0.05046417\n",
      "Iteration 4087, loss = 0.05045519\n",
      "Iteration 4088, loss = 0.05044648\n",
      "Iteration 4089, loss = 0.05043967\n",
      "Iteration 4090, loss = 0.05043207\n",
      "Iteration 4091, loss = 0.05042338\n",
      "Iteration 4092, loss = 0.05041456\n",
      "Iteration 4093, loss = 0.05040300\n",
      "Iteration 4094, loss = 0.05039477\n",
      "Iteration 4095, loss = 0.05038810\n",
      "Iteration 4096, loss = 0.05037875\n",
      "Iteration 4097, loss = 0.05037094\n",
      "Iteration 4098, loss = 0.05036497\n",
      "Iteration 4099, loss = 0.05035678\n",
      "Iteration 4100, loss = 0.05035089\n",
      "Iteration 4101, loss = 0.05034137\n",
      "Iteration 4102, loss = 0.05033105\n",
      "Iteration 4103, loss = 0.05032178\n",
      "Iteration 4104, loss = 0.05031627\n",
      "Iteration 4105, loss = 0.05030731\n",
      "Iteration 4106, loss = 0.05029773\n",
      "Iteration 4107, loss = 0.05028919\n",
      "Iteration 4108, loss = 0.05028258\n",
      "Iteration 4109, loss = 0.05027435\n",
      "Iteration 4110, loss = 0.05026531\n",
      "Iteration 4111, loss = 0.05025631\n",
      "Iteration 4112, loss = 0.05024918\n",
      "Iteration 4113, loss = 0.05024084\n",
      "Iteration 4114, loss = 0.05023367\n",
      "Iteration 4115, loss = 0.05022469\n",
      "Iteration 4116, loss = 0.05021673\n",
      "Iteration 4117, loss = 0.05020845\n",
      "Iteration 4118, loss = 0.05019928\n",
      "Iteration 4119, loss = 0.05019233\n",
      "Iteration 4120, loss = 0.05018461\n",
      "Iteration 4121, loss = 0.05017275\n",
      "Iteration 4122, loss = 0.05016563\n",
      "Iteration 4123, loss = 0.05016005\n",
      "Iteration 4124, loss = 0.05015175\n",
      "Iteration 4125, loss = 0.05014378\n",
      "Iteration 4126, loss = 0.05013506\n",
      "Iteration 4127, loss = 0.05012341\n",
      "Iteration 4128, loss = 0.05011651\n",
      "Iteration 4129, loss = 0.05011261\n",
      "Iteration 4130, loss = 0.05010293\n",
      "Iteration 4131, loss = 0.05009597\n",
      "Iteration 4132, loss = 0.05008626\n",
      "Iteration 4133, loss = 0.05007905\n",
      "Iteration 4134, loss = 0.05007106\n",
      "Iteration 4135, loss = 0.05006290\n",
      "Iteration 4136, loss = 0.05005273\n",
      "Iteration 4137, loss = 0.05004246\n",
      "Iteration 4138, loss = 0.05003232\n",
      "Iteration 4139, loss = 0.05002919\n",
      "Iteration 4140, loss = 0.05002261\n",
      "Iteration 4141, loss = 0.05001482\n",
      "Iteration 4142, loss = 0.05000122\n",
      "Iteration 4143, loss = 0.04999434\n",
      "Iteration 4144, loss = 0.04998504\n",
      "Iteration 4145, loss = 0.04997772\n",
      "Iteration 4146, loss = 0.04997149\n",
      "Iteration 4147, loss = 0.04996148\n",
      "Iteration 4148, loss = 0.04995351\n",
      "Iteration 4149, loss = 0.04994512\n",
      "Iteration 4150, loss = 0.04993769\n",
      "Iteration 4151, loss = 0.04993035\n",
      "Iteration 4152, loss = 0.04991943\n",
      "Iteration 4153, loss = 0.04990986\n",
      "Iteration 4154, loss = 0.04990348\n",
      "Iteration 4155, loss = 0.04989830\n",
      "Iteration 4156, loss = 0.04989308\n",
      "Iteration 4157, loss = 0.04988176\n",
      "Iteration 4158, loss = 0.04987003\n",
      "Iteration 4159, loss = 0.04986123\n",
      "Iteration 4160, loss = 0.04985554\n",
      "Iteration 4161, loss = 0.04984763\n",
      "Iteration 4162, loss = 0.04983641\n",
      "Iteration 4163, loss = 0.04982701\n",
      "Iteration 4164, loss = 0.04982222\n",
      "Iteration 4165, loss = 0.04981335\n",
      "Iteration 4166, loss = 0.04980438\n",
      "Iteration 4167, loss = 0.04979674\n",
      "Iteration 4168, loss = 0.04978897\n",
      "Iteration 4169, loss = 0.04978054\n",
      "Iteration 4170, loss = 0.04977201\n",
      "Iteration 4171, loss = 0.04976371\n",
      "Iteration 4172, loss = 0.04975452\n",
      "Iteration 4173, loss = 0.04974547\n",
      "Iteration 4174, loss = 0.04973420\n",
      "Iteration 4175, loss = 0.04973046\n",
      "Iteration 4176, loss = 0.04972356\n",
      "Iteration 4177, loss = 0.04971319\n",
      "Iteration 4178, loss = 0.04970662\n",
      "Iteration 4179, loss = 0.04969950\n",
      "Iteration 4180, loss = 0.04968943\n",
      "Iteration 4181, loss = 0.04968591\n",
      "Iteration 4182, loss = 0.04967865\n",
      "Iteration 4183, loss = 0.04966666\n",
      "Iteration 4184, loss = 0.04965578\n",
      "Iteration 4185, loss = 0.04964714\n",
      "Iteration 4186, loss = 0.04964011\n",
      "Iteration 4187, loss = 0.04963493\n",
      "Iteration 4188, loss = 0.04962382\n",
      "Iteration 4189, loss = 0.04961444\n",
      "Iteration 4190, loss = 0.04960579\n",
      "Iteration 4191, loss = 0.04960100\n",
      "Iteration 4192, loss = 0.04959441\n",
      "Iteration 4193, loss = 0.04958477\n",
      "Iteration 4194, loss = 0.04957281\n",
      "Iteration 4195, loss = 0.04956347\n",
      "Iteration 4196, loss = 0.04955873\n",
      "Iteration 4197, loss = 0.04955171\n",
      "Iteration 4198, loss = 0.04954124\n",
      "Iteration 4199, loss = 0.04953044\n",
      "Iteration 4200, loss = 0.04952219\n",
      "Iteration 4201, loss = 0.04951423\n",
      "Iteration 4202, loss = 0.04950737\n",
      "Iteration 4203, loss = 0.04949889\n",
      "Iteration 4204, loss = 0.04948852\n",
      "Iteration 4205, loss = 0.04947870\n",
      "Iteration 4206, loss = 0.04947422\n",
      "Iteration 4207, loss = 0.04946682\n",
      "Iteration 4208, loss = 0.04945533\n",
      "Iteration 4209, loss = 0.04944566\n",
      "Iteration 4210, loss = 0.04943798\n",
      "Iteration 4211, loss = 0.04943003\n",
      "Iteration 4212, loss = 0.04942211\n",
      "Iteration 4213, loss = 0.04941452\n",
      "Iteration 4214, loss = 0.04940503\n",
      "Iteration 4215, loss = 0.04939685\n",
      "Iteration 4216, loss = 0.04939000\n",
      "Iteration 4217, loss = 0.04938063\n",
      "Iteration 4218, loss = 0.04937491\n",
      "Iteration 4219, loss = 0.04936489\n",
      "Iteration 4220, loss = 0.04935529\n",
      "Iteration 4221, loss = 0.04934833\n",
      "Iteration 4222, loss = 0.04934270\n",
      "Iteration 4223, loss = 0.04933434\n",
      "Iteration 4224, loss = 0.04932415\n",
      "Iteration 4225, loss = 0.04931420\n",
      "Iteration 4226, loss = 0.04930667\n",
      "Iteration 4227, loss = 0.04929672\n",
      "Iteration 4228, loss = 0.04928889\n",
      "Iteration 4229, loss = 0.04928222\n",
      "Iteration 4230, loss = 0.04927507\n",
      "Iteration 4231, loss = 0.04926251\n",
      "Iteration 4232, loss = 0.04925476\n",
      "Iteration 4233, loss = 0.04924710\n",
      "Iteration 4234, loss = 0.04923801\n",
      "Iteration 4235, loss = 0.04922766\n",
      "Iteration 4236, loss = 0.04922130\n",
      "Iteration 4237, loss = 0.04921385\n",
      "Iteration 4238, loss = 0.04920304\n",
      "Iteration 4239, loss = 0.04919623\n",
      "Iteration 4240, loss = 0.04918829\n",
      "Iteration 4241, loss = 0.04918006\n",
      "Iteration 4242, loss = 0.04917329\n",
      "Iteration 4243, loss = 0.04916398\n",
      "Iteration 4244, loss = 0.04915310\n",
      "Iteration 4245, loss = 0.04914485\n",
      "Iteration 4246, loss = 0.04913603\n",
      "Iteration 4247, loss = 0.04912839\n",
      "Iteration 4248, loss = 0.04911889\n",
      "Iteration 4249, loss = 0.04911399\n",
      "Iteration 4250, loss = 0.04910578\n",
      "Iteration 4251, loss = 0.04909685\n",
      "Iteration 4252, loss = 0.04909007\n",
      "Iteration 4253, loss = 0.04908146\n",
      "Iteration 4254, loss = 0.04907252\n",
      "Iteration 4255, loss = 0.04906192\n",
      "Iteration 4256, loss = 0.04905329\n",
      "Iteration 4257, loss = 0.04904515\n",
      "Iteration 4258, loss = 0.04903286\n",
      "Iteration 4259, loss = 0.04902854\n",
      "Iteration 4260, loss = 0.04902028\n",
      "Iteration 4261, loss = 0.04901594\n",
      "Iteration 4262, loss = 0.04900418\n",
      "Iteration 4263, loss = 0.04899258\n",
      "Iteration 4264, loss = 0.04898685\n",
      "Iteration 4265, loss = 0.04898124\n",
      "Iteration 4266, loss = 0.04897315\n",
      "Iteration 4267, loss = 0.04896403\n",
      "Iteration 4268, loss = 0.04895282\n",
      "Iteration 4269, loss = 0.04894500\n",
      "Iteration 4270, loss = 0.04893636\n",
      "Iteration 4271, loss = 0.04892609\n",
      "Iteration 4272, loss = 0.04891878\n",
      "Iteration 4273, loss = 0.04891190\n",
      "Iteration 4274, loss = 0.04890473\n",
      "Iteration 4275, loss = 0.04889629\n",
      "Iteration 4276, loss = 0.04888617\n",
      "Iteration 4277, loss = 0.04887757\n",
      "Iteration 4278, loss = 0.04887013\n",
      "Iteration 4279, loss = 0.04885961\n",
      "Iteration 4280, loss = 0.04885292\n",
      "Iteration 4281, loss = 0.04884514\n",
      "Iteration 4282, loss = 0.04883562\n",
      "Iteration 4283, loss = 0.04882580\n",
      "Iteration 4284, loss = 0.04881859\n",
      "Iteration 4285, loss = 0.04880826\n",
      "Iteration 4286, loss = 0.04879952\n",
      "Iteration 4287, loss = 0.04879310\n",
      "Iteration 4288, loss = 0.04878415\n",
      "Iteration 4289, loss = 0.04877548\n",
      "Iteration 4290, loss = 0.04876647\n",
      "Iteration 4291, loss = 0.04875823\n",
      "Iteration 4292, loss = 0.04874871\n",
      "Iteration 4293, loss = 0.04873960\n",
      "Iteration 4294, loss = 0.04873200\n",
      "Iteration 4295, loss = 0.04872762\n",
      "Iteration 4296, loss = 0.04871805\n",
      "Iteration 4297, loss = 0.04870928\n",
      "Iteration 4298, loss = 0.04870174\n",
      "Iteration 4299, loss = 0.04869143\n",
      "Iteration 4300, loss = 0.04868221\n",
      "Iteration 4301, loss = 0.04867501\n",
      "Iteration 4302, loss = 0.04866751\n",
      "Iteration 4303, loss = 0.04865974\n",
      "Iteration 4304, loss = 0.04864805\n",
      "Iteration 4305, loss = 0.04863966\n",
      "Iteration 4306, loss = 0.04863211\n",
      "Iteration 4307, loss = 0.04862412\n",
      "Iteration 4308, loss = 0.04861602\n",
      "Iteration 4309, loss = 0.04860916\n",
      "Iteration 4310, loss = 0.04859929\n",
      "Iteration 4311, loss = 0.04858948\n",
      "Iteration 4312, loss = 0.04858121\n",
      "Iteration 4313, loss = 0.04857152\n",
      "Iteration 4314, loss = 0.04856229\n",
      "Iteration 4315, loss = 0.04855665\n",
      "Iteration 4316, loss = 0.04854631\n",
      "Iteration 4317, loss = 0.04853631\n",
      "Iteration 4318, loss = 0.04853014\n",
      "Iteration 4319, loss = 0.04852364\n",
      "Iteration 4320, loss = 0.04851572\n",
      "Iteration 4321, loss = 0.04850561\n",
      "Iteration 4322, loss = 0.04849719\n",
      "Iteration 4323, loss = 0.04848908\n",
      "Iteration 4324, loss = 0.04847966\n",
      "Iteration 4325, loss = 0.04847140\n",
      "Iteration 4326, loss = 0.04846115\n",
      "Iteration 4327, loss = 0.04845313\n",
      "Iteration 4328, loss = 0.04844807\n",
      "Iteration 4329, loss = 0.04843992\n",
      "Iteration 4330, loss = 0.04842688\n",
      "Iteration 4331, loss = 0.04841784\n",
      "Iteration 4332, loss = 0.04841312\n",
      "Iteration 4333, loss = 0.04840489\n",
      "Iteration 4334, loss = 0.04839670\n",
      "Iteration 4335, loss = 0.04838972\n",
      "Iteration 4336, loss = 0.04837765\n",
      "Iteration 4337, loss = 0.04836885\n",
      "Iteration 4338, loss = 0.04836114\n",
      "Iteration 4339, loss = 0.04834947\n",
      "Iteration 4340, loss = 0.04834235\n",
      "Iteration 4341, loss = 0.04833525\n",
      "Iteration 4342, loss = 0.04832956\n",
      "Iteration 4343, loss = 0.04832027\n",
      "Iteration 4344, loss = 0.04830770\n",
      "Iteration 4345, loss = 0.04829953\n",
      "Iteration 4346, loss = 0.04828941\n",
      "Iteration 4347, loss = 0.04828176\n",
      "Iteration 4348, loss = 0.04827239\n",
      "Iteration 4349, loss = 0.04826363\n",
      "Iteration 4350, loss = 0.04825813\n",
      "Iteration 4351, loss = 0.04824958\n",
      "Iteration 4352, loss = 0.04824094\n",
      "Iteration 4353, loss = 0.04823204\n",
      "Iteration 4354, loss = 0.04822351\n",
      "Iteration 4355, loss = 0.04821302\n",
      "Iteration 4356, loss = 0.04820863\n",
      "Iteration 4357, loss = 0.04820073\n",
      "Iteration 4358, loss = 0.04819153\n",
      "Iteration 4359, loss = 0.04818207\n",
      "Iteration 4360, loss = 0.04817181\n",
      "Iteration 4361, loss = 0.04816701\n",
      "Iteration 4362, loss = 0.04816091\n",
      "Iteration 4363, loss = 0.04815110\n",
      "Iteration 4364, loss = 0.04814172\n",
      "Iteration 4365, loss = 0.04813183\n",
      "Iteration 4366, loss = 0.04812526\n",
      "Iteration 4367, loss = 0.04811580\n",
      "Iteration 4368, loss = 0.04810728\n",
      "Iteration 4369, loss = 0.04810031\n",
      "Iteration 4370, loss = 0.04809122\n",
      "Iteration 4371, loss = 0.04808121\n",
      "Iteration 4372, loss = 0.04807376\n",
      "Iteration 4373, loss = 0.04806582\n",
      "Iteration 4374, loss = 0.04805838\n",
      "Iteration 4375, loss = 0.04804907\n",
      "Iteration 4376, loss = 0.04804276\n",
      "Iteration 4377, loss = 0.04803396\n",
      "Iteration 4378, loss = 0.04802098\n",
      "Iteration 4379, loss = 0.04800942\n",
      "Iteration 4380, loss = 0.04800219\n",
      "Iteration 4381, loss = 0.04800002\n",
      "Iteration 4382, loss = 0.04798930\n",
      "Iteration 4383, loss = 0.04797757\n",
      "Iteration 4384, loss = 0.04796748\n",
      "Iteration 4385, loss = 0.04795675\n",
      "Iteration 4386, loss = 0.04795172\n",
      "Iteration 4387, loss = 0.04794352\n",
      "Iteration 4388, loss = 0.04793619\n",
      "Iteration 4389, loss = 0.04792510\n",
      "Iteration 4390, loss = 0.04791739\n",
      "Iteration 4391, loss = 0.04791118\n",
      "Iteration 4392, loss = 0.04790292\n",
      "Iteration 4393, loss = 0.04789436\n",
      "Iteration 4394, loss = 0.04788468\n",
      "Iteration 4395, loss = 0.04787381\n",
      "Iteration 4396, loss = 0.04786978\n",
      "Iteration 4397, loss = 0.04786072\n",
      "Iteration 4398, loss = 0.04785300\n",
      "Iteration 4399, loss = 0.04784139\n",
      "Iteration 4400, loss = 0.04783531\n",
      "Iteration 4401, loss = 0.04782478\n",
      "Iteration 4402, loss = 0.04781495\n",
      "Iteration 4403, loss = 0.04780842\n",
      "Iteration 4404, loss = 0.04780041\n",
      "Iteration 4405, loss = 0.04779334\n",
      "Iteration 4406, loss = 0.04778432\n",
      "Iteration 4407, loss = 0.04777382\n",
      "Iteration 4408, loss = 0.04776378\n",
      "Iteration 4409, loss = 0.04775233\n",
      "Iteration 4410, loss = 0.04774402\n",
      "Iteration 4411, loss = 0.04773733\n",
      "Iteration 4412, loss = 0.04772780\n",
      "Iteration 4413, loss = 0.04771671\n",
      "Iteration 4414, loss = 0.04770944\n",
      "Iteration 4415, loss = 0.04770330\n",
      "Iteration 4416, loss = 0.04769347\n",
      "Iteration 4417, loss = 0.04768399\n",
      "Iteration 4418, loss = 0.04767493\n",
      "Iteration 4419, loss = 0.04766629\n",
      "Iteration 4420, loss = 0.04765980\n",
      "Iteration 4421, loss = 0.04765280\n",
      "Iteration 4422, loss = 0.04764308\n",
      "Iteration 4423, loss = 0.04763343\n",
      "Iteration 4424, loss = 0.04762585\n",
      "Iteration 4425, loss = 0.04762100\n",
      "Iteration 4426, loss = 0.04760922\n",
      "Iteration 4427, loss = 0.04759822\n",
      "Iteration 4428, loss = 0.04759168\n",
      "Iteration 4429, loss = 0.04758600\n",
      "Iteration 4430, loss = 0.04757945\n",
      "Iteration 4431, loss = 0.04757099\n",
      "Iteration 4432, loss = 0.04756024\n",
      "Iteration 4433, loss = 0.04754937\n",
      "Iteration 4434, loss = 0.04753935\n",
      "Iteration 4435, loss = 0.04752692\n",
      "Iteration 4436, loss = 0.04752383\n",
      "Iteration 4437, loss = 0.04751944\n",
      "Iteration 4438, loss = 0.04750912\n",
      "Iteration 4439, loss = 0.04749592\n",
      "Iteration 4440, loss = 0.04748666\n",
      "Iteration 4441, loss = 0.04747813\n",
      "Iteration 4442, loss = 0.04747044\n",
      "Iteration 4443, loss = 0.04746046\n",
      "Iteration 4444, loss = 0.04744962\n",
      "Iteration 4445, loss = 0.04744210\n",
      "Iteration 4446, loss = 0.04743634\n",
      "Iteration 4447, loss = 0.04742831\n",
      "Iteration 4448, loss = 0.04741968\n",
      "Iteration 4449, loss = 0.04740997\n",
      "Iteration 4450, loss = 0.04739994\n",
      "Iteration 4451, loss = 0.04739180\n",
      "Iteration 4452, loss = 0.04738481\n",
      "Iteration 4453, loss = 0.04737571\n",
      "Iteration 4454, loss = 0.04736716\n",
      "Iteration 4455, loss = 0.04735956\n",
      "Iteration 4456, loss = 0.04735066\n",
      "Iteration 4457, loss = 0.04734327\n",
      "Iteration 4458, loss = 0.04733331\n",
      "Iteration 4459, loss = 0.04732184\n",
      "Iteration 4460, loss = 0.04731707\n",
      "Iteration 4461, loss = 0.04731014\n",
      "Iteration 4462, loss = 0.04729937\n",
      "Iteration 4463, loss = 0.04728839\n",
      "Iteration 4464, loss = 0.04728243\n",
      "Iteration 4465, loss = 0.04727453\n",
      "Iteration 4466, loss = 0.04726482\n",
      "Iteration 4467, loss = 0.04725440\n",
      "Iteration 4468, loss = 0.04724699\n",
      "Iteration 4469, loss = 0.04723902\n",
      "Iteration 4470, loss = 0.04723055\n",
      "Iteration 4471, loss = 0.04722128\n",
      "Iteration 4472, loss = 0.04721107\n",
      "Iteration 4473, loss = 0.04720403\n",
      "Iteration 4474, loss = 0.04719655\n",
      "Iteration 4475, loss = 0.04718660\n",
      "Iteration 4476, loss = 0.04717919\n",
      "Iteration 4477, loss = 0.04717171\n",
      "Iteration 4478, loss = 0.04716531\n",
      "Iteration 4479, loss = 0.04715649\n",
      "Iteration 4480, loss = 0.04714827\n",
      "Iteration 4481, loss = 0.04713771\n",
      "Iteration 4482, loss = 0.04712843\n",
      "Iteration 4483, loss = 0.04711867\n",
      "Iteration 4484, loss = 0.04711362\n",
      "Iteration 4485, loss = 0.04710221\n",
      "Iteration 4486, loss = 0.04709336\n",
      "Iteration 4487, loss = 0.04708657\n",
      "Iteration 4488, loss = 0.04708195\n",
      "Iteration 4489, loss = 0.04707316\n",
      "Iteration 4490, loss = 0.04706458\n",
      "Iteration 4491, loss = 0.04705566\n",
      "Iteration 4492, loss = 0.04704384\n",
      "Iteration 4493, loss = 0.04703568\n",
      "Iteration 4494, loss = 0.04702554\n",
      "Iteration 4495, loss = 0.04702348\n",
      "Iteration 4496, loss = 0.04701437\n",
      "Iteration 4497, loss = 0.04700203\n",
      "Iteration 4498, loss = 0.04699066\n",
      "Iteration 4499, loss = 0.04698528\n",
      "Iteration 4500, loss = 0.04697624\n",
      "Iteration 4501, loss = 0.04696738\n",
      "Iteration 4502, loss = 0.04695684\n",
      "Iteration 4503, loss = 0.04694810\n",
      "Iteration 4504, loss = 0.04694055\n",
      "Iteration 4505, loss = 0.04693344\n",
      "Iteration 4506, loss = 0.04692516\n",
      "Iteration 4507, loss = 0.04691495\n",
      "Iteration 4508, loss = 0.04690859\n",
      "Iteration 4509, loss = 0.04689856\n",
      "Iteration 4510, loss = 0.04689178\n",
      "Iteration 4511, loss = 0.04688246\n",
      "Iteration 4512, loss = 0.04687220\n",
      "Iteration 4513, loss = 0.04686496\n",
      "Iteration 4514, loss = 0.04685641\n",
      "Iteration 4515, loss = 0.04684865\n",
      "Iteration 4516, loss = 0.04684150\n",
      "Iteration 4517, loss = 0.04682888\n",
      "Iteration 4518, loss = 0.04682207\n",
      "Iteration 4519, loss = 0.04681692\n",
      "Iteration 4520, loss = 0.04680460\n",
      "Iteration 4521, loss = 0.04679756\n",
      "Iteration 4522, loss = 0.04678910\n",
      "Iteration 4523, loss = 0.04678376\n",
      "Iteration 4524, loss = 0.04677717\n",
      "Iteration 4525, loss = 0.04676538\n",
      "Iteration 4526, loss = 0.04675534\n",
      "Iteration 4527, loss = 0.04674539\n",
      "Iteration 4528, loss = 0.04674263\n",
      "Iteration 4529, loss = 0.04673586\n",
      "Iteration 4530, loss = 0.04672743\n",
      "Iteration 4531, loss = 0.04671521\n",
      "Iteration 4532, loss = 0.04670506\n",
      "Iteration 4533, loss = 0.04669560\n",
      "Iteration 4534, loss = 0.04669000\n",
      "Iteration 4535, loss = 0.04668346\n",
      "Iteration 4536, loss = 0.04667263\n",
      "Iteration 4537, loss = 0.04665926\n",
      "Iteration 4538, loss = 0.04665069\n",
      "Iteration 4539, loss = 0.04663977\n",
      "Iteration 4540, loss = 0.04663519\n",
      "Iteration 4541, loss = 0.04662775\n",
      "Iteration 4542, loss = 0.04661772\n",
      "Iteration 4543, loss = 0.04660541\n",
      "Iteration 4544, loss = 0.04659663\n",
      "Iteration 4545, loss = 0.04659314\n",
      "Iteration 4546, loss = 0.04658357\n",
      "Iteration 4547, loss = 0.04657478\n",
      "Iteration 4548, loss = 0.04656709\n",
      "Iteration 4549, loss = 0.04655944\n",
      "Iteration 4550, loss = 0.04654758\n",
      "Iteration 4551, loss = 0.04653837\n",
      "Iteration 4552, loss = 0.04653384\n",
      "Iteration 4553, loss = 0.04652442\n",
      "Iteration 4554, loss = 0.04651153\n",
      "Iteration 4555, loss = 0.04650662\n",
      "Iteration 4556, loss = 0.04649572\n",
      "Iteration 4557, loss = 0.04648678\n",
      "Iteration 4558, loss = 0.04648002\n",
      "Iteration 4559, loss = 0.04646983\n",
      "Iteration 4560, loss = 0.04645839\n",
      "Iteration 4561, loss = 0.04645031\n",
      "Iteration 4562, loss = 0.04644705\n",
      "Iteration 4563, loss = 0.04643792\n",
      "Iteration 4564, loss = 0.04642767\n",
      "Iteration 4565, loss = 0.04641992\n",
      "Iteration 4566, loss = 0.04641169\n",
      "Iteration 4567, loss = 0.04640406\n",
      "Iteration 4568, loss = 0.04639474\n",
      "Iteration 4569, loss = 0.04638602\n",
      "Iteration 4570, loss = 0.04637627\n",
      "Iteration 4571, loss = 0.04636746\n",
      "Iteration 4572, loss = 0.04635634\n",
      "Iteration 4573, loss = 0.04635244\n",
      "Iteration 4574, loss = 0.04634511\n",
      "Iteration 4575, loss = 0.04633553\n",
      "Iteration 4576, loss = 0.04632410\n",
      "Iteration 4577, loss = 0.04631747\n",
      "Iteration 4578, loss = 0.04631229\n",
      "Iteration 4579, loss = 0.04630306\n",
      "Iteration 4580, loss = 0.04629611\n",
      "Iteration 4581, loss = 0.04628611\n",
      "Iteration 4582, loss = 0.04628115\n",
      "Iteration 4583, loss = 0.04627154\n",
      "Iteration 4584, loss = 0.04625969\n",
      "Iteration 4585, loss = 0.04624827\n",
      "Iteration 4586, loss = 0.04624258\n",
      "Iteration 4587, loss = 0.04623567\n",
      "Iteration 4588, loss = 0.04622806\n",
      "Iteration 4589, loss = 0.04621672\n",
      "Iteration 4590, loss = 0.04620997\n",
      "Iteration 4591, loss = 0.04620116\n",
      "Iteration 4592, loss = 0.04619057\n",
      "Iteration 4593, loss = 0.04617900\n",
      "Iteration 4594, loss = 0.04617395\n",
      "Iteration 4595, loss = 0.04616714\n",
      "Iteration 4596, loss = 0.04615390\n",
      "Iteration 4597, loss = 0.04614606\n",
      "Iteration 4598, loss = 0.04613901\n",
      "Iteration 4599, loss = 0.04613025\n",
      "Iteration 4600, loss = 0.04612281\n",
      "Iteration 4601, loss = 0.04611177\n",
      "Iteration 4602, loss = 0.04610472\n",
      "Iteration 4603, loss = 0.04609750\n",
      "Iteration 4604, loss = 0.04608814\n",
      "Iteration 4605, loss = 0.04607930\n",
      "Iteration 4606, loss = 0.04606991\n",
      "Iteration 4607, loss = 0.04606389\n",
      "Iteration 4608, loss = 0.04605556\n",
      "Iteration 4609, loss = 0.04604496\n",
      "Iteration 4610, loss = 0.04603552\n",
      "Iteration 4611, loss = 0.04602567\n",
      "Iteration 4612, loss = 0.04601328\n",
      "Iteration 4613, loss = 0.04600955\n",
      "Iteration 4614, loss = 0.04599769\n",
      "Iteration 4615, loss = 0.04598737\n",
      "Iteration 4616, loss = 0.04597759\n",
      "Iteration 4617, loss = 0.04596834\n",
      "Iteration 4618, loss = 0.04595621\n",
      "Iteration 4619, loss = 0.04594644\n",
      "Iteration 4620, loss = 0.04593793\n",
      "Iteration 4621, loss = 0.04592586\n",
      "Iteration 4622, loss = 0.04591437\n",
      "Iteration 4623, loss = 0.04590696\n",
      "Iteration 4624, loss = 0.04589599\n",
      "Iteration 4625, loss = 0.04588482\n",
      "Iteration 4626, loss = 0.04587435\n",
      "Iteration 4627, loss = 0.04586290\n",
      "Iteration 4628, loss = 0.04585483\n",
      "Iteration 4629, loss = 0.04584711\n",
      "Iteration 4630, loss = 0.04583425\n",
      "Iteration 4631, loss = 0.04582201\n",
      "Iteration 4632, loss = 0.04581578\n",
      "Iteration 4633, loss = 0.04580575\n",
      "Iteration 4634, loss = 0.04579424\n",
      "Iteration 4635, loss = 0.04578282\n",
      "Iteration 4636, loss = 0.04577291\n",
      "Iteration 4637, loss = 0.04575951\n",
      "Iteration 4638, loss = 0.04574982\n",
      "Iteration 4639, loss = 0.04574093\n",
      "Iteration 4640, loss = 0.04572940\n",
      "Iteration 4641, loss = 0.04571629\n",
      "Iteration 4642, loss = 0.04571121\n",
      "Iteration 4643, loss = 0.04570118\n",
      "Iteration 4644, loss = 0.04568756\n",
      "Iteration 4645, loss = 0.04567797\n",
      "Iteration 4646, loss = 0.04567057\n",
      "Iteration 4647, loss = 0.04566063\n",
      "Iteration 4648, loss = 0.04564797\n",
      "Iteration 4649, loss = 0.04563889\n",
      "Iteration 4650, loss = 0.04562798\n",
      "Iteration 4651, loss = 0.04561687\n",
      "Iteration 4652, loss = 0.04560757\n",
      "Iteration 4653, loss = 0.04559659\n",
      "Iteration 4654, loss = 0.04558675\n",
      "Iteration 4655, loss = 0.04558034\n",
      "Iteration 4656, loss = 0.04557021\n",
      "Iteration 4657, loss = 0.04556102\n",
      "Iteration 4658, loss = 0.04555000\n",
      "Iteration 4659, loss = 0.04554263\n",
      "Iteration 4660, loss = 0.04553592\n",
      "Iteration 4661, loss = 0.04552460\n",
      "Iteration 4662, loss = 0.04551933\n",
      "Iteration 4663, loss = 0.04551361\n",
      "Iteration 4664, loss = 0.04550754\n",
      "Iteration 4665, loss = 0.04549770\n",
      "Iteration 4666, loss = 0.04548773\n",
      "Iteration 4667, loss = 0.04547680\n",
      "Iteration 4668, loss = 0.04546980\n",
      "Iteration 4669, loss = 0.04546336\n",
      "Iteration 4670, loss = 0.04545140\n",
      "Iteration 4671, loss = 0.04543963\n",
      "Iteration 4672, loss = 0.04543390\n",
      "Iteration 4673, loss = 0.04542611\n",
      "Iteration 4674, loss = 0.04541505\n",
      "Iteration 4675, loss = 0.04540535\n",
      "Iteration 4676, loss = 0.04539793\n",
      "Iteration 4677, loss = 0.04538974\n",
      "Iteration 4678, loss = 0.04538226\n",
      "Iteration 4679, loss = 0.04537394\n",
      "Iteration 4680, loss = 0.04536325\n",
      "Iteration 4681, loss = 0.04535631\n",
      "Iteration 4682, loss = 0.04534819\n",
      "Iteration 4683, loss = 0.04533768\n",
      "Iteration 4684, loss = 0.04533117\n",
      "Iteration 4685, loss = 0.04532214\n",
      "Iteration 4686, loss = 0.04531564\n",
      "Iteration 4687, loss = 0.04530613\n",
      "Iteration 4688, loss = 0.04529749\n",
      "Iteration 4689, loss = 0.04528903\n",
      "Iteration 4690, loss = 0.04528035\n",
      "Iteration 4691, loss = 0.04527349\n",
      "Iteration 4692, loss = 0.04526445\n",
      "Iteration 4693, loss = 0.04525584\n",
      "Iteration 4694, loss = 0.04524575\n",
      "Iteration 4695, loss = 0.04523721\n",
      "Iteration 4696, loss = 0.04522981\n",
      "Iteration 4697, loss = 0.04522043\n",
      "Iteration 4698, loss = 0.04521080\n",
      "Iteration 4699, loss = 0.04520586\n",
      "Iteration 4700, loss = 0.04519820\n",
      "Iteration 4701, loss = 0.04518725\n",
      "Iteration 4702, loss = 0.04517926\n",
      "Iteration 4703, loss = 0.04517265\n",
      "Iteration 4704, loss = 0.04516489\n",
      "Iteration 4705, loss = 0.04515445\n",
      "Iteration 4706, loss = 0.04514540\n",
      "Iteration 4707, loss = 0.04513477\n",
      "Iteration 4708, loss = 0.04512844\n",
      "Iteration 4709, loss = 0.04512323\n",
      "Iteration 4710, loss = 0.04511296\n",
      "Iteration 4711, loss = 0.04510107\n",
      "Iteration 4712, loss = 0.04509729\n",
      "Iteration 4713, loss = 0.04509202\n",
      "Iteration 4714, loss = 0.04508243\n",
      "Iteration 4715, loss = 0.04507601\n",
      "Iteration 4716, loss = 0.04506707\n",
      "Iteration 4717, loss = 0.04505610\n",
      "Iteration 4718, loss = 0.04504853\n",
      "Iteration 4719, loss = 0.04504192\n",
      "Iteration 4720, loss = 0.04502958\n",
      "Iteration 4721, loss = 0.04502064\n",
      "Iteration 4722, loss = 0.04501179\n",
      "Iteration 4723, loss = 0.04500878\n",
      "Iteration 4724, loss = 0.04500092\n",
      "Iteration 4725, loss = 0.04498998\n",
      "Iteration 4726, loss = 0.04498019\n",
      "Iteration 4727, loss = 0.04497188\n",
      "Iteration 4728, loss = 0.04496467\n",
      "Iteration 4729, loss = 0.04495890\n",
      "Iteration 4730, loss = 0.04495018\n",
      "Iteration 4731, loss = 0.04494095\n",
      "Iteration 4732, loss = 0.04493179\n",
      "Iteration 4733, loss = 0.04492131\n",
      "Iteration 4734, loss = 0.04490972\n",
      "Iteration 4735, loss = 0.04489964\n",
      "Iteration 4736, loss = 0.04489104\n",
      "Iteration 4737, loss = 0.04488211\n",
      "Iteration 4738, loss = 0.04487438\n",
      "Iteration 4739, loss = 0.04486792\n",
      "Iteration 4740, loss = 0.04486182\n",
      "Iteration 4741, loss = 0.04485268\n",
      "Iteration 4742, loss = 0.04484216\n",
      "Iteration 4743, loss = 0.04483426\n",
      "Iteration 4744, loss = 0.04482673\n",
      "Iteration 4745, loss = 0.04481948\n",
      "Iteration 4746, loss = 0.04480867\n",
      "Iteration 4747, loss = 0.04479859\n",
      "Iteration 4748, loss = 0.04479191\n",
      "Iteration 4749, loss = 0.04478512\n",
      "Iteration 4750, loss = 0.04477670\n",
      "Iteration 4751, loss = 0.04476626\n",
      "Iteration 4752, loss = 0.04475733\n",
      "Iteration 4753, loss = 0.04474931\n",
      "Iteration 4754, loss = 0.04474104\n",
      "Iteration 4755, loss = 0.04473159\n",
      "Iteration 4756, loss = 0.04472352\n",
      "Iteration 4757, loss = 0.04471484\n",
      "Iteration 4758, loss = 0.04470747\n",
      "Iteration 4759, loss = 0.04469945\n",
      "Iteration 4760, loss = 0.04468894\n",
      "Iteration 4761, loss = 0.04468245\n",
      "Iteration 4762, loss = 0.04467300\n",
      "Iteration 4763, loss = 0.04466587\n",
      "Iteration 4764, loss = 0.04466002\n",
      "Iteration 4765, loss = 0.04465155\n",
      "Iteration 4766, loss = 0.04464059\n",
      "Iteration 4767, loss = 0.04463087\n",
      "Iteration 4768, loss = 0.04462193\n",
      "Iteration 4769, loss = 0.04461517\n",
      "Iteration 4770, loss = 0.04460529\n",
      "Iteration 4771, loss = 0.04459461\n",
      "Iteration 4772, loss = 0.04458729\n",
      "Iteration 4773, loss = 0.04457918\n",
      "Iteration 4774, loss = 0.04457155\n",
      "Iteration 4775, loss = 0.04456089\n",
      "Iteration 4776, loss = 0.04455478\n",
      "Iteration 4777, loss = 0.04454630\n",
      "Iteration 4778, loss = 0.04453682\n",
      "Iteration 4779, loss = 0.04452902\n",
      "Iteration 4780, loss = 0.04451963\n",
      "Iteration 4781, loss = 0.04450978\n",
      "Iteration 4782, loss = 0.04450056\n",
      "Iteration 4783, loss = 0.04449340\n",
      "Iteration 4784, loss = 0.04448656\n",
      "Iteration 4785, loss = 0.04447588\n",
      "Iteration 4786, loss = 0.04446827\n",
      "Iteration 4787, loss = 0.04446173\n",
      "Iteration 4788, loss = 0.04445390\n",
      "Iteration 4789, loss = 0.04444783\n",
      "Iteration 4790, loss = 0.04443976\n",
      "Iteration 4791, loss = 0.04442995\n",
      "Iteration 4792, loss = 0.04441942\n",
      "Iteration 4793, loss = 0.04441158\n",
      "Iteration 4794, loss = 0.04440372\n",
      "Iteration 4795, loss = 0.04439455\n",
      "Iteration 4796, loss = 0.04438238\n",
      "Iteration 4797, loss = 0.04437780\n",
      "Iteration 4798, loss = 0.04437004\n",
      "Iteration 4799, loss = 0.04436078\n",
      "Iteration 4800, loss = 0.04435447\n",
      "Iteration 4801, loss = 0.04434374\n",
      "Iteration 4802, loss = 0.04433411\n",
      "Iteration 4803, loss = 0.04432715\n",
      "Iteration 4804, loss = 0.04431831\n",
      "Iteration 4805, loss = 0.04430726\n",
      "Iteration 4806, loss = 0.04430050\n",
      "Iteration 4807, loss = 0.04429078\n",
      "Iteration 4808, loss = 0.04428487\n",
      "Iteration 4809, loss = 0.04427436\n",
      "Iteration 4810, loss = 0.04426563\n",
      "Iteration 4811, loss = 0.04425940\n",
      "Iteration 4812, loss = 0.04424889\n",
      "Iteration 4813, loss = 0.04423734\n",
      "Iteration 4814, loss = 0.04423130\n",
      "Iteration 4815, loss = 0.04422413\n",
      "Iteration 4816, loss = 0.04421516\n",
      "Iteration 4817, loss = 0.04420604\n",
      "Iteration 4818, loss = 0.04419567\n",
      "Iteration 4819, loss = 0.04418941\n",
      "Iteration 4820, loss = 0.04418236\n",
      "Iteration 4821, loss = 0.04417226\n",
      "Iteration 4822, loss = 0.04416301\n",
      "Iteration 4823, loss = 0.04415419\n",
      "Iteration 4824, loss = 0.04414415\n",
      "Iteration 4825, loss = 0.04413629\n",
      "Iteration 4826, loss = 0.04412881\n",
      "Iteration 4827, loss = 0.04411916\n",
      "Iteration 4828, loss = 0.04411132\n",
      "Iteration 4829, loss = 0.04410497\n",
      "Iteration 4830, loss = 0.04409803\n",
      "Iteration 4831, loss = 0.04408898\n",
      "Iteration 4832, loss = 0.04407810\n",
      "Iteration 4833, loss = 0.04406863\n",
      "Iteration 4834, loss = 0.04406042\n",
      "Iteration 4835, loss = 0.04405227\n",
      "Iteration 4836, loss = 0.04404281\n",
      "Iteration 4837, loss = 0.04403294\n",
      "Iteration 4838, loss = 0.04402632\n",
      "Iteration 4839, loss = 0.04402038\n",
      "Iteration 4840, loss = 0.04401009\n",
      "Iteration 4841, loss = 0.04399850\n",
      "Iteration 4842, loss = 0.04399378\n",
      "Iteration 4843, loss = 0.04398649\n",
      "Iteration 4844, loss = 0.04397671\n",
      "Iteration 4845, loss = 0.04396577\n",
      "Iteration 4846, loss = 0.04395809\n",
      "Iteration 4847, loss = 0.04395052\n",
      "Iteration 4848, loss = 0.04394057\n",
      "Iteration 4849, loss = 0.04393148\n",
      "Iteration 4850, loss = 0.04392348\n",
      "Iteration 4851, loss = 0.04391515\n",
      "Iteration 4852, loss = 0.04390614\n",
      "Iteration 4853, loss = 0.04389740\n",
      "Iteration 4854, loss = 0.04389016\n",
      "Iteration 4855, loss = 0.04388180\n",
      "Iteration 4856, loss = 0.04387428\n",
      "Iteration 4857, loss = 0.04386235\n",
      "Iteration 4858, loss = 0.04385624\n",
      "Iteration 4859, loss = 0.04385029\n",
      "Iteration 4860, loss = 0.04383930\n",
      "Iteration 4861, loss = 0.04383058\n",
      "Iteration 4862, loss = 0.04382213\n",
      "Iteration 4863, loss = 0.04381545\n",
      "Iteration 4864, loss = 0.04380854\n",
      "Iteration 4865, loss = 0.04379785\n",
      "Iteration 4866, loss = 0.04378651\n",
      "Iteration 4867, loss = 0.04377841\n",
      "Iteration 4868, loss = 0.04377206\n",
      "Iteration 4869, loss = 0.04376045\n",
      "Iteration 4870, loss = 0.04375412\n",
      "Iteration 4871, loss = 0.04374641\n",
      "Iteration 4872, loss = 0.04373781\n",
      "Iteration 4873, loss = 0.04372919\n",
      "Iteration 4874, loss = 0.04372133\n",
      "Iteration 4875, loss = 0.04371089\n",
      "Iteration 4876, loss = 0.04370196\n",
      "Iteration 4877, loss = 0.04369433\n",
      "Iteration 4878, loss = 0.04368475\n",
      "Iteration 4879, loss = 0.04367910\n",
      "Iteration 4880, loss = 0.04367067\n",
      "Iteration 4881, loss = 0.04366165\n",
      "Iteration 4882, loss = 0.04365201\n",
      "Iteration 4883, loss = 0.04364374\n",
      "Iteration 4884, loss = 0.04363419\n",
      "Iteration 4885, loss = 0.04362248\n",
      "Iteration 4886, loss = 0.04361380\n",
      "Iteration 4887, loss = 0.04360407\n",
      "Iteration 4888, loss = 0.04359573\n",
      "Iteration 4889, loss = 0.04358602\n",
      "Iteration 4890, loss = 0.04357944\n",
      "Iteration 4891, loss = 0.04357095\n",
      "Iteration 4892, loss = 0.04356272\n",
      "Iteration 4893, loss = 0.04355541\n",
      "Iteration 4894, loss = 0.04354779\n",
      "Iteration 4895, loss = 0.04353871\n",
      "Iteration 4896, loss = 0.04352997\n",
      "Iteration 4897, loss = 0.04352054\n",
      "Iteration 4898, loss = 0.04351258\n",
      "Iteration 4899, loss = 0.04350233\n",
      "Iteration 4900, loss = 0.04349675\n",
      "Iteration 4901, loss = 0.04348838\n",
      "Iteration 4902, loss = 0.04347760\n",
      "Iteration 4903, loss = 0.04346960\n",
      "Iteration 4904, loss = 0.04346371\n",
      "Iteration 4905, loss = 0.04345369\n",
      "Iteration 4906, loss = 0.04344390\n",
      "Iteration 4907, loss = 0.04343974\n",
      "Iteration 4908, loss = 0.04343006\n",
      "Iteration 4909, loss = 0.04341855\n",
      "Iteration 4910, loss = 0.04341045\n",
      "Iteration 4911, loss = 0.04340141\n",
      "Iteration 4912, loss = 0.04339549\n",
      "Iteration 4913, loss = 0.04338685\n",
      "Iteration 4914, loss = 0.04337704\n",
      "Iteration 4915, loss = 0.04336580\n",
      "Iteration 4916, loss = 0.04335778\n",
      "Iteration 4917, loss = 0.04335333\n",
      "Iteration 4918, loss = 0.04334317\n",
      "Iteration 4919, loss = 0.04333337\n",
      "Iteration 4920, loss = 0.04332602\n",
      "Iteration 4921, loss = 0.04331651\n",
      "Iteration 4922, loss = 0.04330697\n",
      "Iteration 4923, loss = 0.04330005\n",
      "Iteration 4924, loss = 0.04329125\n",
      "Iteration 4925, loss = 0.04328028\n",
      "Iteration 4926, loss = 0.04327267\n",
      "Iteration 4927, loss = 0.04326873\n",
      "Iteration 4928, loss = 0.04326224\n",
      "Iteration 4929, loss = 0.04325287\n",
      "Iteration 4930, loss = 0.04324300\n",
      "Iteration 4931, loss = 0.04323383\n",
      "Iteration 4932, loss = 0.04322472\n",
      "Iteration 4933, loss = 0.04321600\n",
      "Iteration 4934, loss = 0.04321067\n",
      "Iteration 4935, loss = 0.04320202\n",
      "Iteration 4936, loss = 0.04319292\n",
      "Iteration 4937, loss = 0.04317803\n",
      "Iteration 4938, loss = 0.04317438\n",
      "Iteration 4939, loss = 0.04317083\n",
      "Iteration 4940, loss = 0.04316221\n",
      "Iteration 4941, loss = 0.04315419\n",
      "Iteration 4942, loss = 0.04314457\n",
      "Iteration 4943, loss = 0.04313506\n",
      "Iteration 4944, loss = 0.04312617\n",
      "Iteration 4945, loss = 0.04311457\n",
      "Iteration 4946, loss = 0.04310520\n",
      "Iteration 4947, loss = 0.04310142\n",
      "Iteration 4948, loss = 0.04309185\n",
      "Iteration 4949, loss = 0.04307983\n",
      "Iteration 4950, loss = 0.04306891\n",
      "Iteration 4951, loss = 0.04306265\n",
      "Iteration 4952, loss = 0.04305704\n",
      "Iteration 4953, loss = 0.04304814\n",
      "Iteration 4954, loss = 0.04303911\n",
      "Iteration 4955, loss = 0.04302861\n",
      "Iteration 4956, loss = 0.04301854\n",
      "Iteration 4957, loss = 0.04301026\n",
      "Iteration 4958, loss = 0.04299983\n",
      "Iteration 4959, loss = 0.04299286\n",
      "Iteration 4960, loss = 0.04298412\n",
      "Iteration 4961, loss = 0.04297404\n",
      "Iteration 4962, loss = 0.04296772\n",
      "Iteration 4963, loss = 0.04296025\n",
      "Iteration 4964, loss = 0.04295075\n",
      "Iteration 4965, loss = 0.04294131\n",
      "Iteration 4966, loss = 0.04293533\n",
      "Iteration 4967, loss = 0.04292496\n",
      "Iteration 4968, loss = 0.04291681\n",
      "Iteration 4969, loss = 0.04291165\n",
      "Iteration 4970, loss = 0.04290108\n",
      "Iteration 4971, loss = 0.04289108\n",
      "Iteration 4972, loss = 0.04288514\n",
      "Iteration 4973, loss = 0.04287729\n",
      "Iteration 4974, loss = 0.04286662\n",
      "Iteration 4975, loss = 0.04285755\n",
      "Iteration 4976, loss = 0.04284959\n",
      "Iteration 4977, loss = 0.04284252\n",
      "Iteration 4978, loss = 0.04283279\n",
      "Iteration 4979, loss = 0.04282416\n",
      "Iteration 4980, loss = 0.04281472\n",
      "Iteration 4981, loss = 0.04280354\n",
      "Iteration 4982, loss = 0.04279680\n",
      "Iteration 4983, loss = 0.04278968\n",
      "Iteration 4984, loss = 0.04278206\n",
      "Iteration 4985, loss = 0.04276862\n",
      "Iteration 4986, loss = 0.04276002\n",
      "Iteration 4987, loss = 0.04275219\n",
      "Iteration 4988, loss = 0.04274415\n",
      "Iteration 4989, loss = 0.04273475\n",
      "Iteration 4990, loss = 0.04272514\n",
      "Iteration 4991, loss = 0.04271811\n",
      "Iteration 4992, loss = 0.04270942\n",
      "Iteration 4993, loss = 0.04270041\n",
      "Iteration 4994, loss = 0.04269227\n",
      "Iteration 4995, loss = 0.04268562\n",
      "Iteration 4996, loss = 0.04267852\n",
      "Iteration 4997, loss = 0.04266781\n",
      "Iteration 4998, loss = 0.04265873\n",
      "Iteration 4999, loss = 0.04265058\n",
      "Iteration 5000, loss = 0.04264217\n",
      "Iteration 5001, loss = 0.04263326\n",
      "Iteration 5002, loss = 0.04262482\n",
      "Iteration 5003, loss = 0.04261577\n",
      "Iteration 5004, loss = 0.04260642\n",
      "Iteration 5005, loss = 0.04259893\n",
      "Iteration 5006, loss = 0.04259012\n",
      "Iteration 5007, loss = 0.04258106\n",
      "Iteration 5008, loss = 0.04257425\n",
      "Iteration 5009, loss = 0.04256475\n",
      "Iteration 5010, loss = 0.04255854\n",
      "Iteration 5011, loss = 0.04255050\n",
      "Iteration 5012, loss = 0.04253944\n",
      "Iteration 5013, loss = 0.04253042\n",
      "Iteration 5014, loss = 0.04252140\n",
      "Iteration 5015, loss = 0.04251825\n",
      "Iteration 5016, loss = 0.04251022\n",
      "Iteration 5017, loss = 0.04249878\n",
      "Iteration 5018, loss = 0.04248572\n",
      "Iteration 5019, loss = 0.04248085\n",
      "Iteration 5020, loss = 0.04247423\n",
      "Iteration 5021, loss = 0.04246516\n",
      "Iteration 5022, loss = 0.04245320\n",
      "Iteration 5023, loss = 0.04244667\n",
      "Iteration 5024, loss = 0.04243876\n",
      "Iteration 5025, loss = 0.04242807\n",
      "Iteration 5026, loss = 0.04241847\n",
      "Iteration 5027, loss = 0.04241226\n",
      "Iteration 5028, loss = 0.04240204\n",
      "Iteration 5029, loss = 0.04239096\n",
      "Iteration 5030, loss = 0.04238440\n",
      "Iteration 5031, loss = 0.04237749\n",
      "Iteration 5032, loss = 0.04236748\n",
      "Iteration 5033, loss = 0.04235601\n",
      "Iteration 5034, loss = 0.04234765\n",
      "Iteration 5035, loss = 0.04234023\n",
      "Iteration 5036, loss = 0.04233189\n",
      "Iteration 5037, loss = 0.04232439\n",
      "Iteration 5038, loss = 0.04231509\n",
      "Iteration 5039, loss = 0.04230562\n",
      "Iteration 5040, loss = 0.04229580\n",
      "Iteration 5041, loss = 0.04228676\n",
      "Iteration 5042, loss = 0.04227767\n",
      "Iteration 5043, loss = 0.04227255\n",
      "Iteration 5044, loss = 0.04226502\n",
      "Iteration 5045, loss = 0.04225353\n",
      "Iteration 5046, loss = 0.04224365\n",
      "Iteration 5047, loss = 0.04223564\n",
      "Iteration 5048, loss = 0.04222788\n",
      "Iteration 5049, loss = 0.04221895\n",
      "Iteration 5050, loss = 0.04220806\n",
      "Iteration 5051, loss = 0.04219910\n",
      "Iteration 5052, loss = 0.04219111\n",
      "Iteration 5053, loss = 0.04218477\n",
      "Iteration 5054, loss = 0.04217528\n",
      "Iteration 5055, loss = 0.04216639\n",
      "Iteration 5056, loss = 0.04215792\n",
      "Iteration 5057, loss = 0.04214886\n",
      "Iteration 5058, loss = 0.04213909\n",
      "Iteration 5059, loss = 0.04213022\n",
      "Iteration 5060, loss = 0.04212355\n",
      "Iteration 5061, loss = 0.04211451\n",
      "Iteration 5062, loss = 0.04210341\n",
      "Iteration 5063, loss = 0.04209179\n",
      "Iteration 5064, loss = 0.04208596\n",
      "Iteration 5065, loss = 0.04207772\n",
      "Iteration 5066, loss = 0.04206662\n",
      "Iteration 5067, loss = 0.04205840\n",
      "Iteration 5068, loss = 0.04205077\n",
      "Iteration 5069, loss = 0.04204159\n",
      "Iteration 5070, loss = 0.04203404\n",
      "Iteration 5071, loss = 0.04202362\n",
      "Iteration 5072, loss = 0.04201603\n",
      "Iteration 5073, loss = 0.04201027\n",
      "Iteration 5074, loss = 0.04200117\n",
      "Iteration 5075, loss = 0.04198967\n",
      "Iteration 5076, loss = 0.04198137\n",
      "Iteration 5077, loss = 0.04197409\n",
      "Iteration 5078, loss = 0.04196559\n",
      "Iteration 5079, loss = 0.04195466\n",
      "Iteration 5080, loss = 0.04194691\n",
      "Iteration 5081, loss = 0.04194104\n",
      "Iteration 5082, loss = 0.04193037\n",
      "Iteration 5083, loss = 0.04192096\n",
      "Iteration 5084, loss = 0.04191411\n",
      "Iteration 5085, loss = 0.04190737\n",
      "Iteration 5086, loss = 0.04189842\n",
      "Iteration 5087, loss = 0.04188914\n",
      "Iteration 5088, loss = 0.04187962\n",
      "Iteration 5089, loss = 0.04186947\n",
      "Iteration 5090, loss = 0.04186250\n",
      "Iteration 5091, loss = 0.04185415\n",
      "Iteration 5092, loss = 0.04184568\n",
      "Iteration 5093, loss = 0.04183457\n",
      "Iteration 5094, loss = 0.04182469\n",
      "Iteration 5095, loss = 0.04181327\n",
      "Iteration 5096, loss = 0.04180821\n",
      "Iteration 5097, loss = 0.04180028\n",
      "Iteration 5098, loss = 0.04179096\n",
      "Iteration 5099, loss = 0.04178081\n",
      "Iteration 5100, loss = 0.04177118\n",
      "Iteration 5101, loss = 0.04176494\n",
      "Iteration 5102, loss = 0.04175690\n",
      "Iteration 5103, loss = 0.04174690\n",
      "Iteration 5104, loss = 0.04173795\n",
      "Iteration 5105, loss = 0.04172663\n",
      "Iteration 5106, loss = 0.04171727\n",
      "Iteration 5107, loss = 0.04171267\n",
      "Iteration 5108, loss = 0.04170542\n",
      "Iteration 5109, loss = 0.04169482\n",
      "Iteration 5110, loss = 0.04168557\n",
      "Iteration 5111, loss = 0.04167366\n",
      "Iteration 5112, loss = 0.04166752\n",
      "Iteration 5113, loss = 0.04166107\n",
      "Iteration 5114, loss = 0.04165062\n",
      "Iteration 5115, loss = 0.04164063\n",
      "Iteration 5116, loss = 0.04163040\n",
      "Iteration 5117, loss = 0.04162408\n",
      "Iteration 5118, loss = 0.04161507\n",
      "Iteration 5119, loss = 0.04160415\n",
      "Iteration 5120, loss = 0.04159568\n",
      "Iteration 5121, loss = 0.04158831\n",
      "Iteration 5122, loss = 0.04158020\n",
      "Iteration 5123, loss = 0.04157282\n",
      "Iteration 5124, loss = 0.04156329\n",
      "Iteration 5125, loss = 0.04155298\n",
      "Iteration 5126, loss = 0.04154271\n",
      "Iteration 5127, loss = 0.04153474\n",
      "Iteration 5128, loss = 0.04152543\n",
      "Iteration 5129, loss = 0.04151694\n",
      "Iteration 5130, loss = 0.04150489\n",
      "Iteration 5131, loss = 0.04149790\n",
      "Iteration 5132, loss = 0.04148886\n",
      "Iteration 5133, loss = 0.04148140\n",
      "Iteration 5134, loss = 0.04147138\n",
      "Iteration 5135, loss = 0.04146274\n",
      "Iteration 5136, loss = 0.04145266\n",
      "Iteration 5137, loss = 0.04144130\n",
      "Iteration 5138, loss = 0.04143437\n",
      "Iteration 5139, loss = 0.04142609\n",
      "Iteration 5140, loss = 0.04141728\n",
      "Iteration 5141, loss = 0.04140634\n",
      "Iteration 5142, loss = 0.04139697\n",
      "Iteration 5143, loss = 0.04138770\n",
      "Iteration 5144, loss = 0.04137811\n",
      "Iteration 5145, loss = 0.04137167\n",
      "Iteration 5146, loss = 0.04136369\n",
      "Iteration 5147, loss = 0.04135405\n",
      "Iteration 5148, loss = 0.04134526\n",
      "Iteration 5149, loss = 0.04133696\n",
      "Iteration 5150, loss = 0.04132727\n",
      "Iteration 5151, loss = 0.04131944\n",
      "Iteration 5152, loss = 0.04131220\n",
      "Iteration 5153, loss = 0.04130099\n",
      "Iteration 5154, loss = 0.04129257\n",
      "Iteration 5155, loss = 0.04128608\n",
      "Iteration 5156, loss = 0.04127892\n",
      "Iteration 5157, loss = 0.04126907\n",
      "Iteration 5158, loss = 0.04125758\n",
      "Iteration 5159, loss = 0.04124858\n",
      "Iteration 5160, loss = 0.04123860\n",
      "Iteration 5161, loss = 0.04123031\n",
      "Iteration 5162, loss = 0.04121927\n",
      "Iteration 5163, loss = 0.04120851\n",
      "Iteration 5164, loss = 0.04120213\n",
      "Iteration 5165, loss = 0.04119372\n",
      "Iteration 5166, loss = 0.04118436\n",
      "Iteration 5167, loss = 0.04117411\n",
      "Iteration 5168, loss = 0.04116479\n",
      "Iteration 5169, loss = 0.04115851\n",
      "Iteration 5170, loss = 0.04115092\n",
      "Iteration 5171, loss = 0.04114063\n",
      "Iteration 5172, loss = 0.04113152\n",
      "Iteration 5173, loss = 0.04112173\n",
      "Iteration 5174, loss = 0.04111095\n",
      "Iteration 5175, loss = 0.04110362\n",
      "Iteration 5176, loss = 0.04109255\n",
      "Iteration 5177, loss = 0.04108303\n",
      "Iteration 5178, loss = 0.04107489\n",
      "Iteration 5179, loss = 0.04106470\n",
      "Iteration 5180, loss = 0.04105642\n",
      "Iteration 5181, loss = 0.04104838\n",
      "Iteration 5182, loss = 0.04103778\n",
      "Iteration 5183, loss = 0.04102786\n",
      "Iteration 5184, loss = 0.04101860\n",
      "Iteration 5185, loss = 0.04101278\n",
      "Iteration 5186, loss = 0.04100039\n",
      "Iteration 5187, loss = 0.04099320\n",
      "Iteration 5188, loss = 0.04098698\n",
      "Iteration 5189, loss = 0.04097933\n",
      "Iteration 5190, loss = 0.04096936\n",
      "Iteration 5191, loss = 0.04095914\n",
      "Iteration 5192, loss = 0.04094767\n",
      "Iteration 5193, loss = 0.04094164\n",
      "Iteration 5194, loss = 0.04093316\n",
      "Iteration 5195, loss = 0.04092382\n",
      "Iteration 5196, loss = 0.04091295\n",
      "Iteration 5197, loss = 0.04090330\n",
      "Iteration 5198, loss = 0.04089446\n",
      "Iteration 5199, loss = 0.04088724\n",
      "Iteration 5200, loss = 0.04087776\n",
      "Iteration 5201, loss = 0.04086747\n",
      "Iteration 5202, loss = 0.04085805\n",
      "Iteration 5203, loss = 0.04084734\n",
      "Iteration 5204, loss = 0.04084290\n",
      "Iteration 5205, loss = 0.04083580\n",
      "Iteration 5206, loss = 0.04082465\n",
      "Iteration 5207, loss = 0.04081595\n",
      "Iteration 5208, loss = 0.04080703\n",
      "Iteration 5209, loss = 0.04079872\n",
      "Iteration 5210, loss = 0.04078836\n",
      "Iteration 5211, loss = 0.04077745\n",
      "Iteration 5212, loss = 0.04076565\n",
      "Iteration 5213, loss = 0.04075653\n",
      "Iteration 5214, loss = 0.04074797\n",
      "Iteration 5215, loss = 0.04073862\n",
      "Iteration 5216, loss = 0.04073064\n",
      "Iteration 5217, loss = 0.04072134\n",
      "Iteration 5218, loss = 0.04071280\n",
      "Iteration 5219, loss = 0.04070190\n",
      "Iteration 5220, loss = 0.04069698\n",
      "Iteration 5221, loss = 0.04068820\n",
      "Iteration 5222, loss = 0.04067502\n",
      "Iteration 5223, loss = 0.04066464\n",
      "Iteration 5224, loss = 0.04065758\n",
      "Iteration 5225, loss = 0.04064796\n",
      "Iteration 5226, loss = 0.04063665\n",
      "Iteration 5227, loss = 0.04062939\n",
      "Iteration 5228, loss = 0.04062110\n",
      "Iteration 5229, loss = 0.04061112\n",
      "Iteration 5230, loss = 0.04060116\n",
      "Iteration 5231, loss = 0.04059175\n",
      "Iteration 5232, loss = 0.04058333\n",
      "Iteration 5233, loss = 0.04057569\n",
      "Iteration 5234, loss = 0.04056308\n",
      "Iteration 5235, loss = 0.04055643\n",
      "Iteration 5236, loss = 0.04054731\n",
      "Iteration 5237, loss = 0.04053737\n",
      "Iteration 5238, loss = 0.04053058\n",
      "Iteration 5239, loss = 0.04052055\n",
      "Iteration 5240, loss = 0.04051240\n",
      "Iteration 5241, loss = 0.04050246\n",
      "Iteration 5242, loss = 0.04049302\n",
      "Iteration 5243, loss = 0.04048313\n",
      "Iteration 5244, loss = 0.04047708\n",
      "Iteration 5245, loss = 0.04046777\n",
      "Iteration 5246, loss = 0.04045422\n",
      "Iteration 5247, loss = 0.04044566\n",
      "Iteration 5248, loss = 0.04043783\n",
      "Iteration 5249, loss = 0.04042926\n",
      "Iteration 5250, loss = 0.04041991\n",
      "Iteration 5251, loss = 0.04040838\n",
      "Iteration 5252, loss = 0.04039708\n",
      "Iteration 5253, loss = 0.04039037\n",
      "Iteration 5254, loss = 0.04038357\n",
      "Iteration 5255, loss = 0.04037171\n",
      "Iteration 5256, loss = 0.04036126\n",
      "Iteration 5257, loss = 0.04035312\n",
      "Iteration 5258, loss = 0.04034331\n",
      "Iteration 5259, loss = 0.04033593\n",
      "Iteration 5260, loss = 0.04032613\n",
      "Iteration 5261, loss = 0.04031569\n",
      "Iteration 5262, loss = 0.04030461\n",
      "Iteration 5263, loss = 0.04029898\n",
      "Iteration 5264, loss = 0.04029055\n",
      "Iteration 5265, loss = 0.04028118\n",
      "Iteration 5266, loss = 0.04026901\n",
      "Iteration 5267, loss = 0.04025834\n",
      "Iteration 5268, loss = 0.04025168\n",
      "Iteration 5269, loss = 0.04024228\n",
      "Iteration 5270, loss = 0.04023108\n",
      "Iteration 5271, loss = 0.04022049\n",
      "Iteration 5272, loss = 0.04021400\n",
      "Iteration 5273, loss = 0.04020364\n",
      "Iteration 5274, loss = 0.04019278\n",
      "Iteration 5275, loss = 0.04018515\n",
      "Iteration 5276, loss = 0.04017698\n",
      "Iteration 5277, loss = 0.04016721\n",
      "Iteration 5278, loss = 0.04015965\n",
      "Iteration 5279, loss = 0.04015237\n",
      "Iteration 5280, loss = 0.04014165\n",
      "Iteration 5281, loss = 0.04013315\n",
      "Iteration 5282, loss = 0.04012243\n",
      "Iteration 5283, loss = 0.04011274\n",
      "Iteration 5284, loss = 0.04010071\n",
      "Iteration 5285, loss = 0.04009035\n",
      "Iteration 5286, loss = 0.04008400\n",
      "Iteration 5287, loss = 0.04007547\n",
      "Iteration 5288, loss = 0.04006653\n",
      "Iteration 5289, loss = 0.04005589\n",
      "Iteration 5290, loss = 0.04004350\n",
      "Iteration 5291, loss = 0.04003684\n",
      "Iteration 5292, loss = 0.04002914\n",
      "Iteration 5293, loss = 0.04001734\n",
      "Iteration 5294, loss = 0.04000626\n",
      "Iteration 5295, loss = 0.04000144\n",
      "Iteration 5296, loss = 0.03999479\n",
      "Iteration 5297, loss = 0.03998525\n",
      "Iteration 5298, loss = 0.03997346\n",
      "Iteration 5299, loss = 0.03996240\n",
      "Iteration 5300, loss = 0.03995072\n",
      "Iteration 5301, loss = 0.03994263\n",
      "Iteration 5302, loss = 0.03993470\n",
      "Iteration 5303, loss = 0.03992437\n",
      "Iteration 5304, loss = 0.03991225\n",
      "Iteration 5305, loss = 0.03990493\n",
      "Iteration 5306, loss = 0.03989561\n",
      "Iteration 5307, loss = 0.03988496\n",
      "Iteration 5308, loss = 0.03987502\n",
      "Iteration 5309, loss = 0.03986793\n",
      "Iteration 5310, loss = 0.03985623\n",
      "Iteration 5311, loss = 0.03984839\n",
      "Iteration 5312, loss = 0.03984231\n",
      "Iteration 5313, loss = 0.03983281\n",
      "Iteration 5314, loss = 0.03982206\n",
      "Iteration 5315, loss = 0.03980973\n",
      "Iteration 5316, loss = 0.03980240\n",
      "Iteration 5317, loss = 0.03979242\n",
      "Iteration 5318, loss = 0.03978443\n",
      "Iteration 5319, loss = 0.03977280\n",
      "Iteration 5320, loss = 0.03976684\n",
      "Iteration 5321, loss = 0.03975784\n",
      "Iteration 5322, loss = 0.03974844\n",
      "Iteration 5323, loss = 0.03974059\n",
      "Iteration 5324, loss = 0.03973111\n",
      "Iteration 5325, loss = 0.03971930\n",
      "Iteration 5326, loss = 0.03970916\n",
      "Iteration 5327, loss = 0.03969887\n",
      "Iteration 5328, loss = 0.03968427\n",
      "Iteration 5329, loss = 0.03967998\n",
      "Iteration 5330, loss = 0.03967445\n",
      "Iteration 5331, loss = 0.03966453\n",
      "Iteration 5332, loss = 0.03965154\n",
      "Iteration 5333, loss = 0.03963844\n",
      "Iteration 5334, loss = 0.03963018\n",
      "Iteration 5335, loss = 0.03962382\n",
      "Iteration 5336, loss = 0.03961504\n",
      "Iteration 5337, loss = 0.03960440\n",
      "Iteration 5338, loss = 0.03959091\n",
      "Iteration 5339, loss = 0.03958266\n",
      "Iteration 5340, loss = 0.03957465\n",
      "Iteration 5341, loss = 0.03956436\n",
      "Iteration 5342, loss = 0.03955484\n",
      "Iteration 5343, loss = 0.03954476\n",
      "Iteration 5344, loss = 0.03953630\n",
      "Iteration 5345, loss = 0.03952552\n",
      "Iteration 5346, loss = 0.03951709\n",
      "Iteration 5347, loss = 0.03950709\n",
      "Iteration 5348, loss = 0.03949908\n",
      "Iteration 5349, loss = 0.03948891\n",
      "Iteration 5350, loss = 0.03947696\n",
      "Iteration 5351, loss = 0.03946803\n",
      "Iteration 5352, loss = 0.03946074\n",
      "Iteration 5353, loss = 0.03945166\n",
      "Iteration 5354, loss = 0.03943987\n",
      "Iteration 5355, loss = 0.03943029\n",
      "Iteration 5356, loss = 0.03942147\n",
      "Iteration 5357, loss = 0.03941133\n",
      "Iteration 5358, loss = 0.03940117\n",
      "Iteration 5359, loss = 0.03939043\n",
      "Iteration 5360, loss = 0.03938153\n",
      "Iteration 5361, loss = 0.03937239\n",
      "Iteration 5362, loss = 0.03936221\n",
      "Iteration 5363, loss = 0.03935101\n",
      "Iteration 5364, loss = 0.03934431\n",
      "Iteration 5365, loss = 0.03933634\n",
      "Iteration 5366, loss = 0.03932666\n",
      "Iteration 5367, loss = 0.03931498\n",
      "Iteration 5368, loss = 0.03930666\n",
      "Iteration 5369, loss = 0.03929943\n",
      "Iteration 5370, loss = 0.03928797\n",
      "Iteration 5371, loss = 0.03928051\n",
      "Iteration 5372, loss = 0.03927313\n",
      "Iteration 5373, loss = 0.03926342\n",
      "Iteration 5374, loss = 0.03925476\n",
      "Iteration 5375, loss = 0.03924104\n",
      "Iteration 5376, loss = 0.03923053\n",
      "Iteration 5377, loss = 0.03922171\n",
      "Iteration 5378, loss = 0.03921166\n",
      "Iteration 5379, loss = 0.03920201\n",
      "Iteration 5380, loss = 0.03919577\n",
      "Iteration 5381, loss = 0.03918685\n",
      "Iteration 5382, loss = 0.03917336\n",
      "Iteration 5383, loss = 0.03916536\n",
      "Iteration 5384, loss = 0.03915581\n",
      "Iteration 5385, loss = 0.03914328\n",
      "Iteration 5386, loss = 0.03913343\n",
      "Iteration 5387, loss = 0.03912321\n",
      "Iteration 5388, loss = 0.03911342\n",
      "Iteration 5389, loss = 0.03910356\n",
      "Iteration 5390, loss = 0.03909368\n",
      "Iteration 5391, loss = 0.03908377\n",
      "Iteration 5392, loss = 0.03907616\n",
      "Iteration 5393, loss = 0.03906840\n",
      "Iteration 5394, loss = 0.03905733\n",
      "Iteration 5395, loss = 0.03904636\n",
      "Iteration 5396, loss = 0.03903482\n",
      "Iteration 5397, loss = 0.03902557\n",
      "Iteration 5398, loss = 0.03901740\n",
      "Iteration 5399, loss = 0.03900709\n",
      "Iteration 5400, loss = 0.03899717\n",
      "Iteration 5401, loss = 0.03898720\n",
      "Iteration 5402, loss = 0.03897716\n",
      "Iteration 5403, loss = 0.03896781\n",
      "Iteration 5404, loss = 0.03895763\n",
      "Iteration 5405, loss = 0.03894839\n",
      "Iteration 5406, loss = 0.03893809\n",
      "Iteration 5407, loss = 0.03892979\n",
      "Iteration 5408, loss = 0.03891891\n",
      "Iteration 5409, loss = 0.03890854\n",
      "Iteration 5410, loss = 0.03890028\n",
      "Iteration 5411, loss = 0.03888902\n",
      "Iteration 5412, loss = 0.03888000\n",
      "Iteration 5413, loss = 0.03886916\n",
      "Iteration 5414, loss = 0.03886105\n",
      "Iteration 5415, loss = 0.03885325\n",
      "Iteration 5416, loss = 0.03884322\n",
      "Iteration 5417, loss = 0.03883128\n",
      "Iteration 5418, loss = 0.03882578\n",
      "Iteration 5419, loss = 0.03881718\n",
      "Iteration 5420, loss = 0.03880553\n",
      "Iteration 5421, loss = 0.03879195\n",
      "Iteration 5422, loss = 0.03878473\n",
      "Iteration 5423, loss = 0.03877584\n",
      "Iteration 5424, loss = 0.03876377\n",
      "Iteration 5425, loss = 0.03875904\n",
      "Iteration 5426, loss = 0.03874782\n",
      "Iteration 5427, loss = 0.03873306\n",
      "Iteration 5428, loss = 0.03872850\n",
      "Iteration 5429, loss = 0.03872163\n",
      "Iteration 5430, loss = 0.03871106\n",
      "Iteration 5431, loss = 0.03869943\n",
      "Iteration 5432, loss = 0.03868956\n",
      "Iteration 5433, loss = 0.03867869\n",
      "Iteration 5434, loss = 0.03867129\n",
      "Iteration 5435, loss = 0.03866193\n",
      "Iteration 5436, loss = 0.03865177\n",
      "Iteration 5437, loss = 0.03864087\n",
      "Iteration 5438, loss = 0.03862985\n",
      "Iteration 5439, loss = 0.03861819\n",
      "Iteration 5440, loss = 0.03861488\n",
      "Iteration 5441, loss = 0.03860648\n",
      "Iteration 5442, loss = 0.03859183\n",
      "Iteration 5443, loss = 0.03857908\n",
      "Iteration 5444, loss = 0.03857264\n",
      "Iteration 5445, loss = 0.03856423\n",
      "Iteration 5446, loss = 0.03855358\n",
      "Iteration 5447, loss = 0.03854384\n",
      "Iteration 5448, loss = 0.03853191\n",
      "Iteration 5449, loss = 0.03852033\n",
      "Iteration 5450, loss = 0.03851301\n",
      "Iteration 5451, loss = 0.03850511\n",
      "Iteration 5452, loss = 0.03849538\n",
      "Iteration 5453, loss = 0.03848459\n",
      "Iteration 5454, loss = 0.03847134\n",
      "Iteration 5455, loss = 0.03846246\n",
      "Iteration 5456, loss = 0.03845338\n",
      "Iteration 5457, loss = 0.03844606\n",
      "Iteration 5458, loss = 0.03843594\n",
      "Iteration 5459, loss = 0.03842159\n",
      "Iteration 5460, loss = 0.03841543\n",
      "Iteration 5461, loss = 0.03840765\n",
      "Iteration 5462, loss = 0.03839819\n",
      "Iteration 5463, loss = 0.03838559\n",
      "Iteration 5464, loss = 0.03837395\n",
      "Iteration 5465, loss = 0.03836524\n",
      "Iteration 5466, loss = 0.03835735\n",
      "Iteration 5467, loss = 0.03834582\n",
      "Iteration 5468, loss = 0.03833309\n",
      "Iteration 5469, loss = 0.03832422\n",
      "Iteration 5470, loss = 0.03831513\n",
      "Iteration 5471, loss = 0.03830243\n",
      "Iteration 5472, loss = 0.03829637\n",
      "Iteration 5473, loss = 0.03828751\n",
      "Iteration 5474, loss = 0.03827460\n",
      "Iteration 5475, loss = 0.03826711\n",
      "Iteration 5476, loss = 0.03825935\n",
      "Iteration 5477, loss = 0.03824904\n",
      "Iteration 5478, loss = 0.03823831\n",
      "Iteration 5479, loss = 0.03822528\n",
      "Iteration 5480, loss = 0.03821800\n",
      "Iteration 5481, loss = 0.03820980\n",
      "Iteration 5482, loss = 0.03819889\n",
      "Iteration 5483, loss = 0.03818668\n",
      "Iteration 5484, loss = 0.03817708\n",
      "Iteration 5485, loss = 0.03816602\n",
      "Iteration 5486, loss = 0.03815567\n",
      "Iteration 5487, loss = 0.03814449\n",
      "Iteration 5488, loss = 0.03813616\n",
      "Iteration 5489, loss = 0.03812833\n",
      "Iteration 5490, loss = 0.03811761\n",
      "Iteration 5491, loss = 0.03810494\n",
      "Iteration 5492, loss = 0.03809500\n",
      "Iteration 5493, loss = 0.03808607\n",
      "Iteration 5494, loss = 0.03807643\n",
      "Iteration 5495, loss = 0.03806711\n",
      "Iteration 5496, loss = 0.03805784\n",
      "Iteration 5497, loss = 0.03804635\n",
      "Iteration 5498, loss = 0.03803810\n",
      "Iteration 5499, loss = 0.03802929\n",
      "Iteration 5500, loss = 0.03801752\n",
      "Iteration 5501, loss = 0.03800687\n",
      "Iteration 5502, loss = 0.03799858\n",
      "Iteration 5503, loss = 0.03798935\n",
      "Iteration 5504, loss = 0.03797822\n",
      "Iteration 5505, loss = 0.03796716\n",
      "Iteration 5506, loss = 0.03795953\n",
      "Iteration 5507, loss = 0.03795086\n",
      "Iteration 5508, loss = 0.03793814\n",
      "Iteration 5509, loss = 0.03792932\n",
      "Iteration 5510, loss = 0.03791971\n",
      "Iteration 5511, loss = 0.03790672\n",
      "Iteration 5512, loss = 0.03790033\n",
      "Iteration 5513, loss = 0.03789170\n",
      "Iteration 5514, loss = 0.03788129\n",
      "Iteration 5515, loss = 0.03786911\n",
      "Iteration 5516, loss = 0.03785914\n",
      "Iteration 5517, loss = 0.03785012\n",
      "Iteration 5518, loss = 0.03783827\n",
      "Iteration 5519, loss = 0.03782876\n",
      "Iteration 5520, loss = 0.03781723\n",
      "Iteration 5521, loss = 0.03781298\n",
      "Iteration 5522, loss = 0.03780275\n",
      "Iteration 5523, loss = 0.03778910\n",
      "Iteration 5524, loss = 0.03778115\n",
      "Iteration 5525, loss = 0.03777368\n",
      "Iteration 5526, loss = 0.03776325\n",
      "Iteration 5527, loss = 0.03775234\n",
      "Iteration 5528, loss = 0.03773894\n",
      "Iteration 5529, loss = 0.03773240\n",
      "Iteration 5530, loss = 0.03772427\n",
      "Iteration 5531, loss = 0.03771328\n",
      "Iteration 5532, loss = 0.03770014\n",
      "Iteration 5533, loss = 0.03769256\n",
      "Iteration 5534, loss = 0.03768577\n",
      "Iteration 5535, loss = 0.03767779\n",
      "Iteration 5536, loss = 0.03766641\n",
      "Iteration 5537, loss = 0.03765429\n",
      "Iteration 5538, loss = 0.03763984\n",
      "Iteration 5539, loss = 0.03763318\n",
      "Iteration 5540, loss = 0.03762803\n",
      "Iteration 5541, loss = 0.03761786\n",
      "Iteration 5542, loss = 0.03760208\n",
      "Iteration 5543, loss = 0.03759108\n",
      "Iteration 5544, loss = 0.03758488\n",
      "Iteration 5545, loss = 0.03757508\n",
      "Iteration 5546, loss = 0.03756462\n",
      "Iteration 5547, loss = 0.03755246\n",
      "Iteration 5548, loss = 0.03753777\n",
      "Iteration 5549, loss = 0.03753401\n",
      "Iteration 5550, loss = 0.03752825\n",
      "Iteration 5551, loss = 0.03751781\n",
      "Iteration 5552, loss = 0.03750328\n",
      "Iteration 5553, loss = 0.03748807\n",
      "Iteration 5554, loss = 0.03748208\n",
      "Iteration 5555, loss = 0.03747146\n",
      "Iteration 5556, loss = 0.03745924\n",
      "Iteration 5557, loss = 0.03744735\n",
      "Iteration 5558, loss = 0.03743822\n",
      "Iteration 5559, loss = 0.03742977\n",
      "Iteration 5560, loss = 0.03741957\n",
      "Iteration 5561, loss = 0.03740734\n",
      "Iteration 5562, loss = 0.03739958\n",
      "Iteration 5563, loss = 0.03739139\n",
      "Iteration 5564, loss = 0.03737760\n",
      "Iteration 5565, loss = 0.03736833\n",
      "Iteration 5566, loss = 0.03735606\n",
      "Iteration 5567, loss = 0.03735060\n",
      "Iteration 5568, loss = 0.03733993\n",
      "Iteration 5569, loss = 0.03732657\n",
      "Iteration 5570, loss = 0.03732227\n",
      "Iteration 5571, loss = 0.03731337\n",
      "Iteration 5572, loss = 0.03730330\n",
      "Iteration 5573, loss = 0.03729145\n",
      "Iteration 5574, loss = 0.03727871\n",
      "Iteration 5575, loss = 0.03726823\n",
      "Iteration 5576, loss = 0.03725942\n",
      "Iteration 5577, loss = 0.03724844\n",
      "Iteration 5578, loss = 0.03723796\n",
      "Iteration 5579, loss = 0.03722833\n",
      "Iteration 5580, loss = 0.03721761\n",
      "Iteration 5581, loss = 0.03720595\n",
      "Iteration 5582, loss = 0.03719594\n",
      "Iteration 5583, loss = 0.03718708\n",
      "Iteration 5584, loss = 0.03717690\n",
      "Iteration 5585, loss = 0.03716818\n",
      "Iteration 5586, loss = 0.03715769\n",
      "Iteration 5587, loss = 0.03714480\n",
      "Iteration 5588, loss = 0.03713713\n",
      "Iteration 5589, loss = 0.03712558\n",
      "Iteration 5590, loss = 0.03711533\n",
      "Iteration 5591, loss = 0.03710748\n",
      "Iteration 5592, loss = 0.03709789\n",
      "Iteration 5593, loss = 0.03708664\n",
      "Iteration 5594, loss = 0.03707946\n",
      "Iteration 5595, loss = 0.03706985\n",
      "Iteration 5596, loss = 0.03705902\n",
      "Iteration 5597, loss = 0.03704903\n",
      "Iteration 5598, loss = 0.03704049\n",
      "Iteration 5599, loss = 0.03702908\n",
      "Iteration 5600, loss = 0.03701592\n",
      "Iteration 5601, loss = 0.03700470\n",
      "Iteration 5602, loss = 0.03699811\n",
      "Iteration 5603, loss = 0.03698710\n",
      "Iteration 5604, loss = 0.03697607\n",
      "Iteration 5605, loss = 0.03696671\n",
      "Iteration 5606, loss = 0.03695351\n",
      "Iteration 5607, loss = 0.03694485\n",
      "Iteration 5608, loss = 0.03693665\n",
      "Iteration 5609, loss = 0.03692798\n",
      "Iteration 5610, loss = 0.03691509\n",
      "Iteration 5611, loss = 0.03690642\n",
      "Iteration 5612, loss = 0.03689563\n",
      "Iteration 5613, loss = 0.03688163\n",
      "Iteration 5614, loss = 0.03687508\n",
      "Iteration 5615, loss = 0.03686792\n",
      "Iteration 5616, loss = 0.03685789\n",
      "Iteration 5617, loss = 0.03684683\n",
      "Iteration 5618, loss = 0.03683320\n",
      "Iteration 5619, loss = 0.03682606\n",
      "Iteration 5620, loss = 0.03681684\n",
      "Iteration 5621, loss = 0.03680327\n",
      "Iteration 5622, loss = 0.03679004\n",
      "Iteration 5623, loss = 0.03678602\n",
      "Iteration 5624, loss = 0.03678016\n",
      "Iteration 5625, loss = 0.03677053\n",
      "Iteration 5626, loss = 0.03675938\n",
      "Iteration 5627, loss = 0.03674634\n",
      "Iteration 5628, loss = 0.03672992\n",
      "Iteration 5629, loss = 0.03672435\n",
      "Iteration 5630, loss = 0.03671671\n",
      "Iteration 5631, loss = 0.03670658\n",
      "Iteration 5632, loss = 0.03669176\n",
      "Iteration 5633, loss = 0.03667920\n",
      "Iteration 5634, loss = 0.03667197\n",
      "Iteration 5635, loss = 0.03666146\n",
      "Iteration 5636, loss = 0.03664904\n",
      "Iteration 5637, loss = 0.03663654\n",
      "Iteration 5638, loss = 0.03663181\n",
      "Iteration 5639, loss = 0.03662362\n",
      "Iteration 5640, loss = 0.03661113\n",
      "Iteration 5641, loss = 0.03659463\n",
      "Iteration 5642, loss = 0.03659091\n",
      "Iteration 5643, loss = 0.03658407\n",
      "Iteration 5644, loss = 0.03657580\n",
      "Iteration 5645, loss = 0.03656562\n",
      "Iteration 5646, loss = 0.03655114\n",
      "Iteration 5647, loss = 0.03653737\n",
      "Iteration 5648, loss = 0.03652792\n",
      "Iteration 5649, loss = 0.03652128\n",
      "Iteration 5650, loss = 0.03650740\n",
      "Iteration 5651, loss = 0.03649199\n",
      "Iteration 5652, loss = 0.03648316\n",
      "Iteration 5653, loss = 0.03647332\n",
      "Iteration 5654, loss = 0.03646194\n",
      "Iteration 5655, loss = 0.03645103\n",
      "Iteration 5656, loss = 0.03644275\n",
      "Iteration 5657, loss = 0.03643427\n",
      "Iteration 5658, loss = 0.03642267\n",
      "Iteration 5659, loss = 0.03640960\n",
      "Iteration 5660, loss = 0.03639930\n",
      "Iteration 5661, loss = 0.03639064\n",
      "Iteration 5662, loss = 0.03638196\n",
      "Iteration 5663, loss = 0.03636946\n",
      "Iteration 5664, loss = 0.03636164\n",
      "Iteration 5665, loss = 0.03635113\n",
      "Iteration 5666, loss = 0.03633658\n",
      "Iteration 5667, loss = 0.03632624\n",
      "Iteration 5668, loss = 0.03631934\n",
      "Iteration 5669, loss = 0.03630680\n",
      "Iteration 5670, loss = 0.03629900\n",
      "Iteration 5671, loss = 0.03629104\n",
      "Iteration 5672, loss = 0.03627859\n",
      "Iteration 5673, loss = 0.03626587\n",
      "Iteration 5674, loss = 0.03625715\n",
      "Iteration 5675, loss = 0.03624977\n",
      "Iteration 5676, loss = 0.03623689\n",
      "Iteration 5677, loss = 0.03622523\n",
      "Iteration 5678, loss = 0.03621801\n",
      "Iteration 5679, loss = 0.03620770\n",
      "Iteration 5680, loss = 0.03619346\n",
      "Iteration 5681, loss = 0.03618507\n",
      "Iteration 5682, loss = 0.03617534\n",
      "Iteration 5683, loss = 0.03616271\n",
      "Iteration 5684, loss = 0.03615257\n",
      "Iteration 5685, loss = 0.03614161\n",
      "Iteration 5686, loss = 0.03613174\n",
      "Iteration 5687, loss = 0.03612021\n",
      "Iteration 5688, loss = 0.03611307\n",
      "Iteration 5689, loss = 0.03610358\n",
      "Iteration 5690, loss = 0.03609106\n",
      "Iteration 5691, loss = 0.03608117\n",
      "Iteration 5692, loss = 0.03606808\n",
      "Iteration 5693, loss = 0.03606164\n",
      "Iteration 5694, loss = 0.03605169\n",
      "Iteration 5695, loss = 0.03603737\n",
      "Iteration 5696, loss = 0.03602785\n",
      "Iteration 5697, loss = 0.03601913\n",
      "Iteration 5698, loss = 0.03600745\n",
      "Iteration 5699, loss = 0.03599661\n",
      "Iteration 5700, loss = 0.03598653\n",
      "Iteration 5701, loss = 0.03597544\n",
      "Iteration 5702, loss = 0.03596573\n",
      "Iteration 5703, loss = 0.03595446\n",
      "Iteration 5704, loss = 0.03594520\n",
      "Iteration 5705, loss = 0.03593269\n",
      "Iteration 5706, loss = 0.03592298\n",
      "Iteration 5707, loss = 0.03591576\n",
      "Iteration 5708, loss = 0.03590503\n",
      "Iteration 5709, loss = 0.03589476\n",
      "Iteration 5710, loss = 0.03588514\n",
      "Iteration 5711, loss = 0.03587334\n",
      "Iteration 5712, loss = 0.03586252\n",
      "Iteration 5713, loss = 0.03584960\n",
      "Iteration 5714, loss = 0.03584207\n",
      "Iteration 5715, loss = 0.03583502\n",
      "Iteration 5716, loss = 0.03582304\n",
      "Iteration 5717, loss = 0.03581017\n",
      "Iteration 5718, loss = 0.03580427\n",
      "Iteration 5719, loss = 0.03579572\n",
      "Iteration 5720, loss = 0.03578377\n",
      "Iteration 5721, loss = 0.03576887\n",
      "Iteration 5722, loss = 0.03575937\n",
      "Iteration 5723, loss = 0.03574953\n",
      "Iteration 5724, loss = 0.03573642\n",
      "Iteration 5725, loss = 0.03573115\n",
      "Iteration 5726, loss = 0.03572053\n",
      "Iteration 5727, loss = 0.03570758\n",
      "Iteration 5728, loss = 0.03569582\n",
      "Iteration 5729, loss = 0.03568753\n",
      "Iteration 5730, loss = 0.03567673\n",
      "Iteration 5731, loss = 0.03566306\n",
      "Iteration 5732, loss = 0.03565494\n",
      "Iteration 5733, loss = 0.03564667\n",
      "Iteration 5734, loss = 0.03563247\n",
      "Iteration 5735, loss = 0.03562528\n",
      "Iteration 5736, loss = 0.03561654\n",
      "Iteration 5737, loss = 0.03560687\n",
      "Iteration 5738, loss = 0.03559380\n",
      "Iteration 5739, loss = 0.03557829\n",
      "Iteration 5740, loss = 0.03557404\n",
      "Iteration 5741, loss = 0.03556786\n",
      "Iteration 5742, loss = 0.03555601\n",
      "Iteration 5743, loss = 0.03553833\n",
      "Iteration 5744, loss = 0.03553164\n",
      "Iteration 5745, loss = 0.03552481\n",
      "Iteration 5746, loss = 0.03551676\n",
      "Iteration 5747, loss = 0.03550627\n",
      "Iteration 5748, loss = 0.03549144\n",
      "Iteration 5749, loss = 0.03547435\n",
      "Iteration 5750, loss = 0.03546905\n",
      "Iteration 5751, loss = 0.03546529\n",
      "Iteration 5752, loss = 0.03545378\n",
      "Iteration 5753, loss = 0.03543736\n",
      "Iteration 5754, loss = 0.03542750\n",
      "Iteration 5755, loss = 0.03542014\n",
      "Iteration 5756, loss = 0.03540999\n",
      "Iteration 5757, loss = 0.03539817\n",
      "Iteration 5758, loss = 0.03538435\n",
      "Iteration 5759, loss = 0.03536870\n",
      "Iteration 5760, loss = 0.03536750\n",
      "Iteration 5761, loss = 0.03536232\n",
      "Iteration 5762, loss = 0.03535178\n",
      "Iteration 5763, loss = 0.03533515\n",
      "Iteration 5764, loss = 0.03531700\n",
      "Iteration 5765, loss = 0.03531089\n",
      "Iteration 5766, loss = 0.03530161\n",
      "Iteration 5767, loss = 0.03528924\n",
      "Iteration 5768, loss = 0.03527373\n",
      "Iteration 5769, loss = 0.03526260\n",
      "Iteration 5770, loss = 0.03525720\n",
      "Iteration 5771, loss = 0.03524714\n",
      "Iteration 5772, loss = 0.03523401\n",
      "Iteration 5773, loss = 0.03522387\n",
      "Iteration 5774, loss = 0.03521435\n",
      "Iteration 5775, loss = 0.03520121\n",
      "Iteration 5776, loss = 0.03519036\n",
      "Iteration 5777, loss = 0.03517982\n",
      "Iteration 5778, loss = 0.03516771\n",
      "Iteration 5779, loss = 0.03515892\n",
      "Iteration 5780, loss = 0.03514979\n",
      "Iteration 5781, loss = 0.03513824\n",
      "Iteration 5782, loss = 0.03512622\n",
      "Iteration 5783, loss = 0.03511598\n",
      "Iteration 5784, loss = 0.03510724\n",
      "Iteration 5785, loss = 0.03509447\n",
      "Iteration 5786, loss = 0.03508419\n",
      "Iteration 5787, loss = 0.03507775\n",
      "Iteration 5788, loss = 0.03506626\n",
      "Iteration 5789, loss = 0.03505655\n",
      "Iteration 5790, loss = 0.03504622\n",
      "Iteration 5791, loss = 0.03503374\n",
      "Iteration 5792, loss = 0.03502089\n",
      "Iteration 5793, loss = 0.03501901\n",
      "Iteration 5794, loss = 0.03501033\n",
      "Iteration 5795, loss = 0.03499505\n",
      "Iteration 5796, loss = 0.03498061\n",
      "Iteration 5797, loss = 0.03497301\n",
      "Iteration 5798, loss = 0.03496202\n",
      "Iteration 5799, loss = 0.03494859\n",
      "Iteration 5800, loss = 0.03493923\n",
      "Iteration 5801, loss = 0.03492996\n",
      "Iteration 5802, loss = 0.03491435\n",
      "Iteration 5803, loss = 0.03490490\n",
      "Iteration 5804, loss = 0.03489713\n",
      "Iteration 5805, loss = 0.03488462\n",
      "Iteration 5806, loss = 0.03487709\n",
      "Iteration 5807, loss = 0.03486835\n",
      "Iteration 5808, loss = 0.03485649\n",
      "Iteration 5809, loss = 0.03484224\n",
      "Iteration 5810, loss = 0.03483451\n",
      "Iteration 5811, loss = 0.03482742\n",
      "Iteration 5812, loss = 0.03481368\n",
      "Iteration 5813, loss = 0.03480085\n",
      "Iteration 5814, loss = 0.03479303\n",
      "Iteration 5815, loss = 0.03478239\n",
      "Iteration 5816, loss = 0.03476965\n",
      "Iteration 5817, loss = 0.03476034\n",
      "Iteration 5818, loss = 0.03475067\n",
      "Iteration 5819, loss = 0.03473653\n",
      "Iteration 5820, loss = 0.03472971\n",
      "Iteration 5821, loss = 0.03472132\n",
      "Iteration 5822, loss = 0.03471279\n",
      "Iteration 5823, loss = 0.03469946\n",
      "Iteration 5824, loss = 0.03468374\n",
      "Iteration 5825, loss = 0.03468132\n",
      "Iteration 5826, loss = 0.03467404\n",
      "Iteration 5827, loss = 0.03466047\n",
      "Iteration 5828, loss = 0.03464253\n",
      "Iteration 5829, loss = 0.03463439\n",
      "Iteration 5830, loss = 0.03462837\n",
      "Iteration 5831, loss = 0.03462057\n",
      "Iteration 5832, loss = 0.03460957\n",
      "Iteration 5833, loss = 0.03459510\n",
      "Iteration 5834, loss = 0.03457728\n",
      "Iteration 5835, loss = 0.03457424\n",
      "Iteration 5836, loss = 0.03456917\n",
      "Iteration 5837, loss = 0.03455701\n",
      "Iteration 5838, loss = 0.03453951\n",
      "Iteration 5839, loss = 0.03452616\n",
      "Iteration 5840, loss = 0.03452051\n",
      "Iteration 5841, loss = 0.03451121\n",
      "Iteration 5842, loss = 0.03449842\n",
      "Iteration 5843, loss = 0.03448366\n",
      "Iteration 5844, loss = 0.03447228\n",
      "Iteration 5845, loss = 0.03446542\n",
      "Iteration 5846, loss = 0.03445233\n",
      "Iteration 5847, loss = 0.03444196\n",
      "Iteration 5848, loss = 0.03443196\n",
      "Iteration 5849, loss = 0.03442194\n",
      "Iteration 5850, loss = 0.03440886\n",
      "Iteration 5851, loss = 0.03439954\n",
      "Iteration 5852, loss = 0.03439005\n",
      "Iteration 5853, loss = 0.03437602\n",
      "Iteration 5854, loss = 0.03436962\n",
      "Iteration 5855, loss = 0.03436124\n",
      "Iteration 5856, loss = 0.03435089\n",
      "Iteration 5857, loss = 0.03433743\n",
      "Iteration 5858, loss = 0.03432258\n",
      "Iteration 5859, loss = 0.03432103\n",
      "Iteration 5860, loss = 0.03431342\n",
      "Iteration 5861, loss = 0.03429908\n",
      "Iteration 5862, loss = 0.03428120\n",
      "Iteration 5863, loss = 0.03427488\n",
      "Iteration 5864, loss = 0.03426837\n",
      "Iteration 5865, loss = 0.03425913\n",
      "Iteration 5866, loss = 0.03424864\n",
      "Iteration 5867, loss = 0.03423434\n",
      "Iteration 5868, loss = 0.03421664\n",
      "Iteration 5869, loss = 0.03421086\n",
      "Iteration 5870, loss = 0.03420566\n",
      "Iteration 5871, loss = 0.03419308\n",
      "Iteration 5872, loss = 0.03417612\n",
      "Iteration 5873, loss = 0.03416635\n",
      "Iteration 5874, loss = 0.03416014\n",
      "Iteration 5875, loss = 0.03415239\n",
      "Iteration 5876, loss = 0.03413967\n",
      "Iteration 5877, loss = 0.03412314\n",
      "Iteration 5878, loss = 0.03410961\n",
      "Iteration 5879, loss = 0.03410313\n",
      "Iteration 5880, loss = 0.03408981\n",
      "Iteration 5881, loss = 0.03408172\n",
      "Iteration 5882, loss = 0.03407191\n",
      "Iteration 5883, loss = 0.03406162\n",
      "Iteration 5884, loss = 0.03404911\n",
      "Iteration 5885, loss = 0.03403831\n",
      "Iteration 5886, loss = 0.03402842\n",
      "Iteration 5887, loss = 0.03401284\n",
      "Iteration 5888, loss = 0.03400211\n",
      "Iteration 5889, loss = 0.03399441\n",
      "Iteration 5890, loss = 0.03398365\n",
      "Iteration 5891, loss = 0.03397211\n",
      "Iteration 5892, loss = 0.03396394\n",
      "Iteration 5893, loss = 0.03395343\n",
      "Iteration 5894, loss = 0.03393914\n",
      "Iteration 5895, loss = 0.03392794\n",
      "Iteration 5896, loss = 0.03391717\n",
      "Iteration 5897, loss = 0.03390820\n",
      "Iteration 5898, loss = 0.03389643\n",
      "Iteration 5899, loss = 0.03388863\n",
      "Iteration 5900, loss = 0.03387606\n",
      "Iteration 5901, loss = 0.03386518\n",
      "Iteration 5902, loss = 0.03385638\n",
      "Iteration 5903, loss = 0.03384359\n",
      "Iteration 5904, loss = 0.03383405\n",
      "Iteration 5905, loss = 0.03382365\n",
      "Iteration 5906, loss = 0.03381136\n",
      "Iteration 5907, loss = 0.03380171\n",
      "Iteration 5908, loss = 0.03378845\n",
      "Iteration 5909, loss = 0.03378142\n",
      "Iteration 5910, loss = 0.03377070\n",
      "Iteration 5911, loss = 0.03375650\n",
      "Iteration 5912, loss = 0.03374734\n",
      "Iteration 5913, loss = 0.03373464\n",
      "Iteration 5914, loss = 0.03372988\n",
      "Iteration 5915, loss = 0.03371854\n",
      "Iteration 5916, loss = 0.03370207\n",
      "Iteration 5917, loss = 0.03369918\n",
      "Iteration 5918, loss = 0.03369260\n",
      "Iteration 5919, loss = 0.03368126\n",
      "Iteration 5920, loss = 0.03366906\n",
      "Iteration 5921, loss = 0.03365374\n",
      "Iteration 5922, loss = 0.03363970\n",
      "Iteration 5923, loss = 0.03363217\n",
      "Iteration 5924, loss = 0.03361875\n",
      "Iteration 5925, loss = 0.03360871\n",
      "Iteration 5926, loss = 0.03360108\n",
      "Iteration 5927, loss = 0.03358977\n",
      "Iteration 5928, loss = 0.03357636\n",
      "Iteration 5929, loss = 0.03356796\n",
      "Iteration 5930, loss = 0.03355815\n",
      "Iteration 5931, loss = 0.03354369\n",
      "Iteration 5932, loss = 0.03353593\n",
      "Iteration 5933, loss = 0.03352846\n",
      "Iteration 5934, loss = 0.03351866\n",
      "Iteration 5935, loss = 0.03350477\n",
      "Iteration 5936, loss = 0.03348847\n",
      "Iteration 5937, loss = 0.03348772\n",
      "Iteration 5938, loss = 0.03348094\n",
      "Iteration 5939, loss = 0.03346922\n",
      "Iteration 5940, loss = 0.03345061\n",
      "Iteration 5941, loss = 0.03344138\n",
      "Iteration 5942, loss = 0.03343530\n",
      "Iteration 5943, loss = 0.03342800\n",
      "Iteration 5944, loss = 0.03341642\n",
      "Iteration 5945, loss = 0.03340052\n",
      "Iteration 5946, loss = 0.03338402\n",
      "Iteration 5947, loss = 0.03338057\n",
      "Iteration 5948, loss = 0.03337476\n",
      "Iteration 5949, loss = 0.03336259\n",
      "Iteration 5950, loss = 0.03334422\n",
      "Iteration 5951, loss = 0.03333191\n",
      "Iteration 5952, loss = 0.03332602\n",
      "Iteration 5953, loss = 0.03331689\n",
      "Iteration 5954, loss = 0.03330485\n",
      "Iteration 5955, loss = 0.03328965\n",
      "Iteration 5956, loss = 0.03327554\n",
      "Iteration 5957, loss = 0.03326744\n",
      "Iteration 5958, loss = 0.03325347\n",
      "Iteration 5959, loss = 0.03324692\n",
      "Iteration 5960, loss = 0.03323938\n",
      "Iteration 5961, loss = 0.03322803\n",
      "Iteration 5962, loss = 0.03321402\n",
      "Iteration 5963, loss = 0.03320009\n",
      "Iteration 5964, loss = 0.03319106\n",
      "Iteration 5965, loss = 0.03317909\n",
      "Iteration 5966, loss = 0.03316798\n",
      "Iteration 5967, loss = 0.03315775\n",
      "Iteration 5968, loss = 0.03314720\n",
      "Iteration 5969, loss = 0.03313691\n",
      "Iteration 5970, loss = 0.03312546\n",
      "Iteration 5971, loss = 0.03311553\n",
      "Iteration 5972, loss = 0.03310587\n",
      "Iteration 5973, loss = 0.03309396\n",
      "Iteration 5974, loss = 0.03308810\n",
      "Iteration 5975, loss = 0.03307872\n",
      "Iteration 5976, loss = 0.03306600\n",
      "Iteration 5977, loss = 0.03305144\n",
      "Iteration 5978, loss = 0.03304475\n",
      "Iteration 5979, loss = 0.03303524\n",
      "Iteration 5980, loss = 0.03302099\n",
      "Iteration 5981, loss = 0.03301073\n",
      "Iteration 5982, loss = 0.03300340\n",
      "Iteration 5983, loss = 0.03299284\n",
      "Iteration 5984, loss = 0.03297828\n",
      "Iteration 5985, loss = 0.03296506\n",
      "Iteration 5986, loss = 0.03295697\n",
      "Iteration 5987, loss = 0.03294415\n",
      "Iteration 5988, loss = 0.03293302\n",
      "Iteration 5989, loss = 0.03292138\n",
      "Iteration 5990, loss = 0.03290996\n",
      "Iteration 5991, loss = 0.03290080\n",
      "Iteration 5992, loss = 0.03289198\n",
      "Iteration 5993, loss = 0.03288164\n",
      "Iteration 5994, loss = 0.03286991\n",
      "Iteration 5995, loss = 0.03285840\n",
      "Iteration 5996, loss = 0.03284707\n",
      "Iteration 5997, loss = 0.03283704\n",
      "Iteration 5998, loss = 0.03282688\n",
      "Iteration 5999, loss = 0.03281855\n",
      "Iteration 6000, loss = 0.03280562\n",
      "Iteration 6001, loss = 0.03279818\n",
      "Iteration 6002, loss = 0.03278814\n",
      "Iteration 6003, loss = 0.03277729\n",
      "Iteration 6004, loss = 0.03276269\n",
      "Iteration 6005, loss = 0.03275653\n",
      "Iteration 6006, loss = 0.03274807\n",
      "Iteration 6007, loss = 0.03273211\n",
      "Iteration 6008, loss = 0.03271952\n",
      "Iteration 6009, loss = 0.03271246\n",
      "Iteration 6010, loss = 0.03270141\n",
      "Iteration 6011, loss = 0.03268795\n",
      "Iteration 6012, loss = 0.03267642\n",
      "Iteration 6013, loss = 0.03266663\n",
      "Iteration 6014, loss = 0.03265364\n",
      "Iteration 6015, loss = 0.03264292\n",
      "Iteration 6016, loss = 0.03263218\n",
      "Iteration 6017, loss = 0.03262089\n",
      "Iteration 6018, loss = 0.03261121\n",
      "Iteration 6019, loss = 0.03260076\n",
      "Iteration 6020, loss = 0.03258956\n",
      "Iteration 6021, loss = 0.03257815\n",
      "Iteration 6022, loss = 0.03256793\n",
      "Iteration 6023, loss = 0.03255728\n",
      "Iteration 6024, loss = 0.03254779\n",
      "Iteration 6025, loss = 0.03253646\n",
      "Iteration 6026, loss = 0.03252501\n",
      "Iteration 6027, loss = 0.03251263\n",
      "Iteration 6028, loss = 0.03250411\n",
      "Iteration 6029, loss = 0.03249223\n",
      "Iteration 6030, loss = 0.03248161\n",
      "Iteration 6031, loss = 0.03247311\n",
      "Iteration 6032, loss = 0.03246064\n",
      "Iteration 6033, loss = 0.03245367\n",
      "Iteration 6034, loss = 0.03244487\n",
      "Iteration 6035, loss = 0.03243244\n",
      "Iteration 6036, loss = 0.03241759\n",
      "Iteration 6037, loss = 0.03241428\n",
      "Iteration 6038, loss = 0.03240558\n",
      "Iteration 6039, loss = 0.03239009\n",
      "Iteration 6040, loss = 0.03237698\n",
      "Iteration 6041, loss = 0.03237020\n",
      "Iteration 6042, loss = 0.03235874\n",
      "Iteration 6043, loss = 0.03234542\n",
      "Iteration 6044, loss = 0.03233476\n",
      "Iteration 6045, loss = 0.03232468\n",
      "Iteration 6046, loss = 0.03231064\n",
      "Iteration 6047, loss = 0.03230071\n",
      "Iteration 6048, loss = 0.03229088\n",
      "Iteration 6049, loss = 0.03227801\n",
      "Iteration 6050, loss = 0.03226731\n",
      "Iteration 6051, loss = 0.03225830\n",
      "Iteration 6052, loss = 0.03224693\n",
      "Iteration 6053, loss = 0.03223475\n",
      "Iteration 6054, loss = 0.03222515\n",
      "Iteration 6055, loss = 0.03221347\n",
      "Iteration 6056, loss = 0.03220575\n",
      "Iteration 6057, loss = 0.03219418\n",
      "Iteration 6058, loss = 0.03218124\n",
      "Iteration 6059, loss = 0.03217134\n",
      "Iteration 6060, loss = 0.03216119\n",
      "Iteration 6061, loss = 0.03215158\n",
      "Iteration 6062, loss = 0.03213999\n",
      "Iteration 6063, loss = 0.03212816\n",
      "Iteration 6064, loss = 0.03211666\n",
      "Iteration 6065, loss = 0.03210814\n",
      "Iteration 6066, loss = 0.03209684\n",
      "Iteration 6067, loss = 0.03208542\n",
      "Iteration 6068, loss = 0.03207735\n",
      "Iteration 6069, loss = 0.03206525\n",
      "Iteration 6070, loss = 0.03205702\n",
      "Iteration 6071, loss = 0.03204852\n",
      "Iteration 6072, loss = 0.03203714\n",
      "Iteration 6073, loss = 0.03202233\n",
      "Iteration 6074, loss = 0.03201730\n",
      "Iteration 6075, loss = 0.03200939\n",
      "Iteration 6076, loss = 0.03199432\n",
      "Iteration 6077, loss = 0.03198181\n",
      "Iteration 6078, loss = 0.03197371\n",
      "Iteration 6079, loss = 0.03196315\n",
      "Iteration 6080, loss = 0.03195063\n",
      "Iteration 6081, loss = 0.03193792\n",
      "Iteration 6082, loss = 0.03192755\n",
      "Iteration 6083, loss = 0.03191522\n",
      "Iteration 6084, loss = 0.03190545\n",
      "Iteration 6085, loss = 0.03189245\n",
      "Iteration 6086, loss = 0.03188391\n",
      "Iteration 6087, loss = 0.03187146\n",
      "Iteration 6088, loss = 0.03186542\n",
      "Iteration 6089, loss = 0.03185307\n",
      "Iteration 6090, loss = 0.03184088\n",
      "Iteration 6091, loss = 0.03183246\n",
      "Iteration 6092, loss = 0.03182071\n",
      "Iteration 6093, loss = 0.03180879\n",
      "Iteration 6094, loss = 0.03179697\n",
      "Iteration 6095, loss = 0.03178889\n",
      "Iteration 6096, loss = 0.03177936\n",
      "Iteration 6097, loss = 0.03176729\n",
      "Iteration 6098, loss = 0.03175397\n",
      "Iteration 6099, loss = 0.03174210\n",
      "Iteration 6100, loss = 0.03173563\n",
      "Iteration 6101, loss = 0.03172637\n",
      "Iteration 6102, loss = 0.03171412\n",
      "Iteration 6103, loss = 0.03170116\n",
      "Iteration 6104, loss = 0.03168992\n",
      "Iteration 6105, loss = 0.03168168\n",
      "Iteration 6106, loss = 0.03167186\n",
      "Iteration 6107, loss = 0.03165981\n",
      "Iteration 6108, loss = 0.03164872\n",
      "Iteration 6109, loss = 0.03163710\n",
      "Iteration 6110, loss = 0.03162728\n",
      "Iteration 6111, loss = 0.03161728\n",
      "Iteration 6112, loss = 0.03160549\n",
      "Iteration 6113, loss = 0.03159677\n",
      "Iteration 6114, loss = 0.03158519\n",
      "Iteration 6115, loss = 0.03157300\n",
      "Iteration 6116, loss = 0.03156408\n",
      "Iteration 6117, loss = 0.03155170\n",
      "Iteration 6118, loss = 0.03154457\n",
      "Iteration 6119, loss = 0.03153298\n",
      "Iteration 6120, loss = 0.03151896\n",
      "Iteration 6121, loss = 0.03150963\n",
      "Iteration 6122, loss = 0.03149653\n",
      "Iteration 6123, loss = 0.03149149\n",
      "Iteration 6124, loss = 0.03148041\n",
      "Iteration 6125, loss = 0.03146436\n",
      "Iteration 6126, loss = 0.03145564\n",
      "Iteration 6127, loss = 0.03144275\n",
      "Iteration 6128, loss = 0.03143421\n",
      "Iteration 6129, loss = 0.03142405\n",
      "Iteration 6130, loss = 0.03141367\n",
      "Iteration 6131, loss = 0.03140117\n",
      "Iteration 6132, loss = 0.03139571\n",
      "Iteration 6133, loss = 0.03138706\n",
      "Iteration 6134, loss = 0.03137461\n",
      "Iteration 6135, loss = 0.03135865\n",
      "Iteration 6136, loss = 0.03135446\n",
      "Iteration 6137, loss = 0.03134688\n",
      "Iteration 6138, loss = 0.03133054\n",
      "Iteration 6139, loss = 0.03131924\n",
      "Iteration 6140, loss = 0.03131217\n",
      "Iteration 6141, loss = 0.03130243\n",
      "Iteration 6142, loss = 0.03128932\n",
      "Iteration 6143, loss = 0.03127295\n",
      "Iteration 6144, loss = 0.03126287\n",
      "Iteration 6145, loss = 0.03125515\n",
      "Iteration 6146, loss = 0.03124463\n",
      "Iteration 6147, loss = 0.03123057\n",
      "Iteration 6148, loss = 0.03122437\n",
      "Iteration 6149, loss = 0.03121414\n",
      "Iteration 6150, loss = 0.03119802\n",
      "Iteration 6151, loss = 0.03118816\n",
      "Iteration 6152, loss = 0.03117792\n",
      "Iteration 6153, loss = 0.03116722\n",
      "Iteration 6154, loss = 0.03115555\n",
      "Iteration 6155, loss = 0.03114861\n",
      "Iteration 6156, loss = 0.03113560\n",
      "Iteration 6157, loss = 0.03112697\n",
      "Iteration 6158, loss = 0.03111872\n",
      "Iteration 6159, loss = 0.03110592\n",
      "Iteration 6160, loss = 0.03109059\n",
      "Iteration 6161, loss = 0.03108966\n",
      "Iteration 6162, loss = 0.03108198\n",
      "Iteration 6163, loss = 0.03106662\n",
      "Iteration 6164, loss = 0.03105016\n",
      "Iteration 6165, loss = 0.03104394\n",
      "Iteration 6166, loss = 0.03103387\n",
      "Iteration 6167, loss = 0.03101973\n",
      "Iteration 6168, loss = 0.03100823\n",
      "Iteration 6169, loss = 0.03099809\n",
      "Iteration 6170, loss = 0.03098596\n",
      "Iteration 6171, loss = 0.03097654\n",
      "Iteration 6172, loss = 0.03096283\n",
      "Iteration 6173, loss = 0.03095420\n",
      "Iteration 6174, loss = 0.03094357\n",
      "Iteration 6175, loss = 0.03093554\n",
      "Iteration 6176, loss = 0.03092314\n",
      "Iteration 6177, loss = 0.03091427\n",
      "Iteration 6178, loss = 0.03090544\n",
      "Iteration 6179, loss = 0.03089369\n",
      "Iteration 6180, loss = 0.03087838\n",
      "Iteration 6181, loss = 0.03087744\n",
      "Iteration 6182, loss = 0.03086936\n",
      "Iteration 6183, loss = 0.03085385\n",
      "Iteration 6184, loss = 0.03083858\n",
      "Iteration 6185, loss = 0.03083150\n",
      "Iteration 6186, loss = 0.03082100\n",
      "Iteration 6187, loss = 0.03080712\n",
      "Iteration 6188, loss = 0.03079585\n",
      "Iteration 6189, loss = 0.03078638\n",
      "Iteration 6190, loss = 0.03077326\n",
      "Iteration 6191, loss = 0.03076274\n",
      "Iteration 6192, loss = 0.03075035\n",
      "Iteration 6193, loss = 0.03074278\n",
      "Iteration 6194, loss = 0.03073070\n",
      "Iteration 6195, loss = 0.03072225\n",
      "Iteration 6196, loss = 0.03071054\n",
      "Iteration 6197, loss = 0.03070329\n",
      "Iteration 6198, loss = 0.03069493\n",
      "Iteration 6199, loss = 0.03068243\n",
      "Iteration 6200, loss = 0.03066710\n",
      "Iteration 6201, loss = 0.03066441\n",
      "Iteration 6202, loss = 0.03065646\n",
      "Iteration 6203, loss = 0.03064199\n",
      "Iteration 6204, loss = 0.03062785\n",
      "Iteration 6205, loss = 0.03062017\n",
      "Iteration 6206, loss = 0.03060972\n",
      "Iteration 6207, loss = 0.03059669\n",
      "Iteration 6208, loss = 0.03058391\n",
      "Iteration 6209, loss = 0.03057433\n",
      "Iteration 6210, loss = 0.03056417\n",
      "Iteration 6211, loss = 0.03055422\n",
      "Iteration 6212, loss = 0.03054003\n",
      "Iteration 6213, loss = 0.03053441\n",
      "Iteration 6214, loss = 0.03052467\n",
      "Iteration 6215, loss = 0.03050902\n",
      "Iteration 6216, loss = 0.03049994\n",
      "Iteration 6217, loss = 0.03048805\n",
      "Iteration 6218, loss = 0.03047686\n",
      "Iteration 6219, loss = 0.03046552\n",
      "Iteration 6220, loss = 0.03045838\n",
      "Iteration 6221, loss = 0.03044612\n",
      "Iteration 6222, loss = 0.03043916\n",
      "Iteration 6223, loss = 0.03042974\n",
      "Iteration 6224, loss = 0.03041766\n",
      "Iteration 6225, loss = 0.03040297\n",
      "Iteration 6226, loss = 0.03039771\n",
      "Iteration 6227, loss = 0.03038882\n",
      "Iteration 6228, loss = 0.03037233\n",
      "Iteration 6229, loss = 0.03036358\n",
      "Iteration 6230, loss = 0.03035708\n",
      "Iteration 6231, loss = 0.03034698\n",
      "Iteration 6232, loss = 0.03033363\n",
      "Iteration 6233, loss = 0.03031739\n",
      "Iteration 6234, loss = 0.03031636\n",
      "Iteration 6235, loss = 0.03030924\n",
      "Iteration 6236, loss = 0.03029493\n",
      "Iteration 6237, loss = 0.03027360\n",
      "Iteration 6238, loss = 0.03027445\n",
      "Iteration 6239, loss = 0.03027056\n",
      "Iteration 6240, loss = 0.03026345\n",
      "Iteration 6241, loss = 0.03025212\n",
      "Iteration 6242, loss = 0.03023719\n",
      "Iteration 6243, loss = 0.03022070\n",
      "Iteration 6244, loss = 0.03020097\n",
      "Iteration 6245, loss = 0.03020614\n",
      "Iteration 6246, loss = 0.03020337\n",
      "Iteration 6247, loss = 0.03019252\n",
      "Iteration 6248, loss = 0.03017439\n",
      "Iteration 6249, loss = 0.03014942\n",
      "Iteration 6250, loss = 0.03014881\n",
      "Iteration 6251, loss = 0.03014771\n",
      "Iteration 6252, loss = 0.03014159\n",
      "Iteration 6253, loss = 0.03013126\n",
      "Iteration 6254, loss = 0.03011698\n",
      "Iteration 6255, loss = 0.03010176\n",
      "Iteration 6256, loss = 0.03008321\n",
      "Iteration 6257, loss = 0.03006402\n",
      "Iteration 6258, loss = 0.03005932\n",
      "Iteration 6259, loss = 0.03004747\n",
      "Iteration 6260, loss = 0.03003235\n",
      "Iteration 6261, loss = 0.03002405\n",
      "Iteration 6262, loss = 0.03001118\n",
      "Iteration 6263, loss = 0.03000311\n",
      "Iteration 6264, loss = 0.02999190\n",
      "Iteration 6265, loss = 0.02998248\n",
      "Iteration 6266, loss = 0.02997361\n",
      "Iteration 6267, loss = 0.02996114\n",
      "Iteration 6268, loss = 0.02994681\n",
      "Iteration 6269, loss = 0.02993521\n",
      "Iteration 6270, loss = 0.02993003\n",
      "Iteration 6271, loss = 0.02991572\n",
      "Iteration 6272, loss = 0.02991194\n",
      "Iteration 6273, loss = 0.02990396\n",
      "Iteration 6274, loss = 0.02989296\n",
      "Iteration 6275, loss = 0.02987887\n",
      "Iteration 6276, loss = 0.02986285\n",
      "Iteration 6277, loss = 0.02986640\n",
      "Iteration 6278, loss = 0.02986042\n",
      "Iteration 6279, loss = 0.02984528\n",
      "Iteration 6280, loss = 0.02982377\n",
      "Iteration 6281, loss = 0.02981815\n",
      "Iteration 6282, loss = 0.02981568\n",
      "Iteration 6283, loss = 0.02980897\n",
      "Iteration 6284, loss = 0.02979718\n",
      "Iteration 6285, loss = 0.02978148\n",
      "Iteration 6286, loss = 0.02976389\n",
      "Iteration 6287, loss = 0.02975088\n",
      "Iteration 6288, loss = 0.02974597\n",
      "Iteration 6289, loss = 0.02973156\n",
      "Iteration 6290, loss = 0.02971759\n",
      "Iteration 6291, loss = 0.02971081\n",
      "Iteration 6292, loss = 0.02969956\n",
      "Iteration 6293, loss = 0.02968386\n",
      "Iteration 6294, loss = 0.02968129\n",
      "Iteration 6295, loss = 0.02967272\n",
      "Iteration 6296, loss = 0.02965610\n",
      "Iteration 6297, loss = 0.02964763\n",
      "Iteration 6298, loss = 0.02964160\n",
      "Iteration 6299, loss = 0.02963148\n",
      "Iteration 6300, loss = 0.02961777\n",
      "Iteration 6301, loss = 0.02960069\n",
      "Iteration 6302, loss = 0.02959668\n",
      "Iteration 6303, loss = 0.02959045\n",
      "Iteration 6304, loss = 0.02957504\n",
      "Iteration 6305, loss = 0.02955867\n",
      "Iteration 6306, loss = 0.02955232\n",
      "Iteration 6307, loss = 0.02954146\n",
      "Iteration 6308, loss = 0.02952798\n",
      "Iteration 6309, loss = 0.02952130\n",
      "Iteration 6310, loss = 0.02951153\n",
      "Iteration 6311, loss = 0.02949409\n",
      "Iteration 6312, loss = 0.02948382\n",
      "Iteration 6313, loss = 0.02947477\n",
      "Iteration 6314, loss = 0.02946334\n",
      "Iteration 6315, loss = 0.02945239\n",
      "Iteration 6316, loss = 0.02944793\n",
      "Iteration 6317, loss = 0.02943450\n",
      "Iteration 6318, loss = 0.02942376\n",
      "Iteration 6319, loss = 0.02941642\n",
      "Iteration 6320, loss = 0.02940516\n",
      "Iteration 6321, loss = 0.02939029\n",
      "Iteration 6322, loss = 0.02938886\n",
      "Iteration 6323, loss = 0.02938043\n",
      "Iteration 6324, loss = 0.02936307\n",
      "Iteration 6325, loss = 0.02935069\n",
      "Iteration 6326, loss = 0.02934513\n",
      "Iteration 6327, loss = 0.02933521\n",
      "Iteration 6328, loss = 0.02932255\n",
      "Iteration 6329, loss = 0.02930582\n",
      "Iteration 6330, loss = 0.02930593\n",
      "Iteration 6331, loss = 0.02929975\n",
      "Iteration 6332, loss = 0.02928453\n",
      "Iteration 6333, loss = 0.02926378\n",
      "Iteration 6334, loss = 0.02925728\n",
      "Iteration 6335, loss = 0.02924680\n",
      "Iteration 6336, loss = 0.02923277\n",
      "Iteration 6337, loss = 0.02922902\n",
      "Iteration 6338, loss = 0.02922006\n",
      "Iteration 6339, loss = 0.02920254\n",
      "Iteration 6340, loss = 0.02919792\n",
      "Iteration 6341, loss = 0.02919224\n",
      "Iteration 6342, loss = 0.02918268\n",
      "Iteration 6343, loss = 0.02917021\n",
      "Iteration 6344, loss = 0.02915422\n",
      "Iteration 6345, loss = 0.02914043\n",
      "Iteration 6346, loss = 0.02913391\n",
      "Iteration 6347, loss = 0.02911816\n",
      "Iteration 6348, loss = 0.02911400\n",
      "Iteration 6349, loss = 0.02910727\n",
      "Iteration 6350, loss = 0.02909650\n",
      "Iteration 6351, loss = 0.02908291\n",
      "Iteration 6352, loss = 0.02906630\n",
      "Iteration 6353, loss = 0.02906519\n",
      "Iteration 6354, loss = 0.02905909\n",
      "Iteration 6355, loss = 0.02904385\n",
      "Iteration 6356, loss = 0.02902280\n",
      "Iteration 6357, loss = 0.02901602\n",
      "Iteration 6358, loss = 0.02900614\n",
      "Iteration 6359, loss = 0.02899188\n",
      "Iteration 6360, loss = 0.02898396\n",
      "Iteration 6361, loss = 0.02897186\n",
      "Iteration 6362, loss = 0.02896750\n",
      "Iteration 6363, loss = 0.02895533\n",
      "Iteration 6364, loss = 0.02894189\n",
      "Iteration 6365, loss = 0.02893329\n",
      "Iteration 6366, loss = 0.02892191\n",
      "Iteration 6367, loss = 0.02891071\n",
      "Iteration 6368, loss = 0.02889817\n",
      "Iteration 6369, loss = 0.02889309\n",
      "Iteration 6370, loss = 0.02888486\n",
      "Iteration 6371, loss = 0.02887243\n",
      "Iteration 6372, loss = 0.02885731\n",
      "Iteration 6373, loss = 0.02885468\n",
      "Iteration 6374, loss = 0.02884665\n",
      "Iteration 6375, loss = 0.02883032\n",
      "Iteration 6376, loss = 0.02881977\n",
      "Iteration 6377, loss = 0.02881461\n",
      "Iteration 6378, loss = 0.02880476\n",
      "Iteration 6379, loss = 0.02879108\n",
      "Iteration 6380, loss = 0.02877422\n",
      "Iteration 6381, loss = 0.02877451\n",
      "Iteration 6382, loss = 0.02876827\n",
      "Iteration 6383, loss = 0.02875245\n",
      "Iteration 6384, loss = 0.02873293\n",
      "Iteration 6385, loss = 0.02872619\n",
      "Iteration 6386, loss = 0.02871606\n",
      "Iteration 6387, loss = 0.02870206\n",
      "Iteration 6388, loss = 0.02869718\n",
      "Iteration 6389, loss = 0.02868798\n",
      "Iteration 6390, loss = 0.02866967\n",
      "Iteration 6391, loss = 0.02866840\n",
      "Iteration 6392, loss = 0.02866377\n",
      "Iteration 6393, loss = 0.02865460\n",
      "Iteration 6394, loss = 0.02864206\n",
      "Iteration 6395, loss = 0.02862591\n",
      "Iteration 6396, loss = 0.02860726\n",
      "Iteration 6397, loss = 0.02860064\n",
      "Iteration 6398, loss = 0.02858792\n",
      "Iteration 6399, loss = 0.02857801\n",
      "Iteration 6400, loss = 0.02857076\n",
      "Iteration 6401, loss = 0.02855675\n",
      "Iteration 6402, loss = 0.02855286\n",
      "Iteration 6403, loss = 0.02854517\n",
      "Iteration 6404, loss = 0.02853339\n",
      "Iteration 6405, loss = 0.02851976\n",
      "Iteration 6406, loss = 0.02850776\n",
      "Iteration 6407, loss = 0.02849883\n",
      "Iteration 6408, loss = 0.02848448\n",
      "Iteration 6409, loss = 0.02847499\n",
      "Iteration 6410, loss = 0.02846458\n",
      "Iteration 6411, loss = 0.02845433\n",
      "Iteration 6412, loss = 0.02844316\n",
      "Iteration 6413, loss = 0.02843703\n",
      "Iteration 6414, loss = 0.02842444\n",
      "Iteration 6415, loss = 0.02841615\n",
      "Iteration 6416, loss = 0.02840812\n",
      "Iteration 6417, loss = 0.02839691\n",
      "Iteration 6418, loss = 0.02838248\n",
      "Iteration 6419, loss = 0.02837819\n",
      "Iteration 6420, loss = 0.02836957\n",
      "Iteration 6421, loss = 0.02835245\n",
      "Iteration 6422, loss = 0.02834623\n",
      "Iteration 6423, loss = 0.02834154\n",
      "Iteration 6424, loss = 0.02833197\n",
      "Iteration 6425, loss = 0.02831891\n",
      "Iteration 6426, loss = 0.02830239\n",
      "Iteration 6427, loss = 0.02829364\n",
      "Iteration 6428, loss = 0.02828740\n",
      "Iteration 6429, loss = 0.02827202\n",
      "Iteration 6430, loss = 0.02826160\n",
      "Iteration 6431, loss = 0.02825564\n",
      "Iteration 6432, loss = 0.02824605\n",
      "Iteration 6433, loss = 0.02823243\n",
      "Iteration 6434, loss = 0.02821552\n",
      "Iteration 6435, loss = 0.02822000\n",
      "Iteration 6436, loss = 0.02821409\n",
      "Iteration 6437, loss = 0.02819882\n",
      "Iteration 6438, loss = 0.02817617\n",
      "Iteration 6439, loss = 0.02817641\n",
      "Iteration 6440, loss = 0.02817408\n",
      "Iteration 6441, loss = 0.02816747\n",
      "Iteration 6442, loss = 0.02815724\n",
      "Iteration 6443, loss = 0.02814327\n",
      "Iteration 6444, loss = 0.02812624\n",
      "Iteration 6445, loss = 0.02810638\n",
      "Iteration 6446, loss = 0.02810684\n",
      "Iteration 6447, loss = 0.02810530\n",
      "Iteration 6448, loss = 0.02809386\n",
      "Iteration 6449, loss = 0.02807273\n",
      "Iteration 6450, loss = 0.02805522\n",
      "Iteration 6451, loss = 0.02805160\n",
      "Iteration 6452, loss = 0.02804364\n",
      "Iteration 6453, loss = 0.02803210\n",
      "Iteration 6454, loss = 0.02801723\n",
      "Iteration 6455, loss = 0.02800255\n",
      "Iteration 6456, loss = 0.02799501\n",
      "Iteration 6457, loss = 0.02798122\n",
      "Iteration 6458, loss = 0.02797120\n",
      "Iteration 6459, loss = 0.02796278\n",
      "Iteration 6460, loss = 0.02795023\n",
      "Iteration 6461, loss = 0.02793990\n",
      "Iteration 6462, loss = 0.02793234\n",
      "Iteration 6463, loss = 0.02792332\n",
      "Iteration 6464, loss = 0.02791035\n",
      "Iteration 6465, loss = 0.02790491\n",
      "Iteration 6466, loss = 0.02789464\n",
      "Iteration 6467, loss = 0.02787916\n",
      "Iteration 6468, loss = 0.02787002\n",
      "Iteration 6469, loss = 0.02785812\n",
      "Iteration 6470, loss = 0.02785555\n",
      "Iteration 6471, loss = 0.02784389\n",
      "Iteration 6472, loss = 0.02782839\n",
      "Iteration 6473, loss = 0.02782037\n",
      "Iteration 6474, loss = 0.02780780\n",
      "Iteration 6475, loss = 0.02780104\n",
      "Iteration 6476, loss = 0.02778947\n",
      "Iteration 6477, loss = 0.02778037\n",
      "Iteration 6478, loss = 0.02777200\n",
      "Iteration 6479, loss = 0.02776056\n",
      "Iteration 6480, loss = 0.02774587\n",
      "Iteration 6481, loss = 0.02774605\n",
      "Iteration 6482, loss = 0.02773764\n",
      "Iteration 6483, loss = 0.02772062\n",
      "Iteration 6484, loss = 0.02770944\n",
      "Iteration 6485, loss = 0.02770446\n",
      "Iteration 6486, loss = 0.02769548\n",
      "Iteration 6487, loss = 0.02768248\n",
      "Iteration 6488, loss = 0.02766606\n",
      "Iteration 6489, loss = 0.02766309\n",
      "Iteration 6490, loss = 0.02765679\n",
      "Iteration 6491, loss = 0.02764121\n",
      "Iteration 6492, loss = 0.02762741\n",
      "Iteration 6493, loss = 0.02762130\n",
      "Iteration 6494, loss = 0.02761158\n",
      "Iteration 6495, loss = 0.02759844\n",
      "Iteration 6496, loss = 0.02758406\n",
      "Iteration 6497, loss = 0.02757374\n",
      "Iteration 6498, loss = 0.02756614\n",
      "Iteration 6499, loss = 0.02755886\n",
      "Iteration 6500, loss = 0.02754651\n",
      "Iteration 6501, loss = 0.02753477\n",
      "Iteration 6502, loss = 0.02752389\n",
      "Iteration 6503, loss = 0.02751746\n",
      "Iteration 6504, loss = 0.02750942\n",
      "Iteration 6505, loss = 0.02749763\n",
      "Iteration 6506, loss = 0.02748202\n",
      "Iteration 6507, loss = 0.02748325\n",
      "Iteration 6508, loss = 0.02747591\n",
      "Iteration 6509, loss = 0.02745978\n",
      "Iteration 6510, loss = 0.02744544\n",
      "Iteration 6511, loss = 0.02744023\n",
      "Iteration 6512, loss = 0.02743022\n",
      "Iteration 6513, loss = 0.02741825\n",
      "Iteration 6514, loss = 0.02740250\n",
      "Iteration 6515, loss = 0.02740288\n",
      "Iteration 6516, loss = 0.02739665\n",
      "Iteration 6517, loss = 0.02738086\n",
      "Iteration 6518, loss = 0.02736336\n",
      "Iteration 6519, loss = 0.02735725\n",
      "Iteration 6520, loss = 0.02734766\n",
      "Iteration 6521, loss = 0.02733400\n",
      "Iteration 6522, loss = 0.02732287\n",
      "Iteration 6523, loss = 0.02731355\n",
      "Iteration 6524, loss = 0.02730438\n",
      "Iteration 6525, loss = 0.02729530\n",
      "Iteration 6526, loss = 0.02728234\n",
      "Iteration 6527, loss = 0.02727377\n",
      "Iteration 6528, loss = 0.02726272\n",
      "Iteration 6529, loss = 0.02725378\n",
      "Iteration 6530, loss = 0.02724620\n",
      "Iteration 6531, loss = 0.02723482\n",
      "Iteration 6532, loss = 0.02721939\n",
      "Iteration 6533, loss = 0.02722097\n",
      "Iteration 6534, loss = 0.02721368\n",
      "Iteration 6535, loss = 0.02719627\n",
      "Iteration 6536, loss = 0.02718258\n",
      "Iteration 6537, loss = 0.02717810\n",
      "Iteration 6538, loss = 0.02716959\n",
      "Iteration 6539, loss = 0.02715662\n",
      "Iteration 6540, loss = 0.02713981\n",
      "Iteration 6541, loss = 0.02713900\n",
      "Iteration 6542, loss = 0.02713264\n",
      "Iteration 6543, loss = 0.02711713\n",
      "Iteration 6544, loss = 0.02710279\n",
      "Iteration 6545, loss = 0.02709740\n",
      "Iteration 6546, loss = 0.02708750\n",
      "Iteration 6547, loss = 0.02707403\n",
      "Iteration 6548, loss = 0.02705885\n",
      "Iteration 6549, loss = 0.02704817\n",
      "Iteration 6550, loss = 0.02704338\n",
      "Iteration 6551, loss = 0.02703546\n",
      "Iteration 6552, loss = 0.02702320\n",
      "Iteration 6553, loss = 0.02700884\n",
      "Iteration 6554, loss = 0.02699767\n",
      "Iteration 6555, loss = 0.02699145\n",
      "Iteration 6556, loss = 0.02697845\n",
      "Iteration 6557, loss = 0.02696778\n",
      "Iteration 6558, loss = 0.02696411\n",
      "Iteration 6559, loss = 0.02695100\n",
      "Iteration 6560, loss = 0.02694305\n",
      "Iteration 6561, loss = 0.02693609\n",
      "Iteration 6562, loss = 0.02692554\n",
      "Iteration 6563, loss = 0.02691151\n",
      "Iteration 6564, loss = 0.02690275\n",
      "Iteration 6565, loss = 0.02689346\n",
      "Iteration 6566, loss = 0.02687819\n",
      "Iteration 6567, loss = 0.02686970\n",
      "Iteration 6568, loss = 0.02685868\n",
      "Iteration 6569, loss = 0.02685048\n",
      "Iteration 6570, loss = 0.02683985\n",
      "Iteration 6571, loss = 0.02683070\n",
      "Iteration 6572, loss = 0.02681786\n",
      "Iteration 6573, loss = 0.02681048\n",
      "Iteration 6574, loss = 0.02680113\n",
      "Iteration 6575, loss = 0.02679083\n",
      "Iteration 6576, loss = 0.02677788\n",
      "Iteration 6577, loss = 0.02677096\n",
      "Iteration 6578, loss = 0.02675931\n",
      "Iteration 6579, loss = 0.02675329\n",
      "Iteration 6580, loss = 0.02674113\n",
      "Iteration 6581, loss = 0.02673579\n",
      "Iteration 6582, loss = 0.02672884\n",
      "Iteration 6583, loss = 0.02671751\n",
      "Iteration 6584, loss = 0.02670240\n",
      "Iteration 6585, loss = 0.02669300\n",
      "Iteration 6586, loss = 0.02668610\n",
      "Iteration 6587, loss = 0.02667077\n",
      "Iteration 6588, loss = 0.02666097\n",
      "Iteration 6589, loss = 0.02665009\n",
      "Iteration 6590, loss = 0.02664064\n",
      "Iteration 6591, loss = 0.02663055\n",
      "Iteration 6592, loss = 0.02662295\n",
      "Iteration 6593, loss = 0.02660937\n",
      "Iteration 6594, loss = 0.02660603\n",
      "Iteration 6595, loss = 0.02660042\n",
      "Iteration 6596, loss = 0.02658978\n",
      "Iteration 6597, loss = 0.02657499\n",
      "Iteration 6598, loss = 0.02656078\n",
      "Iteration 6599, loss = 0.02655206\n",
      "Iteration 6600, loss = 0.02654306\n",
      "Iteration 6601, loss = 0.02653456\n",
      "Iteration 6602, loss = 0.02652191\n",
      "Iteration 6603, loss = 0.02651589\n",
      "Iteration 6604, loss = 0.02650510\n",
      "Iteration 6605, loss = 0.02649258\n",
      "Iteration 6606, loss = 0.02648416\n",
      "Iteration 6607, loss = 0.02647305\n",
      "Iteration 6608, loss = 0.02646231\n",
      "Iteration 6609, loss = 0.02645073\n",
      "Iteration 6610, loss = 0.02644639\n",
      "Iteration 6611, loss = 0.02643958\n",
      "Iteration 6612, loss = 0.02642820\n",
      "Iteration 6613, loss = 0.02641345\n",
      "Iteration 6614, loss = 0.02640852\n",
      "Iteration 6615, loss = 0.02640057\n",
      "Iteration 6616, loss = 0.02638205\n",
      "Iteration 6617, loss = 0.02638013\n",
      "Iteration 6618, loss = 0.02637637\n",
      "Iteration 6619, loss = 0.02636775\n",
      "Iteration 6620, loss = 0.02635506\n",
      "Iteration 6621, loss = 0.02633960\n",
      "Iteration 6622, loss = 0.02632176\n",
      "Iteration 6623, loss = 0.02631501\n",
      "Iteration 6624, loss = 0.02630378\n",
      "Iteration 6625, loss = 0.02629379\n",
      "Iteration 6626, loss = 0.02628410\n",
      "Iteration 6627, loss = 0.02627367\n",
      "Iteration 6628, loss = 0.02626241\n",
      "Iteration 6629, loss = 0.02625897\n",
      "Iteration 6630, loss = 0.02624681\n",
      "Iteration 6631, loss = 0.02623824\n",
      "Iteration 6632, loss = 0.02623130\n",
      "Iteration 6633, loss = 0.02622060\n",
      "Iteration 6634, loss = 0.02620607\n",
      "Iteration 6635, loss = 0.02620044\n",
      "Iteration 6636, loss = 0.02619252\n",
      "Iteration 6637, loss = 0.02617396\n",
      "Iteration 6638, loss = 0.02617445\n",
      "Iteration 6639, loss = 0.02617057\n",
      "Iteration 6640, loss = 0.02616274\n",
      "Iteration 6641, loss = 0.02615077\n",
      "Iteration 6642, loss = 0.02613505\n",
      "Iteration 6643, loss = 0.02611702\n",
      "Iteration 6644, loss = 0.02612052\n",
      "Iteration 6645, loss = 0.02611739\n",
      "Iteration 6646, loss = 0.02610438\n",
      "Iteration 6647, loss = 0.02608166\n",
      "Iteration 6648, loss = 0.02607567\n",
      "Iteration 6649, loss = 0.02607416\n",
      "Iteration 6650, loss = 0.02606818\n",
      "Iteration 6651, loss = 0.02605747\n",
      "Iteration 6652, loss = 0.02604315\n",
      "Iteration 6653, loss = 0.02602618\n",
      "Iteration 6654, loss = 0.02600814\n",
      "Iteration 6655, loss = 0.02600341\n",
      "Iteration 6656, loss = 0.02598830\n",
      "Iteration 6657, loss = 0.02598592\n",
      "Iteration 6658, loss = 0.02598034\n",
      "Iteration 6659, loss = 0.02597110\n",
      "Iteration 6660, loss = 0.02595796\n",
      "Iteration 6661, loss = 0.02594128\n",
      "Iteration 6662, loss = 0.02593931\n",
      "Iteration 6663, loss = 0.02593469\n",
      "Iteration 6664, loss = 0.02591868\n",
      "Iteration 6665, loss = 0.02590295\n",
      "Iteration 6666, loss = 0.02589829\n",
      "Iteration 6667, loss = 0.02588921\n",
      "Iteration 6668, loss = 0.02587600\n",
      "Iteration 6669, loss = 0.02586208\n",
      "Iteration 6670, loss = 0.02585169\n",
      "Iteration 6671, loss = 0.02584595\n",
      "Iteration 6672, loss = 0.02583837\n",
      "Iteration 6673, loss = 0.02582685\n",
      "Iteration 6674, loss = 0.02581201\n",
      "Iteration 6675, loss = 0.02581488\n",
      "Iteration 6676, loss = 0.02580782\n",
      "Iteration 6677, loss = 0.02579080\n",
      "Iteration 6678, loss = 0.02577774\n",
      "Iteration 6679, loss = 0.02577346\n",
      "Iteration 6680, loss = 0.02576500\n",
      "Iteration 6681, loss = 0.02575320\n",
      "Iteration 6682, loss = 0.02573773\n",
      "Iteration 6683, loss = 0.02573284\n",
      "Iteration 6684, loss = 0.02572649\n",
      "Iteration 6685, loss = 0.02570980\n",
      "Iteration 6686, loss = 0.02570164\n",
      "Iteration 6687, loss = 0.02569693\n",
      "Iteration 6688, loss = 0.02568875\n",
      "Iteration 6689, loss = 0.02567655\n",
      "Iteration 6690, loss = 0.02566041\n",
      "Iteration 6691, loss = 0.02565437\n",
      "Iteration 6692, loss = 0.02564824\n",
      "Iteration 6693, loss = 0.02563171\n",
      "Iteration 6694, loss = 0.02562426\n",
      "Iteration 6695, loss = 0.02561994\n",
      "Iteration 6696, loss = 0.02561166\n",
      "Iteration 6697, loss = 0.02559883\n",
      "Iteration 6698, loss = 0.02558309\n",
      "Iteration 6699, loss = 0.02557778\n",
      "Iteration 6700, loss = 0.02557206\n",
      "Iteration 6701, loss = 0.02555601\n",
      "Iteration 6702, loss = 0.02554791\n",
      "Iteration 6703, loss = 0.02554324\n",
      "Iteration 6704, loss = 0.02553398\n",
      "Iteration 6705, loss = 0.02552112\n",
      "Iteration 6706, loss = 0.02550584\n",
      "Iteration 6707, loss = 0.02550099\n",
      "Iteration 6708, loss = 0.02549472\n",
      "Iteration 6709, loss = 0.02547839\n",
      "Iteration 6710, loss = 0.02547020\n",
      "Iteration 6711, loss = 0.02546588\n",
      "Iteration 6712, loss = 0.02545681\n",
      "Iteration 6713, loss = 0.02544423\n",
      "Iteration 6714, loss = 0.02542812\n",
      "Iteration 6715, loss = 0.02542378\n",
      "Iteration 6716, loss = 0.02541777\n",
      "Iteration 6717, loss = 0.02540104\n",
      "Iteration 6718, loss = 0.02539215\n",
      "Iteration 6719, loss = 0.02538790\n",
      "Iteration 6720, loss = 0.02537976\n",
      "Iteration 6721, loss = 0.02536736\n",
      "Iteration 6722, loss = 0.02535165\n",
      "Iteration 6723, loss = 0.02534710\n",
      "Iteration 6724, loss = 0.02534062\n",
      "Iteration 6725, loss = 0.02532451\n",
      "Iteration 6726, loss = 0.02531638\n",
      "Iteration 6727, loss = 0.02531151\n",
      "Iteration 6728, loss = 0.02530242\n",
      "Iteration 6729, loss = 0.02529084\n",
      "Iteration 6730, loss = 0.02527545\n",
      "Iteration 6731, loss = 0.02526964\n",
      "Iteration 6732, loss = 0.02526343\n",
      "Iteration 6733, loss = 0.02524716\n",
      "Iteration 6734, loss = 0.02523994\n",
      "Iteration 6735, loss = 0.02523524\n",
      "Iteration 6736, loss = 0.02522650\n",
      "Iteration 6737, loss = 0.02521406\n",
      "Iteration 6738, loss = 0.02519862\n",
      "Iteration 6739, loss = 0.02519250\n",
      "Iteration 6740, loss = 0.02518634\n",
      "Iteration 6741, loss = 0.02516977\n",
      "Iteration 6742, loss = 0.02516346\n",
      "Iteration 6743, loss = 0.02515910\n",
      "Iteration 6744, loss = 0.02515087\n",
      "Iteration 6745, loss = 0.02513906\n",
      "Iteration 6746, loss = 0.02512353\n",
      "Iteration 6747, loss = 0.02511458\n",
      "Iteration 6748, loss = 0.02510859\n",
      "Iteration 6749, loss = 0.02509220\n",
      "Iteration 6750, loss = 0.02508808\n",
      "Iteration 6751, loss = 0.02508369\n",
      "Iteration 6752, loss = 0.02507564\n",
      "Iteration 6753, loss = 0.02506347\n",
      "Iteration 6754, loss = 0.02504781\n",
      "Iteration 6755, loss = 0.02503636\n",
      "Iteration 6756, loss = 0.02502997\n",
      "Iteration 6757, loss = 0.02501327\n",
      "Iteration 6758, loss = 0.02501373\n",
      "Iteration 6759, loss = 0.02500939\n",
      "Iteration 6760, loss = 0.02500096\n",
      "Iteration 6761, loss = 0.02498847\n",
      "Iteration 6762, loss = 0.02497347\n",
      "Iteration 6763, loss = 0.02495851\n",
      "Iteration 6764, loss = 0.02495165\n",
      "Iteration 6765, loss = 0.02493861\n",
      "Iteration 6766, loss = 0.02492937\n",
      "Iteration 6767, loss = 0.02492089\n",
      "Iteration 6768, loss = 0.02491016\n",
      "Iteration 6769, loss = 0.02489999\n",
      "Iteration 6770, loss = 0.02489604\n",
      "Iteration 6771, loss = 0.02488284\n",
      "Iteration 6772, loss = 0.02487752\n",
      "Iteration 6773, loss = 0.02487171\n",
      "Iteration 6774, loss = 0.02486155\n",
      "Iteration 6775, loss = 0.02484781\n",
      "Iteration 6776, loss = 0.02483475\n",
      "Iteration 6777, loss = 0.02482604\n",
      "Iteration 6778, loss = 0.02481798\n",
      "Iteration 6779, loss = 0.02480994\n",
      "Iteration 6780, loss = 0.02479871\n",
      "Iteration 6781, loss = 0.02478831\n",
      "Iteration 6782, loss = 0.02477708\n",
      "Iteration 6783, loss = 0.02477333\n",
      "Iteration 6784, loss = 0.02476679\n",
      "Iteration 6785, loss = 0.02475604\n",
      "Iteration 6786, loss = 0.02474214\n",
      "Iteration 6787, loss = 0.02473558\n",
      "Iteration 6788, loss = 0.02472788\n",
      "Iteration 6789, loss = 0.02471096\n",
      "Iteration 6790, loss = 0.02470260\n",
      "Iteration 6791, loss = 0.02469236\n",
      "Iteration 6792, loss = 0.02468508\n",
      "Iteration 6793, loss = 0.02467551\n",
      "Iteration 6794, loss = 0.02466427\n",
      "Iteration 6795, loss = 0.02465585\n",
      "Iteration 6796, loss = 0.02464508\n",
      "Iteration 6797, loss = 0.02464065\n",
      "Iteration 6798, loss = 0.02462780\n",
      "Iteration 6799, loss = 0.02462222\n",
      "Iteration 6800, loss = 0.02461658\n",
      "Iteration 6801, loss = 0.02460651\n",
      "Iteration 6802, loss = 0.02459332\n",
      "Iteration 6803, loss = 0.02458186\n",
      "Iteration 6804, loss = 0.02457280\n",
      "Iteration 6805, loss = 0.02456312\n",
      "Iteration 6806, loss = 0.02455525\n",
      "Iteration 6807, loss = 0.02454375\n",
      "Iteration 6808, loss = 0.02453548\n",
      "Iteration 6809, loss = 0.02452367\n",
      "Iteration 6810, loss = 0.02451829\n",
      "Iteration 6811, loss = 0.02451208\n",
      "Iteration 6812, loss = 0.02450184\n",
      "Iteration 6813, loss = 0.02448787\n",
      "Iteration 6814, loss = 0.02448285\n",
      "Iteration 6815, loss = 0.02447479\n",
      "Iteration 6816, loss = 0.02445696\n",
      "Iteration 6817, loss = 0.02444927\n",
      "Iteration 6818, loss = 0.02443941\n",
      "Iteration 6819, loss = 0.02443174\n",
      "Iteration 6820, loss = 0.02442226\n",
      "Iteration 6821, loss = 0.02441074\n",
      "Iteration 6822, loss = 0.02440304\n",
      "Iteration 6823, loss = 0.02439276\n",
      "Iteration 6824, loss = 0.02438687\n",
      "Iteration 6825, loss = 0.02437394\n",
      "Iteration 6826, loss = 0.02437030\n",
      "Iteration 6827, loss = 0.02436478\n",
      "Iteration 6828, loss = 0.02435511\n",
      "Iteration 6829, loss = 0.02434164\n",
      "Iteration 6830, loss = 0.02432694\n",
      "Iteration 6831, loss = 0.02431812\n",
      "Iteration 6832, loss = 0.02431246\n",
      "Iteration 6833, loss = 0.02430471\n",
      "Iteration 6834, loss = 0.02429339\n",
      "Iteration 6835, loss = 0.02428030\n",
      "Iteration 6836, loss = 0.02427121\n",
      "Iteration 6837, loss = 0.02426389\n",
      "Iteration 6838, loss = 0.02425428\n",
      "Iteration 6839, loss = 0.02424477\n",
      "Iteration 6840, loss = 0.02423491\n",
      "Iteration 6841, loss = 0.02422525\n",
      "Iteration 6842, loss = 0.02421484\n",
      "Iteration 6843, loss = 0.02421085\n",
      "Iteration 6844, loss = 0.02419812\n",
      "Iteration 6845, loss = 0.02419318\n",
      "Iteration 6846, loss = 0.02418713\n",
      "Iteration 6847, loss = 0.02417780\n",
      "Iteration 6848, loss = 0.02416488\n",
      "Iteration 6849, loss = 0.02415148\n",
      "Iteration 6850, loss = 0.02414274\n",
      "Iteration 6851, loss = 0.02413611\n",
      "Iteration 6852, loss = 0.02412829\n",
      "Iteration 6853, loss = 0.02411693\n",
      "Iteration 6854, loss = 0.02410383\n",
      "Iteration 6855, loss = 0.02409508\n",
      "Iteration 6856, loss = 0.02408775\n",
      "Iteration 6857, loss = 0.02407878\n",
      "Iteration 6858, loss = 0.02406910\n",
      "Iteration 6859, loss = 0.02405849\n",
      "Iteration 6860, loss = 0.02405017\n",
      "Iteration 6861, loss = 0.02404030\n",
      "Iteration 6862, loss = 0.02403545\n",
      "Iteration 6863, loss = 0.02402227\n",
      "Iteration 6864, loss = 0.02401852\n",
      "Iteration 6865, loss = 0.02401323\n",
      "Iteration 6866, loss = 0.02400383\n",
      "Iteration 6867, loss = 0.02399067\n",
      "Iteration 6868, loss = 0.02397499\n",
      "Iteration 6869, loss = 0.02398203\n",
      "Iteration 6870, loss = 0.02397665\n",
      "Iteration 6871, loss = 0.02396036\n",
      "Iteration 6872, loss = 0.02393892\n",
      "Iteration 6873, loss = 0.02393493\n",
      "Iteration 6874, loss = 0.02392662\n",
      "Iteration 6875, loss = 0.02391498\n",
      "Iteration 6876, loss = 0.02390343\n",
      "Iteration 6877, loss = 0.02389296\n",
      "Iteration 6878, loss = 0.02388879\n",
      "Iteration 6879, loss = 0.02388165\n",
      "Iteration 6880, loss = 0.02387068\n",
      "Iteration 6881, loss = 0.02385728\n",
      "Iteration 6882, loss = 0.02385457\n",
      "Iteration 6883, loss = 0.02384668\n",
      "Iteration 6884, loss = 0.02382735\n",
      "Iteration 6885, loss = 0.02382832\n",
      "Iteration 6886, loss = 0.02382618\n",
      "Iteration 6887, loss = 0.02381936\n",
      "Iteration 6888, loss = 0.02380847\n",
      "Iteration 6889, loss = 0.02379410\n",
      "Iteration 6890, loss = 0.02377680\n",
      "Iteration 6891, loss = 0.02377397\n",
      "Iteration 6892, loss = 0.02377068\n",
      "Iteration 6893, loss = 0.02375612\n",
      "Iteration 6894, loss = 0.02373823\n",
      "Iteration 6895, loss = 0.02373362\n",
      "Iteration 6896, loss = 0.02372507\n",
      "Iteration 6897, loss = 0.02371258\n",
      "Iteration 6898, loss = 0.02370417\n",
      "Iteration 6899, loss = 0.02369454\n",
      "Iteration 6900, loss = 0.02368564\n",
      "Iteration 6901, loss = 0.02367831\n",
      "Iteration 6902, loss = 0.02366827\n",
      "Iteration 6903, loss = 0.02365454\n",
      "Iteration 6904, loss = 0.02365813\n",
      "Iteration 6905, loss = 0.02364962\n",
      "Iteration 6906, loss = 0.02363129\n",
      "Iteration 6907, loss = 0.02362466\n",
      "Iteration 6908, loss = 0.02362208\n",
      "Iteration 6909, loss = 0.02361522\n",
      "Iteration 6910, loss = 0.02360460\n",
      "Iteration 6911, loss = 0.02359047\n",
      "Iteration 6912, loss = 0.02357330\n",
      "Iteration 6913, loss = 0.02357888\n",
      "Iteration 6914, loss = 0.02357525\n",
      "Iteration 6915, loss = 0.02356056\n",
      "Iteration 6916, loss = 0.02353719\n",
      "Iteration 6917, loss = 0.02353974\n",
      "Iteration 6918, loss = 0.02353897\n",
      "Iteration 6919, loss = 0.02353423\n",
      "Iteration 6920, loss = 0.02352602\n",
      "Iteration 6921, loss = 0.02351380\n",
      "Iteration 6922, loss = 0.02349819\n",
      "Iteration 6923, loss = 0.02348000\n",
      "Iteration 6924, loss = 0.02346668\n",
      "Iteration 6925, loss = 0.02346524\n",
      "Iteration 6926, loss = 0.02345221\n",
      "Iteration 6927, loss = 0.02343813\n",
      "Iteration 6928, loss = 0.02343294\n",
      "Iteration 6929, loss = 0.02342389\n",
      "Iteration 6930, loss = 0.02341173\n",
      "Iteration 6931, loss = 0.02340317\n",
      "Iteration 6932, loss = 0.02339361\n",
      "Iteration 6933, loss = 0.02338368\n",
      "Iteration 6934, loss = 0.02337627\n",
      "Iteration 6935, loss = 0.02336523\n",
      "Iteration 6936, loss = 0.02335418\n",
      "Iteration 6937, loss = 0.02334393\n",
      "Iteration 6938, loss = 0.02333641\n",
      "Iteration 6939, loss = 0.02332812\n",
      "Iteration 6940, loss = 0.02331962\n",
      "Iteration 6941, loss = 0.02330763\n",
      "Iteration 6942, loss = 0.02330752\n",
      "Iteration 6943, loss = 0.02329759\n",
      "Iteration 6944, loss = 0.02328175\n",
      "Iteration 6945, loss = 0.02327505\n",
      "Iteration 6946, loss = 0.02326466\n",
      "Iteration 6947, loss = 0.02325546\n",
      "Iteration 6948, loss = 0.02324377\n",
      "Iteration 6949, loss = 0.02323736\n",
      "Iteration 6950, loss = 0.02322821\n",
      "Iteration 6951, loss = 0.02322025\n",
      "Iteration 6952, loss = 0.02320851\n",
      "Iteration 6953, loss = 0.02320831\n",
      "Iteration 6954, loss = 0.02319805\n",
      "Iteration 6955, loss = 0.02318262\n",
      "Iteration 6956, loss = 0.02317608\n",
      "Iteration 6957, loss = 0.02316579\n",
      "Iteration 6958, loss = 0.02315467\n",
      "Iteration 6959, loss = 0.02314580\n",
      "Iteration 6960, loss = 0.02313663\n",
      "Iteration 6961, loss = 0.02313100\n",
      "Iteration 6962, loss = 0.02312191\n",
      "Iteration 6963, loss = 0.02311059\n",
      "Iteration 6964, loss = 0.02310598\n",
      "Iteration 6965, loss = 0.02309525\n",
      "Iteration 6966, loss = 0.02308504\n",
      "Iteration 6967, loss = 0.02307847\n",
      "Iteration 6968, loss = 0.02306842\n",
      "Iteration 6969, loss = 0.02305560\n",
      "Iteration 6970, loss = 0.02305655\n",
      "Iteration 6971, loss = 0.02304846\n",
      "Iteration 6972, loss = 0.02302944\n",
      "Iteration 6973, loss = 0.02302863\n",
      "Iteration 6974, loss = 0.02302619\n",
      "Iteration 6975, loss = 0.02301958\n",
      "Iteration 6976, loss = 0.02300939\n",
      "Iteration 6977, loss = 0.02299585\n",
      "Iteration 6978, loss = 0.02297913\n",
      "Iteration 6979, loss = 0.02297333\n",
      "Iteration 6980, loss = 0.02296981\n",
      "Iteration 6981, loss = 0.02295505\n",
      "Iteration 6982, loss = 0.02294288\n",
      "Iteration 6983, loss = 0.02293855\n",
      "Iteration 6984, loss = 0.02293055\n",
      "Iteration 6985, loss = 0.02291908\n",
      "Iteration 6986, loss = 0.02290419\n",
      "Iteration 6987, loss = 0.02290545\n",
      "Iteration 6988, loss = 0.02289952\n",
      "Iteration 6989, loss = 0.02288257\n",
      "Iteration 6990, loss = 0.02287257\n",
      "Iteration 6991, loss = 0.02286945\n",
      "Iteration 6992, loss = 0.02286239\n",
      "Iteration 6993, loss = 0.02285136\n",
      "Iteration 6994, loss = 0.02283731\n",
      "Iteration 6995, loss = 0.02282438\n",
      "Iteration 6996, loss = 0.02281775\n",
      "Iteration 6997, loss = 0.02280710\n",
      "Iteration 6998, loss = 0.02279903\n",
      "Iteration 6999, loss = 0.02278777\n",
      "Iteration 7000, loss = 0.02278418\n",
      "Iteration 7001, loss = 0.02277329\n",
      "Iteration 7002, loss = 0.02276436\n",
      "Iteration 7003, loss = 0.02275818\n",
      "Iteration 7004, loss = 0.02274855\n",
      "Iteration 7005, loss = 0.02273566\n",
      "Iteration 7006, loss = 0.02273225\n",
      "Iteration 7007, loss = 0.02272392\n",
      "Iteration 7008, loss = 0.02270741\n",
      "Iteration 7009, loss = 0.02270029\n",
      "Iteration 7010, loss = 0.02268959\n",
      "Iteration 7011, loss = 0.02268649\n",
      "Iteration 7012, loss = 0.02267465\n",
      "Iteration 7013, loss = 0.02266783\n",
      "Iteration 7014, loss = 0.02266213\n",
      "Iteration 7015, loss = 0.02265332\n",
      "Iteration 7016, loss = 0.02264069\n",
      "Iteration 7017, loss = 0.02262992\n",
      "Iteration 7018, loss = 0.02262106\n",
      "Iteration 7019, loss = 0.02261352\n",
      "Iteration 7020, loss = 0.02260689\n",
      "Iteration 7021, loss = 0.02259654\n",
      "Iteration 7022, loss = 0.02258259\n",
      "Iteration 7023, loss = 0.02258685\n",
      "Iteration 7024, loss = 0.02257941\n",
      "Iteration 7025, loss = 0.02256030\n",
      "Iteration 7026, loss = 0.02255566\n",
      "Iteration 7027, loss = 0.02255327\n",
      "Iteration 7028, loss = 0.02254690\n",
      "Iteration 7029, loss = 0.02253678\n",
      "Iteration 7030, loss = 0.02252340\n",
      "Iteration 7031, loss = 0.02250690\n",
      "Iteration 7032, loss = 0.02250590\n",
      "Iteration 7033, loss = 0.02250250\n",
      "Iteration 7034, loss = 0.02248721\n",
      "Iteration 7035, loss = 0.02247148\n",
      "Iteration 7036, loss = 0.02246792\n",
      "Iteration 7037, loss = 0.02246022\n",
      "Iteration 7038, loss = 0.02244874\n",
      "Iteration 7039, loss = 0.02243395\n",
      "Iteration 7040, loss = 0.02243704\n",
      "Iteration 7041, loss = 0.02243122\n",
      "Iteration 7042, loss = 0.02241323\n",
      "Iteration 7043, loss = 0.02240477\n",
      "Iteration 7044, loss = 0.02240237\n",
      "Iteration 7045, loss = 0.02239564\n",
      "Iteration 7046, loss = 0.02238531\n",
      "Iteration 7047, loss = 0.02237160\n",
      "Iteration 7048, loss = 0.02235486\n",
      "Iteration 7049, loss = 0.02236398\n",
      "Iteration 7050, loss = 0.02236045\n",
      "Iteration 7051, loss = 0.02234608\n",
      "Iteration 7052, loss = 0.02232118\n",
      "Iteration 7053, loss = 0.02232477\n",
      "Iteration 7054, loss = 0.02232529\n",
      "Iteration 7055, loss = 0.02232200\n",
      "Iteration 7056, loss = 0.02231427\n",
      "Iteration 7057, loss = 0.02230259\n",
      "Iteration 7058, loss = 0.02228789\n",
      "Iteration 7059, loss = 0.02227055\n",
      "Iteration 7060, loss = 0.02225224\n",
      "Iteration 7061, loss = 0.02226328\n",
      "Iteration 7062, loss = 0.02226370\n",
      "Iteration 7063, loss = 0.02225247\n",
      "Iteration 7064, loss = 0.02223075\n",
      "Iteration 7065, loss = 0.02221085\n",
      "Iteration 7066, loss = 0.02221093\n",
      "Iteration 7067, loss = 0.02220589\n",
      "Iteration 7068, loss = 0.02219635\n",
      "Iteration 7069, loss = 0.02218328\n",
      "Iteration 7070, loss = 0.02216764\n",
      "Iteration 7071, loss = 0.02216213\n",
      "Iteration 7072, loss = 0.02215736\n",
      "Iteration 7073, loss = 0.02214155\n",
      "Iteration 7074, loss = 0.02213567\n",
      "Iteration 7075, loss = 0.02213249\n",
      "Iteration 7076, loss = 0.02212563\n",
      "Iteration 7077, loss = 0.02211500\n",
      "Iteration 7078, loss = 0.02210121\n",
      "Iteration 7079, loss = 0.02208450\n",
      "Iteration 7080, loss = 0.02207752\n",
      "Iteration 7081, loss = 0.02207128\n",
      "Iteration 7082, loss = 0.02206412\n",
      "Iteration 7083, loss = 0.02205303\n",
      "Iteration 7084, loss = 0.02204511\n",
      "Iteration 7085, loss = 0.02203416\n",
      "Iteration 7086, loss = 0.02203063\n",
      "Iteration 7087, loss = 0.02202523\n",
      "Iteration 7088, loss = 0.02201607\n",
      "Iteration 7089, loss = 0.02200401\n",
      "Iteration 7090, loss = 0.02199189\n",
      "Iteration 7091, loss = 0.02198316\n",
      "Iteration 7092, loss = 0.02197771\n",
      "Iteration 7093, loss = 0.02197157\n",
      "Iteration 7094, loss = 0.02196173\n",
      "Iteration 7095, loss = 0.02194847\n",
      "Iteration 7096, loss = 0.02194892\n",
      "Iteration 7097, loss = 0.02194151\n",
      "Iteration 7098, loss = 0.02192265\n",
      "Iteration 7099, loss = 0.02192277\n",
      "Iteration 7100, loss = 0.02192100\n",
      "Iteration 7101, loss = 0.02191516\n",
      "Iteration 7102, loss = 0.02190533\n",
      "Iteration 7103, loss = 0.02189250\n",
      "Iteration 7104, loss = 0.02187681\n",
      "Iteration 7105, loss = 0.02186611\n",
      "Iteration 7106, loss = 0.02186219\n",
      "Iteration 7107, loss = 0.02184618\n",
      "Iteration 7108, loss = 0.02184350\n",
      "Iteration 7109, loss = 0.02184044\n",
      "Iteration 7110, loss = 0.02183324\n",
      "Iteration 7111, loss = 0.02182206\n",
      "Iteration 7112, loss = 0.02180831\n",
      "Iteration 7113, loss = 0.02179360\n",
      "Iteration 7114, loss = 0.02178730\n",
      "Iteration 7115, loss = 0.02177809\n",
      "Iteration 7116, loss = 0.02177077\n",
      "Iteration 7117, loss = 0.02176046\n",
      "Iteration 7118, loss = 0.02175563\n",
      "Iteration 7119, loss = 0.02174409\n",
      "Iteration 7120, loss = 0.02173806\n",
      "Iteration 7121, loss = 0.02173309\n",
      "Iteration 7122, loss = 0.02172452\n",
      "Iteration 7123, loss = 0.02171240\n",
      "Iteration 7124, loss = 0.02170192\n",
      "Iteration 7125, loss = 0.02169288\n",
      "Iteration 7126, loss = 0.02168671\n",
      "Iteration 7127, loss = 0.02168016\n",
      "Iteration 7128, loss = 0.02167065\n",
      "Iteration 7129, loss = 0.02165806\n",
      "Iteration 7130, loss = 0.02165866\n",
      "Iteration 7131, loss = 0.02165057\n",
      "Iteration 7132, loss = 0.02163137\n",
      "Iteration 7133, loss = 0.02163330\n",
      "Iteration 7134, loss = 0.02163207\n",
      "Iteration 7135, loss = 0.02162610\n",
      "Iteration 7136, loss = 0.02161628\n",
      "Iteration 7137, loss = 0.02160377\n",
      "Iteration 7138, loss = 0.02158844\n",
      "Iteration 7139, loss = 0.02157402\n",
      "Iteration 7140, loss = 0.02157005\n",
      "Iteration 7141, loss = 0.02155505\n",
      "Iteration 7142, loss = 0.02154685\n",
      "Iteration 7143, loss = 0.02154217\n",
      "Iteration 7144, loss = 0.02153018\n",
      "Iteration 7145, loss = 0.02152184\n",
      "Iteration 7146, loss = 0.02151768\n",
      "Iteration 7147, loss = 0.02150446\n",
      "Iteration 7148, loss = 0.02149649\n",
      "Iteration 7149, loss = 0.02149182\n",
      "Iteration 7150, loss = 0.02148490\n",
      "Iteration 7151, loss = 0.02147509\n",
      "Iteration 7152, loss = 0.02146275\n",
      "Iteration 7153, loss = 0.02145472\n",
      "Iteration 7154, loss = 0.02144574\n",
      "Iteration 7155, loss = 0.02144074\n",
      "Iteration 7156, loss = 0.02143307\n",
      "Iteration 7157, loss = 0.02142222\n",
      "Iteration 7158, loss = 0.02141686\n",
      "Iteration 7159, loss = 0.02140612\n",
      "Iteration 7160, loss = 0.02139941\n",
      "Iteration 7161, loss = 0.02139407\n",
      "Iteration 7162, loss = 0.02138505\n",
      "Iteration 7163, loss = 0.02137285\n",
      "Iteration 7164, loss = 0.02136564\n",
      "Iteration 7165, loss = 0.02135707\n",
      "Iteration 7166, loss = 0.02134733\n",
      "Iteration 7167, loss = 0.02134142\n",
      "Iteration 7168, loss = 0.02133182\n",
      "Iteration 7169, loss = 0.02131907\n",
      "Iteration 7170, loss = 0.02132316\n",
      "Iteration 7171, loss = 0.02131523\n",
      "Iteration 7172, loss = 0.02129581\n",
      "Iteration 7173, loss = 0.02129550\n",
      "Iteration 7174, loss = 0.02129467\n",
      "Iteration 7175, loss = 0.02128918\n",
      "Iteration 7176, loss = 0.02127952\n",
      "Iteration 7177, loss = 0.02126721\n",
      "Iteration 7178, loss = 0.02125214\n",
      "Iteration 7179, loss = 0.02123807\n",
      "Iteration 7180, loss = 0.02123407\n",
      "Iteration 7181, loss = 0.02122015\n",
      "Iteration 7182, loss = 0.02121210\n",
      "Iteration 7183, loss = 0.02120500\n",
      "Iteration 7184, loss = 0.02119576\n",
      "Iteration 7185, loss = 0.02118778\n",
      "Iteration 7186, loss = 0.02118019\n",
      "Iteration 7187, loss = 0.02117085\n",
      "Iteration 7188, loss = 0.02116190\n",
      "Iteration 7189, loss = 0.02115723\n",
      "Iteration 7190, loss = 0.02114397\n",
      "Iteration 7191, loss = 0.02113696\n",
      "Iteration 7192, loss = 0.02113183\n",
      "Iteration 7193, loss = 0.02112552\n",
      "Iteration 7194, loss = 0.02111528\n",
      "Iteration 7195, loss = 0.02110310\n",
      "Iteration 7196, loss = 0.02109575\n",
      "Iteration 7197, loss = 0.02108634\n",
      "Iteration 7198, loss = 0.02108211\n",
      "Iteration 7199, loss = 0.02107514\n",
      "Iteration 7200, loss = 0.02106441\n",
      "Iteration 7201, loss = 0.02105785\n",
      "Iteration 7202, loss = 0.02104740\n",
      "Iteration 7203, loss = 0.02104241\n",
      "Iteration 7204, loss = 0.02103740\n",
      "Iteration 7205, loss = 0.02102892\n",
      "Iteration 7206, loss = 0.02101710\n",
      "Iteration 7207, loss = 0.02100655\n",
      "Iteration 7208, loss = 0.02099762\n",
      "Iteration 7209, loss = 0.02099203\n",
      "Iteration 7210, loss = 0.02098657\n",
      "Iteration 7211, loss = 0.02097756\n",
      "Iteration 7212, loss = 0.02096498\n",
      "Iteration 7213, loss = 0.02096195\n",
      "Iteration 7214, loss = 0.02095421\n",
      "Iteration 7215, loss = 0.02093925\n",
      "Iteration 7216, loss = 0.02093314\n",
      "Iteration 7217, loss = 0.02092356\n",
      "Iteration 7218, loss = 0.02091640\n",
      "Iteration 7219, loss = 0.02090503\n",
      "Iteration 7220, loss = 0.02089889\n",
      "Iteration 7221, loss = 0.02089215\n",
      "Iteration 7222, loss = 0.02088522\n",
      "Iteration 7223, loss = 0.02087489\n",
      "Iteration 7224, loss = 0.02086807\n",
      "Iteration 7225, loss = 0.02085713\n",
      "Iteration 7226, loss = 0.02085437\n",
      "Iteration 7227, loss = 0.02084958\n",
      "Iteration 7228, loss = 0.02084092\n",
      "Iteration 7229, loss = 0.02082919\n",
      "Iteration 7230, loss = 0.02081513\n",
      "Iteration 7231, loss = 0.02082311\n",
      "Iteration 7232, loss = 0.02081822\n",
      "Iteration 7233, loss = 0.02080079\n",
      "Iteration 7234, loss = 0.02078641\n",
      "Iteration 7235, loss = 0.02078446\n",
      "Iteration 7236, loss = 0.02077842\n",
      "Iteration 7237, loss = 0.02076860\n",
      "Iteration 7238, loss = 0.02075591\n",
      "Iteration 7239, loss = 0.02074309\n",
      "Iteration 7240, loss = 0.02073565\n",
      "Iteration 7241, loss = 0.02072924\n",
      "Iteration 7242, loss = 0.02072303\n",
      "Iteration 7243, loss = 0.02071351\n",
      "Iteration 7244, loss = 0.02070103\n",
      "Iteration 7245, loss = 0.02070512\n",
      "Iteration 7246, loss = 0.02069777\n",
      "Iteration 7247, loss = 0.02067860\n",
      "Iteration 7248, loss = 0.02067841\n",
      "Iteration 7249, loss = 0.02067719\n",
      "Iteration 7250, loss = 0.02067200\n",
      "Iteration 7251, loss = 0.02066311\n",
      "Iteration 7252, loss = 0.02065112\n",
      "Iteration 7253, loss = 0.02063629\n",
      "Iteration 7254, loss = 0.02062078\n",
      "Iteration 7255, loss = 0.02061688\n",
      "Iteration 7256, loss = 0.02060525\n",
      "Iteration 7257, loss = 0.02059793\n",
      "Iteration 7258, loss = 0.02058756\n",
      "Iteration 7259, loss = 0.02058337\n",
      "Iteration 7260, loss = 0.02057555\n",
      "Iteration 7261, loss = 0.02056420\n",
      "Iteration 7262, loss = 0.02056444\n",
      "Iteration 7263, loss = 0.02055499\n",
      "Iteration 7264, loss = 0.02054134\n",
      "Iteration 7265, loss = 0.02053626\n",
      "Iteration 7266, loss = 0.02052768\n",
      "Iteration 7267, loss = 0.02051586\n",
      "Iteration 7268, loss = 0.02051726\n",
      "Iteration 7269, loss = 0.02050872\n",
      "Iteration 7270, loss = 0.02049172\n",
      "Iteration 7271, loss = 0.02048621\n",
      "Iteration 7272, loss = 0.02047728\n",
      "Iteration 7273, loss = 0.02046854\n",
      "Iteration 7274, loss = 0.02045951\n",
      "Iteration 7275, loss = 0.02045069\n",
      "Iteration 7276, loss = 0.02044933\n",
      "Iteration 7277, loss = 0.02043627\n",
      "Iteration 7278, loss = 0.02043294\n",
      "Iteration 7279, loss = 0.02042961\n",
      "Iteration 7280, loss = 0.02042219\n",
      "Iteration 7281, loss = 0.02041135\n",
      "Iteration 7282, loss = 0.02039790\n",
      "Iteration 7283, loss = 0.02039453\n",
      "Iteration 7284, loss = 0.02038859\n",
      "Iteration 7285, loss = 0.02037057\n",
      "Iteration 7286, loss = 0.02037244\n",
      "Iteration 7287, loss = 0.02037119\n",
      "Iteration 7288, loss = 0.02036567\n",
      "Iteration 7289, loss = 0.02035636\n",
      "Iteration 7290, loss = 0.02034424\n",
      "Iteration 7291, loss = 0.02032941\n",
      "Iteration 7292, loss = 0.02031846\n",
      "Iteration 7293, loss = 0.02031502\n",
      "Iteration 7294, loss = 0.02029878\n",
      "Iteration 7295, loss = 0.02029945\n",
      "Iteration 7296, loss = 0.02029746\n",
      "Iteration 7297, loss = 0.02029136\n",
      "Iteration 7298, loss = 0.02028179\n",
      "Iteration 7299, loss = 0.02026904\n",
      "Iteration 7300, loss = 0.02025380\n",
      "Iteration 7301, loss = 0.02025344\n",
      "Iteration 7302, loss = 0.02025058\n",
      "Iteration 7303, loss = 0.02023526\n",
      "Iteration 7304, loss = 0.02022247\n",
      "Iteration 7305, loss = 0.02021997\n",
      "Iteration 7306, loss = 0.02021349\n",
      "Iteration 7307, loss = 0.02020355\n",
      "Iteration 7308, loss = 0.02019053\n",
      "Iteration 7309, loss = 0.02018181\n",
      "Iteration 7310, loss = 0.02017478\n",
      "Iteration 7311, loss = 0.02016420\n",
      "Iteration 7312, loss = 0.02015816\n",
      "Iteration 7313, loss = 0.02014868\n",
      "Iteration 7314, loss = 0.02013924\n",
      "Iteration 7315, loss = 0.02013010\n",
      "Iteration 7316, loss = 0.02012128\n",
      "Iteration 7317, loss = 0.02011798\n",
      "Iteration 7318, loss = 0.02011136\n",
      "Iteration 7319, loss = 0.02010109\n",
      "Iteration 7320, loss = 0.02009039\n",
      "Iteration 7321, loss = 0.02008174\n",
      "Iteration 7322, loss = 0.02007447\n",
      "Iteration 7323, loss = 0.02006861\n",
      "Iteration 7324, loss = 0.02006161\n",
      "Iteration 7325, loss = 0.02005160\n",
      "Iteration 7326, loss = 0.02004602\n",
      "Iteration 7327, loss = 0.02003507\n",
      "Iteration 7328, loss = 0.02003149\n",
      "Iteration 7329, loss = 0.02002684\n",
      "Iteration 7330, loss = 0.02001905\n",
      "Iteration 7331, loss = 0.02000820\n",
      "Iteration 7332, loss = 0.01999420\n",
      "Iteration 7333, loss = 0.01999987\n",
      "Iteration 7334, loss = 0.01999478\n",
      "Iteration 7335, loss = 0.01997730\n",
      "Iteration 7336, loss = 0.01996767\n",
      "Iteration 7337, loss = 0.01996621\n",
      "Iteration 7338, loss = 0.01996043\n",
      "Iteration 7339, loss = 0.01995147\n",
      "Iteration 7340, loss = 0.01993935\n",
      "Iteration 7341, loss = 0.01992464\n",
      "Iteration 7342, loss = 0.01992500\n",
      "Iteration 7343, loss = 0.01991595\n",
      "Iteration 7344, loss = 0.01989186\n",
      "Iteration 7345, loss = 0.01987331\n",
      "Iteration 7346, loss = 0.01986199\n",
      "Iteration 7347, loss = 0.01984632\n",
      "Iteration 7348, loss = 0.01982482\n",
      "Iteration 7349, loss = 0.01980885\n",
      "Iteration 7350, loss = 0.01979543\n",
      "Iteration 7351, loss = 0.01978760\n",
      "Iteration 7352, loss = 0.01977433\n",
      "Iteration 7353, loss = 0.01976773\n",
      "Iteration 7354, loss = 0.01975844\n",
      "Iteration 7355, loss = 0.01975614\n",
      "Iteration 7356, loss = 0.01974278\n",
      "Iteration 7357, loss = 0.01973763\n",
      "Iteration 7358, loss = 0.01973245\n",
      "Iteration 7359, loss = 0.01972414\n",
      "Iteration 7360, loss = 0.01971323\n",
      "Iteration 7361, loss = 0.01969983\n",
      "Iteration 7362, loss = 0.01969075\n",
      "Iteration 7363, loss = 0.01968383\n",
      "Iteration 7364, loss = 0.01967072\n",
      "Iteration 7365, loss = 0.01966367\n",
      "Iteration 7366, loss = 0.01965345\n",
      "Iteration 7367, loss = 0.01964666\n",
      "Iteration 7368, loss = 0.01963490\n",
      "Iteration 7369, loss = 0.01962771\n",
      "Iteration 7370, loss = 0.01962221\n",
      "Iteration 7371, loss = 0.01961535\n",
      "Iteration 7372, loss = 0.01960585\n",
      "Iteration 7373, loss = 0.01959348\n",
      "Iteration 7374, loss = 0.01959354\n",
      "Iteration 7375, loss = 0.01958452\n",
      "Iteration 7376, loss = 0.01956817\n",
      "Iteration 7377, loss = 0.01956225\n",
      "Iteration 7378, loss = 0.01955299\n",
      "Iteration 7379, loss = 0.01954111\n",
      "Iteration 7380, loss = 0.01954483\n",
      "Iteration 7381, loss = 0.01953530\n",
      "Iteration 7382, loss = 0.01951688\n",
      "Iteration 7383, loss = 0.01951134\n",
      "Iteration 7384, loss = 0.01950279\n",
      "Iteration 7385, loss = 0.01949129\n",
      "Iteration 7386, loss = 0.01949238\n",
      "Iteration 7387, loss = 0.01948224\n",
      "Iteration 7388, loss = 0.01946793\n",
      "Iteration 7389, loss = 0.01946289\n",
      "Iteration 7390, loss = 0.01945435\n",
      "Iteration 7391, loss = 0.01944275\n",
      "Iteration 7392, loss = 0.01943799\n",
      "Iteration 7393, loss = 0.01942783\n",
      "Iteration 7394, loss = 0.01941985\n",
      "Iteration 7395, loss = 0.01941467\n",
      "Iteration 7396, loss = 0.01940626\n",
      "Iteration 7397, loss = 0.01939495\n",
      "Iteration 7398, loss = 0.01938364\n",
      "Iteration 7399, loss = 0.01937370\n",
      "Iteration 7400, loss = 0.01936929\n",
      "Iteration 7401, loss = 0.01935935\n",
      "Iteration 7402, loss = 0.01935177\n",
      "Iteration 7403, loss = 0.01934126\n",
      "Iteration 7404, loss = 0.01933991\n",
      "Iteration 7405, loss = 0.01932816\n",
      "Iteration 7406, loss = 0.01932085\n",
      "Iteration 7407, loss = 0.01931643\n",
      "Iteration 7408, loss = 0.01930877\n",
      "Iteration 7409, loss = 0.01929807\n",
      "Iteration 7410, loss = 0.01928464\n",
      "Iteration 7411, loss = 0.01928660\n",
      "Iteration 7412, loss = 0.01927978\n",
      "Iteration 7413, loss = 0.01926042\n",
      "Iteration 7414, loss = 0.01925937\n",
      "Iteration 7415, loss = 0.01925842\n",
      "Iteration 7416, loss = 0.01925329\n",
      "Iteration 7417, loss = 0.01924475\n",
      "Iteration 7418, loss = 0.01923318\n",
      "Iteration 7419, loss = 0.01921940\n",
      "Iteration 7420, loss = 0.01920342\n",
      "Iteration 7421, loss = 0.01920898\n",
      "Iteration 7422, loss = 0.01920694\n",
      "Iteration 7423, loss = 0.01919157\n",
      "Iteration 7424, loss = 0.01916943\n",
      "Iteration 7425, loss = 0.01916679\n",
      "Iteration 7426, loss = 0.01916068\n",
      "Iteration 7427, loss = 0.01915061\n",
      "Iteration 7428, loss = 0.01913808\n",
      "Iteration 7429, loss = 0.01913529\n",
      "Iteration 7430, loss = 0.01912742\n",
      "Iteration 7431, loss = 0.01911291\n",
      "Iteration 7432, loss = 0.01910709\n",
      "Iteration 7433, loss = 0.01909845\n",
      "Iteration 7434, loss = 0.01908687\n",
      "Iteration 7435, loss = 0.01909068\n",
      "Iteration 7436, loss = 0.01908133\n",
      "Iteration 7437, loss = 0.01906356\n",
      "Iteration 7438, loss = 0.01905850\n",
      "Iteration 7439, loss = 0.01905033\n",
      "Iteration 7440, loss = 0.01903927\n",
      "Iteration 7441, loss = 0.01903974\n",
      "Iteration 7442, loss = 0.01902974\n",
      "Iteration 7443, loss = 0.01901658\n",
      "Iteration 7444, loss = 0.01901183\n",
      "Iteration 7445, loss = 0.01900380\n",
      "Iteration 7446, loss = 0.01899257\n",
      "Iteration 7447, loss = 0.01898705\n",
      "Iteration 7448, loss = 0.01897685\n",
      "Iteration 7449, loss = 0.01897135\n",
      "Iteration 7450, loss = 0.01896674\n",
      "Iteration 7451, loss = 0.01895862\n",
      "Iteration 7452, loss = 0.01894772\n",
      "Iteration 7453, loss = 0.01893438\n",
      "Iteration 7454, loss = 0.01894025\n",
      "Iteration 7455, loss = 0.01893421\n",
      "Iteration 7456, loss = 0.01891550\n",
      "Iteration 7457, loss = 0.01890978\n",
      "Iteration 7458, loss = 0.01890846\n",
      "Iteration 7459, loss = 0.01890332\n",
      "Iteration 7460, loss = 0.01889524\n",
      "Iteration 7461, loss = 0.01888419\n",
      "Iteration 7462, loss = 0.01887014\n",
      "Iteration 7463, loss = 0.01885460\n",
      "Iteration 7464, loss = 0.01886577\n",
      "Iteration 7465, loss = 0.01886382\n",
      "Iteration 7466, loss = 0.01884841\n",
      "Iteration 7467, loss = 0.01882257\n",
      "Iteration 7468, loss = 0.01882003\n",
      "Iteration 7469, loss = 0.01881394\n",
      "Iteration 7470, loss = 0.01880469\n",
      "Iteration 7471, loss = 0.01879285\n",
      "Iteration 7472, loss = 0.01879159\n",
      "Iteration 7473, loss = 0.01878330\n",
      "Iteration 7474, loss = 0.01876825\n",
      "Iteration 7475, loss = 0.01876305\n",
      "Iteration 7476, loss = 0.01875484\n",
      "Iteration 7477, loss = 0.01874394\n",
      "Iteration 7478, loss = 0.01874555\n",
      "Iteration 7479, loss = 0.01873561\n",
      "Iteration 7480, loss = 0.01872175\n",
      "Iteration 7481, loss = 0.01871735\n",
      "Iteration 7482, loss = 0.01870942\n",
      "Iteration 7483, loss = 0.01869870\n",
      "Iteration 7484, loss = 0.01869288\n",
      "Iteration 7485, loss = 0.01868266\n",
      "Iteration 7486, loss = 0.01867752\n",
      "Iteration 7487, loss = 0.01867319\n",
      "Iteration 7488, loss = 0.01866569\n",
      "Iteration 7489, loss = 0.01865504\n",
      "Iteration 7490, loss = 0.01864195\n",
      "Iteration 7491, loss = 0.01864634\n",
      "Iteration 7492, loss = 0.01864023\n",
      "Iteration 7493, loss = 0.01862101\n",
      "Iteration 7494, loss = 0.01861829\n",
      "Iteration 7495, loss = 0.01861736\n",
      "Iteration 7496, loss = 0.01861294\n",
      "Iteration 7497, loss = 0.01860502\n",
      "Iteration 7498, loss = 0.01859393\n",
      "Iteration 7499, loss = 0.01858083\n",
      "Iteration 7500, loss = 0.01856556\n",
      "Iteration 7501, loss = 0.01856913\n",
      "Iteration 7502, loss = 0.01856696\n",
      "Iteration 7503, loss = 0.01855143\n",
      "Iteration 7504, loss = 0.01853454\n",
      "Iteration 7505, loss = 0.01853261\n",
      "Iteration 7506, loss = 0.01852712\n",
      "Iteration 7507, loss = 0.01851830\n",
      "Iteration 7508, loss = 0.01850667\n",
      "Iteration 7509, loss = 0.01849253\n",
      "Iteration 7510, loss = 0.01848479\n",
      "Iteration 7511, loss = 0.01848167\n",
      "Iteration 7512, loss = 0.01847111\n",
      "Iteration 7513, loss = 0.01846396\n",
      "Iteration 7514, loss = 0.01845399\n",
      "Iteration 7515, loss = 0.01844975\n",
      "Iteration 7516, loss = 0.01844300\n",
      "Iteration 7517, loss = 0.01843342\n",
      "Iteration 7518, loss = 0.01842756\n",
      "Iteration 7519, loss = 0.01841613\n",
      "Iteration 7520, loss = 0.01841449\n",
      "Iteration 7521, loss = 0.01841056\n",
      "Iteration 7522, loss = 0.01840340\n",
      "Iteration 7523, loss = 0.01839335\n",
      "Iteration 7524, loss = 0.01838107\n",
      "Iteration 7525, loss = 0.01837458\n",
      "Iteration 7526, loss = 0.01836783\n",
      "Iteration 7527, loss = 0.01835649\n",
      "Iteration 7528, loss = 0.01835096\n",
      "Iteration 7529, loss = 0.01834272\n",
      "Iteration 7530, loss = 0.01833156\n",
      "Iteration 7531, loss = 0.01833297\n",
      "Iteration 7532, loss = 0.01832408\n",
      "Iteration 7533, loss = 0.01830997\n",
      "Iteration 7534, loss = 0.01830526\n",
      "Iteration 7535, loss = 0.01829750\n",
      "Iteration 7536, loss = 0.01828705\n",
      "Iteration 7537, loss = 0.01828262\n",
      "Iteration 7538, loss = 0.01827243\n",
      "Iteration 7539, loss = 0.01826715\n",
      "Iteration 7540, loss = 0.01826304\n",
      "Iteration 7541, loss = 0.01825587\n",
      "Iteration 7542, loss = 0.01824602\n",
      "Iteration 7543, loss = 0.01823355\n",
      "Iteration 7544, loss = 0.01823409\n",
      "Iteration 7545, loss = 0.01822729\n",
      "Iteration 7546, loss = 0.01820849\n",
      "Iteration 7547, loss = 0.01820335\n",
      "Iteration 7548, loss = 0.01819502\n",
      "Iteration 7549, loss = 0.01818925\n",
      "Iteration 7550, loss = 0.01817903\n",
      "Iteration 7551, loss = 0.01817126\n",
      "Iteration 7552, loss = 0.01816738\n",
      "Iteration 7553, loss = 0.01815637\n",
      "Iteration 7554, loss = 0.01814905\n",
      "Iteration 7555, loss = 0.01814287\n",
      "Iteration 7556, loss = 0.01813500\n",
      "Iteration 7557, loss = 0.01812803\n",
      "Iteration 7558, loss = 0.01811817\n",
      "Iteration 7559, loss = 0.01811876\n",
      "Iteration 7560, loss = 0.01810751\n",
      "Iteration 7561, loss = 0.01810016\n",
      "Iteration 7562, loss = 0.01809665\n",
      "Iteration 7563, loss = 0.01808966\n",
      "Iteration 7564, loss = 0.01808028\n",
      "Iteration 7565, loss = 0.01806829\n",
      "Iteration 7566, loss = 0.01806620\n",
      "Iteration 7567, loss = 0.01805918\n",
      "Iteration 7568, loss = 0.01804448\n",
      "Iteration 7569, loss = 0.01803925\n",
      "Iteration 7570, loss = 0.01803115\n",
      "Iteration 7571, loss = 0.01802044\n",
      "Iteration 7572, loss = 0.01802314\n",
      "Iteration 7573, loss = 0.01801411\n",
      "Iteration 7574, loss = 0.01800032\n",
      "Iteration 7575, loss = 0.01799608\n",
      "Iteration 7576, loss = 0.01798858\n",
      "Iteration 7577, loss = 0.01797860\n",
      "Iteration 7578, loss = 0.01797075\n",
      "Iteration 7579, loss = 0.01796019\n",
      "Iteration 7580, loss = 0.01795558\n",
      "Iteration 7581, loss = 0.01794845\n",
      "Iteration 7582, loss = 0.01794257\n",
      "Iteration 7583, loss = 0.01793393\n",
      "Iteration 7584, loss = 0.01792385\n",
      "Iteration 7585, loss = 0.01791728\n",
      "Iteration 7586, loss = 0.01790947\n",
      "Iteration 7587, loss = 0.01790458\n",
      "Iteration 7588, loss = 0.01789460\n",
      "Iteration 7589, loss = 0.01788726\n",
      "Iteration 7590, loss = 0.01788164\n",
      "Iteration 7591, loss = 0.01787303\n",
      "Iteration 7592, loss = 0.01786606\n",
      "Iteration 7593, loss = 0.01785613\n",
      "Iteration 7594, loss = 0.01785872\n",
      "Iteration 7595, loss = 0.01784774\n",
      "Iteration 7596, loss = 0.01783805\n",
      "Iteration 7597, loss = 0.01783489\n",
      "Iteration 7598, loss = 0.01782852\n",
      "Iteration 7599, loss = 0.01781927\n",
      "Iteration 7600, loss = 0.01780763\n",
      "Iteration 7601, loss = 0.01780547\n",
      "Iteration 7602, loss = 0.01779817\n",
      "Iteration 7603, loss = 0.01778430\n",
      "Iteration 7604, loss = 0.01777939\n",
      "Iteration 7605, loss = 0.01777171\n",
      "Iteration 7606, loss = 0.01776148\n",
      "Iteration 7607, loss = 0.01776154\n",
      "Iteration 7608, loss = 0.01775194\n",
      "Iteration 7609, loss = 0.01774176\n",
      "Iteration 7610, loss = 0.01773801\n",
      "Iteration 7611, loss = 0.01773129\n",
      "Iteration 7612, loss = 0.01772165\n",
      "Iteration 7613, loss = 0.01770962\n",
      "Iteration 7614, loss = 0.01771524\n",
      "Iteration 7615, loss = 0.01770832\n",
      "Iteration 7616, loss = 0.01768867\n",
      "Iteration 7617, loss = 0.01768969\n",
      "Iteration 7618, loss = 0.01768958\n",
      "Iteration 7619, loss = 0.01768609\n",
      "Iteration 7620, loss = 0.01767934\n",
      "Iteration 7621, loss = 0.01766955\n",
      "Iteration 7622, loss = 0.01765785\n",
      "Iteration 7623, loss = 0.01764395\n",
      "Iteration 7624, loss = 0.01763044\n",
      "Iteration 7625, loss = 0.01762706\n",
      "Iteration 7626, loss = 0.01761556\n",
      "Iteration 7627, loss = 0.01760972\n",
      "Iteration 7628, loss = 0.01760106\n",
      "Iteration 7629, loss = 0.01759793\n",
      "Iteration 7630, loss = 0.01758573\n",
      "Iteration 7631, loss = 0.01758534\n",
      "Iteration 7632, loss = 0.01758228\n",
      "Iteration 7633, loss = 0.01757577\n",
      "Iteration 7634, loss = 0.01756690\n",
      "Iteration 7635, loss = 0.01755576\n",
      "Iteration 7636, loss = 0.01754265\n",
      "Iteration 7637, loss = 0.01755139\n",
      "Iteration 7638, loss = 0.01754711\n",
      "Iteration 7639, loss = 0.01752918\n",
      "Iteration 7640, loss = 0.01751856\n",
      "Iteration 7641, loss = 0.01751807\n",
      "Iteration 7642, loss = 0.01751412\n",
      "Iteration 7643, loss = 0.01750659\n",
      "Iteration 7644, loss = 0.01749614\n",
      "Iteration 7645, loss = 0.01748391\n",
      "Iteration 7646, loss = 0.01746991\n",
      "Iteration 7647, loss = 0.01748260\n",
      "Iteration 7648, loss = 0.01748027\n",
      "Iteration 7649, loss = 0.01746378\n",
      "Iteration 7650, loss = 0.01744291\n",
      "Iteration 7651, loss = 0.01744198\n",
      "Iteration 7652, loss = 0.01743753\n",
      "Iteration 7653, loss = 0.01743017\n",
      "Iteration 7654, loss = 0.01741987\n",
      "Iteration 7655, loss = 0.01740701\n",
      "Iteration 7656, loss = 0.01740917\n",
      "Iteration 7657, loss = 0.01740393\n",
      "Iteration 7658, loss = 0.01738495\n",
      "Iteration 7659, loss = 0.01738589\n",
      "Iteration 7660, loss = 0.01738578\n",
      "Iteration 7661, loss = 0.01738206\n",
      "Iteration 7662, loss = 0.01737549\n",
      "Iteration 7663, loss = 0.01736612\n",
      "Iteration 7664, loss = 0.01735454\n",
      "Iteration 7665, loss = 0.01734088\n",
      "Iteration 7666, loss = 0.01732934\n",
      "Iteration 7667, loss = 0.01732632\n",
      "Iteration 7668, loss = 0.01731342\n",
      "Iteration 7669, loss = 0.01730729\n",
      "Iteration 7670, loss = 0.01729863\n",
      "Iteration 7671, loss = 0.01729716\n",
      "Iteration 7672, loss = 0.01728506\n",
      "Iteration 7673, loss = 0.01728393\n",
      "Iteration 7674, loss = 0.01728109\n",
      "Iteration 7675, loss = 0.01727520\n",
      "Iteration 7676, loss = 0.01726661\n",
      "Iteration 7677, loss = 0.01725584\n",
      "Iteration 7678, loss = 0.01724312\n",
      "Iteration 7679, loss = 0.01724805\n",
      "Iteration 7680, loss = 0.01724344\n",
      "Iteration 7681, loss = 0.01722522\n",
      "Iteration 7682, loss = 0.01722056\n",
      "Iteration 7683, loss = 0.01722028\n",
      "Iteration 7684, loss = 0.01721675\n",
      "Iteration 7685, loss = 0.01720968\n",
      "Iteration 7686, loss = 0.01719973\n",
      "Iteration 7687, loss = 0.01718777\n",
      "Iteration 7688, loss = 0.01717408\n",
      "Iteration 7689, loss = 0.01717598\n",
      "Iteration 7690, loss = 0.01717324\n",
      "Iteration 7691, loss = 0.01715624\n",
      "Iteration 7692, loss = 0.01714893\n",
      "Iteration 7693, loss = 0.01714826\n",
      "Iteration 7694, loss = 0.01714446\n",
      "Iteration 7695, loss = 0.01713731\n",
      "Iteration 7696, loss = 0.01712721\n",
      "Iteration 7697, loss = 0.01711485\n",
      "Iteration 7698, loss = 0.01710100\n",
      "Iteration 7699, loss = 0.01711277\n",
      "Iteration 7700, loss = 0.01711077\n",
      "Iteration 7701, loss = 0.01709435\n",
      "Iteration 7702, loss = 0.01707542\n",
      "Iteration 7703, loss = 0.01707448\n",
      "Iteration 7704, loss = 0.01707056\n",
      "Iteration 7705, loss = 0.01706343\n",
      "Iteration 7706, loss = 0.01705330\n",
      "Iteration 7707, loss = 0.01704131\n",
      "Iteration 7708, loss = 0.01703868\n",
      "Iteration 7709, loss = 0.01703282\n",
      "Iteration 7710, loss = 0.01701817\n",
      "Iteration 7711, loss = 0.01701373\n",
      "Iteration 7712, loss = 0.01700649\n",
      "Iteration 7713, loss = 0.01699658\n",
      "Iteration 7714, loss = 0.01699947\n",
      "Iteration 7715, loss = 0.01698990\n",
      "Iteration 7716, loss = 0.01697892\n",
      "Iteration 7717, loss = 0.01697544\n",
      "Iteration 7718, loss = 0.01696906\n",
      "Iteration 7719, loss = 0.01696021\n",
      "Iteration 7720, loss = 0.01694897\n",
      "Iteration 7721, loss = 0.01695166\n",
      "Iteration 7722, loss = 0.01694450\n",
      "Iteration 7723, loss = 0.01692781\n",
      "Iteration 7724, loss = 0.01692375\n",
      "Iteration 7725, loss = 0.01691707\n",
      "Iteration 7726, loss = 0.01690781\n",
      "Iteration 7727, loss = 0.01690610\n",
      "Iteration 7728, loss = 0.01689599\n",
      "Iteration 7729, loss = 0.01689120\n",
      "Iteration 7730, loss = 0.01688808\n",
      "Iteration 7731, loss = 0.01688209\n",
      "Iteration 7732, loss = 0.01687344\n",
      "Iteration 7733, loss = 0.01686255\n",
      "Iteration 7734, loss = 0.01685466\n",
      "Iteration 7735, loss = 0.01684698\n",
      "Iteration 7736, loss = 0.01684222\n",
      "Iteration 7737, loss = 0.01683837\n",
      "Iteration 7738, loss = 0.01683173\n",
      "Iteration 7739, loss = 0.01682254\n",
      "Iteration 7740, loss = 0.01681106\n",
      "Iteration 7741, loss = 0.01681540\n",
      "Iteration 7742, loss = 0.01680901\n",
      "Iteration 7743, loss = 0.01678954\n",
      "Iteration 7744, loss = 0.01679345\n",
      "Iteration 7745, loss = 0.01679362\n",
      "Iteration 7746, loss = 0.01679061\n",
      "Iteration 7747, loss = 0.01678464\n",
      "Iteration 7748, loss = 0.01677604\n",
      "Iteration 7749, loss = 0.01676561\n",
      "Iteration 7750, loss = 0.01675301\n",
      "Iteration 7751, loss = 0.01673876\n",
      "Iteration 7752, loss = 0.01674320\n",
      "Iteration 7753, loss = 0.01674201\n",
      "Iteration 7754, loss = 0.01672713\n",
      "Iteration 7755, loss = 0.01671179\n",
      "Iteration 7756, loss = 0.01671077\n",
      "Iteration 7757, loss = 0.01670608\n",
      "Iteration 7758, loss = 0.01669841\n",
      "Iteration 7759, loss = 0.01668816\n",
      "Iteration 7760, loss = 0.01667616\n",
      "Iteration 7761, loss = 0.01667965\n",
      "Iteration 7762, loss = 0.01667445\n",
      "Iteration 7763, loss = 0.01665497\n",
      "Iteration 7764, loss = 0.01665733\n",
      "Iteration 7765, loss = 0.01665773\n",
      "Iteration 7766, loss = 0.01665508\n",
      "Iteration 7767, loss = 0.01664917\n",
      "Iteration 7768, loss = 0.01664022\n",
      "Iteration 7769, loss = 0.01662925\n",
      "Iteration 7770, loss = 0.01661650\n",
      "Iteration 7771, loss = 0.01660207\n",
      "Iteration 7772, loss = 0.01661225\n",
      "Iteration 7773, loss = 0.01661160\n",
      "Iteration 7774, loss = 0.01659672\n",
      "Iteration 7775, loss = 0.01657466\n",
      "Iteration 7776, loss = 0.01657357\n",
      "Iteration 7777, loss = 0.01656953\n",
      "Iteration 7778, loss = 0.01656214\n",
      "Iteration 7779, loss = 0.01655190\n",
      "Iteration 7780, loss = 0.01654032\n",
      "Iteration 7781, loss = 0.01654867\n",
      "Iteration 7782, loss = 0.01654321\n",
      "Iteration 7783, loss = 0.01652401\n",
      "Iteration 7784, loss = 0.01652166\n",
      "Iteration 7785, loss = 0.01652217\n",
      "Iteration 7786, loss = 0.01651929\n",
      "Iteration 7787, loss = 0.01651355\n",
      "Iteration 7788, loss = 0.01650514\n",
      "Iteration 7789, loss = 0.01649439\n",
      "Iteration 7790, loss = 0.01648180\n",
      "Iteration 7791, loss = 0.01646771\n",
      "Iteration 7792, loss = 0.01647996\n",
      "Iteration 7793, loss = 0.01647914\n",
      "Iteration 7794, loss = 0.01646411\n",
      "Iteration 7795, loss = 0.01644111\n",
      "Iteration 7796, loss = 0.01644038\n",
      "Iteration 7797, loss = 0.01643612\n",
      "Iteration 7798, loss = 0.01642879\n",
      "Iteration 7799, loss = 0.01641912\n",
      "Iteration 7800, loss = 0.01640757\n",
      "Iteration 7801, loss = 0.01641452\n",
      "Iteration 7802, loss = 0.01640903\n",
      "Iteration 7803, loss = 0.01638968\n",
      "Iteration 7804, loss = 0.01638944\n",
      "Iteration 7805, loss = 0.01638996\n",
      "Iteration 7806, loss = 0.01638766\n",
      "Iteration 7807, loss = 0.01638202\n",
      "Iteration 7808, loss = 0.01637352\n",
      "Iteration 7809, loss = 0.01636305\n",
      "Iteration 7810, loss = 0.01635074\n",
      "Iteration 7811, loss = 0.01633686\n",
      "Iteration 7812, loss = 0.01634371\n",
      "Iteration 7813, loss = 0.01634264\n",
      "Iteration 7814, loss = 0.01632764\n",
      "Iteration 7815, loss = 0.01631105\n",
      "Iteration 7816, loss = 0.01631055\n",
      "Iteration 7817, loss = 0.01630654\n",
      "Iteration 7818, loss = 0.01629932\n",
      "Iteration 7819, loss = 0.01628968\n",
      "Iteration 7820, loss = 0.01627817\n",
      "Iteration 7821, loss = 0.01627747\n",
      "Iteration 7822, loss = 0.01627183\n",
      "Iteration 7823, loss = 0.01625687\n",
      "Iteration 7824, loss = 0.01625290\n",
      "Iteration 7825, loss = 0.01624642\n",
      "Iteration 7826, loss = 0.01623748\n",
      "Iteration 7827, loss = 0.01623649\n",
      "Iteration 7828, loss = 0.01622631\n",
      "Iteration 7829, loss = 0.01622205\n",
      "Iteration 7830, loss = 0.01621929\n",
      "Iteration 7831, loss = 0.01621368\n",
      "Iteration 7832, loss = 0.01620562\n",
      "Iteration 7833, loss = 0.01619564\n",
      "Iteration 7834, loss = 0.01618480\n",
      "Iteration 7835, loss = 0.01617774\n",
      "Iteration 7836, loss = 0.01617416\n",
      "Iteration 7837, loss = 0.01616671\n",
      "Iteration 7838, loss = 0.01616180\n",
      "Iteration 7839, loss = 0.01615431\n",
      "Iteration 7840, loss = 0.01614613\n",
      "Iteration 7841, loss = 0.01614035\n",
      "Iteration 7842, loss = 0.01613389\n",
      "Iteration 7843, loss = 0.01612722\n",
      "Iteration 7844, loss = 0.01612162\n",
      "Iteration 7845, loss = 0.01611587\n",
      "Iteration 7846, loss = 0.01610774\n",
      "Iteration 7847, loss = 0.01610504\n",
      "Iteration 7848, loss = 0.01609321\n",
      "Iteration 7849, loss = 0.01609422\n",
      "Iteration 7850, loss = 0.01609238\n",
      "Iteration 7851, loss = 0.01608760\n",
      "Iteration 7852, loss = 0.01608004\n",
      "Iteration 7853, loss = 0.01607045\n",
      "Iteration 7854, loss = 0.01605883\n",
      "Iteration 7855, loss = 0.01605481\n",
      "Iteration 7856, loss = 0.01604984\n",
      "Iteration 7857, loss = 0.01603692\n",
      "Iteration 7858, loss = 0.01603286\n",
      "Iteration 7859, loss = 0.01602620\n",
      "Iteration 7860, loss = 0.01601713\n",
      "Iteration 7861, loss = 0.01601880\n",
      "Iteration 7862, loss = 0.01600910\n",
      "Iteration 7863, loss = 0.01600107\n",
      "Iteration 7864, loss = 0.01599852\n",
      "Iteration 7865, loss = 0.01599309\n",
      "Iteration 7866, loss = 0.01598523\n",
      "Iteration 7867, loss = 0.01597529\n",
      "Iteration 7868, loss = 0.01596851\n",
      "Iteration 7869, loss = 0.01596030\n",
      "Iteration 7870, loss = 0.01595764\n",
      "Iteration 7871, loss = 0.01595466\n",
      "Iteration 7872, loss = 0.01594892\n",
      "Iteration 7873, loss = 0.01594071\n",
      "Iteration 7874, loss = 0.01593018\n",
      "Iteration 7875, loss = 0.01592579\n",
      "Iteration 7876, loss = 0.01591885\n",
      "Iteration 7877, loss = 0.01591142\n",
      "Iteration 7878, loss = 0.01590794\n",
      "Iteration 7879, loss = 0.01590207\n",
      "Iteration 7880, loss = 0.01589392\n",
      "Iteration 7881, loss = 0.01588337\n",
      "Iteration 7882, loss = 0.01588799\n",
      "Iteration 7883, loss = 0.01588106\n",
      "Iteration 7884, loss = 0.01586442\n",
      "Iteration 7885, loss = 0.01586098\n",
      "Iteration 7886, loss = 0.01585496\n",
      "Iteration 7887, loss = 0.01584644\n",
      "Iteration 7888, loss = 0.01584273\n",
      "Iteration 7889, loss = 0.01583206\n",
      "Iteration 7890, loss = 0.01583227\n",
      "Iteration 7891, loss = 0.01583027\n",
      "Iteration 7892, loss = 0.01582536\n",
      "Iteration 7893, loss = 0.01581811\n",
      "Iteration 7894, loss = 0.01580865\n",
      "Iteration 7895, loss = 0.01579726\n",
      "Iteration 7896, loss = 0.01579673\n",
      "Iteration 7897, loss = 0.01579170\n",
      "Iteration 7898, loss = 0.01577582\n",
      "Iteration 7899, loss = 0.01577192\n",
      "Iteration 7900, loss = 0.01576539\n",
      "Iteration 7901, loss = 0.01575645\n",
      "Iteration 7902, loss = 0.01576047\n",
      "Iteration 7903, loss = 0.01575088\n",
      "Iteration 7904, loss = 0.01574113\n",
      "Iteration 7905, loss = 0.01573862\n",
      "Iteration 7906, loss = 0.01573379\n",
      "Iteration 7907, loss = 0.01572629\n",
      "Iteration 7908, loss = 0.01571646\n",
      "Iteration 7909, loss = 0.01570930\n",
      "Iteration 7910, loss = 0.01570093\n",
      "Iteration 7911, loss = 0.01569973\n",
      "Iteration 7912, loss = 0.01569704\n",
      "Iteration 7913, loss = 0.01569171\n",
      "Iteration 7914, loss = 0.01568374\n",
      "Iteration 7915, loss = 0.01567361\n",
      "Iteration 7916, loss = 0.01566513\n",
      "Iteration 7917, loss = 0.01565775\n",
      "Iteration 7918, loss = 0.01565551\n",
      "Iteration 7919, loss = 0.01565250\n",
      "Iteration 7920, loss = 0.01564667\n",
      "Iteration 7921, loss = 0.01563863\n",
      "Iteration 7922, loss = 0.01562861\n",
      "Iteration 7923, loss = 0.01562526\n",
      "Iteration 7924, loss = 0.01561808\n",
      "Iteration 7925, loss = 0.01561073\n",
      "Iteration 7926, loss = 0.01560758\n",
      "Iteration 7927, loss = 0.01560177\n",
      "Iteration 7928, loss = 0.01559349\n",
      "Iteration 7929, loss = 0.01558344\n",
      "Iteration 7930, loss = 0.01558673\n",
      "Iteration 7931, loss = 0.01557963\n",
      "Iteration 7932, loss = 0.01556520\n",
      "Iteration 7933, loss = 0.01556205\n",
      "Iteration 7934, loss = 0.01555628\n",
      "Iteration 7935, loss = 0.01554843\n",
      "Iteration 7936, loss = 0.01554024\n",
      "Iteration 7937, loss = 0.01553400\n",
      "Iteration 7938, loss = 0.01552743\n",
      "Iteration 7939, loss = 0.01552451\n",
      "Iteration 7940, loss = 0.01551572\n",
      "Iteration 7941, loss = 0.01551019\n",
      "Iteration 7942, loss = 0.01550229\n",
      "Iteration 7943, loss = 0.01550395\n",
      "Iteration 7944, loss = 0.01549255\n",
      "Iteration 7945, loss = 0.01548989\n",
      "Iteration 7946, loss = 0.01548805\n",
      "Iteration 7947, loss = 0.01548362\n",
      "Iteration 7948, loss = 0.01547674\n",
      "Iteration 7949, loss = 0.01546767\n",
      "Iteration 7950, loss = 0.01545690\n",
      "Iteration 7951, loss = 0.01545390\n",
      "Iteration 7952, loss = 0.01544835\n",
      "Iteration 7953, loss = 0.01543672\n",
      "Iteration 7954, loss = 0.01543351\n",
      "Iteration 7955, loss = 0.01542749\n",
      "Iteration 7956, loss = 0.01541891\n",
      "Iteration 7957, loss = 0.01541483\n",
      "Iteration 7958, loss = 0.01540492\n",
      "Iteration 7959, loss = 0.01540484\n",
      "Iteration 7960, loss = 0.01540268\n",
      "Iteration 7961, loss = 0.01539795\n",
      "Iteration 7962, loss = 0.01539083\n",
      "Iteration 7963, loss = 0.01538165\n",
      "Iteration 7964, loss = 0.01537075\n",
      "Iteration 7965, loss = 0.01537120\n",
      "Iteration 7966, loss = 0.01536609\n",
      "Iteration 7967, loss = 0.01535050\n",
      "Iteration 7968, loss = 0.01534696\n",
      "Iteration 7969, loss = 0.01534092\n",
      "Iteration 7970, loss = 0.01533230\n",
      "Iteration 7971, loss = 0.01533404\n",
      "Iteration 7972, loss = 0.01532425\n",
      "Iteration 7973, loss = 0.01531813\n",
      "Iteration 7974, loss = 0.01531612\n",
      "Iteration 7975, loss = 0.01531139\n",
      "Iteration 7976, loss = 0.01530444\n",
      "Iteration 7977, loss = 0.01529535\n",
      "Iteration 7978, loss = 0.01528455\n",
      "Iteration 7979, loss = 0.01529076\n",
      "Iteration 7980, loss = 0.01528537\n",
      "Iteration 7981, loss = 0.01526656\n",
      "Iteration 7982, loss = 0.01526886\n",
      "Iteration 7983, loss = 0.01526999\n",
      "Iteration 7984, loss = 0.01526755\n",
      "Iteration 7985, loss = 0.01526220\n",
      "Iteration 7986, loss = 0.01525474\n",
      "Iteration 7987, loss = 0.01524588\n",
      "Iteration 7988, loss = 0.01523517\n",
      "Iteration 7989, loss = 0.01522262\n",
      "Iteration 7990, loss = 0.01522052\n",
      "Iteration 7991, loss = 0.01521828\n",
      "Iteration 7992, loss = 0.01520236\n",
      "Iteration 7993, loss = 0.01520211\n",
      "Iteration 7994, loss = 0.01520260\n",
      "Iteration 7995, loss = 0.01519904\n",
      "Iteration 7996, loss = 0.01519249\n",
      "Iteration 7997, loss = 0.01518383\n",
      "Iteration 7998, loss = 0.01517416\n",
      "Iteration 7999, loss = 0.01516349\n",
      "Iteration 8000, loss = 0.01515983\n",
      "Iteration 8001, loss = 0.01515519\n",
      "Iteration 8002, loss = 0.01514075\n",
      "Iteration 8003, loss = 0.01513726\n",
      "Iteration 8004, loss = 0.01513148\n",
      "Iteration 8005, loss = 0.01512308\n",
      "Iteration 8006, loss = 0.01512676\n",
      "Iteration 8007, loss = 0.01511751\n",
      "Iteration 8008, loss = 0.01510777\n",
      "Iteration 8009, loss = 0.01510533\n",
      "Iteration 8010, loss = 0.01510051\n",
      "Iteration 8011, loss = 0.01509386\n",
      "Iteration 8012, loss = 0.01508509\n",
      "Iteration 8013, loss = 0.01507633\n",
      "Iteration 8014, loss = 0.01506903\n",
      "Iteration 8015, loss = 0.01506376\n",
      "Iteration 8016, loss = 0.01506078\n",
      "Iteration 8017, loss = 0.01505707\n",
      "Iteration 8018, loss = 0.01505061\n",
      "Iteration 8019, loss = 0.01504144\n",
      "Iteration 8020, loss = 0.01503916\n",
      "Iteration 8021, loss = 0.01503046\n",
      "Iteration 8022, loss = 0.01502663\n",
      "Iteration 8023, loss = 0.01502455\n",
      "Iteration 8024, loss = 0.01501982\n",
      "Iteration 8025, loss = 0.01501285\n",
      "Iteration 8026, loss = 0.01500394\n",
      "Iteration 8027, loss = 0.01499315\n",
      "Iteration 8028, loss = 0.01499967\n",
      "Iteration 8029, loss = 0.01499484\n",
      "Iteration 8030, loss = 0.01497614\n",
      "Iteration 8031, loss = 0.01497839\n",
      "Iteration 8032, loss = 0.01497943\n",
      "Iteration 8033, loss = 0.01497705\n",
      "Iteration 8034, loss = 0.01497213\n",
      "Iteration 8035, loss = 0.01496514\n",
      "Iteration 8036, loss = 0.01495642\n",
      "Iteration 8037, loss = 0.01494608\n",
      "Iteration 8038, loss = 0.01493383\n",
      "Iteration 8039, loss = 0.01492880\n",
      "Iteration 8040, loss = 0.01492628\n",
      "Iteration 8041, loss = 0.01491036\n",
      "Iteration 8042, loss = 0.01490667\n",
      "Iteration 8043, loss = 0.01490000\n",
      "Iteration 8044, loss = 0.01489965\n",
      "Iteration 8045, loss = 0.01488713\n",
      "Iteration 8046, loss = 0.01488136\n",
      "Iteration 8047, loss = 0.01488008\n",
      "Iteration 8048, loss = 0.01487140\n",
      "Iteration 8049, loss = 0.01486696\n",
      "Iteration 8050, loss = 0.01486019\n",
      "Iteration 8051, loss = 0.01485438\n",
      "Iteration 8052, loss = 0.01484816\n",
      "Iteration 8053, loss = 0.01484293\n",
      "Iteration 8054, loss = 0.01483539\n",
      "Iteration 8055, loss = 0.01483738\n",
      "Iteration 8056, loss = 0.01482663\n",
      "Iteration 8057, loss = 0.01482359\n",
      "Iteration 8058, loss = 0.01482196\n",
      "Iteration 8059, loss = 0.01481762\n",
      "Iteration 8060, loss = 0.01481123\n",
      "Iteration 8061, loss = 0.01480309\n",
      "Iteration 8062, loss = 0.01479301\n",
      "Iteration 8063, loss = 0.01478860\n",
      "Iteration 8064, loss = 0.01478260\n",
      "Iteration 8065, loss = 0.01477464\n",
      "Iteration 8066, loss = 0.01477185\n",
      "Iteration 8067, loss = 0.01476661\n",
      "Iteration 8068, loss = 0.01475873\n",
      "Iteration 8069, loss = 0.01474876\n",
      "Iteration 8070, loss = 0.01475586\n",
      "Iteration 8071, loss = 0.01474946\n",
      "Iteration 8072, loss = 0.01473184\n",
      "Iteration 8073, loss = 0.01472914\n",
      "Iteration 8074, loss = 0.01472413\n",
      "Iteration 8075, loss = 0.01471680\n",
      "Iteration 8076, loss = 0.01470920\n",
      "Iteration 8077, loss = 0.01470429\n",
      "Iteration 8078, loss = 0.01469852\n",
      "Iteration 8079, loss = 0.01469191\n",
      "Iteration 8080, loss = 0.01468853\n",
      "Iteration 8081, loss = 0.01468371\n",
      "Iteration 8082, loss = 0.01467675\n",
      "Iteration 8083, loss = 0.01466935\n",
      "Iteration 8084, loss = 0.01466462\n",
      "Iteration 8085, loss = 0.01465909\n",
      "Iteration 8086, loss = 0.01465141\n",
      "Iteration 8087, loss = 0.01465477\n",
      "Iteration 8088, loss = 0.01464397\n",
      "Iteration 8089, loss = 0.01463973\n",
      "Iteration 8090, loss = 0.01463849\n",
      "Iteration 8091, loss = 0.01463445\n",
      "Iteration 8092, loss = 0.01462795\n",
      "Iteration 8093, loss = 0.01461960\n",
      "Iteration 8094, loss = 0.01460964\n",
      "Iteration 8095, loss = 0.01460683\n",
      "Iteration 8096, loss = 0.01460113\n",
      "Iteration 8097, loss = 0.01459172\n",
      "Iteration 8098, loss = 0.01458898\n",
      "Iteration 8099, loss = 0.01458379\n",
      "Iteration 8100, loss = 0.01457623\n",
      "Iteration 8101, loss = 0.01456669\n",
      "Iteration 8102, loss = 0.01457380\n",
      "Iteration 8103, loss = 0.01456718\n",
      "Iteration 8104, loss = 0.01455037\n",
      "Iteration 8105, loss = 0.01454773\n",
      "Iteration 8106, loss = 0.01454299\n",
      "Iteration 8107, loss = 0.01453589\n",
      "Iteration 8108, loss = 0.01452672\n",
      "Iteration 8109, loss = 0.01453390\n",
      "Iteration 8110, loss = 0.01452628\n",
      "Iteration 8111, loss = 0.01451182\n",
      "Iteration 8112, loss = 0.01450960\n",
      "Iteration 8113, loss = 0.01450496\n",
      "Iteration 8114, loss = 0.01449807\n",
      "Iteration 8115, loss = 0.01448913\n",
      "Iteration 8116, loss = 0.01449028\n",
      "Iteration 8117, loss = 0.01448227\n",
      "Iteration 8118, loss = 0.01447455\n",
      "Iteration 8119, loss = 0.01447252\n",
      "Iteration 8120, loss = 0.01446794\n",
      "Iteration 8121, loss = 0.01446129\n",
      "Iteration 8122, loss = 0.01445247\n",
      "Iteration 8123, loss = 0.01444468\n",
      "Iteration 8124, loss = 0.01443736\n",
      "Iteration 8125, loss = 0.01443386\n",
      "Iteration 8126, loss = 0.01442937\n",
      "Iteration 8127, loss = 0.01442567\n",
      "Iteration 8128, loss = 0.01441962\n",
      "Iteration 8129, loss = 0.01441151\n",
      "Iteration 8130, loss = 0.01440916\n",
      "Iteration 8131, loss = 0.01439974\n",
      "Iteration 8132, loss = 0.01439861\n",
      "Iteration 8133, loss = 0.01439714\n",
      "Iteration 8134, loss = 0.01439299\n",
      "Iteration 8135, loss = 0.01438663\n",
      "Iteration 8136, loss = 0.01437831\n",
      "Iteration 8137, loss = 0.01436835\n",
      "Iteration 8138, loss = 0.01436712\n",
      "Iteration 8139, loss = 0.01436165\n",
      "Iteration 8140, loss = 0.01435054\n",
      "Iteration 8141, loss = 0.01434784\n",
      "Iteration 8142, loss = 0.01434270\n",
      "Iteration 8143, loss = 0.01433525\n",
      "Iteration 8144, loss = 0.01432690\n",
      "Iteration 8145, loss = 0.01432229\n",
      "Iteration 8146, loss = 0.01431650\n",
      "Iteration 8147, loss = 0.01431203\n",
      "Iteration 8148, loss = 0.01430650\n",
      "Iteration 8149, loss = 0.01430191\n",
      "Iteration 8150, loss = 0.01429524\n",
      "Iteration 8151, loss = 0.01429009\n",
      "Iteration 8152, loss = 0.01428356\n",
      "Iteration 8153, loss = 0.01427835\n",
      "Iteration 8154, loss = 0.01427115\n",
      "Iteration 8155, loss = 0.01426919\n",
      "Iteration 8156, loss = 0.01426502\n",
      "Iteration 8157, loss = 0.01425853\n",
      "Iteration 8158, loss = 0.01425022\n",
      "Iteration 8159, loss = 0.01425344\n",
      "Iteration 8160, loss = 0.01424494\n",
      "Iteration 8161, loss = 0.01423670\n",
      "Iteration 8162, loss = 0.01423499\n",
      "Iteration 8163, loss = 0.01423079\n",
      "Iteration 8164, loss = 0.01422435\n",
      "Iteration 8165, loss = 0.01421611\n",
      "Iteration 8166, loss = 0.01420611\n",
      "Iteration 8167, loss = 0.01421531\n",
      "Iteration 8168, loss = 0.01421029\n",
      "Iteration 8169, loss = 0.01419129\n",
      "Iteration 8170, loss = 0.01419323\n",
      "Iteration 8171, loss = 0.01419503\n",
      "Iteration 8172, loss = 0.01419351\n",
      "Iteration 8173, loss = 0.01418917\n",
      "Iteration 8174, loss = 0.01418275\n",
      "Iteration 8175, loss = 0.01417476\n",
      "Iteration 8176, loss = 0.01416529\n",
      "Iteration 8177, loss = 0.01415436\n",
      "Iteration 8178, loss = 0.01414159\n",
      "Iteration 8179, loss = 0.01415560\n",
      "Iteration 8180, loss = 0.01415560\n",
      "Iteration 8181, loss = 0.01414180\n",
      "Iteration 8182, loss = 0.01412158\n",
      "Iteration 8183, loss = 0.01412174\n",
      "Iteration 8184, loss = 0.01411799\n",
      "Iteration 8185, loss = 0.01411200\n",
      "Iteration 8186, loss = 0.01410488\n",
      "Iteration 8187, loss = 0.01409660\n",
      "Iteration 8188, loss = 0.01409095\n",
      "Iteration 8189, loss = 0.01408296\n",
      "Iteration 8190, loss = 0.01408091\n",
      "Iteration 8191, loss = 0.01407910\n",
      "Iteration 8192, loss = 0.01407528\n",
      "Iteration 8193, loss = 0.01406886\n",
      "Iteration 8194, loss = 0.01405987\n",
      "Iteration 8195, loss = 0.01404914\n",
      "Iteration 8196, loss = 0.01405852\n",
      "Iteration 8197, loss = 0.01405459\n",
      "Iteration 8198, loss = 0.01403646\n",
      "Iteration 8199, loss = 0.01403525\n",
      "Iteration 8200, loss = 0.01403664\n",
      "Iteration 8201, loss = 0.01403537\n",
      "Iteration 8202, loss = 0.01403157\n",
      "Iteration 8203, loss = 0.01402537\n",
      "Iteration 8204, loss = 0.01401728\n",
      "Iteration 8205, loss = 0.01400762\n",
      "Iteration 8206, loss = 0.01399651\n",
      "Iteration 8207, loss = 0.01398602\n",
      "Iteration 8208, loss = 0.01398327\n",
      "Iteration 8209, loss = 0.01397592\n",
      "Iteration 8210, loss = 0.01397271\n",
      "Iteration 8211, loss = 0.01396711\n",
      "Iteration 8212, loss = 0.01395916\n",
      "Iteration 8213, loss = 0.01395948\n",
      "Iteration 8214, loss = 0.01395033\n",
      "Iteration 8215, loss = 0.01394647\n",
      "Iteration 8216, loss = 0.01394506\n",
      "Iteration 8217, loss = 0.01394132\n",
      "Iteration 8218, loss = 0.01393534\n",
      "Iteration 8219, loss = 0.01392777\n",
      "Iteration 8220, loss = 0.01391823\n",
      "Iteration 8221, loss = 0.01391604\n",
      "Iteration 8222, loss = 0.01391003\n",
      "Iteration 8223, loss = 0.01390220\n",
      "Iteration 8224, loss = 0.01390012\n",
      "Iteration 8225, loss = 0.01389533\n",
      "Iteration 8226, loss = 0.01388809\n",
      "Iteration 8227, loss = 0.01387918\n",
      "Iteration 8228, loss = 0.01388248\n",
      "Iteration 8229, loss = 0.01387536\n",
      "Iteration 8230, loss = 0.01386478\n",
      "Iteration 8231, loss = 0.01386287\n",
      "Iteration 8232, loss = 0.01385859\n",
      "Iteration 8233, loss = 0.01385234\n",
      "Iteration 8234, loss = 0.01384413\n",
      "Iteration 8235, loss = 0.01383830\n",
      "Iteration 8236, loss = 0.01383024\n",
      "Iteration 8237, loss = 0.01383124\n",
      "Iteration 8238, loss = 0.01382961\n",
      "Iteration 8239, loss = 0.01382555\n",
      "Iteration 8240, loss = 0.01381946\n",
      "Iteration 8241, loss = 0.01381150\n",
      "Iteration 8242, loss = 0.01380194\n",
      "Iteration 8243, loss = 0.01380164\n",
      "Iteration 8244, loss = 0.01379614\n",
      "Iteration 8245, loss = 0.01378526\n",
      "Iteration 8246, loss = 0.01378295\n",
      "Iteration 8247, loss = 0.01377816\n",
      "Iteration 8248, loss = 0.01377133\n",
      "Iteration 8249, loss = 0.01376253\n",
      "Iteration 8250, loss = 0.01376924\n",
      "Iteration 8251, loss = 0.01376220\n",
      "Iteration 8252, loss = 0.01374815\n",
      "Iteration 8253, loss = 0.01374627\n",
      "Iteration 8254, loss = 0.01374214\n",
      "Iteration 8255, loss = 0.01373594\n",
      "Iteration 8256, loss = 0.01372786\n",
      "Iteration 8257, loss = 0.01372609\n",
      "Iteration 8258, loss = 0.01371763\n",
      "Iteration 8259, loss = 0.01371498\n",
      "Iteration 8260, loss = 0.01371354\n",
      "Iteration 8261, loss = 0.01370968\n",
      "Iteration 8262, loss = 0.01370360\n",
      "Iteration 8263, loss = 0.01369553\n",
      "Iteration 8264, loss = 0.01368611\n",
      "Iteration 8265, loss = 0.01368837\n",
      "Iteration 8266, loss = 0.01368290\n",
      "Iteration 8267, loss = 0.01366980\n",
      "Iteration 8268, loss = 0.01366773\n",
      "Iteration 8269, loss = 0.01366320\n",
      "Iteration 8270, loss = 0.01365625\n",
      "Iteration 8271, loss = 0.01364764\n",
      "Iteration 8272, loss = 0.01365544\n",
      "Iteration 8273, loss = 0.01364833\n",
      "Iteration 8274, loss = 0.01363382\n",
      "Iteration 8275, loss = 0.01363215\n",
      "Iteration 8276, loss = 0.01362797\n",
      "Iteration 8277, loss = 0.01362190\n",
      "Iteration 8278, loss = 0.01361393\n",
      "Iteration 8279, loss = 0.01361154\n",
      "Iteration 8280, loss = 0.01360295\n",
      "Iteration 8281, loss = 0.01360164\n",
      "Iteration 8282, loss = 0.01360035\n",
      "Iteration 8283, loss = 0.01359641\n",
      "Iteration 8284, loss = 0.01359028\n",
      "Iteration 8285, loss = 0.01358265\n",
      "Iteration 8286, loss = 0.01357339\n",
      "Iteration 8287, loss = 0.01357295\n",
      "Iteration 8288, loss = 0.01356738\n",
      "Iteration 8289, loss = 0.01355736\n",
      "Iteration 8290, loss = 0.01355518\n",
      "Iteration 8291, loss = 0.01355076\n",
      "Iteration 8292, loss = 0.01354410\n",
      "Iteration 8293, loss = 0.01353549\n",
      "Iteration 8294, loss = 0.01353917\n",
      "Iteration 8295, loss = 0.01353187\n",
      "Iteration 8296, loss = 0.01352182\n",
      "Iteration 8297, loss = 0.01352013\n",
      "Iteration 8298, loss = 0.01351628\n",
      "Iteration 8299, loss = 0.01351030\n",
      "Iteration 8300, loss = 0.01350245\n",
      "Iteration 8301, loss = 0.01349466\n",
      "Iteration 8302, loss = 0.01348913\n",
      "Iteration 8303, loss = 0.01348331\n",
      "Iteration 8304, loss = 0.01348489\n",
      "Iteration 8305, loss = 0.01347342\n",
      "Iteration 8306, loss = 0.01346901\n",
      "Iteration 8307, loss = 0.01346385\n",
      "Iteration 8308, loss = 0.01346131\n",
      "Iteration 8309, loss = 0.01345788\n",
      "Iteration 8310, loss = 0.01345231\n",
      "Iteration 8311, loss = 0.01344498\n",
      "Iteration 8312, loss = 0.01344244\n",
      "Iteration 8313, loss = 0.01343287\n",
      "Iteration 8314, loss = 0.01343404\n",
      "Iteration 8315, loss = 0.01343311\n",
      "Iteration 8316, loss = 0.01342962\n",
      "Iteration 8317, loss = 0.01342401\n",
      "Iteration 8318, loss = 0.01341656\n",
      "Iteration 8319, loss = 0.01340752\n",
      "Iteration 8320, loss = 0.01339766\n",
      "Iteration 8321, loss = 0.01339222\n",
      "Iteration 8322, loss = 0.01339098\n",
      "Iteration 8323, loss = 0.01338426\n",
      "Iteration 8324, loss = 0.01338080\n",
      "Iteration 8325, loss = 0.01337507\n",
      "Iteration 8326, loss = 0.01336738\n",
      "Iteration 8327, loss = 0.01337194\n",
      "Iteration 8328, loss = 0.01336323\n",
      "Iteration 8329, loss = 0.01335565\n",
      "Iteration 8330, loss = 0.01335457\n",
      "Iteration 8331, loss = 0.01335119\n",
      "Iteration 8332, loss = 0.01334555\n",
      "Iteration 8333, loss = 0.01333818\n",
      "Iteration 8334, loss = 0.01332928\n",
      "Iteration 8335, loss = 0.01332966\n",
      "Iteration 8336, loss = 0.01332373\n",
      "Iteration 8337, loss = 0.01331453\n",
      "Iteration 8338, loss = 0.01331265\n",
      "Iteration 8339, loss = 0.01330841\n",
      "Iteration 8340, loss = 0.01330196\n",
      "Iteration 8341, loss = 0.01329375\n",
      "Iteration 8342, loss = 0.01329367\n",
      "Iteration 8343, loss = 0.01328602\n",
      "Iteration 8344, loss = 0.01328107\n",
      "Iteration 8345, loss = 0.01327966\n",
      "Iteration 8346, loss = 0.01327602\n",
      "Iteration 8347, loss = 0.01327028\n",
      "Iteration 8348, loss = 0.01326266\n",
      "Iteration 8349, loss = 0.01325338\n",
      "Iteration 8350, loss = 0.01325708\n",
      "Iteration 8351, loss = 0.01325182\n",
      "Iteration 8352, loss = 0.01323813\n",
      "Iteration 8353, loss = 0.01323609\n",
      "Iteration 8354, loss = 0.01323185\n",
      "Iteration 8355, loss = 0.01322549\n",
      "Iteration 8356, loss = 0.01321738\n",
      "Iteration 8357, loss = 0.01322356\n",
      "Iteration 8358, loss = 0.01321600\n",
      "Iteration 8359, loss = 0.01320478\n",
      "Iteration 8360, loss = 0.01320331\n",
      "Iteration 8361, loss = 0.01319960\n",
      "Iteration 8362, loss = 0.01319392\n",
      "Iteration 8363, loss = 0.01318642\n",
      "Iteration 8364, loss = 0.01317728\n",
      "Iteration 8365, loss = 0.01318754\n",
      "Iteration 8366, loss = 0.01318217\n",
      "Iteration 8367, loss = 0.01316268\n",
      "Iteration 8368, loss = 0.01316754\n",
      "Iteration 8369, loss = 0.01316994\n",
      "Iteration 8370, loss = 0.01316900\n",
      "Iteration 8371, loss = 0.01316558\n",
      "Iteration 8372, loss = 0.01316035\n",
      "Iteration 8373, loss = 0.01315373\n",
      "Iteration 8374, loss = 0.01314544\n",
      "Iteration 8375, loss = 0.01313538\n",
      "Iteration 8376, loss = 0.01312375\n",
      "Iteration 8377, loss = 0.01311946\n",
      "Iteration 8378, loss = 0.01311883\n",
      "Iteration 8379, loss = 0.01310436\n",
      "Iteration 8380, loss = 0.01310847\n",
      "Iteration 8381, loss = 0.01310892\n",
      "Iteration 8382, loss = 0.01310588\n",
      "Iteration 8383, loss = 0.01310105\n",
      "Iteration 8384, loss = 0.01309537\n",
      "Iteration 8385, loss = 0.01308862\n",
      "Iteration 8386, loss = 0.01307970\n",
      "Iteration 8387, loss = 0.01306842\n",
      "Iteration 8388, loss = 0.01306963\n",
      "Iteration 8389, loss = 0.01306699\n",
      "Iteration 8390, loss = 0.01305131\n",
      "Iteration 8391, loss = 0.01305604\n",
      "Iteration 8392, loss = 0.01305701\n",
      "Iteration 8393, loss = 0.01305385\n",
      "Iteration 8394, loss = 0.01304882\n",
      "Iteration 8395, loss = 0.01304363\n",
      "Iteration 8396, loss = 0.01303754\n",
      "Iteration 8397, loss = 0.01302907\n",
      "Iteration 8398, loss = 0.01301804\n",
      "Iteration 8399, loss = 0.01300886\n",
      "Iteration 8400, loss = 0.01300542\n",
      "Iteration 8401, loss = 0.01300055\n",
      "Iteration 8402, loss = 0.01299893\n",
      "Iteration 8403, loss = 0.01299339\n",
      "Iteration 8404, loss = 0.01298476\n",
      "Iteration 8405, loss = 0.01298066\n",
      "Iteration 8406, loss = 0.01297260\n",
      "Iteration 8407, loss = 0.01296871\n",
      "Iteration 8408, loss = 0.01296759\n",
      "Iteration 8409, loss = 0.01296494\n",
      "Iteration 8410, loss = 0.01296065\n",
      "Iteration 8411, loss = 0.01295463\n",
      "Iteration 8412, loss = 0.01294689\n",
      "Iteration 8413, loss = 0.01294109\n",
      "Iteration 8414, loss = 0.01293371\n",
      "Iteration 8415, loss = 0.01293450\n",
      "Iteration 8416, loss = 0.01293282\n",
      "Iteration 8417, loss = 0.01292910\n",
      "Iteration 8418, loss = 0.01292372\n",
      "Iteration 8419, loss = 0.01291656\n",
      "Iteration 8420, loss = 0.01290785\n",
      "Iteration 8421, loss = 0.01290648\n",
      "Iteration 8422, loss = 0.01290045\n",
      "Iteration 8423, loss = 0.01289358\n",
      "Iteration 8424, loss = 0.01289215\n",
      "Iteration 8425, loss = 0.01288806\n",
      "Iteration 8426, loss = 0.01288159\n",
      "Iteration 8427, loss = 0.01287344\n",
      "Iteration 8428, loss = 0.01287124\n",
      "Iteration 8429, loss = 0.01286381\n",
      "Iteration 8430, loss = 0.01286169\n",
      "Iteration 8431, loss = 0.01286051\n",
      "Iteration 8432, loss = 0.01285718\n",
      "Iteration 8433, loss = 0.01285188\n",
      "Iteration 8434, loss = 0.01284480\n",
      "Iteration 8435, loss = 0.01283611\n",
      "Iteration 8436, loss = 0.01283313\n",
      "Iteration 8437, loss = 0.01282757\n",
      "Iteration 8438, loss = 0.01282180\n",
      "Iteration 8439, loss = 0.01282010\n",
      "Iteration 8440, loss = 0.01281626\n",
      "Iteration 8441, loss = 0.01281033\n",
      "Iteration 8442, loss = 0.01280247\n",
      "Iteration 8443, loss = 0.01279727\n",
      "Iteration 8444, loss = 0.01279010\n",
      "Iteration 8445, loss = 0.01278665\n",
      "Iteration 8446, loss = 0.01278423\n",
      "Iteration 8447, loss = 0.01278169\n",
      "Iteration 8448, loss = 0.01277698\n",
      "Iteration 8449, loss = 0.01277034\n",
      "Iteration 8450, loss = 0.01276214\n",
      "Iteration 8451, loss = 0.01276781\n",
      "Iteration 8452, loss = 0.01276107\n",
      "Iteration 8453, loss = 0.01274946\n",
      "Iteration 8454, loss = 0.01274813\n",
      "Iteration 8455, loss = 0.01274455\n",
      "Iteration 8456, loss = 0.01273888\n",
      "Iteration 8457, loss = 0.01273158\n",
      "Iteration 8458, loss = 0.01272525\n",
      "Iteration 8459, loss = 0.01271948\n",
      "Iteration 8460, loss = 0.01271426\n",
      "Iteration 8461, loss = 0.01271512\n",
      "Iteration 8462, loss = 0.01270561\n",
      "Iteration 8463, loss = 0.01270182\n",
      "Iteration 8464, loss = 0.01269607\n",
      "Iteration 8465, loss = 0.01269602\n",
      "Iteration 8466, loss = 0.01268660\n",
      "Iteration 8467, loss = 0.01268234\n",
      "Iteration 8468, loss = 0.01267690\n",
      "Iteration 8469, loss = 0.01267539\n",
      "Iteration 8470, loss = 0.01267221\n",
      "Iteration 8471, loss = 0.01266735\n",
      "Iteration 8472, loss = 0.01266069\n",
      "Iteration 8473, loss = 0.01265489\n",
      "Iteration 8474, loss = 0.01264929\n",
      "Iteration 8475, loss = 0.01264447\n",
      "Iteration 8476, loss = 0.01264192\n",
      "Iteration 8477, loss = 0.01263646\n",
      "Iteration 8478, loss = 0.01263299\n",
      "Iteration 8479, loss = 0.01262742\n",
      "Iteration 8480, loss = 0.01262019\n",
      "Iteration 8481, loss = 0.01261819\n",
      "Iteration 8482, loss = 0.01261423\n",
      "Iteration 8483, loss = 0.01260844\n",
      "Iteration 8484, loss = 0.01260312\n",
      "Iteration 8485, loss = 0.01259868\n",
      "Iteration 8486, loss = 0.01259450\n",
      "Iteration 8487, loss = 0.01258863\n",
      "Iteration 8488, loss = 0.01258860\n",
      "Iteration 8489, loss = 0.01257857\n",
      "Iteration 8490, loss = 0.01257418\n",
      "Iteration 8491, loss = 0.01257173\n",
      "Iteration 8492, loss = 0.01256702\n",
      "Iteration 8493, loss = 0.01256384\n",
      "Iteration 8494, loss = 0.01255869\n",
      "Iteration 8495, loss = 0.01255194\n",
      "Iteration 8496, loss = 0.01255242\n",
      "Iteration 8497, loss = 0.01254301\n",
      "Iteration 8498, loss = 0.01254239\n",
      "Iteration 8499, loss = 0.01254206\n",
      "Iteration 8500, loss = 0.01253930\n",
      "Iteration 8501, loss = 0.01253425\n",
      "Iteration 8502, loss = 0.01252765\n",
      "Iteration 8503, loss = 0.01251967\n",
      "Iteration 8504, loss = 0.01251042\n",
      "Iteration 8505, loss = 0.01251893\n",
      "Iteration 8506, loss = 0.01251521\n",
      "Iteration 8507, loss = 0.01249741\n",
      "Iteration 8508, loss = 0.01249953\n",
      "Iteration 8509, loss = 0.01250179\n",
      "Iteration 8510, loss = 0.01250043\n",
      "Iteration 8511, loss = 0.01249681\n",
      "Iteration 8512, loss = 0.01249168\n",
      "Iteration 8513, loss = 0.01248548\n",
      "Iteration 8514, loss = 0.01247780\n",
      "Iteration 8515, loss = 0.01246816\n",
      "Iteration 8516, loss = 0.01245695\n",
      "Iteration 8517, loss = 0.01246188\n",
      "Iteration 8518, loss = 0.01246127\n",
      "Iteration 8519, loss = 0.01244692\n",
      "Iteration 8520, loss = 0.01244355\n",
      "Iteration 8521, loss = 0.01244398\n",
      "Iteration 8522, loss = 0.01244114\n",
      "Iteration 8523, loss = 0.01243702\n",
      "Iteration 8524, loss = 0.01243228\n",
      "Iteration 8525, loss = 0.01242621\n",
      "Iteration 8526, loss = 0.01241783\n",
      "Iteration 8527, loss = 0.01240721\n",
      "Iteration 8528, loss = 0.01240785\n",
      "Iteration 8529, loss = 0.01240496\n",
      "Iteration 8530, loss = 0.01239188\n",
      "Iteration 8531, loss = 0.01239016\n",
      "Iteration 8532, loss = 0.01238468\n",
      "Iteration 8533, loss = 0.01237692\n",
      "Iteration 8534, loss = 0.01237934\n",
      "Iteration 8535, loss = 0.01237031\n",
      "Iteration 8536, loss = 0.01236862\n",
      "Iteration 8537, loss = 0.01236801\n",
      "Iteration 8538, loss = 0.01236532\n",
      "Iteration 8539, loss = 0.01236106\n",
      "Iteration 8540, loss = 0.01235514\n",
      "Iteration 8541, loss = 0.01234736\n",
      "Iteration 8542, loss = 0.01233816\n",
      "Iteration 8543, loss = 0.01233830\n",
      "Iteration 8544, loss = 0.01233483\n",
      "Iteration 8545, loss = 0.01232290\n",
      "Iteration 8546, loss = 0.01232088\n",
      "Iteration 8547, loss = 0.01231676\n",
      "Iteration 8548, loss = 0.01231075\n",
      "Iteration 8549, loss = 0.01230323\n",
      "Iteration 8550, loss = 0.01230098\n",
      "Iteration 8551, loss = 0.01229682\n",
      "Iteration 8552, loss = 0.01229089\n",
      "Iteration 8553, loss = 0.01229040\n",
      "Iteration 8554, loss = 0.01228111\n",
      "Iteration 8555, loss = 0.01227685\n",
      "Iteration 8556, loss = 0.01227386\n",
      "Iteration 8557, loss = 0.01227001\n",
      "Iteration 8558, loss = 0.01226707\n",
      "Iteration 8559, loss = 0.01226222\n",
      "Iteration 8560, loss = 0.01225552\n",
      "Iteration 8561, loss = 0.01225450\n",
      "Iteration 8562, loss = 0.01224512\n",
      "Iteration 8563, loss = 0.01224661\n",
      "Iteration 8564, loss = 0.01224628\n",
      "Iteration 8565, loss = 0.01224384\n",
      "Iteration 8566, loss = 0.01223914\n",
      "Iteration 8567, loss = 0.01223264\n",
      "Iteration 8568, loss = 0.01222499\n",
      "Iteration 8569, loss = 0.01221605\n",
      "Iteration 8570, loss = 0.01221947\n",
      "Iteration 8571, loss = 0.01221559\n",
      "Iteration 8572, loss = 0.01220085\n",
      "Iteration 8573, loss = 0.01219917\n",
      "Iteration 8574, loss = 0.01219515\n",
      "Iteration 8575, loss = 0.01218889\n",
      "Iteration 8576, loss = 0.01218452\n",
      "Iteration 8577, loss = 0.01217883\n",
      "Iteration 8578, loss = 0.01217472\n",
      "Iteration 8579, loss = 0.01216881\n",
      "Iteration 8580, loss = 0.01217269\n",
      "Iteration 8581, loss = 0.01216151\n",
      "Iteration 8582, loss = 0.01216186\n",
      "Iteration 8583, loss = 0.01216236\n",
      "Iteration 8584, loss = 0.01216010\n",
      "Iteration 8585, loss = 0.01215561\n",
      "Iteration 8586, loss = 0.01214956\n",
      "Iteration 8587, loss = 0.01214225\n",
      "Iteration 8588, loss = 0.01213369\n",
      "Iteration 8589, loss = 0.01212873\n",
      "Iteration 8590, loss = 0.01212404\n",
      "Iteration 8591, loss = 0.01211908\n",
      "Iteration 8592, loss = 0.01211779\n",
      "Iteration 8593, loss = 0.01211408\n",
      "Iteration 8594, loss = 0.01210787\n",
      "Iteration 8595, loss = 0.01210011\n",
      "Iteration 8596, loss = 0.01209948\n",
      "Iteration 8597, loss = 0.01209239\n",
      "Iteration 8598, loss = 0.01208936\n",
      "Iteration 8599, loss = 0.01208839\n",
      "Iteration 8600, loss = 0.01208547\n",
      "Iteration 8601, loss = 0.01208079\n",
      "Iteration 8602, loss = 0.01207439\n",
      "Iteration 8603, loss = 0.01206636\n",
      "Iteration 8604, loss = 0.01206101\n",
      "Iteration 8605, loss = 0.01205516\n",
      "Iteration 8606, loss = 0.01205400\n",
      "Iteration 8607, loss = 0.01205282\n",
      "Iteration 8608, loss = 0.01204944\n",
      "Iteration 8609, loss = 0.01204409\n",
      "Iteration 8610, loss = 0.01203726\n",
      "Iteration 8611, loss = 0.01202914\n",
      "Iteration 8612, loss = 0.01203279\n",
      "Iteration 8613, loss = 0.01202735\n",
      "Iteration 8614, loss = 0.01201625\n",
      "Iteration 8615, loss = 0.01201500\n",
      "Iteration 8616, loss = 0.01201160\n",
      "Iteration 8617, loss = 0.01200611\n",
      "Iteration 8618, loss = 0.01199904\n",
      "Iteration 8619, loss = 0.01199691\n",
      "Iteration 8620, loss = 0.01198887\n",
      "Iteration 8621, loss = 0.01198928\n",
      "Iteration 8622, loss = 0.01198872\n",
      "Iteration 8623, loss = 0.01198613\n",
      "Iteration 8624, loss = 0.01198147\n",
      "Iteration 8625, loss = 0.01197515\n",
      "Iteration 8626, loss = 0.01196741\n",
      "Iteration 8627, loss = 0.01195834\n",
      "Iteration 8628, loss = 0.01196736\n",
      "Iteration 8629, loss = 0.01196388\n",
      "Iteration 8630, loss = 0.01194624\n",
      "Iteration 8631, loss = 0.01194862\n",
      "Iteration 8632, loss = 0.01195089\n",
      "Iteration 8633, loss = 0.01195003\n",
      "Iteration 8634, loss = 0.01194690\n",
      "Iteration 8635, loss = 0.01194238\n",
      "Iteration 8636, loss = 0.01193668\n",
      "Iteration 8637, loss = 0.01192937\n",
      "Iteration 8638, loss = 0.01192036\n",
      "Iteration 8639, loss = 0.01190987\n",
      "Iteration 8640, loss = 0.01190776\n",
      "Iteration 8641, loss = 0.01190683\n",
      "Iteration 8642, loss = 0.01189358\n",
      "Iteration 8643, loss = 0.01189116\n",
      "Iteration 8644, loss = 0.01188582\n",
      "Iteration 8645, loss = 0.01188019\n",
      "Iteration 8646, loss = 0.01187743\n",
      "Iteration 8647, loss = 0.01187450\n",
      "Iteration 8648, loss = 0.01186985\n",
      "Iteration 8649, loss = 0.01186326\n",
      "Iteration 8650, loss = 0.01186562\n",
      "Iteration 8651, loss = 0.01185598\n",
      "Iteration 8652, loss = 0.01185501\n",
      "Iteration 8653, loss = 0.01185525\n",
      "Iteration 8654, loss = 0.01185253\n",
      "Iteration 8655, loss = 0.01184733\n",
      "Iteration 8656, loss = 0.01184097\n",
      "Iteration 8657, loss = 0.01183386\n",
      "Iteration 8658, loss = 0.01182560\n",
      "Iteration 8659, loss = 0.01183293\n",
      "Iteration 8660, loss = 0.01182825\n",
      "Iteration 8661, loss = 0.01181053\n",
      "Iteration 8662, loss = 0.01180953\n",
      "Iteration 8663, loss = 0.01180610\n",
      "Iteration 8664, loss = 0.01179989\n",
      "Iteration 8665, loss = 0.01179615\n",
      "Iteration 8666, loss = 0.01178994\n",
      "Iteration 8667, loss = 0.01178640\n",
      "Iteration 8668, loss = 0.01178105\n",
      "Iteration 8669, loss = 0.01178400\n",
      "Iteration 8670, loss = 0.01177227\n",
      "Iteration 8671, loss = 0.01177466\n",
      "Iteration 8672, loss = 0.01177562\n",
      "Iteration 8673, loss = 0.01177385\n",
      "Iteration 8674, loss = 0.01176944\n",
      "Iteration 8675, loss = 0.01176338\n",
      "Iteration 8676, loss = 0.01175654\n",
      "Iteration 8677, loss = 0.01174852\n",
      "Iteration 8678, loss = 0.01173900\n",
      "Iteration 8679, loss = 0.01175161\n",
      "Iteration 8680, loss = 0.01174916\n",
      "Iteration 8681, loss = 0.01173314\n",
      "Iteration 8682, loss = 0.01172821\n",
      "Iteration 8683, loss = 0.01173016\n",
      "Iteration 8684, loss = 0.01172831\n",
      "Iteration 8685, loss = 0.01172436\n",
      "Iteration 8686, loss = 0.01171995\n",
      "Iteration 8687, loss = 0.01171465\n",
      "Iteration 8688, loss = 0.01170750\n",
      "Iteration 8689, loss = 0.01169812\n",
      "Iteration 8690, loss = 0.01168726\n",
      "Iteration 8691, loss = 0.01170120\n",
      "Iteration 8692, loss = 0.01170088\n",
      "Iteration 8693, loss = 0.01168647\n",
      "Iteration 8694, loss = 0.01167670\n",
      "Iteration 8695, loss = 0.01167701\n",
      "Iteration 8696, loss = 0.01167462\n",
      "Iteration 8697, loss = 0.01167160\n",
      "Iteration 8698, loss = 0.01166806\n",
      "Iteration 8699, loss = 0.01166252\n",
      "Iteration 8700, loss = 0.01165446\n",
      "Iteration 8701, loss = 0.01164468\n",
      "Iteration 8702, loss = 0.01164137\n",
      "Iteration 8703, loss = 0.01163814\n",
      "Iteration 8704, loss = 0.01163243\n",
      "Iteration 8705, loss = 0.01163085\n",
      "Iteration 8706, loss = 0.01162566\n",
      "Iteration 8707, loss = 0.01161891\n",
      "Iteration 8708, loss = 0.01161204\n",
      "Iteration 8709, loss = 0.01161873\n",
      "Iteration 8710, loss = 0.01161143\n",
      "Iteration 8711, loss = 0.01160192\n",
      "Iteration 8712, loss = 0.01160108\n",
      "Iteration 8713, loss = 0.01159898\n",
      "Iteration 8714, loss = 0.01159494\n",
      "Iteration 8715, loss = 0.01158851\n",
      "Iteration 8716, loss = 0.01158036\n",
      "Iteration 8717, loss = 0.01157834\n",
      "Iteration 8718, loss = 0.01157282\n",
      "Iteration 8719, loss = 0.01156892\n",
      "Iteration 8720, loss = 0.01156784\n",
      "Iteration 8721, loss = 0.01156488\n",
      "Iteration 8722, loss = 0.01156021\n",
      "Iteration 8723, loss = 0.01155389\n",
      "Iteration 8724, loss = 0.01154619\n",
      "Iteration 8725, loss = 0.01154724\n",
      "Iteration 8726, loss = 0.01154159\n",
      "Iteration 8727, loss = 0.01153457\n",
      "Iteration 8728, loss = 0.01153361\n",
      "Iteration 8729, loss = 0.01153048\n",
      "Iteration 8730, loss = 0.01152551\n",
      "Iteration 8731, loss = 0.01151909\n",
      "Iteration 8732, loss = 0.01151138\n",
      "Iteration 8733, loss = 0.01151941\n",
      "Iteration 8734, loss = 0.01151367\n",
      "Iteration 8735, loss = 0.01149966\n",
      "Iteration 8736, loss = 0.01149885\n",
      "Iteration 8737, loss = 0.01149587\n",
      "Iteration 8738, loss = 0.01149080\n",
      "Iteration 8739, loss = 0.01148416\n",
      "Iteration 8740, loss = 0.01148109\n",
      "Iteration 8741, loss = 0.01147386\n",
      "Iteration 8742, loss = 0.01146942\n",
      "Iteration 8743, loss = 0.01147036\n",
      "Iteration 8744, loss = 0.01146235\n",
      "Iteration 8745, loss = 0.01145958\n",
      "Iteration 8746, loss = 0.01145493\n",
      "Iteration 8747, loss = 0.01144858\n",
      "Iteration 8748, loss = 0.01144726\n",
      "Iteration 8749, loss = 0.01144397\n",
      "Iteration 8750, loss = 0.01143907\n",
      "Iteration 8751, loss = 0.01143274\n",
      "Iteration 8752, loss = 0.01143745\n",
      "Iteration 8753, loss = 0.01142872\n",
      "Iteration 8754, loss = 0.01142440\n",
      "Iteration 8755, loss = 0.01142438\n",
      "Iteration 8756, loss = 0.01142207\n",
      "Iteration 8757, loss = 0.01141782\n",
      "Iteration 8758, loss = 0.01141191\n",
      "Iteration 8759, loss = 0.01140470\n",
      "Iteration 8760, loss = 0.01139645\n",
      "Iteration 8761, loss = 0.01140485\n",
      "Iteration 8762, loss = 0.01140070\n",
      "Iteration 8763, loss = 0.01138295\n",
      "Iteration 8764, loss = 0.01138182\n",
      "Iteration 8765, loss = 0.01137852\n",
      "Iteration 8766, loss = 0.01137293\n",
      "Iteration 8767, loss = 0.01136789\n",
      "Iteration 8768, loss = 0.01136421\n",
      "Iteration 8769, loss = 0.01136081\n",
      "Iteration 8770, loss = 0.01135566\n",
      "Iteration 8771, loss = 0.01135385\n",
      "Iteration 8772, loss = 0.01134732\n",
      "Iteration 8773, loss = 0.01134397\n",
      "Iteration 8774, loss = 0.01133905\n",
      "Iteration 8775, loss = 0.01133845\n",
      "Iteration 8776, loss = 0.01133084\n",
      "Iteration 8777, loss = 0.01132725\n",
      "Iteration 8778, loss = 0.01132197\n",
      "Iteration 8779, loss = 0.01132412\n",
      "Iteration 8780, loss = 0.01131376\n",
      "Iteration 8781, loss = 0.01131028\n",
      "Iteration 8782, loss = 0.01130571\n",
      "Iteration 8783, loss = 0.01130501\n",
      "Iteration 8784, loss = 0.01130290\n",
      "Iteration 8785, loss = 0.01129880\n",
      "Iteration 8786, loss = 0.01129300\n",
      "Iteration 8787, loss = 0.01128586\n",
      "Iteration 8788, loss = 0.01129157\n",
      "Iteration 8789, loss = 0.01128480\n",
      "Iteration 8790, loss = 0.01127582\n",
      "Iteration 8791, loss = 0.01127536\n",
      "Iteration 8792, loss = 0.01127280\n",
      "Iteration 8793, loss = 0.01126822\n",
      "Iteration 8794, loss = 0.01126211\n",
      "Iteration 8795, loss = 0.01125474\n",
      "Iteration 8796, loss = 0.01125663\n",
      "Iteration 8797, loss = 0.01125044\n",
      "Iteration 8798, loss = 0.01124397\n",
      "Iteration 8799, loss = 0.01124338\n",
      "Iteration 8800, loss = 0.01124069\n",
      "Iteration 8801, loss = 0.01123593\n",
      "Iteration 8802, loss = 0.01122959\n",
      "Iteration 8803, loss = 0.01122224\n",
      "Iteration 8804, loss = 0.01122565\n",
      "Iteration 8805, loss = 0.01121974\n",
      "Iteration 8806, loss = 0.01121116\n",
      "Iteration 8807, loss = 0.01121046\n",
      "Iteration 8808, loss = 0.01120779\n",
      "Iteration 8809, loss = 0.01120305\n",
      "Iteration 8810, loss = 0.01119668\n",
      "Iteration 8811, loss = 0.01118919\n",
      "Iteration 8812, loss = 0.01119613\n",
      "Iteration 8813, loss = 0.01119050\n",
      "Iteration 8814, loss = 0.01117811\n",
      "Iteration 8815, loss = 0.01117731\n",
      "Iteration 8816, loss = 0.01117466\n",
      "Iteration 8817, loss = 0.01116998\n",
      "Iteration 8818, loss = 0.01116371\n",
      "Iteration 8819, loss = 0.01115660\n",
      "Iteration 8820, loss = 0.01115379\n",
      "Iteration 8821, loss = 0.01114968\n",
      "Iteration 8822, loss = 0.01114548\n",
      "Iteration 8823, loss = 0.01114312\n",
      "Iteration 8824, loss = 0.01114059\n",
      "Iteration 8825, loss = 0.01113622\n",
      "Iteration 8826, loss = 0.01113016\n",
      "Iteration 8827, loss = 0.01112922\n",
      "Iteration 8828, loss = 0.01112046\n",
      "Iteration 8829, loss = 0.01111636\n",
      "Iteration 8830, loss = 0.01111705\n",
      "Iteration 8831, loss = 0.01111567\n",
      "Iteration 8832, loss = 0.01111246\n",
      "Iteration 8833, loss = 0.01110758\n",
      "Iteration 8834, loss = 0.01110123\n",
      "Iteration 8835, loss = 0.01109362\n",
      "Iteration 8836, loss = 0.01110029\n",
      "Iteration 8837, loss = 0.01109510\n",
      "Iteration 8838, loss = 0.01108236\n",
      "Iteration 8839, loss = 0.01108153\n",
      "Iteration 8840, loss = 0.01107852\n",
      "Iteration 8841, loss = 0.01107357\n",
      "Iteration 8842, loss = 0.01106727\n",
      "Iteration 8843, loss = 0.01106437\n",
      "Iteration 8844, loss = 0.01105750\n",
      "Iteration 8845, loss = 0.01105326\n",
      "Iteration 8846, loss = 0.01105307\n",
      "Iteration 8847, loss = 0.01104688\n",
      "Iteration 8848, loss = 0.01104438\n",
      "Iteration 8849, loss = 0.01103992\n",
      "Iteration 8850, loss = 0.01103387\n",
      "Iteration 8851, loss = 0.01103742\n",
      "Iteration 8852, loss = 0.01102858\n",
      "Iteration 8853, loss = 0.01102642\n",
      "Iteration 8854, loss = 0.01102650\n",
      "Iteration 8855, loss = 0.01102460\n",
      "Iteration 8856, loss = 0.01102086\n",
      "Iteration 8857, loss = 0.01101554\n",
      "Iteration 8858, loss = 0.01100891\n",
      "Iteration 8859, loss = 0.01100104\n",
      "Iteration 8860, loss = 0.01099989\n",
      "Iteration 8861, loss = 0.01099555\n",
      "Iteration 8862, loss = 0.01098899\n",
      "Iteration 8863, loss = 0.01098804\n",
      "Iteration 8864, loss = 0.01098478\n",
      "Iteration 8865, loss = 0.01097966\n",
      "Iteration 8866, loss = 0.01097327\n",
      "Iteration 8867, loss = 0.01096883\n",
      "Iteration 8868, loss = 0.01096335\n",
      "Iteration 8869, loss = 0.01095911\n",
      "Iteration 8870, loss = 0.01095857\n",
      "Iteration 8871, loss = 0.01095264\n",
      "Iteration 8872, loss = 0.01095010\n",
      "Iteration 8873, loss = 0.01094566\n",
      "Iteration 8874, loss = 0.01093951\n",
      "Iteration 8875, loss = 0.01094380\n",
      "Iteration 8876, loss = 0.01093509\n",
      "Iteration 8877, loss = 0.01093211\n",
      "Iteration 8878, loss = 0.01093217\n",
      "Iteration 8879, loss = 0.01093025\n",
      "Iteration 8880, loss = 0.01092656\n",
      "Iteration 8881, loss = 0.01092144\n",
      "Iteration 8882, loss = 0.01091481\n",
      "Iteration 8883, loss = 0.01090695\n",
      "Iteration 8884, loss = 0.01090682\n",
      "Iteration 8885, loss = 0.01090238\n",
      "Iteration 8886, loss = 0.01089510\n",
      "Iteration 8887, loss = 0.01089426\n",
      "Iteration 8888, loss = 0.01089104\n",
      "Iteration 8889, loss = 0.01088599\n",
      "Iteration 8890, loss = 0.01087963\n",
      "Iteration 8891, loss = 0.01087536\n",
      "Iteration 8892, loss = 0.01086995\n",
      "Iteration 8893, loss = 0.01086576\n",
      "Iteration 8894, loss = 0.01086492\n",
      "Iteration 8895, loss = 0.01085938\n",
      "Iteration 8896, loss = 0.01085697\n",
      "Iteration 8897, loss = 0.01085270\n",
      "Iteration 8898, loss = 0.01084664\n",
      "Iteration 8899, loss = 0.01084984\n",
      "Iteration 8900, loss = 0.01084110\n",
      "Iteration 8901, loss = 0.01083932\n",
      "Iteration 8902, loss = 0.01083945\n",
      "Iteration 8903, loss = 0.01083761\n",
      "Iteration 8904, loss = 0.01083399\n",
      "Iteration 8905, loss = 0.01082893\n",
      "Iteration 8906, loss = 0.01082240\n",
      "Iteration 8907, loss = 0.01081462\n",
      "Iteration 8908, loss = 0.01081204\n",
      "Iteration 8909, loss = 0.01080754\n",
      "Iteration 8910, loss = 0.01080308\n",
      "Iteration 8911, loss = 0.01080219\n",
      "Iteration 8912, loss = 0.01079904\n",
      "Iteration 8913, loss = 0.01079405\n",
      "Iteration 8914, loss = 0.01078788\n",
      "Iteration 8915, loss = 0.01078080\n",
      "Iteration 8916, loss = 0.01079070\n",
      "Iteration 8917, loss = 0.01078511\n",
      "Iteration 8918, loss = 0.01076984\n",
      "Iteration 8919, loss = 0.01076935\n",
      "Iteration 8920, loss = 0.01076685\n",
      "Iteration 8921, loss = 0.01076231\n",
      "Iteration 8922, loss = 0.01075614\n",
      "Iteration 8923, loss = 0.01075234\n",
      "Iteration 8924, loss = 0.01074671\n",
      "Iteration 8925, loss = 0.01074284\n",
      "Iteration 8926, loss = 0.01074091\n",
      "Iteration 8927, loss = 0.01073688\n",
      "Iteration 8928, loss = 0.01073460\n",
      "Iteration 8929, loss = 0.01073053\n",
      "Iteration 8930, loss = 0.01072472\n",
      "Iteration 8931, loss = 0.01072360\n",
      "Iteration 8932, loss = 0.01071558\n",
      "Iteration 8933, loss = 0.01071189\n",
      "Iteration 8934, loss = 0.01071097\n",
      "Iteration 8935, loss = 0.01070618\n",
      "Iteration 8936, loss = 0.01070395\n",
      "Iteration 8937, loss = 0.01069996\n",
      "Iteration 8938, loss = 0.01069438\n",
      "Iteration 8939, loss = 0.01069227\n",
      "Iteration 8940, loss = 0.01068549\n",
      "Iteration 8941, loss = 0.01068203\n",
      "Iteration 8942, loss = 0.01067872\n",
      "Iteration 8943, loss = 0.01067647\n",
      "Iteration 8944, loss = 0.01067427\n",
      "Iteration 8945, loss = 0.01067030\n",
      "Iteration 8946, loss = 0.01066481\n",
      "Iteration 8947, loss = 0.01065951\n",
      "Iteration 8948, loss = 0.01065612\n",
      "Iteration 8949, loss = 0.01065261\n",
      "Iteration 8950, loss = 0.01064749\n",
      "Iteration 8951, loss = 0.01064974\n",
      "Iteration 8952, loss = 0.01063942\n",
      "Iteration 8953, loss = 0.01063615\n",
      "Iteration 8954, loss = 0.01063359\n",
      "Iteration 8955, loss = 0.01063109\n",
      "Iteration 8956, loss = 0.01062897\n",
      "Iteration 8957, loss = 0.01062519\n",
      "Iteration 8958, loss = 0.01061997\n",
      "Iteration 8959, loss = 0.01061349\n",
      "Iteration 8960, loss = 0.01062048\n",
      "Iteration 8961, loss = 0.01061336\n",
      "Iteration 8962, loss = 0.01060472\n",
      "Iteration 8963, loss = 0.01060465\n",
      "Iteration 8964, loss = 0.01060243\n",
      "Iteration 8965, loss = 0.01059817\n",
      "Iteration 8966, loss = 0.01059257\n",
      "Iteration 8967, loss = 0.01058609\n",
      "Iteration 8968, loss = 0.01058367\n",
      "Iteration 8969, loss = 0.01057686\n",
      "Iteration 8970, loss = 0.01057672\n",
      "Iteration 8971, loss = 0.01057665\n",
      "Iteration 8972, loss = 0.01057447\n",
      "Iteration 8973, loss = 0.01057017\n",
      "Iteration 8974, loss = 0.01056431\n",
      "Iteration 8975, loss = 0.01055757\n",
      "Iteration 8976, loss = 0.01054997\n",
      "Iteration 8977, loss = 0.01056157\n",
      "Iteration 8978, loss = 0.01055760\n",
      "Iteration 8979, loss = 0.01053984\n",
      "Iteration 8980, loss = 0.01054329\n",
      "Iteration 8981, loss = 0.01054623\n",
      "Iteration 8982, loss = 0.01054569\n",
      "Iteration 8983, loss = 0.01054314\n",
      "Iteration 8984, loss = 0.01053977\n",
      "Iteration 8985, loss = 0.01053551\n",
      "Iteration 8986, loss = 0.01052951\n",
      "Iteration 8987, loss = 0.01052154\n",
      "Iteration 8988, loss = 0.01051239\n",
      "Iteration 8989, loss = 0.01050285\n",
      "Iteration 8990, loss = 0.01051103\n",
      "Iteration 8991, loss = 0.01051208\n",
      "Iteration 8992, loss = 0.01049844\n",
      "Iteration 8993, loss = 0.01049168\n",
      "Iteration 8994, loss = 0.01049234\n",
      "Iteration 8995, loss = 0.01049140\n",
      "Iteration 8996, loss = 0.01048951\n",
      "Iteration 8997, loss = 0.01048614\n",
      "Iteration 8998, loss = 0.01048073\n",
      "Iteration 8999, loss = 0.01047352\n",
      "Iteration 9000, loss = 0.01046543\n",
      "Iteration 9001, loss = 0.01045680\n",
      "Iteration 9002, loss = 0.01046961\n",
      "Iteration 9003, loss = 0.01046900\n",
      "Iteration 9004, loss = 0.01045382\n",
      "Iteration 9005, loss = 0.01044707\n",
      "Iteration 9006, loss = 0.01044845\n",
      "Iteration 9007, loss = 0.01044817\n",
      "Iteration 9008, loss = 0.01044654\n",
      "Iteration 9009, loss = 0.01044328\n",
      "Iteration 9010, loss = 0.01043808\n",
      "Iteration 9011, loss = 0.01043136\n",
      "Iteration 9012, loss = 0.01042356\n",
      "Iteration 9013, loss = 0.01041504\n",
      "Iteration 9014, loss = 0.01041694\n",
      "Iteration 9015, loss = 0.01041588\n",
      "Iteration 9016, loss = 0.01040179\n",
      "Iteration 9017, loss = 0.01039962\n",
      "Iteration 9018, loss = 0.01039579\n",
      "Iteration 9019, loss = 0.01039094\n",
      "Iteration 9020, loss = 0.01039164\n",
      "Iteration 9021, loss = 0.01038338\n",
      "Iteration 9022, loss = 0.01038008\n",
      "Iteration 9023, loss = 0.01037547\n",
      "Iteration 9024, loss = 0.01037716\n",
      "Iteration 9025, loss = 0.01036864\n",
      "Iteration 9026, loss = 0.01036566\n",
      "Iteration 9027, loss = 0.01036092\n",
      "Iteration 9028, loss = 0.01036238\n",
      "Iteration 9029, loss = 0.01035369\n",
      "Iteration 9030, loss = 0.01035082\n",
      "Iteration 9031, loss = 0.01034653\n",
      "Iteration 9032, loss = 0.01034637\n",
      "Iteration 9033, loss = 0.01033980\n",
      "Iteration 9034, loss = 0.01033716\n",
      "Iteration 9035, loss = 0.01033291\n",
      "Iteration 9036, loss = 0.01032865\n",
      "Iteration 9037, loss = 0.01032599\n",
      "Iteration 9038, loss = 0.01032331\n",
      "Iteration 9039, loss = 0.01031906\n",
      "Iteration 9040, loss = 0.01031333\n",
      "Iteration 9041, loss = 0.01031812\n",
      "Iteration 9042, loss = 0.01030930\n",
      "Iteration 9043, loss = 0.01030683\n",
      "Iteration 9044, loss = 0.01030733\n",
      "Iteration 9045, loss = 0.01030570\n",
      "Iteration 9046, loss = 0.01030204\n",
      "Iteration 9047, loss = 0.01029706\n",
      "Iteration 9048, loss = 0.01029106\n",
      "Iteration 9049, loss = 0.01028390\n",
      "Iteration 9050, loss = 0.01028148\n",
      "Iteration 9051, loss = 0.01027649\n",
      "Iteration 9052, loss = 0.01027311\n",
      "Iteration 9053, loss = 0.01027287\n",
      "Iteration 9054, loss = 0.01027013\n",
      "Iteration 9055, loss = 0.01026521\n",
      "Iteration 9056, loss = 0.01025927\n",
      "Iteration 9057, loss = 0.01025275\n",
      "Iteration 9058, loss = 0.01025843\n",
      "Iteration 9059, loss = 0.01025238\n",
      "Iteration 9060, loss = 0.01024271\n",
      "Iteration 9061, loss = 0.01024272\n",
      "Iteration 9062, loss = 0.01024076\n",
      "Iteration 9063, loss = 0.01023643\n",
      "Iteration 9064, loss = 0.01023055\n",
      "Iteration 9065, loss = 0.01022381\n",
      "Iteration 9066, loss = 0.01022768\n",
      "Iteration 9067, loss = 0.01022173\n",
      "Iteration 9068, loss = 0.01021455\n",
      "Iteration 9069, loss = 0.01021435\n",
      "Iteration 9070, loss = 0.01021244\n",
      "Iteration 9071, loss = 0.01020842\n",
      "Iteration 9072, loss = 0.01020284\n",
      "Iteration 9073, loss = 0.01019628\n",
      "Iteration 9074, loss = 0.01019413\n",
      "Iteration 9075, loss = 0.01018795\n",
      "Iteration 9076, loss = 0.01018735\n",
      "Iteration 9077, loss = 0.01018713\n",
      "Iteration 9078, loss = 0.01018513\n",
      "Iteration 9079, loss = 0.01018120\n",
      "Iteration 9080, loss = 0.01017582\n",
      "Iteration 9081, loss = 0.01016933\n",
      "Iteration 9082, loss = 0.01016189\n",
      "Iteration 9083, loss = 0.01017182\n",
      "Iteration 9084, loss = 0.01016789\n",
      "Iteration 9085, loss = 0.01015045\n",
      "Iteration 9086, loss = 0.01014983\n",
      "Iteration 9087, loss = 0.01014709\n",
      "Iteration 9088, loss = 0.01014236\n",
      "Iteration 9089, loss = 0.01013647\n",
      "Iteration 9090, loss = 0.01014204\n",
      "Iteration 9091, loss = 0.01013392\n",
      "Iteration 9092, loss = 0.01012958\n",
      "Iteration 9093, loss = 0.01013000\n",
      "Iteration 9094, loss = 0.01012855\n",
      "Iteration 9095, loss = 0.01012516\n",
      "Iteration 9096, loss = 0.01012028\n",
      "Iteration 9097, loss = 0.01011418\n",
      "Iteration 9098, loss = 0.01010702\n",
      "Iteration 9099, loss = 0.01010665\n",
      "Iteration 9100, loss = 0.01010197\n",
      "Iteration 9101, loss = 0.01009646\n",
      "Iteration 9102, loss = 0.01009601\n",
      "Iteration 9103, loss = 0.01009346\n",
      "Iteration 9104, loss = 0.01008892\n",
      "Iteration 9105, loss = 0.01008319\n",
      "Iteration 9106, loss = 0.01007667\n",
      "Iteration 9107, loss = 0.01008285\n",
      "Iteration 9108, loss = 0.01007694\n",
      "Iteration 9109, loss = 0.01006719\n",
      "Iteration 9110, loss = 0.01006704\n",
      "Iteration 9111, loss = 0.01006508\n",
      "Iteration 9112, loss = 0.01006099\n",
      "Iteration 9113, loss = 0.01005544\n",
      "Iteration 9114, loss = 0.01004896\n",
      "Iteration 9115, loss = 0.01005146\n",
      "Iteration 9116, loss = 0.01004522\n",
      "Iteration 9117, loss = 0.01004015\n",
      "Iteration 9118, loss = 0.01004004\n",
      "Iteration 9119, loss = 0.01003817\n",
      "Iteration 9120, loss = 0.01003437\n",
      "Iteration 9121, loss = 0.01002901\n",
      "Iteration 9122, loss = 0.01002258\n",
      "Iteration 9123, loss = 0.01001718\n",
      "Iteration 9124, loss = 0.01001255\n",
      "Iteration 9125, loss = 0.01000879\n",
      "Iteration 9126, loss = 0.01000871\n",
      "Iteration 9127, loss = 0.01000765\n",
      "Iteration 9128, loss = 0.01000453\n",
      "Iteration 9129, loss = 0.00999951\n",
      "Iteration 9130, loss = 0.00999333\n",
      "Iteration 9131, loss = 0.00999059\n",
      "Iteration 9132, loss = 0.00998413\n",
      "Iteration 9133, loss = 0.00998046\n",
      "Iteration 9134, loss = 0.00998081\n",
      "Iteration 9135, loss = 0.00997993\n",
      "Iteration 9136, loss = 0.00997708\n",
      "Iteration 9137, loss = 0.00997243\n",
      "Iteration 9138, loss = 0.00996657\n",
      "Iteration 9139, loss = 0.00995981\n",
      "Iteration 9140, loss = 0.00996841\n",
      "Iteration 9141, loss = 0.00996318\n",
      "Iteration 9142, loss = 0.00995003\n",
      "Iteration 9143, loss = 0.00994975\n",
      "Iteration 9144, loss = 0.00994767\n",
      "Iteration 9145, loss = 0.00994355\n",
      "Iteration 9146, loss = 0.00993794\n",
      "Iteration 9147, loss = 0.00993156\n",
      "Iteration 9148, loss = 0.00994022\n",
      "Iteration 9149, loss = 0.00993404\n",
      "Iteration 9150, loss = 0.00992271\n",
      "Iteration 9151, loss = 0.00992271\n",
      "Iteration 9152, loss = 0.00992080\n",
      "Iteration 9153, loss = 0.00991692\n",
      "Iteration 9154, loss = 0.00991160\n",
      "Iteration 9155, loss = 0.00990531\n",
      "Iteration 9156, loss = 0.00990631\n",
      "Iteration 9157, loss = 0.00989989\n",
      "Iteration 9158, loss = 0.00989698\n",
      "Iteration 9159, loss = 0.00989705\n",
      "Iteration 9160, loss = 0.00989527\n",
      "Iteration 9161, loss = 0.00989152\n",
      "Iteration 9162, loss = 0.00988629\n",
      "Iteration 9163, loss = 0.00988012\n",
      "Iteration 9164, loss = 0.00987304\n",
      "Iteration 9165, loss = 0.00988212\n",
      "Iteration 9166, loss = 0.00987792\n",
      "Iteration 9167, loss = 0.00986217\n",
      "Iteration 9168, loss = 0.00986176\n",
      "Iteration 9169, loss = 0.00985923\n",
      "Iteration 9170, loss = 0.00985469\n",
      "Iteration 9171, loss = 0.00984899\n",
      "Iteration 9172, loss = 0.00985100\n",
      "Iteration 9173, loss = 0.00984280\n",
      "Iteration 9174, loss = 0.00984278\n",
      "Iteration 9175, loss = 0.00984322\n",
      "Iteration 9176, loss = 0.00984193\n",
      "Iteration 9177, loss = 0.00983873\n",
      "Iteration 9178, loss = 0.00983399\n",
      "Iteration 9179, loss = 0.00982814\n",
      "Iteration 9180, loss = 0.00982124\n",
      "Iteration 9181, loss = 0.00981387\n",
      "Iteration 9182, loss = 0.00980996\n",
      "Iteration 9183, loss = 0.00980868\n",
      "Iteration 9184, loss = 0.00980574\n",
      "Iteration 9185, loss = 0.00980419\n",
      "Iteration 9186, loss = 0.00980048\n",
      "Iteration 9187, loss = 0.00979531\n",
      "Iteration 9188, loss = 0.00978932\n",
      "Iteration 9189, loss = 0.00979774\n",
      "Iteration 9190, loss = 0.00979078\n",
      "Iteration 9191, loss = 0.00978149\n",
      "Iteration 9192, loss = 0.00978168\n",
      "Iteration 9193, loss = 0.00978009\n",
      "Iteration 9194, loss = 0.00977658\n",
      "Iteration 9195, loss = 0.00977148\n",
      "Iteration 9196, loss = 0.00976538\n",
      "Iteration 9197, loss = 0.00975888\n",
      "Iteration 9198, loss = 0.00975587\n",
      "Iteration 9199, loss = 0.00975190\n",
      "Iteration 9200, loss = 0.00975161\n",
      "Iteration 9201, loss = 0.00974641\n",
      "Iteration 9202, loss = 0.00974420\n",
      "Iteration 9203, loss = 0.00974006\n",
      "Iteration 9204, loss = 0.00973465\n",
      "Iteration 9205, loss = 0.00973937\n",
      "Iteration 9206, loss = 0.00973076\n",
      "Iteration 9207, loss = 0.00972895\n",
      "Iteration 9208, loss = 0.00972948\n",
      "Iteration 9209, loss = 0.00972827\n",
      "Iteration 9210, loss = 0.00972527\n",
      "Iteration 9211, loss = 0.00972086\n",
      "Iteration 9212, loss = 0.00971526\n",
      "Iteration 9213, loss = 0.00970858\n",
      "Iteration 9214, loss = 0.00970078\n",
      "Iteration 9215, loss = 0.00971252\n",
      "Iteration 9216, loss = 0.00970994\n",
      "Iteration 9217, loss = 0.00969398\n",
      "Iteration 9218, loss = 0.00969459\n",
      "Iteration 9219, loss = 0.00969677\n",
      "Iteration 9220, loss = 0.00969631\n",
      "Iteration 9221, loss = 0.00969444\n",
      "Iteration 9222, loss = 0.00969185\n",
      "Iteration 9223, loss = 0.00968793\n",
      "Iteration 9224, loss = 0.00968211\n",
      "Iteration 9225, loss = 0.00967473\n",
      "Iteration 9226, loss = 0.00966670\n",
      "Iteration 9227, loss = 0.00965822\n",
      "Iteration 9228, loss = 0.00966608\n",
      "Iteration 9229, loss = 0.00966663\n",
      "Iteration 9230, loss = 0.00965280\n",
      "Iteration 9231, loss = 0.00964826\n",
      "Iteration 9232, loss = 0.00964954\n",
      "Iteration 9233, loss = 0.00964928\n",
      "Iteration 9234, loss = 0.00964784\n",
      "Iteration 9235, loss = 0.00964473\n",
      "Iteration 9236, loss = 0.00963993\n",
      "Iteration 9237, loss = 0.00963373\n",
      "Iteration 9238, loss = 0.00962663\n",
      "Iteration 9239, loss = 0.00961879\n",
      "Iteration 9240, loss = 0.00961923\n",
      "Iteration 9241, loss = 0.00961788\n",
      "Iteration 9242, loss = 0.00960672\n",
      "Iteration 9243, loss = 0.00960508\n",
      "Iteration 9244, loss = 0.00960187\n",
      "Iteration 9245, loss = 0.00959769\n",
      "Iteration 9246, loss = 0.00959243\n",
      "Iteration 9247, loss = 0.00959105\n",
      "Iteration 9248, loss = 0.00958829\n",
      "Iteration 9249, loss = 0.00958439\n",
      "Iteration 9250, loss = 0.00957919\n",
      "Iteration 9251, loss = 0.00958312\n",
      "Iteration 9252, loss = 0.00957499\n",
      "Iteration 9253, loss = 0.00957320\n",
      "Iteration 9254, loss = 0.00957344\n",
      "Iteration 9255, loss = 0.00957189\n",
      "Iteration 9256, loss = 0.00956901\n",
      "Iteration 9257, loss = 0.00956482\n",
      "Iteration 9258, loss = 0.00955929\n",
      "Iteration 9259, loss = 0.00955252\n",
      "Iteration 9260, loss = 0.00954555\n",
      "Iteration 9261, loss = 0.00954195\n",
      "Iteration 9262, loss = 0.00954037\n",
      "Iteration 9263, loss = 0.00953836\n",
      "Iteration 9264, loss = 0.00953656\n",
      "Iteration 9265, loss = 0.00953286\n",
      "Iteration 9266, loss = 0.00952818\n",
      "Iteration 9267, loss = 0.00952282\n",
      "Iteration 9268, loss = 0.00952780\n",
      "Iteration 9269, loss = 0.00952003\n",
      "Iteration 9270, loss = 0.00951549\n",
      "Iteration 9271, loss = 0.00951627\n",
      "Iteration 9272, loss = 0.00951477\n",
      "Iteration 9273, loss = 0.00951104\n",
      "Iteration 9274, loss = 0.00950606\n",
      "Iteration 9275, loss = 0.00950055\n",
      "Iteration 9276, loss = 0.00949415\n",
      "Iteration 9277, loss = 0.00949837\n",
      "Iteration 9278, loss = 0.00949313\n",
      "Iteration 9279, loss = 0.00948392\n",
      "Iteration 9280, loss = 0.00948414\n",
      "Iteration 9281, loss = 0.00948190\n",
      "Iteration 9282, loss = 0.00947735\n",
      "Iteration 9283, loss = 0.00947177\n",
      "Iteration 9284, loss = 0.00946599\n",
      "Iteration 9285, loss = 0.00947437\n",
      "Iteration 9286, loss = 0.00946794\n",
      "Iteration 9287, loss = 0.00945725\n",
      "Iteration 9288, loss = 0.00945757\n",
      "Iteration 9289, loss = 0.00945616\n",
      "Iteration 9290, loss = 0.00945240\n",
      "Iteration 9291, loss = 0.00944707\n",
      "Iteration 9292, loss = 0.00944110\n",
      "Iteration 9293, loss = 0.00944058\n",
      "Iteration 9294, loss = 0.00943402\n",
      "Iteration 9295, loss = 0.00943331\n",
      "Iteration 9296, loss = 0.00943353\n",
      "Iteration 9297, loss = 0.00943218\n",
      "Iteration 9298, loss = 0.00942874\n",
      "Iteration 9299, loss = 0.00942369\n",
      "Iteration 9300, loss = 0.00941777\n",
      "Iteration 9301, loss = 0.00941117\n",
      "Iteration 9302, loss = 0.00941497\n",
      "Iteration 9303, loss = 0.00941050\n",
      "Iteration 9304, loss = 0.00940115\n",
      "Iteration 9305, loss = 0.00940104\n",
      "Iteration 9306, loss = 0.00939879\n",
      "Iteration 9307, loss = 0.00939447\n",
      "Iteration 9308, loss = 0.00938907\n",
      "Iteration 9309, loss = 0.00938313\n",
      "Iteration 9310, loss = 0.00939210\n",
      "Iteration 9311, loss = 0.00938594\n",
      "Iteration 9312, loss = 0.00937470\n",
      "Iteration 9313, loss = 0.00937497\n",
      "Iteration 9314, loss = 0.00937343\n",
      "Iteration 9315, loss = 0.00936975\n",
      "Iteration 9316, loss = 0.00936455\n",
      "Iteration 9317, loss = 0.00935865\n",
      "Iteration 9318, loss = 0.00935859\n",
      "Iteration 9319, loss = 0.00935205\n",
      "Iteration 9320, loss = 0.00935111\n",
      "Iteration 9321, loss = 0.00935141\n",
      "Iteration 9322, loss = 0.00934995\n",
      "Iteration 9323, loss = 0.00934645\n",
      "Iteration 9324, loss = 0.00934162\n",
      "Iteration 9325, loss = 0.00933587\n",
      "Iteration 9326, loss = 0.00932927\n",
      "Iteration 9327, loss = 0.00933259\n",
      "Iteration 9328, loss = 0.00932801\n",
      "Iteration 9329, loss = 0.00931956\n",
      "Iteration 9330, loss = 0.00931959\n",
      "Iteration 9331, loss = 0.00931733\n",
      "Iteration 9332, loss = 0.00931300\n",
      "Iteration 9333, loss = 0.00930765\n",
      "Iteration 9334, loss = 0.00930181\n",
      "Iteration 9335, loss = 0.00930949\n",
      "Iteration 9336, loss = 0.00930324\n",
      "Iteration 9337, loss = 0.00929351\n",
      "Iteration 9338, loss = 0.00929387\n",
      "Iteration 9339, loss = 0.00929239\n",
      "Iteration 9340, loss = 0.00928871\n",
      "Iteration 9341, loss = 0.00928358\n",
      "Iteration 9342, loss = 0.00927785\n",
      "Iteration 9343, loss = 0.00927597\n",
      "Iteration 9344, loss = 0.00926922\n",
      "Iteration 9345, loss = 0.00927024\n",
      "Iteration 9346, loss = 0.00927065\n",
      "Iteration 9347, loss = 0.00926925\n",
      "Iteration 9348, loss = 0.00926580\n",
      "Iteration 9349, loss = 0.00926086\n",
      "Iteration 9350, loss = 0.00925517\n",
      "Iteration 9351, loss = 0.00924872\n",
      "Iteration 9352, loss = 0.00924968\n",
      "Iteration 9353, loss = 0.00924499\n",
      "Iteration 9354, loss = 0.00923897\n",
      "Iteration 9355, loss = 0.00923903\n",
      "Iteration 9356, loss = 0.00923684\n",
      "Iteration 9357, loss = 0.00923258\n",
      "Iteration 9358, loss = 0.00922726\n",
      "Iteration 9359, loss = 0.00922152\n",
      "Iteration 9360, loss = 0.00922643\n",
      "Iteration 9361, loss = 0.00922007\n",
      "Iteration 9362, loss = 0.00921326\n",
      "Iteration 9363, loss = 0.00921371\n",
      "Iteration 9364, loss = 0.00921223\n",
      "Iteration 9365, loss = 0.00920854\n",
      "Iteration 9366, loss = 0.00920343\n",
      "Iteration 9367, loss = 0.00919772\n",
      "Iteration 9368, loss = 0.00919272\n",
      "Iteration 9369, loss = 0.00918883\n",
      "Iteration 9370, loss = 0.00918496\n",
      "Iteration 9371, loss = 0.00918528\n",
      "Iteration 9372, loss = 0.00918003\n",
      "Iteration 9373, loss = 0.00917820\n",
      "Iteration 9374, loss = 0.00917428\n",
      "Iteration 9375, loss = 0.00916906\n",
      "Iteration 9376, loss = 0.00917279\n",
      "Iteration 9377, loss = 0.00916428\n",
      "Iteration 9378, loss = 0.00916409\n",
      "Iteration 9379, loss = 0.00916481\n",
      "Iteration 9380, loss = 0.00916395\n",
      "Iteration 9381, loss = 0.00916135\n",
      "Iteration 9382, loss = 0.00915721\n",
      "Iteration 9383, loss = 0.00915199\n",
      "Iteration 9384, loss = 0.00914578\n",
      "Iteration 9385, loss = 0.00913853\n",
      "Iteration 9386, loss = 0.00914373\n",
      "Iteration 9387, loss = 0.00914087\n",
      "Iteration 9388, loss = 0.00912802\n",
      "Iteration 9389, loss = 0.00912751\n",
      "Iteration 9390, loss = 0.00912471\n",
      "Iteration 9391, loss = 0.00912030\n",
      "Iteration 9392, loss = 0.00911519\n",
      "Iteration 9393, loss = 0.00911883\n",
      "Iteration 9394, loss = 0.00911067\n",
      "Iteration 9395, loss = 0.00910951\n",
      "Iteration 9396, loss = 0.00911030\n",
      "Iteration 9397, loss = 0.00910941\n",
      "Iteration 9398, loss = 0.00910644\n",
      "Iteration 9399, loss = 0.00910196\n",
      "Iteration 9400, loss = 0.00909662\n",
      "Iteration 9401, loss = 0.00909054\n",
      "Iteration 9402, loss = 0.00908338\n",
      "Iteration 9403, loss = 0.00909480\n",
      "Iteration 9404, loss = 0.00909196\n",
      "Iteration 9405, loss = 0.00907611\n",
      "Iteration 9406, loss = 0.00907825\n",
      "Iteration 9407, loss = 0.00908061\n",
      "Iteration 9408, loss = 0.00908006\n",
      "Iteration 9409, loss = 0.00907841\n",
      "Iteration 9410, loss = 0.00907634\n",
      "Iteration 9411, loss = 0.00907293\n",
      "Iteration 9412, loss = 0.00906736\n",
      "Iteration 9413, loss = 0.00906036\n",
      "Iteration 9414, loss = 0.00905297\n",
      "Iteration 9415, loss = 0.00904523\n",
      "Iteration 9416, loss = 0.00904546\n",
      "Iteration 9417, loss = 0.00904556\n",
      "Iteration 9418, loss = 0.00903256\n",
      "Iteration 9419, loss = 0.00903059\n",
      "Iteration 9420, loss = 0.00902743\n",
      "Iteration 9421, loss = 0.00902353\n",
      "Iteration 9422, loss = 0.00902503\n",
      "Iteration 9423, loss = 0.00901695\n",
      "Iteration 9424, loss = 0.00901437\n",
      "Iteration 9425, loss = 0.00901073\n",
      "Iteration 9426, loss = 0.00901022\n",
      "Iteration 9427, loss = 0.00900529\n",
      "Iteration 9428, loss = 0.00900276\n",
      "Iteration 9429, loss = 0.00899888\n",
      "Iteration 9430, loss = 0.00899456\n",
      "Iteration 9431, loss = 0.00899357\n",
      "Iteration 9432, loss = 0.00899146\n",
      "Iteration 9433, loss = 0.00898797\n",
      "Iteration 9434, loss = 0.00898332\n",
      "Iteration 9435, loss = 0.00898097\n",
      "Iteration 9436, loss = 0.00897629\n",
      "Iteration 9437, loss = 0.00897347\n",
      "Iteration 9438, loss = 0.00896920\n",
      "Iteration 9439, loss = 0.00897240\n",
      "Iteration 9440, loss = 0.00896279\n",
      "Iteration 9441, loss = 0.00896038\n",
      "Iteration 9442, loss = 0.00895660\n",
      "Iteration 9443, loss = 0.00895892\n",
      "Iteration 9444, loss = 0.00895099\n",
      "Iteration 9445, loss = 0.00894888\n",
      "Iteration 9446, loss = 0.00894528\n",
      "Iteration 9447, loss = 0.00894194\n",
      "Iteration 9448, loss = 0.00894000\n",
      "Iteration 9449, loss = 0.00893792\n",
      "Iteration 9450, loss = 0.00893453\n",
      "Iteration 9451, loss = 0.00892993\n",
      "Iteration 9452, loss = 0.00892914\n",
      "Iteration 9453, loss = 0.00892283\n",
      "Iteration 9454, loss = 0.00892005\n",
      "Iteration 9455, loss = 0.00891593\n",
      "Iteration 9456, loss = 0.00891977\n",
      "Iteration 9457, loss = 0.00890976\n",
      "Iteration 9458, loss = 0.00890734\n",
      "Iteration 9459, loss = 0.00890347\n",
      "Iteration 9460, loss = 0.00890634\n",
      "Iteration 9461, loss = 0.00889791\n",
      "Iteration 9462, loss = 0.00889576\n",
      "Iteration 9463, loss = 0.00889228\n",
      "Iteration 9464, loss = 0.00888938\n",
      "Iteration 9465, loss = 0.00888713\n",
      "Iteration 9466, loss = 0.00888518\n",
      "Iteration 9467, loss = 0.00888175\n",
      "Iteration 9468, loss = 0.00887707\n",
      "Iteration 9469, loss = 0.00887613\n",
      "Iteration 9470, loss = 0.00887003\n",
      "Iteration 9471, loss = 0.00886718\n",
      "Iteration 9472, loss = 0.00886320\n",
      "Iteration 9473, loss = 0.00886696\n",
      "Iteration 9474, loss = 0.00885714\n",
      "Iteration 9475, loss = 0.00885480\n",
      "Iteration 9476, loss = 0.00885103\n",
      "Iteration 9477, loss = 0.00885332\n",
      "Iteration 9478, loss = 0.00884539\n",
      "Iteration 9479, loss = 0.00884328\n",
      "Iteration 9480, loss = 0.00883978\n",
      "Iteration 9481, loss = 0.00883643\n",
      "Iteration 9482, loss = 0.00883467\n",
      "Iteration 9483, loss = 0.00883277\n",
      "Iteration 9484, loss = 0.00882945\n",
      "Iteration 9485, loss = 0.00882483\n",
      "Iteration 9486, loss = 0.00882314\n",
      "Iteration 9487, loss = 0.00881778\n",
      "Iteration 9488, loss = 0.00881505\n",
      "Iteration 9489, loss = 0.00881097\n",
      "Iteration 9490, loss = 0.00881400\n",
      "Iteration 9491, loss = 0.00880480\n",
      "Iteration 9492, loss = 0.00880254\n",
      "Iteration 9493, loss = 0.00879885\n",
      "Iteration 9494, loss = 0.00880022\n",
      "Iteration 9495, loss = 0.00879323\n",
      "Iteration 9496, loss = 0.00879123\n",
      "Iteration 9497, loss = 0.00878776\n",
      "Iteration 9498, loss = 0.00878333\n",
      "Iteration 9499, loss = 0.00878271\n",
      "Iteration 9500, loss = 0.00878079\n",
      "Iteration 9501, loss = 0.00877746\n",
      "Iteration 9502, loss = 0.00877289\n",
      "Iteration 9503, loss = 0.00877011\n",
      "Iteration 9504, loss = 0.00876593\n",
      "Iteration 9505, loss = 0.00876319\n",
      "Iteration 9506, loss = 0.00875916\n",
      "Iteration 9507, loss = 0.00876072\n",
      "Iteration 9508, loss = 0.00875329\n",
      "Iteration 9509, loss = 0.00875096\n",
      "Iteration 9510, loss = 0.00874725\n",
      "Iteration 9511, loss = 0.00874696\n",
      "Iteration 9512, loss = 0.00874178\n",
      "Iteration 9513, loss = 0.00873973\n",
      "Iteration 9514, loss = 0.00873628\n",
      "Iteration 9515, loss = 0.00873168\n",
      "Iteration 9516, loss = 0.00873578\n",
      "Iteration 9517, loss = 0.00872705\n",
      "Iteration 9518, loss = 0.00872741\n",
      "Iteration 9519, loss = 0.00872832\n",
      "Iteration 9520, loss = 0.00872720\n",
      "Iteration 9521, loss = 0.00872456\n",
      "Iteration 9522, loss = 0.00872089\n",
      "Iteration 9523, loss = 0.00871624\n",
      "Iteration 9524, loss = 0.00871033\n",
      "Iteration 9525, loss = 0.00870321\n",
      "Iteration 9526, loss = 0.00870596\n",
      "Iteration 9527, loss = 0.00870312\n",
      "Iteration 9528, loss = 0.00869419\n",
      "Iteration 9529, loss = 0.00869351\n",
      "Iteration 9530, loss = 0.00869055\n",
      "Iteration 9531, loss = 0.00868667\n",
      "Iteration 9532, loss = 0.00868233\n",
      "Iteration 9533, loss = 0.00868029\n",
      "Iteration 9534, loss = 0.00867491\n",
      "Iteration 9535, loss = 0.00867184\n",
      "Iteration 9536, loss = 0.00866802\n",
      "Iteration 9537, loss = 0.00867152\n",
      "Iteration 9538, loss = 0.00866219\n",
      "Iteration 9539, loss = 0.00865950\n",
      "Iteration 9540, loss = 0.00865593\n",
      "Iteration 9541, loss = 0.00865589\n",
      "Iteration 9542, loss = 0.00865477\n",
      "Iteration 9543, loss = 0.00865227\n",
      "Iteration 9544, loss = 0.00864840\n",
      "Iteration 9545, loss = 0.00864337\n",
      "Iteration 9546, loss = 0.00863793\n",
      "Iteration 9547, loss = 0.00863591\n",
      "Iteration 9548, loss = 0.00863300\n",
      "Iteration 9549, loss = 0.00862857\n",
      "Iteration 9550, loss = 0.00863344\n",
      "Iteration 9551, loss = 0.00862420\n",
      "Iteration 9552, loss = 0.00862474\n",
      "Iteration 9553, loss = 0.00862572\n",
      "Iteration 9554, loss = 0.00862506\n",
      "Iteration 9555, loss = 0.00862282\n",
      "Iteration 9556, loss = 0.00861930\n",
      "Iteration 9557, loss = 0.00861472\n",
      "Iteration 9558, loss = 0.00860896\n",
      "Iteration 9559, loss = 0.00860220\n",
      "Iteration 9560, loss = 0.00859832\n",
      "Iteration 9561, loss = 0.00859519\n",
      "Iteration 9562, loss = 0.00859336\n",
      "Iteration 9563, loss = 0.00859283\n",
      "Iteration 9564, loss = 0.00859023\n",
      "Iteration 9565, loss = 0.00858653\n",
      "Iteration 9566, loss = 0.00858213\n",
      "Iteration 9567, loss = 0.00857678\n",
      "Iteration 9568, loss = 0.00857937\n",
      "Iteration 9569, loss = 0.00857284\n",
      "Iteration 9570, loss = 0.00856973\n",
      "Iteration 9571, loss = 0.00857056\n",
      "Iteration 9572, loss = 0.00856870\n",
      "Iteration 9573, loss = 0.00856486\n",
      "Iteration 9574, loss = 0.00856067\n",
      "Iteration 9575, loss = 0.00855601\n",
      "Iteration 9576, loss = 0.00854997\n",
      "Iteration 9577, loss = 0.00855537\n",
      "Iteration 9578, loss = 0.00855025\n",
      "Iteration 9579, loss = 0.00854161\n",
      "Iteration 9580, loss = 0.00854212\n",
      "Iteration 9581, loss = 0.00853973\n",
      "Iteration 9582, loss = 0.00853544\n",
      "Iteration 9583, loss = 0.00853099\n",
      "Iteration 9584, loss = 0.00852636\n",
      "Iteration 9585, loss = 0.00853041\n",
      "Iteration 9586, loss = 0.00852288\n",
      "Iteration 9587, loss = 0.00851900\n",
      "Iteration 9588, loss = 0.00852035\n",
      "Iteration 9589, loss = 0.00851911\n",
      "Iteration 9590, loss = 0.00851521\n",
      "Iteration 9591, loss = 0.00851044\n",
      "Iteration 9592, loss = 0.00850578\n",
      "Iteration 9593, loss = 0.00850027\n",
      "Iteration 9594, loss = 0.00850401\n",
      "Iteration 9595, loss = 0.00849834\n",
      "Iteration 9596, loss = 0.00849140\n",
      "Iteration 9597, loss = 0.00849237\n",
      "Iteration 9598, loss = 0.00849030\n",
      "Iteration 9599, loss = 0.00848586\n",
      "Iteration 9600, loss = 0.00848109\n",
      "Iteration 9601, loss = 0.00847641\n",
      "Iteration 9602, loss = 0.00847850\n",
      "Iteration 9603, loss = 0.00847125\n",
      "Iteration 9604, loss = 0.00846907\n",
      "Iteration 9605, loss = 0.00847029\n",
      "Iteration 9606, loss = 0.00846932\n",
      "Iteration 9607, loss = 0.00846571\n",
      "Iteration 9608, loss = 0.00846086\n",
      "Iteration 9609, loss = 0.00845591\n",
      "Iteration 9610, loss = 0.00845046\n",
      "Iteration 9611, loss = 0.00845226\n",
      "Iteration 9612, loss = 0.00844681\n",
      "Iteration 9613, loss = 0.00844173\n",
      "Iteration 9614, loss = 0.00844259\n",
      "Iteration 9615, loss = 0.00844069\n",
      "Iteration 9616, loss = 0.00843642\n",
      "Iteration 9617, loss = 0.00843158\n",
      "Iteration 9618, loss = 0.00842675\n",
      "Iteration 9619, loss = 0.00842684\n",
      "Iteration 9620, loss = 0.00841978\n",
      "Iteration 9621, loss = 0.00841960\n",
      "Iteration 9622, loss = 0.00842066\n",
      "Iteration 9623, loss = 0.00841976\n",
      "Iteration 9624, loss = 0.00841628\n",
      "Iteration 9625, loss = 0.00841148\n",
      "Iteration 9626, loss = 0.00840659\n",
      "Iteration 9627, loss = 0.00840107\n",
      "Iteration 9628, loss = 0.00840078\n",
      "Iteration 9629, loss = 0.00839536\n",
      "Iteration 9630, loss = 0.00839242\n",
      "Iteration 9631, loss = 0.00839324\n",
      "Iteration 9632, loss = 0.00839136\n",
      "Iteration 9633, loss = 0.00838715\n",
      "Iteration 9634, loss = 0.00838233\n",
      "Iteration 9635, loss = 0.00837752\n",
      "Iteration 9636, loss = 0.00837547\n",
      "Iteration 9637, loss = 0.00836893\n",
      "Iteration 9638, loss = 0.00836574\n",
      "Iteration 9639, loss = 0.00836677\n",
      "Iteration 9640, loss = 0.00836672\n",
      "Iteration 9641, loss = 0.00836393\n",
      "Iteration 9642, loss = 0.00835936\n",
      "Iteration 9643, loss = 0.00835451\n",
      "Iteration 9644, loss = 0.00834955\n",
      "Iteration 9645, loss = 0.00835681\n",
      "Iteration 9646, loss = 0.00835053\n",
      "Iteration 9647, loss = 0.00834157\n",
      "Iteration 9648, loss = 0.00834258\n",
      "Iteration 9649, loss = 0.00834122\n",
      "Iteration 9650, loss = 0.00833744\n",
      "Iteration 9651, loss = 0.00833269\n",
      "Iteration 9652, loss = 0.00832783\n",
      "Iteration 9653, loss = 0.00832561\n",
      "Iteration 9654, loss = 0.00831980\n",
      "Iteration 9655, loss = 0.00831624\n",
      "Iteration 9656, loss = 0.00831703\n",
      "Iteration 9657, loss = 0.00831277\n",
      "Iteration 9658, loss = 0.00831114\n",
      "Iteration 9659, loss = 0.00830737\n",
      "Iteration 9660, loss = 0.00830279\n",
      "Iteration 9661, loss = 0.00830375\n",
      "Iteration 9662, loss = 0.00829684\n",
      "Iteration 9663, loss = 0.00829414\n",
      "Iteration 9664, loss = 0.00829031\n",
      "Iteration 9665, loss = 0.00829229\n",
      "Iteration 9666, loss = 0.00828550\n",
      "Iteration 9667, loss = 0.00828358\n",
      "Iteration 9668, loss = 0.00827981\n",
      "Iteration 9669, loss = 0.00827745\n",
      "Iteration 9670, loss = 0.00827473\n",
      "Iteration 9671, loss = 0.00827295\n",
      "Iteration 9672, loss = 0.00826991\n",
      "Iteration 9673, loss = 0.00826563\n",
      "Iteration 9674, loss = 0.00826464\n",
      "Iteration 9675, loss = 0.00825930\n",
      "Iteration 9676, loss = 0.00825702\n",
      "Iteration 9677, loss = 0.00825319\n",
      "Iteration 9678, loss = 0.00825501\n",
      "Iteration 9679, loss = 0.00824750\n",
      "Iteration 9680, loss = 0.00824541\n",
      "Iteration 9681, loss = 0.00824213\n",
      "Iteration 9682, loss = 0.00824090\n",
      "Iteration 9683, loss = 0.00823728\n",
      "Iteration 9684, loss = 0.00823556\n",
      "Iteration 9685, loss = 0.00823248\n",
      "Iteration 9686, loss = 0.00822812\n",
      "Iteration 9687, loss = 0.00822810\n",
      "Iteration 9688, loss = 0.00822159\n",
      "Iteration 9689, loss = 0.00821909\n",
      "Iteration 9690, loss = 0.00821543\n",
      "Iteration 9691, loss = 0.00821899\n",
      "Iteration 9692, loss = 0.00821000\n",
      "Iteration 9693, loss = 0.00820795\n",
      "Iteration 9694, loss = 0.00820459\n",
      "Iteration 9695, loss = 0.00820445\n",
      "Iteration 9696, loss = 0.00819974\n",
      "Iteration 9697, loss = 0.00819794\n",
      "Iteration 9698, loss = 0.00819482\n",
      "Iteration 9699, loss = 0.00819054\n",
      "Iteration 9700, loss = 0.00819221\n",
      "Iteration 9701, loss = 0.00818415\n",
      "Iteration 9702, loss = 0.00818177\n",
      "Iteration 9703, loss = 0.00817902\n",
      "Iteration 9704, loss = 0.00817848\n",
      "Iteration 9705, loss = 0.00817732\n",
      "Iteration 9706, loss = 0.00817469\n",
      "Iteration 9707, loss = 0.00817089\n",
      "Iteration 9708, loss = 0.00816611\n",
      "Iteration 9709, loss = 0.00816497\n",
      "Iteration 9710, loss = 0.00815869\n",
      "Iteration 9711, loss = 0.00815587\n",
      "Iteration 9712, loss = 0.00815589\n",
      "Iteration 9713, loss = 0.00815211\n",
      "Iteration 9714, loss = 0.00815050\n",
      "Iteration 9715, loss = 0.00814746\n",
      "Iteration 9716, loss = 0.00814342\n",
      "Iteration 9717, loss = 0.00813991\n",
      "Iteration 9718, loss = 0.00813753\n",
      "Iteration 9719, loss = 0.00813516\n",
      "Iteration 9720, loss = 0.00813170\n",
      "Iteration 9721, loss = 0.00812774\n",
      "Iteration 9722, loss = 0.00812677\n",
      "Iteration 9723, loss = 0.00812476\n",
      "Iteration 9724, loss = 0.00812136\n",
      "Iteration 9725, loss = 0.00811701\n",
      "Iteration 9726, loss = 0.00811861\n",
      "Iteration 9727, loss = 0.00811043\n",
      "Iteration 9728, loss = 0.00810788\n",
      "Iteration 9729, loss = 0.00810603\n",
      "Iteration 9730, loss = 0.00810476\n",
      "Iteration 9731, loss = 0.00810358\n",
      "Iteration 9732, loss = 0.00810075\n",
      "Iteration 9733, loss = 0.00809683\n",
      "Iteration 9734, loss = 0.00809217\n",
      "Iteration 9735, loss = 0.00809356\n",
      "Iteration 9736, loss = 0.00808615\n",
      "Iteration 9737, loss = 0.00808696\n",
      "Iteration 9738, loss = 0.00808794\n",
      "Iteration 9739, loss = 0.00808698\n",
      "Iteration 9740, loss = 0.00808407\n",
      "Iteration 9741, loss = 0.00808018\n",
      "Iteration 9742, loss = 0.00807575\n",
      "Iteration 9743, loss = 0.00807025\n",
      "Iteration 9744, loss = 0.00806345\n",
      "Iteration 9745, loss = 0.00807250\n",
      "Iteration 9746, loss = 0.00806981\n",
      "Iteration 9747, loss = 0.00805498\n",
      "Iteration 9748, loss = 0.00805455\n",
      "Iteration 9749, loss = 0.00805165\n",
      "Iteration 9750, loss = 0.00804785\n",
      "Iteration 9751, loss = 0.00804385\n",
      "Iteration 9752, loss = 0.00804808\n",
      "Iteration 9753, loss = 0.00803924\n",
      "Iteration 9754, loss = 0.00803933\n",
      "Iteration 9755, loss = 0.00804083\n",
      "Iteration 9756, loss = 0.00804042\n",
      "Iteration 9757, loss = 0.00803767\n",
      "Iteration 9758, loss = 0.00803368\n",
      "Iteration 9759, loss = 0.00802934\n",
      "Iteration 9760, loss = 0.00802425\n",
      "Iteration 9761, loss = 0.00801779\n",
      "Iteration 9762, loss = 0.00801965\n",
      "Iteration 9763, loss = 0.00801622\n",
      "Iteration 9764, loss = 0.00800941\n",
      "Iteration 9765, loss = 0.00800945\n",
      "Iteration 9766, loss = 0.00800664\n",
      "Iteration 9767, loss = 0.00800250\n",
      "Iteration 9768, loss = 0.00799843\n",
      "Iteration 9769, loss = 0.00799413\n",
      "Iteration 9770, loss = 0.00799208\n",
      "Iteration 9771, loss = 0.00798901\n",
      "Iteration 9772, loss = 0.00798543\n",
      "Iteration 9773, loss = 0.00798413\n",
      "Iteration 9774, loss = 0.00798077\n",
      "Iteration 9775, loss = 0.00797837\n",
      "Iteration 9776, loss = 0.00797451\n",
      "Iteration 9777, loss = 0.00797163\n",
      "Iteration 9778, loss = 0.00796985\n",
      "Iteration 9779, loss = 0.00796799\n",
      "Iteration 9780, loss = 0.00796481\n",
      "Iteration 9781, loss = 0.00796066\n",
      "Iteration 9782, loss = 0.00795919\n",
      "Iteration 9783, loss = 0.00795477\n",
      "Iteration 9784, loss = 0.00795239\n",
      "Iteration 9785, loss = 0.00794853\n",
      "Iteration 9786, loss = 0.00795030\n",
      "Iteration 9787, loss = 0.00794316\n",
      "Iteration 9788, loss = 0.00794122\n",
      "Iteration 9789, loss = 0.00793803\n",
      "Iteration 9790, loss = 0.00793596\n",
      "Iteration 9791, loss = 0.00793351\n",
      "Iteration 9792, loss = 0.00793192\n",
      "Iteration 9793, loss = 0.00792889\n",
      "Iteration 9794, loss = 0.00792464\n",
      "Iteration 9795, loss = 0.00792362\n",
      "Iteration 9796, loss = 0.00791854\n",
      "Iteration 9797, loss = 0.00791612\n",
      "Iteration 9798, loss = 0.00791253\n",
      "Iteration 9799, loss = 0.00791395\n",
      "Iteration 9800, loss = 0.00790752\n",
      "Iteration 9801, loss = 0.00790564\n",
      "Iteration 9802, loss = 0.00790232\n",
      "Iteration 9803, loss = 0.00789950\n",
      "Iteration 9804, loss = 0.00789769\n",
      "Iteration 9805, loss = 0.00789601\n",
      "Iteration 9806, loss = 0.00789308\n",
      "Iteration 9807, loss = 0.00788901\n",
      "Iteration 9808, loss = 0.00788694\n",
      "Iteration 9809, loss = 0.00788292\n",
      "Iteration 9810, loss = 0.00788065\n",
      "Iteration 9811, loss = 0.00787708\n",
      "Iteration 9812, loss = 0.00787717\n",
      "Iteration 9813, loss = 0.00787192\n",
      "Iteration 9814, loss = 0.00786994\n",
      "Iteration 9815, loss = 0.00786674\n",
      "Iteration 9816, loss = 0.00786291\n",
      "Iteration 9817, loss = 0.00786226\n",
      "Iteration 9818, loss = 0.00786063\n",
      "Iteration 9819, loss = 0.00785772\n",
      "Iteration 9820, loss = 0.00785362\n",
      "Iteration 9821, loss = 0.00785005\n",
      "Iteration 9822, loss = 0.00784752\n",
      "Iteration 9823, loss = 0.00784519\n",
      "Iteration 9824, loss = 0.00784174\n",
      "Iteration 9825, loss = 0.00784056\n",
      "Iteration 9826, loss = 0.00783660\n",
      "Iteration 9827, loss = 0.00783469\n",
      "Iteration 9828, loss = 0.00783154\n",
      "Iteration 9829, loss = 0.00782728\n",
      "Iteration 9830, loss = 0.00783193\n",
      "Iteration 9831, loss = 0.00782369\n",
      "Iteration 9832, loss = 0.00782355\n",
      "Iteration 9833, loss = 0.00782458\n",
      "Iteration 9834, loss = 0.00782378\n",
      "Iteration 9835, loss = 0.00782156\n",
      "Iteration 9836, loss = 0.00781833\n",
      "Iteration 9837, loss = 0.00781409\n",
      "Iteration 9838, loss = 0.00780879\n",
      "Iteration 9839, loss = 0.00780246\n",
      "Iteration 9840, loss = 0.00780253\n",
      "Iteration 9841, loss = 0.00779954\n",
      "Iteration 9842, loss = 0.00779484\n",
      "Iteration 9843, loss = 0.00779446\n",
      "Iteration 9844, loss = 0.00779194\n",
      "Iteration 9845, loss = 0.00778855\n",
      "Iteration 9846, loss = 0.00778477\n",
      "Iteration 9847, loss = 0.00777998\n",
      "Iteration 9848, loss = 0.00778358\n",
      "Iteration 9849, loss = 0.00777684\n",
      "Iteration 9850, loss = 0.00777394\n",
      "Iteration 9851, loss = 0.00777513\n",
      "Iteration 9852, loss = 0.00777339\n",
      "Iteration 9853, loss = 0.00776979\n",
      "Iteration 9854, loss = 0.00776605\n",
      "Iteration 9855, loss = 0.00776206\n",
      "Iteration 9856, loss = 0.00775660\n",
      "Iteration 9857, loss = 0.00775823\n",
      "Iteration 9858, loss = 0.00775284\n",
      "Iteration 9859, loss = 0.00774958\n",
      "Iteration 9860, loss = 0.00775040\n",
      "Iteration 9861, loss = 0.00774789\n",
      "Iteration 9862, loss = 0.00774384\n",
      "Iteration 9863, loss = 0.00774016\n",
      "Iteration 9864, loss = 0.00773620\n",
      "Iteration 9865, loss = 0.00773247\n",
      "Iteration 9866, loss = 0.00772749\n",
      "Iteration 9867, loss = 0.00772464\n",
      "Iteration 9868, loss = 0.00772408\n",
      "Iteration 9869, loss = 0.00772207\n",
      "Iteration 9870, loss = 0.00771983\n",
      "Iteration 9871, loss = 0.00771607\n",
      "Iteration 9872, loss = 0.00771230\n",
      "Iteration 9873, loss = 0.00771170\n",
      "Iteration 9874, loss = 0.00770680\n",
      "Iteration 9875, loss = 0.00770407\n",
      "Iteration 9876, loss = 0.00770080\n",
      "Iteration 9877, loss = 0.00769907\n",
      "Iteration 9878, loss = 0.00769682\n",
      "Iteration 9879, loss = 0.00769471\n",
      "Iteration 9880, loss = 0.00769108\n",
      "Iteration 9881, loss = 0.00768691\n",
      "Iteration 9882, loss = 0.00769073\n",
      "Iteration 9883, loss = 0.00768229\n",
      "Iteration 9884, loss = 0.00768314\n",
      "Iteration 9885, loss = 0.00768445\n",
      "Iteration 9886, loss = 0.00768408\n",
      "Iteration 9887, loss = 0.00768180\n",
      "Iteration 9888, loss = 0.00767817\n",
      "Iteration 9889, loss = 0.00767396\n",
      "Iteration 9890, loss = 0.00766902\n",
      "Iteration 9891, loss = 0.00766294\n",
      "Iteration 9892, loss = 0.00766103\n",
      "Iteration 9893, loss = 0.00765757\n",
      "Iteration 9894, loss = 0.00765513\n",
      "Iteration 9895, loss = 0.00765523\n",
      "Iteration 9896, loss = 0.00765267\n",
      "Iteration 9897, loss = 0.00764880\n",
      "Iteration 9898, loss = 0.00764502\n",
      "Iteration 9899, loss = 0.00764061\n",
      "Iteration 9900, loss = 0.00764272\n",
      "Iteration 9901, loss = 0.00763579\n",
      "Iteration 9902, loss = 0.00763420\n",
      "Iteration 9903, loss = 0.00763559\n",
      "Iteration 9904, loss = 0.00763423\n",
      "Iteration 9905, loss = 0.00763059\n",
      "Iteration 9906, loss = 0.00762654\n",
      "Iteration 9907, loss = 0.00762262\n",
      "Iteration 9908, loss = 0.00761750\n",
      "Iteration 9909, loss = 0.00761789\n",
      "Iteration 9910, loss = 0.00761223\n",
      "Iteration 9911, loss = 0.00761020\n",
      "Iteration 9912, loss = 0.00761131\n",
      "Iteration 9913, loss = 0.00760911\n",
      "Iteration 9914, loss = 0.00760500\n",
      "Iteration 9915, loss = 0.00760107\n",
      "Iteration 9916, loss = 0.00759726\n",
      "Iteration 9917, loss = 0.00759230\n",
      "Iteration 9918, loss = 0.00758886\n",
      "Iteration 9919, loss = 0.00758579\n",
      "Iteration 9920, loss = 0.00758353\n",
      "Iteration 9921, loss = 0.00758348\n",
      "Iteration 9922, loss = 0.00758145\n",
      "Iteration 9923, loss = 0.00757757\n",
      "Iteration 9924, loss = 0.00757378\n",
      "Iteration 9925, loss = 0.00757143\n",
      "Iteration 9926, loss = 0.00756854\n",
      "Iteration 9927, loss = 0.00756578\n",
      "Iteration 9928, loss = 0.00756246\n",
      "Iteration 9929, loss = 0.00755877\n",
      "Iteration 9930, loss = 0.00755865\n",
      "Iteration 9931, loss = 0.00755669\n",
      "Iteration 9932, loss = 0.00755307\n",
      "Iteration 9933, loss = 0.00754889\n",
      "Iteration 9934, loss = 0.00755053\n",
      "Iteration 9935, loss = 0.00754304\n",
      "Iteration 9936, loss = 0.00754046\n",
      "Iteration 9937, loss = 0.00753755\n",
      "Iteration 9938, loss = 0.00753793\n",
      "Iteration 9939, loss = 0.00753717\n",
      "Iteration 9940, loss = 0.00753442\n",
      "Iteration 9941, loss = 0.00753052\n",
      "Iteration 9942, loss = 0.00752618\n",
      "Iteration 9943, loss = 0.00752527\n",
      "Iteration 9944, loss = 0.00751934\n",
      "Iteration 9945, loss = 0.00751652\n",
      "Iteration 9946, loss = 0.00751552\n",
      "Iteration 9947, loss = 0.00751362\n",
      "Iteration 9948, loss = 0.00751236\n",
      "Iteration 9949, loss = 0.00750936\n",
      "Iteration 9950, loss = 0.00750547\n",
      "Iteration 9951, loss = 0.00750106\n",
      "Iteration 9952, loss = 0.00750740\n",
      "Iteration 9953, loss = 0.00750038\n",
      "Iteration 9954, loss = 0.00749593\n",
      "Iteration 9955, loss = 0.00749706\n",
      "Iteration 9956, loss = 0.00749622\n",
      "Iteration 9957, loss = 0.00749342\n",
      "Iteration 9958, loss = 0.00748963\n",
      "Iteration 9959, loss = 0.00748548\n",
      "Iteration 9960, loss = 0.00748049\n",
      "Iteration 9961, loss = 0.00747701\n",
      "Iteration 9962, loss = 0.00747157\n",
      "Iteration 9963, loss = 0.00747063\n",
      "Iteration 9964, loss = 0.00747019\n",
      "Iteration 9965, loss = 0.00746931\n",
      "Iteration 9966, loss = 0.00746623\n",
      "Iteration 9967, loss = 0.00746243\n",
      "Iteration 9968, loss = 0.00745860\n",
      "Iteration 9969, loss = 0.00745686\n",
      "Iteration 9970, loss = 0.00745172\n",
      "Iteration 9971, loss = 0.00744888\n",
      "Iteration 9972, loss = 0.00744564\n",
      "Iteration 9973, loss = 0.00745004\n",
      "Iteration 9974, loss = 0.00744092\n",
      "Iteration 9975, loss = 0.00744352\n",
      "Iteration 9976, loss = 0.00744423\n",
      "Iteration 9977, loss = 0.00744375\n",
      "Iteration 9978, loss = 0.00744242\n",
      "Iteration 9979, loss = 0.00743979\n",
      "Iteration 9980, loss = 0.00743574\n",
      "Iteration 9981, loss = 0.00743056\n",
      "Iteration 9982, loss = 0.00742478\n",
      "Iteration 9983, loss = 0.00741845\n",
      "Iteration 9984, loss = 0.00742492\n",
      "Iteration 9985, loss = 0.00742382\n",
      "Iteration 9986, loss = 0.00740978\n",
      "Iteration 9987, loss = 0.00741327\n",
      "Iteration 9988, loss = 0.00741540\n",
      "Iteration 9989, loss = 0.00741603\n",
      "Iteration 9990, loss = 0.00741551\n",
      "Iteration 9991, loss = 0.00741376\n",
      "Iteration 9992, loss = 0.00741050\n",
      "Iteration 9993, loss = 0.00740605\n",
      "Iteration 9994, loss = 0.00740083\n",
      "Iteration 9995, loss = 0.00739491\n",
      "Iteration 9996, loss = 0.00738816\n",
      "Iteration 9997, loss = 0.00738069\n",
      "Iteration 9998, loss = 0.00739635\n",
      "Iteration 9999, loss = 0.00739779\n",
      "Iteration 10000, loss = 0.00738606\n",
      "Iteration 10001, loss = 0.00737258\n",
      "Iteration 10002, loss = 0.00737443\n",
      "Iteration 10003, loss = 0.00737451\n",
      "Iteration 10004, loss = 0.00737329\n",
      "Iteration 10005, loss = 0.00737105\n",
      "Iteration 10006, loss = 0.00736769\n",
      "Iteration 10007, loss = 0.00736310\n",
      "Iteration 10008, loss = 0.00735755\n",
      "Iteration 10009, loss = 0.00735140\n",
      "Iteration 10010, loss = 0.00735524\n",
      "Iteration 10011, loss = 0.00735318\n",
      "Iteration 10012, loss = 0.00734342\n",
      "Iteration 10013, loss = 0.00734274\n",
      "Iteration 10014, loss = 0.00734053\n",
      "Iteration 10015, loss = 0.00733758\n",
      "Iteration 10016, loss = 0.00733391\n",
      "Iteration 10017, loss = 0.00733146\n",
      "Iteration 10018, loss = 0.00732754\n",
      "Iteration 10019, loss = 0.00732539\n",
      "Iteration 10020, loss = 0.00732235\n",
      "Iteration 10021, loss = 0.00732224\n",
      "Iteration 10022, loss = 0.00731736\n",
      "Iteration 10023, loss = 0.00731527\n",
      "Iteration 10024, loss = 0.00731223\n",
      "Iteration 10025, loss = 0.00730959\n",
      "Iteration 10026, loss = 0.00730806\n",
      "Iteration 10027, loss = 0.00730653\n",
      "Iteration 10028, loss = 0.00730388\n",
      "Iteration 10029, loss = 0.00730013\n",
      "Iteration 10030, loss = 0.00729624\n",
      "Iteration 10031, loss = 0.00729459\n",
      "Iteration 10032, loss = 0.00729245\n",
      "Iteration 10033, loss = 0.00728912\n",
      "Iteration 10034, loss = 0.00728698\n",
      "Iteration 10035, loss = 0.00728452\n",
      "Iteration 10036, loss = 0.00728281\n",
      "Iteration 10037, loss = 0.00727994\n",
      "Iteration 10038, loss = 0.00727603\n",
      "Iteration 10039, loss = 0.00727729\n",
      "Iteration 10040, loss = 0.00727025\n",
      "Iteration 10041, loss = 0.00726806\n",
      "Iteration 10042, loss = 0.00726545\n",
      "Iteration 10043, loss = 0.00726521\n",
      "Iteration 10044, loss = 0.00726425\n",
      "Iteration 10045, loss = 0.00726201\n",
      "Iteration 10046, loss = 0.00725870\n",
      "Iteration 10047, loss = 0.00725443\n",
      "Iteration 10048, loss = 0.00725165\n",
      "Iteration 10049, loss = 0.00724788\n",
      "Iteration 10050, loss = 0.00724547\n",
      "Iteration 10051, loss = 0.00724234\n",
      "Iteration 10052, loss = 0.00724240\n",
      "Iteration 10053, loss = 0.00724107\n",
      "Iteration 10054, loss = 0.00723847\n",
      "Iteration 10055, loss = 0.00723504\n",
      "Iteration 10056, loss = 0.00723075\n",
      "Iteration 10057, loss = 0.00723287\n",
      "Iteration 10058, loss = 0.00722585\n",
      "Iteration 10059, loss = 0.00722639\n",
      "Iteration 10060, loss = 0.00722747\n",
      "Iteration 10061, loss = 0.00722633\n",
      "Iteration 10062, loss = 0.00722361\n",
      "Iteration 10063, loss = 0.00722040\n",
      "Iteration 10064, loss = 0.00721657\n",
      "Iteration 10065, loss = 0.00721151\n",
      "Iteration 10066, loss = 0.00720520\n",
      "Iteration 10067, loss = 0.00721220\n",
      "Iteration 10068, loss = 0.00720966\n",
      "Iteration 10069, loss = 0.00719832\n",
      "Iteration 10070, loss = 0.00719786\n",
      "Iteration 10071, loss = 0.00719520\n",
      "Iteration 10072, loss = 0.00719200\n",
      "Iteration 10073, loss = 0.00718871\n",
      "Iteration 10074, loss = 0.00718744\n",
      "Iteration 10075, loss = 0.00718257\n",
      "Iteration 10076, loss = 0.00718015\n",
      "Iteration 10077, loss = 0.00717736\n",
      "Iteration 10078, loss = 0.00717707\n",
      "Iteration 10079, loss = 0.00717292\n",
      "Iteration 10080, loss = 0.00717065\n",
      "Iteration 10081, loss = 0.00716753\n",
      "Iteration 10082, loss = 0.00716439\n",
      "Iteration 10083, loss = 0.00716363\n",
      "Iteration 10084, loss = 0.00716199\n",
      "Iteration 10085, loss = 0.00715935\n",
      "Iteration 10086, loss = 0.00715578\n",
      "Iteration 10087, loss = 0.00715112\n",
      "Iteration 10088, loss = 0.00715852\n",
      "Iteration 10089, loss = 0.00715272\n",
      "Iteration 10090, loss = 0.00714609\n",
      "Iteration 10091, loss = 0.00714654\n",
      "Iteration 10092, loss = 0.00714553\n",
      "Iteration 10093, loss = 0.00714338\n",
      "Iteration 10094, loss = 0.00714020\n",
      "Iteration 10095, loss = 0.00713599\n",
      "Iteration 10096, loss = 0.00713076\n",
      "Iteration 10097, loss = 0.00712975\n",
      "Iteration 10098, loss = 0.00712506\n",
      "Iteration 10099, loss = 0.00712533\n",
      "Iteration 10100, loss = 0.00712557\n",
      "Iteration 10101, loss = 0.00712375\n",
      "Iteration 10102, loss = 0.00712102\n",
      "Iteration 10103, loss = 0.00711788\n",
      "Iteration 10104, loss = 0.00711372\n",
      "Iteration 10105, loss = 0.00710815\n",
      "Iteration 10106, loss = 0.00710992\n",
      "Iteration 10107, loss = 0.00710551\n",
      "Iteration 10108, loss = 0.00710244\n",
      "Iteration 10109, loss = 0.00710269\n",
      "Iteration 10110, loss = 0.00710029\n",
      "Iteration 10111, loss = 0.00709704\n",
      "Iteration 10112, loss = 0.00709410\n",
      "Iteration 10113, loss = 0.00709020\n",
      "Iteration 10114, loss = 0.00708473\n",
      "Iteration 10115, loss = 0.00708212\n",
      "Iteration 10116, loss = 0.00707998\n",
      "Iteration 10117, loss = 0.00707695\n",
      "Iteration 10118, loss = 0.00708153\n",
      "Iteration 10119, loss = 0.00707307\n",
      "Iteration 10120, loss = 0.00707310\n",
      "Iteration 10121, loss = 0.00707403\n",
      "Iteration 10122, loss = 0.00707387\n",
      "Iteration 10123, loss = 0.00707244\n",
      "Iteration 10124, loss = 0.00706960\n",
      "Iteration 10125, loss = 0.00706560\n",
      "Iteration 10126, loss = 0.00706085\n",
      "Iteration 10127, loss = 0.00705539\n",
      "Iteration 10128, loss = 0.00704909\n",
      "Iteration 10129, loss = 0.00706045\n",
      "Iteration 10130, loss = 0.00705947\n",
      "Iteration 10131, loss = 0.00704586\n",
      "Iteration 10132, loss = 0.00704409\n",
      "Iteration 10133, loss = 0.00704637\n",
      "Iteration 10134, loss = 0.00704693\n",
      "Iteration 10135, loss = 0.00704630\n",
      "Iteration 10136, loss = 0.00704458\n",
      "Iteration 10137, loss = 0.00704159\n",
      "Iteration 10138, loss = 0.00703745\n",
      "Iteration 10139, loss = 0.00703241\n",
      "Iteration 10140, loss = 0.00702669\n",
      "Iteration 10141, loss = 0.00702027\n",
      "Iteration 10142, loss = 0.00701681\n",
      "Iteration 10143, loss = 0.00701628\n",
      "Iteration 10144, loss = 0.00701097\n",
      "Iteration 10145, loss = 0.00701009\n",
      "Iteration 10146, loss = 0.00700790\n",
      "Iteration 10147, loss = 0.00700484\n",
      "Iteration 10148, loss = 0.00700094\n",
      "Iteration 10149, loss = 0.00700040\n",
      "Iteration 10150, loss = 0.00699466\n",
      "Iteration 10151, loss = 0.00699252\n",
      "Iteration 10152, loss = 0.00699001\n",
      "Iteration 10153, loss = 0.00698991\n",
      "Iteration 10154, loss = 0.00698866\n",
      "Iteration 10155, loss = 0.00698613\n",
      "Iteration 10156, loss = 0.00698290\n",
      "Iteration 10157, loss = 0.00697888\n",
      "Iteration 10158, loss = 0.00697928\n",
      "Iteration 10159, loss = 0.00697210\n",
      "Iteration 10160, loss = 0.00696986\n",
      "Iteration 10161, loss = 0.00697018\n",
      "Iteration 10162, loss = 0.00696713\n",
      "Iteration 10163, loss = 0.00696568\n",
      "Iteration 10164, loss = 0.00696298\n",
      "Iteration 10165, loss = 0.00695970\n",
      "Iteration 10166, loss = 0.00695572\n",
      "Iteration 10167, loss = 0.00696141\n",
      "Iteration 10168, loss = 0.00695419\n",
      "Iteration 10169, loss = 0.00695138\n",
      "Iteration 10170, loss = 0.00695271\n",
      "Iteration 10171, loss = 0.00695162\n",
      "Iteration 10172, loss = 0.00694885\n",
      "Iteration 10173, loss = 0.00694571\n",
      "Iteration 10174, loss = 0.00694215\n",
      "Iteration 10175, loss = 0.00693733\n",
      "Iteration 10176, loss = 0.00693111\n",
      "Iteration 10177, loss = 0.00694024\n",
      "Iteration 10178, loss = 0.00693764\n",
      "Iteration 10179, loss = 0.00692473\n",
      "Iteration 10180, loss = 0.00692433\n",
      "Iteration 10181, loss = 0.00692163\n",
      "Iteration 10182, loss = 0.00691857\n",
      "Iteration 10183, loss = 0.00691560\n",
      "Iteration 10184, loss = 0.00691541\n",
      "Iteration 10185, loss = 0.00690964\n",
      "Iteration 10186, loss = 0.00690736\n",
      "Iteration 10187, loss = 0.00690478\n",
      "Iteration 10188, loss = 0.00690465\n",
      "Iteration 10189, loss = 0.00690057\n",
      "Iteration 10190, loss = 0.00689834\n",
      "Iteration 10191, loss = 0.00689539\n",
      "Iteration 10192, loss = 0.00689203\n",
      "Iteration 10193, loss = 0.00689720\n",
      "Iteration 10194, loss = 0.00688858\n",
      "Iteration 10195, loss = 0.00688883\n",
      "Iteration 10196, loss = 0.00689053\n",
      "Iteration 10197, loss = 0.00689008\n",
      "Iteration 10198, loss = 0.00688767\n",
      "Iteration 10199, loss = 0.00688454\n",
      "Iteration 10200, loss = 0.00688112\n",
      "Iteration 10201, loss = 0.00687668\n",
      "Iteration 10202, loss = 0.00687079\n",
      "Iteration 10203, loss = 0.00686707\n",
      "Iteration 10204, loss = 0.00686376\n",
      "Iteration 10205, loss = 0.00686473\n",
      "Iteration 10206, loss = 0.00686464\n",
      "Iteration 10207, loss = 0.00686191\n",
      "Iteration 10208, loss = 0.00685872\n",
      "Iteration 10209, loss = 0.00685580\n",
      "Iteration 10210, loss = 0.00685184\n",
      "Iteration 10211, loss = 0.00684782\n",
      "Iteration 10212, loss = 0.00684363\n",
      "Iteration 10213, loss = 0.00684162\n",
      "Iteration 10214, loss = 0.00684111\n",
      "Iteration 10215, loss = 0.00683887\n",
      "Iteration 10216, loss = 0.00683660\n",
      "Iteration 10217, loss = 0.00683359\n",
      "Iteration 10218, loss = 0.00683067\n",
      "Iteration 10219, loss = 0.00682929\n",
      "Iteration 10220, loss = 0.00682539\n",
      "Iteration 10221, loss = 0.00682318\n",
      "Iteration 10222, loss = 0.00682059\n",
      "Iteration 10223, loss = 0.00681696\n",
      "Iteration 10224, loss = 0.00682256\n",
      "Iteration 10225, loss = 0.00681538\n",
      "Iteration 10226, loss = 0.00681283\n",
      "Iteration 10227, loss = 0.00681362\n",
      "Iteration 10228, loss = 0.00681316\n",
      "Iteration 10229, loss = 0.00681152\n",
      "Iteration 10230, loss = 0.00680868\n",
      "Iteration 10231, loss = 0.00680476\n",
      "Iteration 10232, loss = 0.00680001\n",
      "Iteration 10233, loss = 0.00679453\n",
      "Iteration 10234, loss = 0.00679549\n",
      "Iteration 10235, loss = 0.00679272\n",
      "Iteration 10236, loss = 0.00678769\n",
      "Iteration 10237, loss = 0.00678740\n",
      "Iteration 10238, loss = 0.00678566\n",
      "Iteration 10239, loss = 0.00678306\n",
      "Iteration 10240, loss = 0.00677974\n",
      "Iteration 10241, loss = 0.00677532\n",
      "Iteration 10242, loss = 0.00677517\n",
      "Iteration 10243, loss = 0.00676900\n",
      "Iteration 10244, loss = 0.00677107\n",
      "Iteration 10245, loss = 0.00677198\n",
      "Iteration 10246, loss = 0.00677045\n",
      "Iteration 10247, loss = 0.00676774\n",
      "Iteration 10248, loss = 0.00676486\n",
      "Iteration 10249, loss = 0.00676129\n",
      "Iteration 10250, loss = 0.00675621\n",
      "Iteration 10251, loss = 0.00674995\n",
      "Iteration 10252, loss = 0.00675878\n",
      "Iteration 10253, loss = 0.00675663\n",
      "Iteration 10254, loss = 0.00674375\n",
      "Iteration 10255, loss = 0.00674311\n",
      "Iteration 10256, loss = 0.00674051\n",
      "Iteration 10257, loss = 0.00673774\n",
      "Iteration 10258, loss = 0.00673489\n",
      "Iteration 10259, loss = 0.00673493\n",
      "Iteration 10260, loss = 0.00672884\n",
      "Iteration 10261, loss = 0.00672680\n",
      "Iteration 10262, loss = 0.00672437\n",
      "Iteration 10263, loss = 0.00672456\n",
      "Iteration 10264, loss = 0.00671998\n",
      "Iteration 10265, loss = 0.00671784\n",
      "Iteration 10266, loss = 0.00671505\n",
      "Iteration 10267, loss = 0.00671198\n",
      "Iteration 10268, loss = 0.00671151\n",
      "Iteration 10269, loss = 0.00670993\n",
      "Iteration 10270, loss = 0.00670757\n",
      "Iteration 10271, loss = 0.00670426\n",
      "Iteration 10272, loss = 0.00669985\n",
      "Iteration 10273, loss = 0.00670537\n",
      "Iteration 10274, loss = 0.00669973\n",
      "Iteration 10275, loss = 0.00669518\n",
      "Iteration 10276, loss = 0.00669573\n",
      "Iteration 10277, loss = 0.00669487\n",
      "Iteration 10278, loss = 0.00669287\n",
      "Iteration 10279, loss = 0.00668987\n",
      "Iteration 10280, loss = 0.00668592\n",
      "Iteration 10281, loss = 0.00668104\n",
      "Iteration 10282, loss = 0.00667682\n",
      "Iteration 10283, loss = 0.00667371\n",
      "Iteration 10284, loss = 0.00667146\n",
      "Iteration 10285, loss = 0.00667209\n",
      "Iteration 10286, loss = 0.00667119\n",
      "Iteration 10287, loss = 0.00666894\n",
      "Iteration 10288, loss = 0.00666609\n",
      "Iteration 10289, loss = 0.00666249\n",
      "Iteration 10290, loss = 0.00665776\n",
      "Iteration 10291, loss = 0.00666368\n",
      "Iteration 10292, loss = 0.00665842\n",
      "Iteration 10293, loss = 0.00665294\n",
      "Iteration 10294, loss = 0.00665362\n",
      "Iteration 10295, loss = 0.00665184\n",
      "Iteration 10296, loss = 0.00664902\n",
      "Iteration 10297, loss = 0.00664614\n",
      "Iteration 10298, loss = 0.00664257\n",
      "Iteration 10299, loss = 0.00663744\n",
      "Iteration 10300, loss = 0.00664157\n",
      "Iteration 10301, loss = 0.00663682\n",
      "Iteration 10302, loss = 0.00663230\n",
      "Iteration 10303, loss = 0.00663286\n",
      "Iteration 10304, loss = 0.00663051\n",
      "Iteration 10305, loss = 0.00662740\n",
      "Iteration 10306, loss = 0.00662469\n",
      "Iteration 10307, loss = 0.00662120\n",
      "Iteration 10308, loss = 0.00661578\n",
      "Iteration 10309, loss = 0.00662335\n",
      "Iteration 10310, loss = 0.00661890\n",
      "Iteration 10311, loss = 0.00661058\n",
      "Iteration 10312, loss = 0.00661108\n",
      "Iteration 10313, loss = 0.00660843\n",
      "Iteration 10314, loss = 0.00660516\n",
      "Iteration 10315, loss = 0.00660261\n",
      "Iteration 10316, loss = 0.00659924\n",
      "Iteration 10317, loss = 0.00659879\n",
      "Iteration 10318, loss = 0.00659123\n",
      "Iteration 10319, loss = 0.00658932\n",
      "Iteration 10320, loss = 0.00659043\n",
      "Iteration 10321, loss = 0.00658727\n",
      "Iteration 10322, loss = 0.00658510\n",
      "Iteration 10323, loss = 0.00658211\n",
      "Iteration 10324, loss = 0.00657939\n",
      "Iteration 10325, loss = 0.00657747\n",
      "Iteration 10326, loss = 0.00657453\n",
      "Iteration 10327, loss = 0.00657235\n",
      "Iteration 10328, loss = 0.00656992\n",
      "Iteration 10329, loss = 0.00656652\n",
      "Iteration 10330, loss = 0.00656976\n",
      "Iteration 10331, loss = 0.00656260\n",
      "Iteration 10332, loss = 0.00656275\n",
      "Iteration 10333, loss = 0.00656351\n",
      "Iteration 10334, loss = 0.00656313\n",
      "Iteration 10335, loss = 0.00656166\n",
      "Iteration 10336, loss = 0.00655895\n",
      "Iteration 10337, loss = 0.00655514\n",
      "Iteration 10338, loss = 0.00655055\n",
      "Iteration 10339, loss = 0.00654529\n",
      "Iteration 10340, loss = 0.00654213\n",
      "Iteration 10341, loss = 0.00653934\n",
      "Iteration 10342, loss = 0.00653874\n",
      "Iteration 10343, loss = 0.00653847\n",
      "Iteration 10344, loss = 0.00653677\n",
      "Iteration 10345, loss = 0.00653433\n",
      "Iteration 10346, loss = 0.00653115\n",
      "Iteration 10347, loss = 0.00652683\n",
      "Iteration 10348, loss = 0.00652185\n",
      "Iteration 10349, loss = 0.00652005\n",
      "Iteration 10350, loss = 0.00651798\n",
      "Iteration 10351, loss = 0.00651581\n",
      "Iteration 10352, loss = 0.00651463\n",
      "Iteration 10353, loss = 0.00651303\n",
      "Iteration 10354, loss = 0.00651062\n",
      "Iteration 10355, loss = 0.00650760\n",
      "Iteration 10356, loss = 0.00650359\n",
      "Iteration 10357, loss = 0.00650956\n",
      "Iteration 10358, loss = 0.00650287\n",
      "Iteration 10359, loss = 0.00649974\n",
      "Iteration 10360, loss = 0.00650097\n",
      "Iteration 10361, loss = 0.00649967\n",
      "Iteration 10362, loss = 0.00649698\n",
      "Iteration 10363, loss = 0.00649420\n",
      "Iteration 10364, loss = 0.00649092\n",
      "Iteration 10365, loss = 0.00648621\n",
      "Iteration 10366, loss = 0.00648034\n",
      "Iteration 10367, loss = 0.00647837\n",
      "Iteration 10368, loss = 0.00647640\n",
      "Iteration 10369, loss = 0.00647726\n",
      "Iteration 10370, loss = 0.00647246\n",
      "Iteration 10371, loss = 0.00647035\n",
      "Iteration 10372, loss = 0.00646785\n",
      "Iteration 10373, loss = 0.00646497\n",
      "Iteration 10374, loss = 0.00646794\n",
      "Iteration 10375, loss = 0.00645944\n",
      "Iteration 10376, loss = 0.00645763\n",
      "Iteration 10377, loss = 0.00645521\n",
      "Iteration 10378, loss = 0.00645727\n",
      "Iteration 10379, loss = 0.00645110\n",
      "Iteration 10380, loss = 0.00644925\n",
      "Iteration 10381, loss = 0.00644664\n",
      "Iteration 10382, loss = 0.00644410\n",
      "Iteration 10383, loss = 0.00644330\n",
      "Iteration 10384, loss = 0.00644196\n",
      "Iteration 10385, loss = 0.00643973\n",
      "Iteration 10386, loss = 0.00643653\n",
      "Iteration 10387, loss = 0.00643230\n",
      "Iteration 10388, loss = 0.00643695\n",
      "Iteration 10389, loss = 0.00643130\n",
      "Iteration 10390, loss = 0.00642803\n",
      "Iteration 10391, loss = 0.00642861\n",
      "Iteration 10392, loss = 0.00642779\n",
      "Iteration 10393, loss = 0.00642588\n",
      "Iteration 10394, loss = 0.00642304\n",
      "Iteration 10395, loss = 0.00641929\n",
      "Iteration 10396, loss = 0.00641456\n",
      "Iteration 10397, loss = 0.00640909\n",
      "Iteration 10398, loss = 0.00641884\n",
      "Iteration 10399, loss = 0.00641645\n",
      "Iteration 10400, loss = 0.00640259\n",
      "Iteration 10401, loss = 0.00640230\n",
      "Iteration 10402, loss = 0.00640042\n",
      "Iteration 10403, loss = 0.00639781\n",
      "Iteration 10404, loss = 0.00639468\n",
      "Iteration 10405, loss = 0.00639406\n",
      "Iteration 10406, loss = 0.00638937\n",
      "Iteration 10407, loss = 0.00638755\n",
      "Iteration 10408, loss = 0.00638497\n",
      "Iteration 10409, loss = 0.00638436\n",
      "Iteration 10410, loss = 0.00638080\n",
      "Iteration 10411, loss = 0.00637909\n",
      "Iteration 10412, loss = 0.00637655\n",
      "Iteration 10413, loss = 0.00637325\n",
      "Iteration 10414, loss = 0.00637656\n",
      "Iteration 10415, loss = 0.00636844\n",
      "Iteration 10416, loss = 0.00637074\n",
      "Iteration 10417, loss = 0.00637236\n",
      "Iteration 10418, loss = 0.00637186\n",
      "Iteration 10419, loss = 0.00636969\n",
      "Iteration 10420, loss = 0.00636703\n",
      "Iteration 10421, loss = 0.00636390\n",
      "Iteration 10422, loss = 0.00635961\n",
      "Iteration 10423, loss = 0.00635405\n",
      "Iteration 10424, loss = 0.00634792\n",
      "Iteration 10425, loss = 0.00635892\n",
      "Iteration 10426, loss = 0.00635813\n",
      "Iteration 10427, loss = 0.00634524\n",
      "Iteration 10428, loss = 0.00634477\n",
      "Iteration 10429, loss = 0.00634633\n",
      "Iteration 10430, loss = 0.00634691\n",
      "Iteration 10431, loss = 0.00634696\n",
      "Iteration 10432, loss = 0.00634570\n",
      "Iteration 10433, loss = 0.00634261\n",
      "Iteration 10434, loss = 0.00633858\n",
      "Iteration 10435, loss = 0.00633423\n",
      "Iteration 10436, loss = 0.00632932\n",
      "Iteration 10437, loss = 0.00632334\n",
      "Iteration 10438, loss = 0.00631650\n",
      "Iteration 10439, loss = 0.00632979\n",
      "Iteration 10440, loss = 0.00633101\n",
      "Iteration 10441, loss = 0.00631992\n",
      "Iteration 10442, loss = 0.00630989\n",
      "Iteration 10443, loss = 0.00631194\n",
      "Iteration 10444, loss = 0.00631227\n",
      "Iteration 10445, loss = 0.00631121\n",
      "Iteration 10446, loss = 0.00630921\n",
      "Iteration 10447, loss = 0.00630640\n",
      "Iteration 10448, loss = 0.00630254\n",
      "Iteration 10449, loss = 0.00629769\n",
      "Iteration 10450, loss = 0.00629218\n",
      "Iteration 10451, loss = 0.00628936\n",
      "Iteration 10452, loss = 0.00628731\n",
      "Iteration 10453, loss = 0.00628592\n",
      "Iteration 10454, loss = 0.00628536\n",
      "Iteration 10455, loss = 0.00628331\n",
      "Iteration 10456, loss = 0.00628083\n",
      "Iteration 10457, loss = 0.00627791\n",
      "Iteration 10458, loss = 0.00627372\n",
      "Iteration 10459, loss = 0.00627318\n",
      "Iteration 10460, loss = 0.00626713\n",
      "Iteration 10461, loss = 0.00626975\n",
      "Iteration 10462, loss = 0.00627089\n",
      "Iteration 10463, loss = 0.00626914\n",
      "Iteration 10464, loss = 0.00626625\n",
      "Iteration 10465, loss = 0.00626372\n",
      "Iteration 10466, loss = 0.00626063\n",
      "Iteration 10467, loss = 0.00625579\n",
      "Iteration 10468, loss = 0.00624967\n",
      "Iteration 10469, loss = 0.00625793\n",
      "Iteration 10470, loss = 0.00625594\n",
      "Iteration 10471, loss = 0.00624443\n",
      "Iteration 10472, loss = 0.00624363\n",
      "Iteration 10473, loss = 0.00624100\n",
      "Iteration 10474, loss = 0.00623862\n",
      "Iteration 10475, loss = 0.00623619\n",
      "Iteration 10476, loss = 0.00623478\n",
      "Iteration 10477, loss = 0.00623027\n",
      "Iteration 10478, loss = 0.00622848\n",
      "Iteration 10479, loss = 0.00622642\n",
      "Iteration 10480, loss = 0.00622406\n",
      "Iteration 10481, loss = 0.00622233\n",
      "Iteration 10482, loss = 0.00622028\n",
      "Iteration 10483, loss = 0.00621776\n",
      "Iteration 10484, loss = 0.00621482\n",
      "Iteration 10485, loss = 0.00621660\n",
      "Iteration 10486, loss = 0.00620942\n",
      "Iteration 10487, loss = 0.00620764\n",
      "Iteration 10488, loss = 0.00620523\n",
      "Iteration 10489, loss = 0.00620695\n",
      "Iteration 10490, loss = 0.00620117\n",
      "Iteration 10491, loss = 0.00619941\n",
      "Iteration 10492, loss = 0.00619691\n",
      "Iteration 10493, loss = 0.00619440\n",
      "Iteration 10494, loss = 0.00619358\n",
      "Iteration 10495, loss = 0.00619233\n",
      "Iteration 10496, loss = 0.00619020\n",
      "Iteration 10497, loss = 0.00618711\n",
      "Iteration 10498, loss = 0.00618302\n",
      "Iteration 10499, loss = 0.00618759\n",
      "Iteration 10500, loss = 0.00618204\n",
      "Iteration 10501, loss = 0.00617893\n",
      "Iteration 10502, loss = 0.00617957\n",
      "Iteration 10503, loss = 0.00617882\n",
      "Iteration 10504, loss = 0.00617701\n",
      "Iteration 10505, loss = 0.00617427\n",
      "Iteration 10506, loss = 0.00617061\n",
      "Iteration 10507, loss = 0.00616607\n",
      "Iteration 10508, loss = 0.00616079\n",
      "Iteration 10509, loss = 0.00616955\n",
      "Iteration 10510, loss = 0.00616716\n",
      "Iteration 10511, loss = 0.00615453\n",
      "Iteration 10512, loss = 0.00615429\n",
      "Iteration 10513, loss = 0.00615255\n",
      "Iteration 10514, loss = 0.00615007\n",
      "Iteration 10515, loss = 0.00614706\n",
      "Iteration 10516, loss = 0.00614491\n",
      "Iteration 10517, loss = 0.00614193\n",
      "Iteration 10518, loss = 0.00614025\n",
      "Iteration 10519, loss = 0.00613778\n",
      "Iteration 10520, loss = 0.00613524\n",
      "Iteration 10521, loss = 0.00613381\n",
      "Iteration 10522, loss = 0.00613216\n",
      "Iteration 10523, loss = 0.00612968\n",
      "Iteration 10524, loss = 0.00612652\n",
      "Iteration 10525, loss = 0.00612743\n",
      "Iteration 10526, loss = 0.00612144\n",
      "Iteration 10527, loss = 0.00611973\n",
      "Iteration 10528, loss = 0.00611715\n",
      "Iteration 10529, loss = 0.00611839\n",
      "Iteration 10530, loss = 0.00611318\n",
      "Iteration 10531, loss = 0.00611162\n",
      "Iteration 10532, loss = 0.00610912\n",
      "Iteration 10533, loss = 0.00610588\n",
      "Iteration 10534, loss = 0.00611062\n",
      "Iteration 10535, loss = 0.00610283\n",
      "Iteration 10536, loss = 0.00610366\n",
      "Iteration 10537, loss = 0.00610512\n",
      "Iteration 10538, loss = 0.00610464\n",
      "Iteration 10539, loss = 0.00610272\n",
      "Iteration 10540, loss = 0.00610021\n",
      "Iteration 10541, loss = 0.00609714\n",
      "Iteration 10542, loss = 0.00609295\n",
      "Iteration 10543, loss = 0.00608765\n",
      "Iteration 10544, loss = 0.00608183\n",
      "Iteration 10545, loss = 0.00609308\n",
      "Iteration 10546, loss = 0.00609224\n",
      "Iteration 10547, loss = 0.00607959\n",
      "Iteration 10548, loss = 0.00607880\n",
      "Iteration 10549, loss = 0.00608044\n",
      "Iteration 10550, loss = 0.00608108\n",
      "Iteration 10551, loss = 0.00608115\n",
      "Iteration 10552, loss = 0.00607991\n",
      "Iteration 10553, loss = 0.00607701\n",
      "Iteration 10554, loss = 0.00607318\n",
      "Iteration 10555, loss = 0.00606895\n",
      "Iteration 10556, loss = 0.00606417\n",
      "Iteration 10557, loss = 0.00605837\n",
      "Iteration 10558, loss = 0.00605179\n",
      "Iteration 10559, loss = 0.00606432\n",
      "Iteration 10560, loss = 0.00606550\n",
      "Iteration 10561, loss = 0.00605463\n",
      "Iteration 10562, loss = 0.00604559\n",
      "Iteration 10563, loss = 0.00604759\n",
      "Iteration 10564, loss = 0.00604789\n",
      "Iteration 10565, loss = 0.00604695\n",
      "Iteration 10566, loss = 0.00604515\n",
      "Iteration 10567, loss = 0.00604245\n",
      "Iteration 10568, loss = 0.00603870\n",
      "Iteration 10569, loss = 0.00603401\n",
      "Iteration 10570, loss = 0.00602873\n",
      "Iteration 10571, loss = 0.00602458\n",
      "Iteration 10572, loss = 0.00602258\n",
      "Iteration 10573, loss = 0.00602273\n",
      "Iteration 10574, loss = 0.00602218\n",
      "Iteration 10575, loss = 0.00602028\n",
      "Iteration 10576, loss = 0.00601796\n",
      "Iteration 10577, loss = 0.00601514\n",
      "Iteration 10578, loss = 0.00601107\n",
      "Iteration 10579, loss = 0.00600831\n",
      "Iteration 10580, loss = 0.00600441\n",
      "Iteration 10581, loss = 0.00600271\n",
      "Iteration 10582, loss = 0.00600286\n",
      "Iteration 10583, loss = 0.00599946\n",
      "Iteration 10584, loss = 0.00599788\n",
      "Iteration 10585, loss = 0.00599572\n",
      "Iteration 10586, loss = 0.00599303\n",
      "Iteration 10587, loss = 0.00599094\n",
      "Iteration 10588, loss = 0.00598822\n",
      "Iteration 10589, loss = 0.00598677\n",
      "Iteration 10590, loss = 0.00598454\n",
      "Iteration 10591, loss = 0.00598099\n",
      "Iteration 10592, loss = 0.00598474\n",
      "Iteration 10593, loss = 0.00597814\n",
      "Iteration 10594, loss = 0.00597772\n",
      "Iteration 10595, loss = 0.00597865\n",
      "Iteration 10596, loss = 0.00597829\n",
      "Iteration 10597, loss = 0.00597679\n",
      "Iteration 10598, loss = 0.00597434\n",
      "Iteration 10599, loss = 0.00597096\n",
      "Iteration 10600, loss = 0.00596678\n",
      "Iteration 10601, loss = 0.00596185\n",
      "Iteration 10602, loss = 0.00595877\n",
      "Iteration 10603, loss = 0.00595593\n",
      "Iteration 10604, loss = 0.00595601\n",
      "Iteration 10605, loss = 0.00595596\n",
      "Iteration 10606, loss = 0.00595437\n",
      "Iteration 10607, loss = 0.00595200\n",
      "Iteration 10608, loss = 0.00594905\n",
      "Iteration 10609, loss = 0.00594517\n",
      "Iteration 10610, loss = 0.00594027\n",
      "Iteration 10611, loss = 0.00594787\n",
      "Iteration 10612, loss = 0.00594437\n",
      "Iteration 10613, loss = 0.00593541\n",
      "Iteration 10614, loss = 0.00593551\n",
      "Iteration 10615, loss = 0.00593362\n",
      "Iteration 10616, loss = 0.00593118\n",
      "Iteration 10617, loss = 0.00592867\n",
      "Iteration 10618, loss = 0.00592513\n",
      "Iteration 10619, loss = 0.00592494\n",
      "Iteration 10620, loss = 0.00591850\n",
      "Iteration 10621, loss = 0.00591692\n",
      "Iteration 10622, loss = 0.00591771\n",
      "Iteration 10623, loss = 0.00591443\n",
      "Iteration 10624, loss = 0.00591278\n",
      "Iteration 10625, loss = 0.00591053\n",
      "Iteration 10626, loss = 0.00590804\n",
      "Iteration 10627, loss = 0.00590453\n",
      "Iteration 10628, loss = 0.00590988\n",
      "Iteration 10629, loss = 0.00590302\n",
      "Iteration 10630, loss = 0.00590128\n",
      "Iteration 10631, loss = 0.00590272\n",
      "Iteration 10632, loss = 0.00590133\n",
      "Iteration 10633, loss = 0.00589871\n",
      "Iteration 10634, loss = 0.00589635\n",
      "Iteration 10635, loss = 0.00589359\n",
      "Iteration 10636, loss = 0.00588919\n",
      "Iteration 10637, loss = 0.00588347\n",
      "Iteration 10638, loss = 0.00588913\n",
      "Iteration 10639, loss = 0.00588680\n",
      "Iteration 10640, loss = 0.00587879\n",
      "Iteration 10641, loss = 0.00587816\n",
      "Iteration 10642, loss = 0.00587565\n",
      "Iteration 10643, loss = 0.00587338\n",
      "Iteration 10644, loss = 0.00587117\n",
      "Iteration 10645, loss = 0.00586738\n",
      "Iteration 10646, loss = 0.00587047\n",
      "Iteration 10647, loss = 0.00586393\n",
      "Iteration 10648, loss = 0.00586378\n",
      "Iteration 10649, loss = 0.00586523\n",
      "Iteration 10650, loss = 0.00586326\n",
      "Iteration 10651, loss = 0.00586019\n",
      "Iteration 10652, loss = 0.00585799\n",
      "Iteration 10653, loss = 0.00585545\n",
      "Iteration 10654, loss = 0.00585083\n",
      "Iteration 10655, loss = 0.00584473\n",
      "Iteration 10656, loss = 0.00585306\n",
      "Iteration 10657, loss = 0.00585117\n",
      "Iteration 10658, loss = 0.00584045\n",
      "Iteration 10659, loss = 0.00583952\n",
      "Iteration 10660, loss = 0.00583679\n",
      "Iteration 10661, loss = 0.00583466\n",
      "Iteration 10662, loss = 0.00583264\n",
      "Iteration 10663, loss = 0.00583017\n",
      "Iteration 10664, loss = 0.00582682\n",
      "Iteration 10665, loss = 0.00582518\n",
      "Iteration 10666, loss = 0.00582346\n",
      "Iteration 10667, loss = 0.00581999\n",
      "Iteration 10668, loss = 0.00582448\n",
      "Iteration 10669, loss = 0.00581826\n",
      "Iteration 10670, loss = 0.00581576\n",
      "Iteration 10671, loss = 0.00581666\n",
      "Iteration 10672, loss = 0.00581631\n",
      "Iteration 10673, loss = 0.00581479\n",
      "Iteration 10674, loss = 0.00581229\n",
      "Iteration 10675, loss = 0.00580896\n",
      "Iteration 10676, loss = 0.00580485\n",
      "Iteration 10677, loss = 0.00579999\n",
      "Iteration 10678, loss = 0.00580018\n",
      "Iteration 10679, loss = 0.00579754\n",
      "Iteration 10680, loss = 0.00579391\n",
      "Iteration 10681, loss = 0.00579388\n",
      "Iteration 10682, loss = 0.00579255\n",
      "Iteration 10683, loss = 0.00579027\n",
      "Iteration 10684, loss = 0.00578725\n",
      "Iteration 10685, loss = 0.00578341\n",
      "Iteration 10686, loss = 0.00578091\n",
      "Iteration 10687, loss = 0.00577741\n",
      "Iteration 10688, loss = 0.00577544\n",
      "Iteration 10689, loss = 0.00577493\n",
      "Iteration 10690, loss = 0.00577265\n",
      "Iteration 10691, loss = 0.00577141\n",
      "Iteration 10692, loss = 0.00576924\n",
      "Iteration 10693, loss = 0.00576646\n",
      "Iteration 10694, loss = 0.00576289\n",
      "Iteration 10695, loss = 0.00576844\n",
      "Iteration 10696, loss = 0.00576214\n",
      "Iteration 10697, loss = 0.00575958\n",
      "Iteration 10698, loss = 0.00576080\n",
      "Iteration 10699, loss = 0.00575975\n",
      "Iteration 10700, loss = 0.00575742\n",
      "Iteration 10701, loss = 0.00575496\n",
      "Iteration 10702, loss = 0.00575203\n",
      "Iteration 10703, loss = 0.00574776\n",
      "Iteration 10704, loss = 0.00574236\n",
      "Iteration 10705, loss = 0.00574916\n",
      "Iteration 10706, loss = 0.00574684\n",
      "Iteration 10707, loss = 0.00573749\n",
      "Iteration 10708, loss = 0.00573702\n",
      "Iteration 10709, loss = 0.00573471\n",
      "Iteration 10710, loss = 0.00573242\n",
      "Iteration 10711, loss = 0.00573010\n",
      "Iteration 10712, loss = 0.00572640\n",
      "Iteration 10713, loss = 0.00573118\n",
      "Iteration 10714, loss = 0.00572486\n",
      "Iteration 10715, loss = 0.00572282\n",
      "Iteration 10716, loss = 0.00572421\n",
      "Iteration 10717, loss = 0.00572246\n",
      "Iteration 10718, loss = 0.00571959\n",
      "Iteration 10719, loss = 0.00571735\n",
      "Iteration 10720, loss = 0.00571475\n",
      "Iteration 10721, loss = 0.00571023\n",
      "Iteration 10722, loss = 0.00570521\n",
      "Iteration 10723, loss = 0.00570298\n",
      "Iteration 10724, loss = 0.00570164\n",
      "Iteration 10725, loss = 0.00570253\n",
      "Iteration 10726, loss = 0.00569779\n",
      "Iteration 10727, loss = 0.00569574\n",
      "Iteration 10728, loss = 0.00569368\n",
      "Iteration 10729, loss = 0.00569131\n",
      "Iteration 10730, loss = 0.00569350\n",
      "Iteration 10731, loss = 0.00568610\n",
      "Iteration 10732, loss = 0.00568461\n",
      "Iteration 10733, loss = 0.00568268\n",
      "Iteration 10734, loss = 0.00568276\n",
      "Iteration 10735, loss = 0.00567889\n",
      "Iteration 10736, loss = 0.00567720\n",
      "Iteration 10737, loss = 0.00567502\n",
      "Iteration 10738, loss = 0.00567228\n",
      "Iteration 10739, loss = 0.00567458\n",
      "Iteration 10740, loss = 0.00566741\n",
      "Iteration 10741, loss = 0.00566591\n",
      "Iteration 10742, loss = 0.00566366\n",
      "Iteration 10743, loss = 0.00566506\n",
      "Iteration 10744, loss = 0.00565991\n",
      "Iteration 10745, loss = 0.00565839\n",
      "Iteration 10746, loss = 0.00565617\n",
      "Iteration 10747, loss = 0.00565327\n",
      "Iteration 10748, loss = 0.00565706\n",
      "Iteration 10749, loss = 0.00564925\n",
      "Iteration 10750, loss = 0.00565131\n",
      "Iteration 10751, loss = 0.00565294\n",
      "Iteration 10752, loss = 0.00565251\n",
      "Iteration 10753, loss = 0.00565057\n",
      "Iteration 10754, loss = 0.00564821\n",
      "Iteration 10755, loss = 0.00564545\n",
      "Iteration 10756, loss = 0.00564157\n",
      "Iteration 10757, loss = 0.00563650\n",
      "Iteration 10758, loss = 0.00563098\n",
      "Iteration 10759, loss = 0.00563898\n",
      "Iteration 10760, loss = 0.00563810\n",
      "Iteration 10761, loss = 0.00562598\n",
      "Iteration 10762, loss = 0.00562855\n",
      "Iteration 10763, loss = 0.00562997\n",
      "Iteration 10764, loss = 0.00563063\n",
      "Iteration 10765, loss = 0.00563087\n",
      "Iteration 10766, loss = 0.00562981\n",
      "Iteration 10767, loss = 0.00562703\n",
      "Iteration 10768, loss = 0.00562336\n",
      "Iteration 10769, loss = 0.00561947\n",
      "Iteration 10770, loss = 0.00561503\n",
      "Iteration 10771, loss = 0.00560958\n",
      "Iteration 10772, loss = 0.00560332\n",
      "Iteration 10773, loss = 0.00561012\n",
      "Iteration 10774, loss = 0.00561118\n",
      "Iteration 10775, loss = 0.00560073\n",
      "Iteration 10776, loss = 0.00559761\n",
      "Iteration 10777, loss = 0.00559962\n",
      "Iteration 10778, loss = 0.00559993\n",
      "Iteration 10779, loss = 0.00559898\n",
      "Iteration 10780, loss = 0.00559730\n",
      "Iteration 10781, loss = 0.00559485\n",
      "Iteration 10782, loss = 0.00559130\n",
      "Iteration 10783, loss = 0.00558686\n",
      "Iteration 10784, loss = 0.00558186\n",
      "Iteration 10785, loss = 0.00557646\n",
      "Iteration 10786, loss = 0.00558394\n",
      "Iteration 10787, loss = 0.00558367\n",
      "Iteration 10788, loss = 0.00557222\n",
      "Iteration 10789, loss = 0.00557244\n",
      "Iteration 10790, loss = 0.00557419\n",
      "Iteration 10791, loss = 0.00557478\n",
      "Iteration 10792, loss = 0.00557457\n",
      "Iteration 10793, loss = 0.00557329\n",
      "Iteration 10794, loss = 0.00557071\n",
      "Iteration 10795, loss = 0.00556720\n",
      "Iteration 10796, loss = 0.00556313\n",
      "Iteration 10797, loss = 0.00555851\n",
      "Iteration 10798, loss = 0.00555312\n",
      "Iteration 10799, loss = 0.00554743\n",
      "Iteration 10800, loss = 0.00554692\n",
      "Iteration 10801, loss = 0.00554544\n",
      "Iteration 10802, loss = 0.00554492\n",
      "Iteration 10803, loss = 0.00554329\n",
      "Iteration 10804, loss = 0.00554087\n",
      "Iteration 10805, loss = 0.00553766\n",
      "Iteration 10806, loss = 0.00553364\n",
      "Iteration 10807, loss = 0.00553908\n",
      "Iteration 10808, loss = 0.00553438\n",
      "Iteration 10809, loss = 0.00552977\n",
      "Iteration 10810, loss = 0.00553039\n",
      "Iteration 10811, loss = 0.00552915\n",
      "Iteration 10812, loss = 0.00552704\n",
      "Iteration 10813, loss = 0.00552464\n",
      "Iteration 10814, loss = 0.00552145\n",
      "Iteration 10815, loss = 0.00551706\n",
      "Iteration 10816, loss = 0.00551740\n",
      "Iteration 10817, loss = 0.00551308\n",
      "Iteration 10818, loss = 0.00551331\n",
      "Iteration 10819, loss = 0.00551376\n",
      "Iteration 10820, loss = 0.00551184\n",
      "Iteration 10821, loss = 0.00550945\n",
      "Iteration 10822, loss = 0.00550734\n",
      "Iteration 10823, loss = 0.00550428\n",
      "Iteration 10824, loss = 0.00549955\n",
      "Iteration 10825, loss = 0.00549889\n",
      "Iteration 10826, loss = 0.00549480\n",
      "Iteration 10827, loss = 0.00549611\n",
      "Iteration 10828, loss = 0.00549641\n",
      "Iteration 10829, loss = 0.00549394\n",
      "Iteration 10830, loss = 0.00549142\n",
      "Iteration 10831, loss = 0.00548963\n",
      "Iteration 10832, loss = 0.00548670\n",
      "Iteration 10833, loss = 0.00548166\n",
      "Iteration 10834, loss = 0.00548149\n",
      "Iteration 10835, loss = 0.00547752\n",
      "Iteration 10836, loss = 0.00547846\n",
      "Iteration 10837, loss = 0.00547863\n",
      "Iteration 10838, loss = 0.00547589\n",
      "Iteration 10839, loss = 0.00547330\n",
      "Iteration 10840, loss = 0.00547168\n",
      "Iteration 10841, loss = 0.00546880\n",
      "Iteration 10842, loss = 0.00546365\n",
      "Iteration 10843, loss = 0.00546435\n",
      "Iteration 10844, loss = 0.00546041\n",
      "Iteration 10845, loss = 0.00546057\n",
      "Iteration 10846, loss = 0.00546073\n",
      "Iteration 10847, loss = 0.00545784\n",
      "Iteration 10848, loss = 0.00545523\n",
      "Iteration 10849, loss = 0.00545372\n",
      "Iteration 10850, loss = 0.00545096\n",
      "Iteration 10851, loss = 0.00544577\n",
      "Iteration 10852, loss = 0.00544718\n",
      "Iteration 10853, loss = 0.00544319\n",
      "Iteration 10854, loss = 0.00544271\n",
      "Iteration 10855, loss = 0.00544295\n",
      "Iteration 10856, loss = 0.00543998\n",
      "Iteration 10857, loss = 0.00543728\n",
      "Iteration 10858, loss = 0.00543580\n",
      "Iteration 10859, loss = 0.00543311\n",
      "Iteration 10860, loss = 0.00542790\n",
      "Iteration 10861, loss = 0.00543011\n",
      "Iteration 10862, loss = 0.00542612\n",
      "Iteration 10863, loss = 0.00542487\n",
      "Iteration 10864, loss = 0.00542512\n",
      "Iteration 10865, loss = 0.00542205\n",
      "Iteration 10866, loss = 0.00541930\n",
      "Iteration 10867, loss = 0.00541788\n",
      "Iteration 10868, loss = 0.00541531\n",
      "Iteration 10869, loss = 0.00541012\n",
      "Iteration 10870, loss = 0.00541307\n",
      "Iteration 10871, loss = 0.00540899\n",
      "Iteration 10872, loss = 0.00540695\n",
      "Iteration 10873, loss = 0.00540732\n",
      "Iteration 10874, loss = 0.00540426\n",
      "Iteration 10875, loss = 0.00540145\n",
      "Iteration 10876, loss = 0.00539997\n",
      "Iteration 10877, loss = 0.00539739\n",
      "Iteration 10878, loss = 0.00539227\n",
      "Iteration 10879, loss = 0.00539608\n",
      "Iteration 10880, loss = 0.00539199\n",
      "Iteration 10881, loss = 0.00538902\n",
      "Iteration 10882, loss = 0.00538949\n",
      "Iteration 10883, loss = 0.00538649\n",
      "Iteration 10884, loss = 0.00538350\n",
      "Iteration 10885, loss = 0.00538193\n",
      "Iteration 10886, loss = 0.00537949\n",
      "Iteration 10887, loss = 0.00537449\n",
      "Iteration 10888, loss = 0.00537944\n",
      "Iteration 10889, loss = 0.00537521\n",
      "Iteration 10890, loss = 0.00537100\n",
      "Iteration 10891, loss = 0.00537161\n",
      "Iteration 10892, loss = 0.00536870\n",
      "Iteration 10893, loss = 0.00536568\n",
      "Iteration 10894, loss = 0.00536403\n",
      "Iteration 10895, loss = 0.00536164\n",
      "Iteration 10896, loss = 0.00535713\n",
      "Iteration 10897, loss = 0.00535417\n",
      "Iteration 10898, loss = 0.00535290\n",
      "Iteration 10899, loss = 0.00535123\n",
      "Iteration 10900, loss = 0.00535249\n",
      "Iteration 10901, loss = 0.00534599\n",
      "Iteration 10902, loss = 0.00534385\n",
      "Iteration 10903, loss = 0.00534187\n",
      "Iteration 10904, loss = 0.00534502\n",
      "Iteration 10905, loss = 0.00533868\n",
      "Iteration 10906, loss = 0.00533714\n",
      "Iteration 10907, loss = 0.00533534\n",
      "Iteration 10908, loss = 0.00533276\n",
      "Iteration 10909, loss = 0.00533340\n",
      "Iteration 10910, loss = 0.00532830\n",
      "Iteration 10911, loss = 0.00532652\n",
      "Iteration 10912, loss = 0.00532416\n",
      "Iteration 10913, loss = 0.00532612\n",
      "Iteration 10914, loss = 0.00532084\n",
      "Iteration 10915, loss = 0.00531963\n",
      "Iteration 10916, loss = 0.00531764\n",
      "Iteration 10917, loss = 0.00531477\n",
      "Iteration 10918, loss = 0.00531649\n",
      "Iteration 10919, loss = 0.00531050\n",
      "Iteration 10920, loss = 0.00530890\n",
      "Iteration 10921, loss = 0.00530647\n",
      "Iteration 10922, loss = 0.00530907\n",
      "Iteration 10923, loss = 0.00530309\n",
      "Iteration 10924, loss = 0.00530197\n",
      "Iteration 10925, loss = 0.00529992\n",
      "Iteration 10926, loss = 0.00529696\n",
      "Iteration 10927, loss = 0.00530039\n",
      "Iteration 10928, loss = 0.00529350\n",
      "Iteration 10929, loss = 0.00529529\n",
      "Iteration 10930, loss = 0.00529638\n",
      "Iteration 10931, loss = 0.00529606\n",
      "Iteration 10932, loss = 0.00529475\n",
      "Iteration 10933, loss = 0.00529269\n",
      "Iteration 10934, loss = 0.00528982\n",
      "Iteration 10935, loss = 0.00528601\n",
      "Iteration 10936, loss = 0.00528145\n",
      "Iteration 10937, loss = 0.00527641\n",
      "Iteration 10938, loss = 0.00528321\n",
      "Iteration 10939, loss = 0.00528219\n",
      "Iteration 10940, loss = 0.00527058\n",
      "Iteration 10941, loss = 0.00527402\n",
      "Iteration 10942, loss = 0.00527572\n",
      "Iteration 10943, loss = 0.00527619\n",
      "Iteration 10944, loss = 0.00527619\n",
      "Iteration 10945, loss = 0.00527526\n",
      "Iteration 10946, loss = 0.00527293\n",
      "Iteration 10947, loss = 0.00526947\n",
      "Iteration 10948, loss = 0.00526557\n",
      "Iteration 10949, loss = 0.00526128\n",
      "Iteration 10950, loss = 0.00525624\n",
      "Iteration 10951, loss = 0.00525040\n",
      "Iteration 10952, loss = 0.00525622\n",
      "Iteration 10953, loss = 0.00525722\n",
      "Iteration 10954, loss = 0.00524734\n",
      "Iteration 10955, loss = 0.00524503\n",
      "Iteration 10956, loss = 0.00524691\n",
      "Iteration 10957, loss = 0.00524723\n",
      "Iteration 10958, loss = 0.00524643\n",
      "Iteration 10959, loss = 0.00524490\n",
      "Iteration 10960, loss = 0.00524255\n",
      "Iteration 10961, loss = 0.00523921\n",
      "Iteration 10962, loss = 0.00523502\n",
      "Iteration 10963, loss = 0.00523033\n",
      "Iteration 10964, loss = 0.00522517\n",
      "Iteration 10965, loss = 0.00523103\n",
      "Iteration 10966, loss = 0.00523076\n",
      "Iteration 10967, loss = 0.00521984\n",
      "Iteration 10968, loss = 0.00522164\n",
      "Iteration 10969, loss = 0.00522334\n",
      "Iteration 10970, loss = 0.00522382\n",
      "Iteration 10971, loss = 0.00522358\n",
      "Iteration 10972, loss = 0.00522242\n",
      "Iteration 10973, loss = 0.00522006\n",
      "Iteration 10974, loss = 0.00521669\n",
      "Iteration 10975, loss = 0.00521280\n",
      "Iteration 10976, loss = 0.00520840\n",
      "Iteration 10977, loss = 0.00520335\n",
      "Iteration 10978, loss = 0.00519761\n",
      "Iteration 10979, loss = 0.00520933\n",
      "Iteration 10980, loss = 0.00521031\n",
      "Iteration 10981, loss = 0.00520048\n",
      "Iteration 10982, loss = 0.00519231\n",
      "Iteration 10983, loss = 0.00519411\n",
      "Iteration 10984, loss = 0.00519445\n",
      "Iteration 10985, loss = 0.00519376\n",
      "Iteration 10986, loss = 0.00519226\n",
      "Iteration 10987, loss = 0.00518991\n",
      "Iteration 10988, loss = 0.00518660\n",
      "Iteration 10989, loss = 0.00518253\n",
      "Iteration 10990, loss = 0.00517793\n",
      "Iteration 10991, loss = 0.00517287\n",
      "Iteration 10992, loss = 0.00518374\n",
      "Iteration 10993, loss = 0.00518336\n",
      "Iteration 10994, loss = 0.00517235\n",
      "Iteration 10995, loss = 0.00516925\n",
      "Iteration 10996, loss = 0.00517105\n",
      "Iteration 10997, loss = 0.00517164\n",
      "Iteration 10998, loss = 0.00517141\n",
      "Iteration 10999, loss = 0.00517026\n",
      "Iteration 11000, loss = 0.00516796\n",
      "Iteration 11001, loss = 0.00516468\n",
      "Iteration 11002, loss = 0.00516083\n",
      "Iteration 11003, loss = 0.00515648\n",
      "Iteration 11004, loss = 0.00515149\n",
      "Iteration 11005, loss = 0.00514799\n",
      "Iteration 11006, loss = 0.00514741\n",
      "Iteration 11007, loss = 0.00514442\n",
      "Iteration 11008, loss = 0.00514400\n",
      "Iteration 11009, loss = 0.00514250\n",
      "Iteration 11010, loss = 0.00514025\n",
      "Iteration 11011, loss = 0.00513731\n",
      "Iteration 11012, loss = 0.00513357\n",
      "Iteration 11013, loss = 0.00513953\n",
      "Iteration 11014, loss = 0.00513496\n",
      "Iteration 11015, loss = 0.00513007\n",
      "Iteration 11016, loss = 0.00513077\n",
      "Iteration 11017, loss = 0.00512961\n",
      "Iteration 11018, loss = 0.00512756\n",
      "Iteration 11019, loss = 0.00512535\n",
      "Iteration 11020, loss = 0.00512246\n",
      "Iteration 11021, loss = 0.00511836\n",
      "Iteration 11022, loss = 0.00511861\n",
      "Iteration 11023, loss = 0.00511437\n",
      "Iteration 11024, loss = 0.00511495\n",
      "Iteration 11025, loss = 0.00511553\n",
      "Iteration 11026, loss = 0.00511366\n",
      "Iteration 11027, loss = 0.00511128\n",
      "Iteration 11028, loss = 0.00510931\n",
      "Iteration 11029, loss = 0.00510657\n",
      "Iteration 11030, loss = 0.00510217\n",
      "Iteration 11031, loss = 0.00510104\n",
      "Iteration 11032, loss = 0.00509695\n",
      "Iteration 11033, loss = 0.00509889\n",
      "Iteration 11034, loss = 0.00509943\n",
      "Iteration 11035, loss = 0.00509711\n",
      "Iteration 11036, loss = 0.00509451\n",
      "Iteration 11037, loss = 0.00509274\n",
      "Iteration 11038, loss = 0.00509019\n",
      "Iteration 11039, loss = 0.00508558\n",
      "Iteration 11040, loss = 0.00508480\n",
      "Iteration 11041, loss = 0.00508078\n",
      "Iteration 11042, loss = 0.00508245\n",
      "Iteration 11043, loss = 0.00508296\n",
      "Iteration 11044, loss = 0.00508031\n",
      "Iteration 11045, loss = 0.00507761\n",
      "Iteration 11046, loss = 0.00507607\n",
      "Iteration 11047, loss = 0.00507362\n",
      "Iteration 11048, loss = 0.00506887\n",
      "Iteration 11049, loss = 0.00506892\n",
      "Iteration 11050, loss = 0.00506491\n",
      "Iteration 11051, loss = 0.00506585\n",
      "Iteration 11052, loss = 0.00506637\n",
      "Iteration 11053, loss = 0.00506351\n",
      "Iteration 11054, loss = 0.00506069\n",
      "Iteration 11055, loss = 0.00505921\n",
      "Iteration 11056, loss = 0.00505693\n",
      "Iteration 11057, loss = 0.00505217\n",
      "Iteration 11058, loss = 0.00505326\n",
      "Iteration 11059, loss = 0.00504916\n",
      "Iteration 11060, loss = 0.00504903\n",
      "Iteration 11061, loss = 0.00504964\n",
      "Iteration 11062, loss = 0.00504677\n",
      "Iteration 11063, loss = 0.00504380\n",
      "Iteration 11064, loss = 0.00504234\n",
      "Iteration 11065, loss = 0.00504012\n",
      "Iteration 11066, loss = 0.00503534\n",
      "Iteration 11067, loss = 0.00503762\n",
      "Iteration 11068, loss = 0.00503351\n",
      "Iteration 11069, loss = 0.00503213\n",
      "Iteration 11070, loss = 0.00503285\n",
      "Iteration 11071, loss = 0.00503000\n",
      "Iteration 11072, loss = 0.00502696\n",
      "Iteration 11073, loss = 0.00502542\n",
      "Iteration 11074, loss = 0.00502329\n",
      "Iteration 11075, loss = 0.00501864\n",
      "Iteration 11076, loss = 0.00502215\n",
      "Iteration 11077, loss = 0.00501793\n",
      "Iteration 11078, loss = 0.00501522\n",
      "Iteration 11079, loss = 0.00501606\n",
      "Iteration 11080, loss = 0.00501326\n",
      "Iteration 11081, loss = 0.00501012\n",
      "Iteration 11082, loss = 0.00500851\n",
      "Iteration 11083, loss = 0.00500642\n",
      "Iteration 11084, loss = 0.00500188\n",
      "Iteration 11085, loss = 0.00500680\n",
      "Iteration 11086, loss = 0.00500254\n",
      "Iteration 11087, loss = 0.00499834\n",
      "Iteration 11088, loss = 0.00499926\n",
      "Iteration 11089, loss = 0.00499654\n",
      "Iteration 11090, loss = 0.00499335\n",
      "Iteration 11091, loss = 0.00499168\n",
      "Iteration 11092, loss = 0.00498965\n",
      "Iteration 11093, loss = 0.00498619\n",
      "Iteration 11094, loss = 0.00498262\n",
      "Iteration 11095, loss = 0.00498120\n",
      "Iteration 11096, loss = 0.00497973\n",
      "Iteration 11097, loss = 0.00498157\n",
      "Iteration 11098, loss = 0.00497511\n",
      "Iteration 11099, loss = 0.00497294\n",
      "Iteration 11100, loss = 0.00497122\n",
      "Iteration 11101, loss = 0.00497182\n",
      "Iteration 11102, loss = 0.00497124\n",
      "Iteration 11103, loss = 0.00496962\n",
      "Iteration 11104, loss = 0.00496741\n",
      "Iteration 11105, loss = 0.00496461\n",
      "Iteration 11106, loss = 0.00496092\n",
      "Iteration 11107, loss = 0.00496300\n",
      "Iteration 11108, loss = 0.00495914\n",
      "Iteration 11109, loss = 0.00495683\n",
      "Iteration 11110, loss = 0.00495722\n",
      "Iteration 11111, loss = 0.00495651\n",
      "Iteration 11112, loss = 0.00495492\n",
      "Iteration 11113, loss = 0.00495254\n",
      "Iteration 11114, loss = 0.00494941\n",
      "Iteration 11115, loss = 0.00494551\n",
      "Iteration 11116, loss = 0.00494364\n",
      "Iteration 11117, loss = 0.00493991\n",
      "Iteration 11118, loss = 0.00494189\n",
      "Iteration 11119, loss = 0.00494229\n",
      "Iteration 11120, loss = 0.00494099\n",
      "Iteration 11121, loss = 0.00493896\n",
      "Iteration 11122, loss = 0.00493675\n",
      "Iteration 11123, loss = 0.00493382\n",
      "Iteration 11124, loss = 0.00492972\n",
      "Iteration 11125, loss = 0.00492764\n",
      "Iteration 11126, loss = 0.00492378\n",
      "Iteration 11127, loss = 0.00492627\n",
      "Iteration 11128, loss = 0.00492683\n",
      "Iteration 11129, loss = 0.00492493\n",
      "Iteration 11130, loss = 0.00492247\n",
      "Iteration 11131, loss = 0.00492050\n",
      "Iteration 11132, loss = 0.00491789\n",
      "Iteration 11133, loss = 0.00491363\n",
      "Iteration 11134, loss = 0.00491286\n",
      "Iteration 11135, loss = 0.00490890\n",
      "Iteration 11136, loss = 0.00491016\n",
      "Iteration 11137, loss = 0.00491087\n",
      "Iteration 11138, loss = 0.00490864\n",
      "Iteration 11139, loss = 0.00490584\n",
      "Iteration 11140, loss = 0.00490396\n",
      "Iteration 11141, loss = 0.00490164\n",
      "Iteration 11142, loss = 0.00489733\n",
      "Iteration 11143, loss = 0.00489867\n",
      "Iteration 11144, loss = 0.00489455\n",
      "Iteration 11145, loss = 0.00489375\n",
      "Iteration 11146, loss = 0.00489466\n",
      "Iteration 11147, loss = 0.00489222\n",
      "Iteration 11148, loss = 0.00488917\n",
      "Iteration 11149, loss = 0.00488732\n",
      "Iteration 11150, loss = 0.00488521\n",
      "Iteration 11151, loss = 0.00488093\n",
      "Iteration 11152, loss = 0.00488449\n",
      "Iteration 11153, loss = 0.00488027\n",
      "Iteration 11154, loss = 0.00487716\n",
      "Iteration 11155, loss = 0.00487820\n",
      "Iteration 11156, loss = 0.00487573\n",
      "Iteration 11157, loss = 0.00487254\n",
      "Iteration 11158, loss = 0.00487068\n",
      "Iteration 11159, loss = 0.00486865\n",
      "Iteration 11160, loss = 0.00486491\n",
      "Iteration 11161, loss = 0.00486194\n",
      "Iteration 11162, loss = 0.00486042\n",
      "Iteration 11163, loss = 0.00485892\n",
      "Iteration 11164, loss = 0.00486065\n",
      "Iteration 11165, loss = 0.00485458\n",
      "Iteration 11166, loss = 0.00485244\n",
      "Iteration 11167, loss = 0.00485057\n",
      "Iteration 11168, loss = 0.00485124\n",
      "Iteration 11169, loss = 0.00485074\n",
      "Iteration 11170, loss = 0.00484917\n",
      "Iteration 11171, loss = 0.00484695\n",
      "Iteration 11172, loss = 0.00484414\n",
      "Iteration 11173, loss = 0.00484055\n",
      "Iteration 11174, loss = 0.00484285\n",
      "Iteration 11175, loss = 0.00483903\n",
      "Iteration 11176, loss = 0.00483663\n",
      "Iteration 11177, loss = 0.00483701\n",
      "Iteration 11178, loss = 0.00483628\n",
      "Iteration 11179, loss = 0.00483473\n",
      "Iteration 11180, loss = 0.00483245\n",
      "Iteration 11181, loss = 0.00482935\n",
      "Iteration 11182, loss = 0.00482551\n",
      "Iteration 11183, loss = 0.00482390\n",
      "Iteration 11184, loss = 0.00482022\n",
      "Iteration 11185, loss = 0.00482212\n",
      "Iteration 11186, loss = 0.00482251\n",
      "Iteration 11187, loss = 0.00482115\n",
      "Iteration 11188, loss = 0.00481917\n",
      "Iteration 11189, loss = 0.00481705\n",
      "Iteration 11190, loss = 0.00481419\n",
      "Iteration 11191, loss = 0.00481014\n",
      "Iteration 11192, loss = 0.00480821\n",
      "Iteration 11193, loss = 0.00480440\n",
      "Iteration 11194, loss = 0.00480682\n",
      "Iteration 11195, loss = 0.00480743\n",
      "Iteration 11196, loss = 0.00480555\n",
      "Iteration 11197, loss = 0.00480305\n",
      "Iteration 11198, loss = 0.00480108\n",
      "Iteration 11199, loss = 0.00479858\n",
      "Iteration 11200, loss = 0.00479443\n",
      "Iteration 11201, loss = 0.00479393\n",
      "Iteration 11202, loss = 0.00478994\n",
      "Iteration 11203, loss = 0.00479100\n",
      "Iteration 11204, loss = 0.00479182\n",
      "Iteration 11205, loss = 0.00478963\n",
      "Iteration 11206, loss = 0.00478677\n",
      "Iteration 11207, loss = 0.00478488\n",
      "Iteration 11208, loss = 0.00478268\n",
      "Iteration 11209, loss = 0.00477851\n",
      "Iteration 11210, loss = 0.00478025\n",
      "Iteration 11211, loss = 0.00477609\n",
      "Iteration 11212, loss = 0.00477483\n",
      "Iteration 11213, loss = 0.00477587\n",
      "Iteration 11214, loss = 0.00477354\n",
      "Iteration 11215, loss = 0.00477043\n",
      "Iteration 11216, loss = 0.00476853\n",
      "Iteration 11217, loss = 0.00476649\n",
      "Iteration 11218, loss = 0.00476239\n",
      "Iteration 11219, loss = 0.00476667\n",
      "Iteration 11220, loss = 0.00476246\n",
      "Iteration 11221, loss = 0.00475855\n",
      "Iteration 11222, loss = 0.00475969\n",
      "Iteration 11223, loss = 0.00475733\n",
      "Iteration 11224, loss = 0.00475414\n",
      "Iteration 11225, loss = 0.00475218\n",
      "Iteration 11226, loss = 0.00475023\n",
      "Iteration 11227, loss = 0.00474768\n",
      "Iteration 11228, loss = 0.00474377\n",
      "Iteration 11229, loss = 0.00474209\n",
      "Iteration 11230, loss = 0.00474057\n",
      "Iteration 11231, loss = 0.00474338\n",
      "Iteration 11232, loss = 0.00473664\n",
      "Iteration 11233, loss = 0.00473448\n",
      "Iteration 11234, loss = 0.00473336\n",
      "Iteration 11235, loss = 0.00473320\n",
      "Iteration 11236, loss = 0.00473276\n",
      "Iteration 11237, loss = 0.00473125\n",
      "Iteration 11238, loss = 0.00472908\n",
      "Iteration 11239, loss = 0.00472629\n",
      "Iteration 11240, loss = 0.00472283\n",
      "Iteration 11241, loss = 0.00472599\n",
      "Iteration 11242, loss = 0.00472224\n",
      "Iteration 11243, loss = 0.00471909\n",
      "Iteration 11244, loss = 0.00471944\n",
      "Iteration 11245, loss = 0.00471872\n",
      "Iteration 11246, loss = 0.00471722\n",
      "Iteration 11247, loss = 0.00471498\n",
      "Iteration 11248, loss = 0.00471196\n",
      "Iteration 11249, loss = 0.00470819\n",
      "Iteration 11250, loss = 0.00470726\n",
      "Iteration 11251, loss = 0.00470363\n",
      "Iteration 11252, loss = 0.00470491\n",
      "Iteration 11253, loss = 0.00470534\n",
      "Iteration 11254, loss = 0.00470401\n",
      "Iteration 11255, loss = 0.00470201\n",
      "Iteration 11256, loss = 0.00469991\n",
      "Iteration 11257, loss = 0.00469716\n",
      "Iteration 11258, loss = 0.00469323\n",
      "Iteration 11259, loss = 0.00469200\n",
      "Iteration 11260, loss = 0.00468820\n",
      "Iteration 11261, loss = 0.00468993\n",
      "Iteration 11262, loss = 0.00469061\n",
      "Iteration 11263, loss = 0.00468877\n",
      "Iteration 11264, loss = 0.00468625\n",
      "Iteration 11265, loss = 0.00468428\n",
      "Iteration 11266, loss = 0.00468190\n",
      "Iteration 11267, loss = 0.00467790\n",
      "Iteration 11268, loss = 0.00467819\n",
      "Iteration 11269, loss = 0.00467419\n",
      "Iteration 11270, loss = 0.00467437\n",
      "Iteration 11271, loss = 0.00467527\n",
      "Iteration 11272, loss = 0.00467317\n",
      "Iteration 11273, loss = 0.00467031\n",
      "Iteration 11274, loss = 0.00466836\n",
      "Iteration 11275, loss = 0.00466625\n",
      "Iteration 11276, loss = 0.00466227\n",
      "Iteration 11277, loss = 0.00466484\n",
      "Iteration 11278, loss = 0.00466070\n",
      "Iteration 11279, loss = 0.00465853\n",
      "Iteration 11280, loss = 0.00465962\n",
      "Iteration 11281, loss = 0.00465742\n",
      "Iteration 11282, loss = 0.00465434\n",
      "Iteration 11283, loss = 0.00465236\n",
      "Iteration 11284, loss = 0.00465038\n",
      "Iteration 11285, loss = 0.00464646\n",
      "Iteration 11286, loss = 0.00465180\n",
      "Iteration 11287, loss = 0.00464755\n",
      "Iteration 11288, loss = 0.00464247\n",
      "Iteration 11289, loss = 0.00464372\n",
      "Iteration 11290, loss = 0.00464157\n",
      "Iteration 11291, loss = 0.00463836\n",
      "Iteration 11292, loss = 0.00463628\n",
      "Iteration 11293, loss = 0.00463438\n",
      "Iteration 11294, loss = 0.00463337\n",
      "Iteration 11295, loss = 0.00462821\n",
      "Iteration 11296, loss = 0.00462641\n",
      "Iteration 11297, loss = 0.00462529\n",
      "Iteration 11298, loss = 0.00462620\n",
      "Iteration 11299, loss = 0.00462471\n",
      "Iteration 11300, loss = 0.00462187\n",
      "Iteration 11301, loss = 0.00461953\n",
      "Iteration 11302, loss = 0.00461736\n",
      "Iteration 11303, loss = 0.00462108\n",
      "Iteration 11304, loss = 0.00461469\n",
      "Iteration 11305, loss = 0.00461374\n",
      "Iteration 11306, loss = 0.00461569\n",
      "Iteration 11307, loss = 0.00461551\n",
      "Iteration 11308, loss = 0.00461299\n",
      "Iteration 11309, loss = 0.00461008\n",
      "Iteration 11310, loss = 0.00460775\n",
      "Iteration 11311, loss = 0.00460491\n",
      "Iteration 11312, loss = 0.00460046\n",
      "Iteration 11313, loss = 0.00460496\n",
      "Iteration 11314, loss = 0.00460225\n",
      "Iteration 11315, loss = 0.00459603\n",
      "Iteration 11316, loss = 0.00459661\n",
      "Iteration 11317, loss = 0.00459433\n",
      "Iteration 11318, loss = 0.00459141\n",
      "Iteration 11319, loss = 0.00458935\n",
      "Iteration 11320, loss = 0.00458705\n",
      "Iteration 11321, loss = 0.00459208\n",
      "Iteration 11322, loss = 0.00458603\n",
      "Iteration 11323, loss = 0.00458288\n",
      "Iteration 11324, loss = 0.00458486\n",
      "Iteration 11325, loss = 0.00458426\n",
      "Iteration 11326, loss = 0.00458140\n",
      "Iteration 11327, loss = 0.00457853\n",
      "Iteration 11328, loss = 0.00457636\n",
      "Iteration 11329, loss = 0.00457343\n",
      "Iteration 11330, loss = 0.00457220\n",
      "Iteration 11331, loss = 0.00456714\n",
      "Iteration 11332, loss = 0.00456884\n",
      "Iteration 11333, loss = 0.00457060\n",
      "Iteration 11334, loss = 0.00456939\n",
      "Iteration 11335, loss = 0.00456621\n",
      "Iteration 11336, loss = 0.00456343\n",
      "Iteration 11337, loss = 0.00456134\n",
      "Iteration 11338, loss = 0.00455815\n",
      "Iteration 11339, loss = 0.00455843\n",
      "Iteration 11340, loss = 0.00455376\n",
      "Iteration 11341, loss = 0.00455338\n",
      "Iteration 11342, loss = 0.00455505\n",
      "Iteration 11343, loss = 0.00455357\n",
      "Iteration 11344, loss = 0.00455022\n",
      "Iteration 11345, loss = 0.00454753\n",
      "Iteration 11346, loss = 0.00454553\n",
      "Iteration 11347, loss = 0.00454228\n",
      "Iteration 11348, loss = 0.00454666\n",
      "Iteration 11349, loss = 0.00454210\n",
      "Iteration 11350, loss = 0.00453742\n",
      "Iteration 11351, loss = 0.00453905\n",
      "Iteration 11352, loss = 0.00453748\n",
      "Iteration 11353, loss = 0.00453414\n",
      "Iteration 11354, loss = 0.00453150\n",
      "Iteration 11355, loss = 0.00452952\n",
      "Iteration 11356, loss = 0.00452918\n",
      "Iteration 11357, loss = 0.00452396\n",
      "Iteration 11358, loss = 0.00452183\n",
      "Iteration 11359, loss = 0.00452139\n",
      "Iteration 11360, loss = 0.00452156\n",
      "Iteration 11361, loss = 0.00452050\n",
      "Iteration 11362, loss = 0.00451767\n",
      "Iteration 11363, loss = 0.00451498\n",
      "Iteration 11364, loss = 0.00451273\n",
      "Iteration 11365, loss = 0.00451812\n",
      "Iteration 11366, loss = 0.00451221\n",
      "Iteration 11367, loss = 0.00450917\n",
      "Iteration 11368, loss = 0.00451084\n",
      "Iteration 11369, loss = 0.00451083\n",
      "Iteration 11370, loss = 0.00450864\n",
      "Iteration 11371, loss = 0.00450571\n",
      "Iteration 11372, loss = 0.00450314\n",
      "Iteration 11373, loss = 0.00450032\n",
      "Iteration 11374, loss = 0.00449621\n",
      "Iteration 11375, loss = 0.00450310\n",
      "Iteration 11376, loss = 0.00450038\n",
      "Iteration 11377, loss = 0.00449143\n",
      "Iteration 11378, loss = 0.00449211\n",
      "Iteration 11379, loss = 0.00449022\n",
      "Iteration 11380, loss = 0.00448733\n",
      "Iteration 11381, loss = 0.00448502\n",
      "Iteration 11382, loss = 0.00448492\n",
      "Iteration 11383, loss = 0.00448172\n",
      "Iteration 11384, loss = 0.00447974\n",
      "Iteration 11385, loss = 0.00447768\n",
      "Iteration 11386, loss = 0.00447610\n",
      "Iteration 11387, loss = 0.00447584\n",
      "Iteration 11388, loss = 0.00447452\n",
      "Iteration 11389, loss = 0.00447202\n",
      "Iteration 11390, loss = 0.00446939\n",
      "Iteration 11391, loss = 0.00447099\n",
      "Iteration 11392, loss = 0.00446599\n",
      "Iteration 11393, loss = 0.00446434\n",
      "Iteration 11394, loss = 0.00446224\n",
      "Iteration 11395, loss = 0.00446330\n",
      "Iteration 11396, loss = 0.00445998\n",
      "Iteration 11397, loss = 0.00445889\n",
      "Iteration 11398, loss = 0.00445666\n",
      "Iteration 11399, loss = 0.00445401\n",
      "Iteration 11400, loss = 0.00445769\n",
      "Iteration 11401, loss = 0.00445172\n",
      "Iteration 11402, loss = 0.00445234\n",
      "Iteration 11403, loss = 0.00445358\n",
      "Iteration 11404, loss = 0.00445368\n",
      "Iteration 11405, loss = 0.00445240\n",
      "Iteration 11406, loss = 0.00445025\n",
      "Iteration 11407, loss = 0.00444769\n",
      "Iteration 11408, loss = 0.00444460\n",
      "Iteration 11409, loss = 0.00444070\n",
      "Iteration 11410, loss = 0.00443613\n",
      "Iteration 11411, loss = 0.00444439\n",
      "Iteration 11412, loss = 0.00444349\n",
      "Iteration 11413, loss = 0.00443356\n",
      "Iteration 11414, loss = 0.00443487\n",
      "Iteration 11415, loss = 0.00443616\n",
      "Iteration 11416, loss = 0.00443612\n",
      "Iteration 11417, loss = 0.00443604\n",
      "Iteration 11418, loss = 0.00443566\n",
      "Iteration 11419, loss = 0.00443385\n",
      "Iteration 11420, loss = 0.00443057\n",
      "Iteration 11421, loss = 0.00442684\n",
      "Iteration 11422, loss = 0.00442319\n",
      "Iteration 11423, loss = 0.00441913\n",
      "Iteration 11424, loss = 0.00441408\n",
      "Iteration 11425, loss = 0.00442165\n",
      "Iteration 11426, loss = 0.00442261\n",
      "Iteration 11427, loss = 0.00441408\n",
      "Iteration 11428, loss = 0.00440915\n",
      "Iteration 11429, loss = 0.00441078\n",
      "Iteration 11430, loss = 0.00441127\n",
      "Iteration 11431, loss = 0.00441075\n",
      "Iteration 11432, loss = 0.00440943\n",
      "Iteration 11433, loss = 0.00440735\n",
      "Iteration 11434, loss = 0.00440452\n",
      "Iteration 11435, loss = 0.00440099\n",
      "Iteration 11436, loss = 0.00439691\n",
      "Iteration 11437, loss = 0.00439239\n",
      "Iteration 11438, loss = 0.00439975\n",
      "Iteration 11439, loss = 0.00439951\n",
      "Iteration 11440, loss = 0.00438992\n",
      "Iteration 11441, loss = 0.00438979\n",
      "Iteration 11442, loss = 0.00439135\n",
      "Iteration 11443, loss = 0.00439170\n",
      "Iteration 11444, loss = 0.00439149\n",
      "Iteration 11445, loss = 0.00439064\n",
      "Iteration 11446, loss = 0.00438874\n",
      "Iteration 11447, loss = 0.00438578\n",
      "Iteration 11448, loss = 0.00438224\n",
      "Iteration 11449, loss = 0.00437842\n",
      "Iteration 11450, loss = 0.00437412\n",
      "Iteration 11451, loss = 0.00436964\n",
      "Iteration 11452, loss = 0.00436925\n",
      "Iteration 11453, loss = 0.00436809\n",
      "Iteration 11454, loss = 0.00436761\n",
      "Iteration 11455, loss = 0.00436625\n",
      "Iteration 11456, loss = 0.00436437\n",
      "Iteration 11457, loss = 0.00436186\n",
      "Iteration 11458, loss = 0.00435855\n",
      "Iteration 11459, loss = 0.00436302\n",
      "Iteration 11460, loss = 0.00435894\n",
      "Iteration 11461, loss = 0.00435571\n",
      "Iteration 11462, loss = 0.00435647\n",
      "Iteration 11463, loss = 0.00435541\n",
      "Iteration 11464, loss = 0.00435348\n",
      "Iteration 11465, loss = 0.00435149\n",
      "Iteration 11466, loss = 0.00434908\n",
      "Iteration 11467, loss = 0.00434562\n",
      "Iteration 11468, loss = 0.00434574\n",
      "Iteration 11469, loss = 0.00434178\n",
      "Iteration 11470, loss = 0.00434248\n",
      "Iteration 11471, loss = 0.00434340\n",
      "Iteration 11472, loss = 0.00434185\n",
      "Iteration 11473, loss = 0.00433937\n",
      "Iteration 11474, loss = 0.00433737\n",
      "Iteration 11475, loss = 0.00433524\n",
      "Iteration 11476, loss = 0.00433170\n",
      "Iteration 11477, loss = 0.00433183\n",
      "Iteration 11478, loss = 0.00432779\n",
      "Iteration 11479, loss = 0.00432818\n",
      "Iteration 11480, loss = 0.00432933\n",
      "Iteration 11481, loss = 0.00432754\n",
      "Iteration 11482, loss = 0.00432467\n",
      "Iteration 11483, loss = 0.00432263\n",
      "Iteration 11484, loss = 0.00432073\n",
      "Iteration 11485, loss = 0.00431725\n",
      "Iteration 11486, loss = 0.00431947\n",
      "Iteration 11487, loss = 0.00431527\n",
      "Iteration 11488, loss = 0.00431335\n",
      "Iteration 11489, loss = 0.00431471\n",
      "Iteration 11490, loss = 0.00431287\n",
      "Iteration 11491, loss = 0.00430982\n",
      "Iteration 11492, loss = 0.00430769\n",
      "Iteration 11493, loss = 0.00430588\n",
      "Iteration 11494, loss = 0.00430250\n",
      "Iteration 11495, loss = 0.00430758\n",
      "Iteration 11496, loss = 0.00430333\n",
      "Iteration 11497, loss = 0.00429834\n",
      "Iteration 11498, loss = 0.00429979\n",
      "Iteration 11499, loss = 0.00429803\n",
      "Iteration 11500, loss = 0.00429486\n",
      "Iteration 11501, loss = 0.00429264\n",
      "Iteration 11502, loss = 0.00429088\n",
      "Iteration 11503, loss = 0.00429051\n",
      "Iteration 11504, loss = 0.00428536\n",
      "Iteration 11505, loss = 0.00428346\n",
      "Iteration 11506, loss = 0.00428275\n",
      "Iteration 11507, loss = 0.00428337\n",
      "Iteration 11508, loss = 0.00428222\n",
      "Iteration 11509, loss = 0.00427947\n",
      "Iteration 11510, loss = 0.00427707\n",
      "Iteration 11511, loss = 0.00427502\n",
      "Iteration 11512, loss = 0.00427937\n",
      "Iteration 11513, loss = 0.00427351\n",
      "Iteration 11514, loss = 0.00427159\n",
      "Iteration 11515, loss = 0.00427336\n",
      "Iteration 11516, loss = 0.00427335\n",
      "Iteration 11517, loss = 0.00427115\n",
      "Iteration 11518, loss = 0.00426832\n",
      "Iteration 11519, loss = 0.00426595\n",
      "Iteration 11520, loss = 0.00426330\n",
      "Iteration 11521, loss = 0.00425932\n",
      "Iteration 11522, loss = 0.00426481\n",
      "Iteration 11523, loss = 0.00426214\n",
      "Iteration 11524, loss = 0.00425489\n",
      "Iteration 11525, loss = 0.00425561\n",
      "Iteration 11526, loss = 0.00425370\n",
      "Iteration 11527, loss = 0.00425088\n",
      "Iteration 11528, loss = 0.00424870\n",
      "Iteration 11529, loss = 0.00424760\n",
      "Iteration 11530, loss = 0.00424557\n",
      "Iteration 11531, loss = 0.00424362\n",
      "Iteration 11532, loss = 0.00424167\n",
      "Iteration 11533, loss = 0.00423944\n",
      "Iteration 11534, loss = 0.00424375\n",
      "Iteration 11535, loss = 0.00423867\n",
      "Iteration 11536, loss = 0.00423723\n",
      "Iteration 11537, loss = 0.00423771\n",
      "Iteration 11538, loss = 0.00423751\n",
      "Iteration 11539, loss = 0.00423672\n",
      "Iteration 11540, loss = 0.00423501\n",
      "Iteration 11541, loss = 0.00423231\n",
      "Iteration 11542, loss = 0.00422896\n",
      "Iteration 11543, loss = 0.00422529\n",
      "Iteration 11544, loss = 0.00422427\n",
      "Iteration 11545, loss = 0.00422220\n",
      "Iteration 11546, loss = 0.00422154\n",
      "Iteration 11547, loss = 0.00422133\n",
      "Iteration 11548, loss = 0.00422006\n",
      "Iteration 11549, loss = 0.00421852\n",
      "Iteration 11550, loss = 0.00421656\n",
      "Iteration 11551, loss = 0.00421367\n",
      "Iteration 11552, loss = 0.00420983\n",
      "Iteration 11553, loss = 0.00421549\n",
      "Iteration 11554, loss = 0.00421265\n",
      "Iteration 11555, loss = 0.00420687\n",
      "Iteration 11556, loss = 0.00420714\n",
      "Iteration 11557, loss = 0.00420554\n",
      "Iteration 11558, loss = 0.00420356\n",
      "Iteration 11559, loss = 0.00420181\n",
      "Iteration 11560, loss = 0.00419942\n",
      "Iteration 11561, loss = 0.00419828\n",
      "Iteration 11562, loss = 0.00419419\n",
      "Iteration 11563, loss = 0.00419299\n",
      "Iteration 11564, loss = 0.00419199\n",
      "Iteration 11565, loss = 0.00419197\n",
      "Iteration 11566, loss = 0.00419069\n",
      "Iteration 11567, loss = 0.00418859\n",
      "Iteration 11568, loss = 0.00418663\n",
      "Iteration 11569, loss = 0.00418442\n",
      "Iteration 11570, loss = 0.00418747\n",
      "Iteration 11571, loss = 0.00418176\n",
      "Iteration 11572, loss = 0.00418158\n",
      "Iteration 11573, loss = 0.00418327\n",
      "Iteration 11574, loss = 0.00418285\n",
      "Iteration 11575, loss = 0.00418062\n",
      "Iteration 11576, loss = 0.00417821\n",
      "Iteration 11577, loss = 0.00417611\n",
      "Iteration 11578, loss = 0.00417334\n",
      "Iteration 11579, loss = 0.00416918\n",
      "Iteration 11580, loss = 0.00417277\n",
      "Iteration 11581, loss = 0.00417043\n",
      "Iteration 11582, loss = 0.00416541\n",
      "Iteration 11583, loss = 0.00416585\n",
      "Iteration 11584, loss = 0.00416378\n",
      "Iteration 11585, loss = 0.00416118\n",
      "Iteration 11586, loss = 0.00415925\n",
      "Iteration 11587, loss = 0.00415711\n",
      "Iteration 11588, loss = 0.00416121\n",
      "Iteration 11589, loss = 0.00415565\n",
      "Iteration 11590, loss = 0.00415336\n",
      "Iteration 11591, loss = 0.00415522\n",
      "Iteration 11592, loss = 0.00415475\n",
      "Iteration 11593, loss = 0.00415216\n",
      "Iteration 11594, loss = 0.00414946\n",
      "Iteration 11595, loss = 0.00414741\n",
      "Iteration 11596, loss = 0.00414475\n",
      "Iteration 11597, loss = 0.00414333\n",
      "Iteration 11598, loss = 0.00413868\n",
      "Iteration 11599, loss = 0.00414053\n",
      "Iteration 11600, loss = 0.00414220\n",
      "Iteration 11601, loss = 0.00414119\n",
      "Iteration 11602, loss = 0.00413823\n",
      "Iteration 11603, loss = 0.00413557\n",
      "Iteration 11604, loss = 0.00413362\n",
      "Iteration 11605, loss = 0.00413078\n",
      "Iteration 11606, loss = 0.00413129\n",
      "Iteration 11607, loss = 0.00412692\n",
      "Iteration 11608, loss = 0.00412627\n",
      "Iteration 11609, loss = 0.00412789\n",
      "Iteration 11610, loss = 0.00412664\n",
      "Iteration 11611, loss = 0.00412355\n",
      "Iteration 11612, loss = 0.00412097\n",
      "Iteration 11613, loss = 0.00411909\n",
      "Iteration 11614, loss = 0.00411617\n",
      "Iteration 11615, loss = 0.00412091\n",
      "Iteration 11616, loss = 0.00411669\n",
      "Iteration 11617, loss = 0.00411160\n",
      "Iteration 11618, loss = 0.00411316\n",
      "Iteration 11619, loss = 0.00411186\n",
      "Iteration 11620, loss = 0.00410878\n",
      "Iteration 11621, loss = 0.00410622\n",
      "Iteration 11622, loss = 0.00410434\n",
      "Iteration 11623, loss = 0.00410524\n",
      "Iteration 11624, loss = 0.00409964\n",
      "Iteration 11625, loss = 0.00410059\n",
      "Iteration 11626, loss = 0.00410236\n",
      "Iteration 11627, loss = 0.00410252\n",
      "Iteration 11628, loss = 0.00410025\n",
      "Iteration 11629, loss = 0.00409725\n",
      "Iteration 11630, loss = 0.00409484\n",
      "Iteration 11631, loss = 0.00409234\n",
      "Iteration 11632, loss = 0.00408852\n",
      "Iteration 11633, loss = 0.00409332\n",
      "Iteration 11634, loss = 0.00409076\n",
      "Iteration 11635, loss = 0.00408394\n",
      "Iteration 11636, loss = 0.00408479\n",
      "Iteration 11637, loss = 0.00408297\n",
      "Iteration 11638, loss = 0.00408001\n",
      "Iteration 11639, loss = 0.00407773\n",
      "Iteration 11640, loss = 0.00407768\n",
      "Iteration 11641, loss = 0.00407488\n",
      "Iteration 11642, loss = 0.00407291\n",
      "Iteration 11643, loss = 0.00407086\n",
      "Iteration 11644, loss = 0.00406975\n",
      "Iteration 11645, loss = 0.00406935\n",
      "Iteration 11646, loss = 0.00406820\n",
      "Iteration 11647, loss = 0.00406578\n",
      "Iteration 11648, loss = 0.00406324\n",
      "Iteration 11649, loss = 0.00406549\n",
      "Iteration 11650, loss = 0.00406020\n",
      "Iteration 11651, loss = 0.00405864\n",
      "Iteration 11652, loss = 0.00405660\n",
      "Iteration 11653, loss = 0.00405855\n",
      "Iteration 11654, loss = 0.00405462\n",
      "Iteration 11655, loss = 0.00405369\n",
      "Iteration 11656, loss = 0.00405159\n",
      "Iteration 11657, loss = 0.00404934\n",
      "Iteration 11658, loss = 0.00404926\n",
      "Iteration 11659, loss = 0.00404859\n",
      "Iteration 11660, loss = 0.00404708\n",
      "Iteration 11661, loss = 0.00404486\n",
      "Iteration 11662, loss = 0.00404205\n",
      "Iteration 11663, loss = 0.00404500\n",
      "Iteration 11664, loss = 0.00404074\n",
      "Iteration 11665, loss = 0.00404009\n",
      "Iteration 11666, loss = 0.00404067\n",
      "Iteration 11667, loss = 0.00403998\n",
      "Iteration 11668, loss = 0.00403871\n",
      "Iteration 11669, loss = 0.00403702\n",
      "Iteration 11670, loss = 0.00403464\n",
      "Iteration 11671, loss = 0.00403137\n",
      "Iteration 11672, loss = 0.00402749\n",
      "Iteration 11673, loss = 0.00403165\n",
      "Iteration 11674, loss = 0.00402985\n",
      "Iteration 11675, loss = 0.00402405\n",
      "Iteration 11676, loss = 0.00402396\n",
      "Iteration 11677, loss = 0.00402241\n",
      "Iteration 11678, loss = 0.00402058\n",
      "Iteration 11679, loss = 0.00401875\n",
      "Iteration 11680, loss = 0.00401621\n",
      "Iteration 11681, loss = 0.00401913\n",
      "Iteration 11682, loss = 0.00401415\n",
      "Iteration 11683, loss = 0.00401344\n",
      "Iteration 11684, loss = 0.00401490\n",
      "Iteration 11685, loss = 0.00401407\n",
      "Iteration 11686, loss = 0.00401179\n",
      "Iteration 11687, loss = 0.00400964\n",
      "Iteration 11688, loss = 0.00400769\n",
      "Iteration 11689, loss = 0.00400483\n",
      "Iteration 11690, loss = 0.00400131\n",
      "Iteration 11691, loss = 0.00399891\n",
      "Iteration 11692, loss = 0.00399769\n",
      "Iteration 11693, loss = 0.00399881\n",
      "Iteration 11694, loss = 0.00399606\n",
      "Iteration 11695, loss = 0.00399442\n",
      "Iteration 11696, loss = 0.00399224\n",
      "Iteration 11697, loss = 0.00399034\n",
      "Iteration 11698, loss = 0.00399358\n",
      "Iteration 11699, loss = 0.00398744\n",
      "Iteration 11700, loss = 0.00398870\n",
      "Iteration 11701, loss = 0.00399027\n",
      "Iteration 11702, loss = 0.00399065\n",
      "Iteration 11703, loss = 0.00398924\n",
      "Iteration 11704, loss = 0.00398688\n",
      "Iteration 11705, loss = 0.00398457\n",
      "Iteration 11706, loss = 0.00398207\n",
      "Iteration 11707, loss = 0.00397862\n",
      "Iteration 11708, loss = 0.00397419\n",
      "Iteration 11709, loss = 0.00398050\n",
      "Iteration 11710, loss = 0.00397963\n",
      "Iteration 11711, loss = 0.00397075\n",
      "Iteration 11712, loss = 0.00397371\n",
      "Iteration 11713, loss = 0.00397459\n",
      "Iteration 11714, loss = 0.00397424\n",
      "Iteration 11715, loss = 0.00397431\n",
      "Iteration 11716, loss = 0.00397426\n",
      "Iteration 11717, loss = 0.00397264\n",
      "Iteration 11718, loss = 0.00396941\n",
      "Iteration 11719, loss = 0.00396587\n",
      "Iteration 11720, loss = 0.00396266\n",
      "Iteration 11721, loss = 0.00395909\n",
      "Iteration 11722, loss = 0.00395445\n",
      "Iteration 11723, loss = 0.00395976\n",
      "Iteration 11724, loss = 0.00396073\n",
      "Iteration 11725, loss = 0.00395301\n",
      "Iteration 11726, loss = 0.00394978\n",
      "Iteration 11727, loss = 0.00395129\n",
      "Iteration 11728, loss = 0.00395178\n",
      "Iteration 11729, loss = 0.00395134\n",
      "Iteration 11730, loss = 0.00395013\n",
      "Iteration 11731, loss = 0.00394822\n",
      "Iteration 11732, loss = 0.00394565\n",
      "Iteration 11733, loss = 0.00394246\n",
      "Iteration 11734, loss = 0.00393875\n",
      "Iteration 11735, loss = 0.00393460\n",
      "Iteration 11736, loss = 0.00394046\n",
      "Iteration 11737, loss = 0.00394027\n",
      "Iteration 11738, loss = 0.00393157\n",
      "Iteration 11739, loss = 0.00393241\n",
      "Iteration 11740, loss = 0.00393384\n",
      "Iteration 11741, loss = 0.00393406\n",
      "Iteration 11742, loss = 0.00393385\n",
      "Iteration 11743, loss = 0.00393315\n",
      "Iteration 11744, loss = 0.00393146\n",
      "Iteration 11745, loss = 0.00392872\n",
      "Iteration 11746, loss = 0.00392545\n",
      "Iteration 11747, loss = 0.00392197\n",
      "Iteration 11748, loss = 0.00391811\n",
      "Iteration 11749, loss = 0.00391368\n",
      "Iteration 11750, loss = 0.00391334\n",
      "Iteration 11751, loss = 0.00391262\n",
      "Iteration 11752, loss = 0.00391217\n",
      "Iteration 11753, loss = 0.00391098\n",
      "Iteration 11754, loss = 0.00390929\n",
      "Iteration 11755, loss = 0.00390700\n",
      "Iteration 11756, loss = 0.00390400\n",
      "Iteration 11757, loss = 0.00390796\n",
      "Iteration 11758, loss = 0.00390421\n",
      "Iteration 11759, loss = 0.00390156\n",
      "Iteration 11760, loss = 0.00390230\n",
      "Iteration 11761, loss = 0.00390127\n",
      "Iteration 11762, loss = 0.00389947\n",
      "Iteration 11763, loss = 0.00389769\n",
      "Iteration 11764, loss = 0.00389556\n",
      "Iteration 11765, loss = 0.00389240\n",
      "Iteration 11766, loss = 0.00389249\n",
      "Iteration 11767, loss = 0.00388880\n",
      "Iteration 11768, loss = 0.00388955\n",
      "Iteration 11769, loss = 0.00389050\n",
      "Iteration 11770, loss = 0.00388912\n",
      "Iteration 11771, loss = 0.00388677\n",
      "Iteration 11772, loss = 0.00388487\n",
      "Iteration 11773, loss = 0.00388300\n",
      "Iteration 11774, loss = 0.00387987\n",
      "Iteration 11775, loss = 0.00388037\n",
      "Iteration 11776, loss = 0.00387656\n",
      "Iteration 11777, loss = 0.00387653\n",
      "Iteration 11778, loss = 0.00387775\n",
      "Iteration 11779, loss = 0.00387621\n",
      "Iteration 11780, loss = 0.00387348\n",
      "Iteration 11781, loss = 0.00387147\n",
      "Iteration 11782, loss = 0.00386977\n",
      "Iteration 11783, loss = 0.00386674\n",
      "Iteration 11784, loss = 0.00386970\n",
      "Iteration 11785, loss = 0.00386575\n",
      "Iteration 11786, loss = 0.00386297\n",
      "Iteration 11787, loss = 0.00386438\n",
      "Iteration 11788, loss = 0.00386287\n",
      "Iteration 11789, loss = 0.00385999\n",
      "Iteration 11790, loss = 0.00385785\n",
      "Iteration 11791, loss = 0.00385621\n",
      "Iteration 11792, loss = 0.00385453\n",
      "Iteration 11793, loss = 0.00385126\n",
      "Iteration 11794, loss = 0.00384945\n",
      "Iteration 11795, loss = 0.00384803\n",
      "Iteration 11796, loss = 0.00385103\n",
      "Iteration 11797, loss = 0.00384541\n",
      "Iteration 11798, loss = 0.00384701\n",
      "Iteration 11799, loss = 0.00384735\n",
      "Iteration 11800, loss = 0.00384741\n",
      "Iteration 11801, loss = 0.00384715\n",
      "Iteration 11802, loss = 0.00384581\n",
      "Iteration 11803, loss = 0.00384326\n",
      "Iteration 11804, loss = 0.00384015\n",
      "Iteration 11805, loss = 0.00383696\n",
      "Iteration 11806, loss = 0.00383345\n",
      "Iteration 11807, loss = 0.00383502\n",
      "Iteration 11808, loss = 0.00383417\n",
      "Iteration 11809, loss = 0.00382838\n",
      "Iteration 11810, loss = 0.00382804\n",
      "Iteration 11811, loss = 0.00382703\n",
      "Iteration 11812, loss = 0.00382555\n",
      "Iteration 11813, loss = 0.00382340\n",
      "Iteration 11814, loss = 0.00382122\n",
      "Iteration 11815, loss = 0.00381991\n",
      "Iteration 11816, loss = 0.00381898\n",
      "Iteration 11817, loss = 0.00381730\n",
      "Iteration 11818, loss = 0.00381548\n",
      "Iteration 11819, loss = 0.00381451\n",
      "Iteration 11820, loss = 0.00381349\n",
      "Iteration 11821, loss = 0.00381191\n",
      "Iteration 11822, loss = 0.00380980\n",
      "Iteration 11823, loss = 0.00381099\n",
      "Iteration 11824, loss = 0.00380647\n",
      "Iteration 11825, loss = 0.00380543\n",
      "Iteration 11826, loss = 0.00380374\n",
      "Iteration 11827, loss = 0.00380496\n",
      "Iteration 11828, loss = 0.00380127\n",
      "Iteration 11829, loss = 0.00380029\n",
      "Iteration 11830, loss = 0.00379864\n",
      "Iteration 11831, loss = 0.00379650\n",
      "Iteration 11832, loss = 0.00379996\n",
      "Iteration 11833, loss = 0.00379447\n",
      "Iteration 11834, loss = 0.00379536\n",
      "Iteration 11835, loss = 0.00379660\n",
      "Iteration 11836, loss = 0.00379648\n",
      "Iteration 11837, loss = 0.00379523\n",
      "Iteration 11838, loss = 0.00379349\n",
      "Iteration 11839, loss = 0.00379145\n",
      "Iteration 11840, loss = 0.00378877\n",
      "Iteration 11841, loss = 0.00378525\n",
      "Iteration 11842, loss = 0.00378120\n",
      "Iteration 11843, loss = 0.00378827\n",
      "Iteration 11844, loss = 0.00378760\n",
      "Iteration 11845, loss = 0.00377883\n",
      "Iteration 11846, loss = 0.00378036\n",
      "Iteration 11847, loss = 0.00378142\n",
      "Iteration 11848, loss = 0.00378152\n",
      "Iteration 11849, loss = 0.00378168\n",
      "Iteration 11850, loss = 0.00378137\n",
      "Iteration 11851, loss = 0.00377968\n",
      "Iteration 11852, loss = 0.00377682\n",
      "Iteration 11853, loss = 0.00377369\n",
      "Iteration 11854, loss = 0.00377058\n",
      "Iteration 11855, loss = 0.00376696\n",
      "Iteration 11856, loss = 0.00376250\n",
      "Iteration 11857, loss = 0.00376821\n",
      "Iteration 11858, loss = 0.00376913\n",
      "Iteration 11859, loss = 0.00376161\n",
      "Iteration 11860, loss = 0.00375836\n",
      "Iteration 11861, loss = 0.00375988\n",
      "Iteration 11862, loss = 0.00376033\n",
      "Iteration 11863, loss = 0.00375987\n",
      "Iteration 11864, loss = 0.00375872\n",
      "Iteration 11865, loss = 0.00375693\n",
      "Iteration 11866, loss = 0.00375446\n",
      "Iteration 11867, loss = 0.00375137\n",
      "Iteration 11868, loss = 0.00374781\n",
      "Iteration 11869, loss = 0.00374385\n",
      "Iteration 11870, loss = 0.00374984\n",
      "Iteration 11871, loss = 0.00374967\n",
      "Iteration 11872, loss = 0.00374127\n",
      "Iteration 11873, loss = 0.00374172\n",
      "Iteration 11874, loss = 0.00374310\n",
      "Iteration 11875, loss = 0.00374340\n",
      "Iteration 11876, loss = 0.00374326\n",
      "Iteration 11877, loss = 0.00374257\n",
      "Iteration 11878, loss = 0.00374092\n",
      "Iteration 11879, loss = 0.00373830\n",
      "Iteration 11880, loss = 0.00373522\n",
      "Iteration 11881, loss = 0.00373191\n",
      "Iteration 11882, loss = 0.00372818\n",
      "Iteration 11883, loss = 0.00372398\n",
      "Iteration 11884, loss = 0.00372366\n",
      "Iteration 11885, loss = 0.00372292\n",
      "Iteration 11886, loss = 0.00372255\n",
      "Iteration 11887, loss = 0.00372138\n",
      "Iteration 11888, loss = 0.00371975\n",
      "Iteration 11889, loss = 0.00371758\n",
      "Iteration 11890, loss = 0.00371471\n",
      "Iteration 11891, loss = 0.00371862\n",
      "Iteration 11892, loss = 0.00371503\n",
      "Iteration 11893, loss = 0.00371234\n",
      "Iteration 11894, loss = 0.00371307\n",
      "Iteration 11895, loss = 0.00371212\n",
      "Iteration 11896, loss = 0.00371039\n",
      "Iteration 11897, loss = 0.00370866\n",
      "Iteration 11898, loss = 0.00370665\n",
      "Iteration 11899, loss = 0.00370365\n",
      "Iteration 11900, loss = 0.00370399\n",
      "Iteration 11901, loss = 0.00370047\n",
      "Iteration 11902, loss = 0.00370090\n",
      "Iteration 11903, loss = 0.00370183\n",
      "Iteration 11904, loss = 0.00370049\n",
      "Iteration 11905, loss = 0.00369824\n",
      "Iteration 11906, loss = 0.00369647\n",
      "Iteration 11907, loss = 0.00369467\n",
      "Iteration 11908, loss = 0.00369164\n",
      "Iteration 11909, loss = 0.00369241\n",
      "Iteration 11910, loss = 0.00368881\n",
      "Iteration 11911, loss = 0.00368856\n",
      "Iteration 11912, loss = 0.00368969\n",
      "Iteration 11913, loss = 0.00368816\n",
      "Iteration 11914, loss = 0.00368557\n",
      "Iteration 11915, loss = 0.00368374\n",
      "Iteration 11916, loss = 0.00368213\n",
      "Iteration 11917, loss = 0.00367916\n",
      "Iteration 11918, loss = 0.00368215\n",
      "Iteration 11919, loss = 0.00367844\n",
      "Iteration 11920, loss = 0.00367572\n",
      "Iteration 11921, loss = 0.00367700\n",
      "Iteration 11922, loss = 0.00367543\n",
      "Iteration 11923, loss = 0.00367268\n",
      "Iteration 11924, loss = 0.00367077\n",
      "Iteration 11925, loss = 0.00366925\n",
      "Iteration 11926, loss = 0.00366761\n",
      "Iteration 11927, loss = 0.00366438\n",
      "Iteration 11928, loss = 0.00366277\n",
      "Iteration 11929, loss = 0.00366150\n",
      "Iteration 11930, loss = 0.00366429\n",
      "Iteration 11931, loss = 0.00365884\n",
      "Iteration 11932, loss = 0.00366034\n",
      "Iteration 11933, loss = 0.00366072\n",
      "Iteration 11934, loss = 0.00366086\n",
      "Iteration 11935, loss = 0.00366062\n",
      "Iteration 11936, loss = 0.00365927\n",
      "Iteration 11937, loss = 0.00365681\n",
      "Iteration 11938, loss = 0.00365388\n",
      "Iteration 11939, loss = 0.00365087\n",
      "Iteration 11940, loss = 0.00364747\n",
      "Iteration 11941, loss = 0.00364908\n",
      "Iteration 11942, loss = 0.00364828\n",
      "Iteration 11943, loss = 0.00364255\n",
      "Iteration 11944, loss = 0.00364228\n",
      "Iteration 11945, loss = 0.00364135\n",
      "Iteration 11946, loss = 0.00363990\n",
      "Iteration 11947, loss = 0.00363782\n",
      "Iteration 11948, loss = 0.00363588\n",
      "Iteration 11949, loss = 0.00363452\n",
      "Iteration 11950, loss = 0.00363365\n",
      "Iteration 11951, loss = 0.00363200\n",
      "Iteration 11952, loss = 0.00363057\n",
      "Iteration 11953, loss = 0.00362933\n",
      "Iteration 11954, loss = 0.00362838\n",
      "Iteration 11955, loss = 0.00362687\n",
      "Iteration 11956, loss = 0.00362483\n",
      "Iteration 11957, loss = 0.00362627\n",
      "Iteration 11958, loss = 0.00362169\n",
      "Iteration 11959, loss = 0.00362069\n",
      "Iteration 11960, loss = 0.00361908\n",
      "Iteration 11961, loss = 0.00362057\n",
      "Iteration 11962, loss = 0.00361666\n",
      "Iteration 11963, loss = 0.00361577\n",
      "Iteration 11964, loss = 0.00361421\n",
      "Iteration 11965, loss = 0.00361241\n",
      "Iteration 11966, loss = 0.00361230\n",
      "Iteration 11967, loss = 0.00361167\n",
      "Iteration 11968, loss = 0.00361038\n",
      "Iteration 11969, loss = 0.00360840\n",
      "Iteration 11970, loss = 0.00360582\n",
      "Iteration 11971, loss = 0.00360865\n",
      "Iteration 11972, loss = 0.00360490\n",
      "Iteration 11973, loss = 0.00360373\n",
      "Iteration 11974, loss = 0.00360438\n",
      "Iteration 11975, loss = 0.00360398\n",
      "Iteration 11976, loss = 0.00360281\n",
      "Iteration 11977, loss = 0.00360112\n",
      "Iteration 11978, loss = 0.00359889\n",
      "Iteration 11979, loss = 0.00359603\n",
      "Iteration 11980, loss = 0.00359260\n",
      "Iteration 11981, loss = 0.00359745\n",
      "Iteration 11982, loss = 0.00359578\n",
      "Iteration 11983, loss = 0.00358918\n",
      "Iteration 11984, loss = 0.00358926\n",
      "Iteration 11985, loss = 0.00358799\n",
      "Iteration 11986, loss = 0.00358628\n",
      "Iteration 11987, loss = 0.00358451\n",
      "Iteration 11988, loss = 0.00358220\n",
      "Iteration 11989, loss = 0.00358650\n",
      "Iteration 11990, loss = 0.00358214\n",
      "Iteration 11991, loss = 0.00357974\n",
      "Iteration 11992, loss = 0.00358099\n",
      "Iteration 11993, loss = 0.00358032\n",
      "Iteration 11994, loss = 0.00357837\n",
      "Iteration 11995, loss = 0.00357644\n",
      "Iteration 11996, loss = 0.00357464\n",
      "Iteration 11997, loss = 0.00357200\n",
      "Iteration 11998, loss = 0.00357054\n",
      "Iteration 11999, loss = 0.00356676\n",
      "Iteration 12000, loss = 0.00356636\n",
      "Iteration 12001, loss = 0.00356737\n",
      "Iteration 12002, loss = 0.00356697\n",
      "Iteration 12003, loss = 0.00356490\n",
      "Iteration 12004, loss = 0.00356284\n",
      "Iteration 12005, loss = 0.00356121\n",
      "Iteration 12006, loss = 0.00355905\n",
      "Iteration 12007, loss = 0.00355756\n",
      "Iteration 12008, loss = 0.00355602\n",
      "Iteration 12009, loss = 0.00355458\n",
      "Iteration 12010, loss = 0.00355371\n",
      "Iteration 12011, loss = 0.00355255\n",
      "Iteration 12012, loss = 0.00355118\n",
      "Iteration 12013, loss = 0.00354922\n",
      "Iteration 12014, loss = 0.00354735\n",
      "Iteration 12015, loss = 0.00354738\n",
      "Iteration 12016, loss = 0.00354656\n",
      "Iteration 12017, loss = 0.00354509\n",
      "Iteration 12018, loss = 0.00354318\n",
      "Iteration 12019, loss = 0.00354070\n",
      "Iteration 12020, loss = 0.00354431\n",
      "Iteration 12021, loss = 0.00354079\n",
      "Iteration 12022, loss = 0.00353850\n",
      "Iteration 12023, loss = 0.00353894\n",
      "Iteration 12024, loss = 0.00353852\n",
      "Iteration 12025, loss = 0.00353751\n",
      "Iteration 12026, loss = 0.00353594\n",
      "Iteration 12027, loss = 0.00353368\n",
      "Iteration 12028, loss = 0.00353076\n",
      "Iteration 12029, loss = 0.00352742\n",
      "Iteration 12030, loss = 0.00353349\n",
      "Iteration 12031, loss = 0.00353196\n",
      "Iteration 12032, loss = 0.00352405\n",
      "Iteration 12033, loss = 0.00352397\n",
      "Iteration 12034, loss = 0.00352287\n",
      "Iteration 12035, loss = 0.00352139\n",
      "Iteration 12036, loss = 0.00351965\n",
      "Iteration 12037, loss = 0.00351793\n",
      "Iteration 12038, loss = 0.00351655\n",
      "Iteration 12039, loss = 0.00351553\n",
      "Iteration 12040, loss = 0.00351413\n",
      "Iteration 12041, loss = 0.00351190\n",
      "Iteration 12042, loss = 0.00351534\n",
      "Iteration 12043, loss = 0.00351117\n",
      "Iteration 12044, loss = 0.00350994\n",
      "Iteration 12045, loss = 0.00351061\n",
      "Iteration 12046, loss = 0.00351047\n",
      "Iteration 12047, loss = 0.00350961\n",
      "Iteration 12048, loss = 0.00350808\n",
      "Iteration 12049, loss = 0.00350590\n",
      "Iteration 12050, loss = 0.00350316\n",
      "Iteration 12051, loss = 0.00349998\n",
      "Iteration 12052, loss = 0.00350004\n",
      "Iteration 12053, loss = 0.00349828\n",
      "Iteration 12054, loss = 0.00349666\n",
      "Iteration 12055, loss = 0.00349672\n",
      "Iteration 12056, loss = 0.00349568\n",
      "Iteration 12057, loss = 0.00349416\n",
      "Iteration 12058, loss = 0.00349240\n",
      "Iteration 12059, loss = 0.00349004\n",
      "Iteration 12060, loss = 0.00348837\n",
      "Iteration 12061, loss = 0.00348588\n",
      "Iteration 12062, loss = 0.00348481\n",
      "Iteration 12063, loss = 0.00348426\n",
      "Iteration 12064, loss = 0.00348347\n",
      "Iteration 12065, loss = 0.00348252\n",
      "Iteration 12066, loss = 0.00348094\n",
      "Iteration 12067, loss = 0.00347925\n",
      "Iteration 12068, loss = 0.00347714\n",
      "Iteration 12069, loss = 0.00347657\n",
      "Iteration 12070, loss = 0.00347551\n",
      "Iteration 12071, loss = 0.00347410\n",
      "Iteration 12072, loss = 0.00347206\n",
      "Iteration 12073, loss = 0.00347312\n",
      "Iteration 12074, loss = 0.00346877\n",
      "Iteration 12075, loss = 0.00346755\n",
      "Iteration 12076, loss = 0.00346755\n",
      "Iteration 12077, loss = 0.00346621\n",
      "Iteration 12078, loss = 0.00346585\n",
      "Iteration 12079, loss = 0.00346478\n",
      "Iteration 12080, loss = 0.00346306\n",
      "Iteration 12081, loss = 0.00346076\n",
      "Iteration 12082, loss = 0.00345996\n",
      "Iteration 12083, loss = 0.00345750\n",
      "Iteration 12084, loss = 0.00345633\n",
      "Iteration 12085, loss = 0.00345486\n",
      "Iteration 12086, loss = 0.00345490\n",
      "Iteration 12087, loss = 0.00345445\n",
      "Iteration 12088, loss = 0.00345324\n",
      "Iteration 12089, loss = 0.00345147\n",
      "Iteration 12090, loss = 0.00344920\n",
      "Iteration 12091, loss = 0.00344920\n",
      "Iteration 12092, loss = 0.00344573\n",
      "Iteration 12093, loss = 0.00344459\n",
      "Iteration 12094, loss = 0.00344410\n",
      "Iteration 12095, loss = 0.00344332\n",
      "Iteration 12096, loss = 0.00344273\n",
      "Iteration 12097, loss = 0.00344137\n",
      "Iteration 12098, loss = 0.00343962\n",
      "Iteration 12099, loss = 0.00343744\n",
      "Iteration 12100, loss = 0.00343931\n",
      "Iteration 12101, loss = 0.00343497\n",
      "Iteration 12102, loss = 0.00343566\n",
      "Iteration 12103, loss = 0.00343674\n",
      "Iteration 12104, loss = 0.00343630\n",
      "Iteration 12105, loss = 0.00343480\n",
      "Iteration 12106, loss = 0.00343311\n",
      "Iteration 12107, loss = 0.00343127\n",
      "Iteration 12108, loss = 0.00342866\n",
      "Iteration 12109, loss = 0.00342517\n",
      "Iteration 12110, loss = 0.00342715\n",
      "Iteration 12111, loss = 0.00342542\n",
      "Iteration 12112, loss = 0.00342230\n",
      "Iteration 12113, loss = 0.00342243\n",
      "Iteration 12114, loss = 0.00342083\n",
      "Iteration 12115, loss = 0.00341897\n",
      "Iteration 12116, loss = 0.00341746\n",
      "Iteration 12117, loss = 0.00341545\n",
      "Iteration 12118, loss = 0.00341724\n",
      "Iteration 12119, loss = 0.00341268\n",
      "Iteration 12120, loss = 0.00341282\n",
      "Iteration 12121, loss = 0.00341434\n",
      "Iteration 12122, loss = 0.00341361\n",
      "Iteration 12123, loss = 0.00341139\n",
      "Iteration 12124, loss = 0.00340947\n",
      "Iteration 12125, loss = 0.00340793\n",
      "Iteration 12126, loss = 0.00340546\n",
      "Iteration 12127, loss = 0.00340244\n",
      "Iteration 12128, loss = 0.00340007\n",
      "Iteration 12129, loss = 0.00339914\n",
      "Iteration 12130, loss = 0.00340058\n",
      "Iteration 12131, loss = 0.00339773\n",
      "Iteration 12132, loss = 0.00339617\n",
      "Iteration 12133, loss = 0.00339431\n",
      "Iteration 12134, loss = 0.00339283\n",
      "Iteration 12135, loss = 0.00339642\n",
      "Iteration 12136, loss = 0.00339089\n",
      "Iteration 12137, loss = 0.00339132\n",
      "Iteration 12138, loss = 0.00339288\n",
      "Iteration 12139, loss = 0.00339323\n",
      "Iteration 12140, loss = 0.00339185\n",
      "Iteration 12141, loss = 0.00338976\n",
      "Iteration 12142, loss = 0.00338787\n",
      "Iteration 12143, loss = 0.00338576\n",
      "Iteration 12144, loss = 0.00338265\n",
      "Iteration 12145, loss = 0.00337871\n",
      "Iteration 12146, loss = 0.00338543\n",
      "Iteration 12147, loss = 0.00338489\n",
      "Iteration 12148, loss = 0.00337716\n",
      "Iteration 12149, loss = 0.00337847\n",
      "Iteration 12150, loss = 0.00337909\n",
      "Iteration 12151, loss = 0.00337894\n",
      "Iteration 12152, loss = 0.00337925\n",
      "Iteration 12153, loss = 0.00337923\n",
      "Iteration 12154, loss = 0.00337763\n",
      "Iteration 12155, loss = 0.00337480\n",
      "Iteration 12156, loss = 0.00337192\n",
      "Iteration 12157, loss = 0.00336928\n",
      "Iteration 12158, loss = 0.00336606\n",
      "Iteration 12159, loss = 0.00336186\n",
      "Iteration 12160, loss = 0.00336746\n",
      "Iteration 12161, loss = 0.00336842\n",
      "Iteration 12162, loss = 0.00336168\n",
      "Iteration 12163, loss = 0.00335799\n",
      "Iteration 12164, loss = 0.00335940\n",
      "Iteration 12165, loss = 0.00335986\n",
      "Iteration 12166, loss = 0.00335947\n",
      "Iteration 12167, loss = 0.00335839\n",
      "Iteration 12168, loss = 0.00335675\n",
      "Iteration 12169, loss = 0.00335453\n",
      "Iteration 12170, loss = 0.00335175\n",
      "Iteration 12171, loss = 0.00334851\n",
      "Iteration 12172, loss = 0.00334491\n",
      "Iteration 12173, loss = 0.00335144\n",
      "Iteration 12174, loss = 0.00335131\n",
      "Iteration 12175, loss = 0.00334371\n",
      "Iteration 12176, loss = 0.00334313\n",
      "Iteration 12177, loss = 0.00334436\n",
      "Iteration 12178, loss = 0.00334462\n",
      "Iteration 12179, loss = 0.00334452\n",
      "Iteration 12180, loss = 0.00334394\n",
      "Iteration 12181, loss = 0.00334243\n",
      "Iteration 12182, loss = 0.00334005\n",
      "Iteration 12183, loss = 0.00333727\n",
      "Iteration 12184, loss = 0.00333432\n",
      "Iteration 12185, loss = 0.00333094\n",
      "Iteration 12186, loss = 0.00332829\n",
      "Iteration 12187, loss = 0.00332803\n",
      "Iteration 12188, loss = 0.00332609\n",
      "Iteration 12189, loss = 0.00332577\n",
      "Iteration 12190, loss = 0.00332480\n",
      "Iteration 12191, loss = 0.00332334\n",
      "Iteration 12192, loss = 0.00332134\n",
      "Iteration 12193, loss = 0.00331875\n",
      "Iteration 12194, loss = 0.00331818\n",
      "Iteration 12195, loss = 0.00331730\n",
      "Iteration 12196, loss = 0.00331574\n",
      "Iteration 12197, loss = 0.00331472\n",
      "Iteration 12198, loss = 0.00331324\n",
      "Iteration 12199, loss = 0.00331233\n",
      "Iteration 12200, loss = 0.00331089\n",
      "Iteration 12201, loss = 0.00330899\n",
      "Iteration 12202, loss = 0.00331140\n",
      "Iteration 12203, loss = 0.00330653\n",
      "Iteration 12204, loss = 0.00330801\n",
      "Iteration 12205, loss = 0.00330916\n",
      "Iteration 12206, loss = 0.00330894\n",
      "Iteration 12207, loss = 0.00330774\n",
      "Iteration 12208, loss = 0.00330627\n",
      "Iteration 12209, loss = 0.00330453\n",
      "Iteration 12210, loss = 0.00330211\n",
      "Iteration 12211, loss = 0.00329890\n",
      "Iteration 12212, loss = 0.00329531\n",
      "Iteration 12213, loss = 0.00330198\n",
      "Iteration 12214, loss = 0.00330151\n",
      "Iteration 12215, loss = 0.00329365\n",
      "Iteration 12216, loss = 0.00329455\n",
      "Iteration 12217, loss = 0.00329549\n",
      "Iteration 12218, loss = 0.00329578\n",
      "Iteration 12219, loss = 0.00329606\n",
      "Iteration 12220, loss = 0.00329573\n",
      "Iteration 12221, loss = 0.00329413\n",
      "Iteration 12222, loss = 0.00329163\n",
      "Iteration 12223, loss = 0.00328901\n",
      "Iteration 12224, loss = 0.00328629\n",
      "Iteration 12225, loss = 0.00328298\n",
      "Iteration 12226, loss = 0.00327895\n",
      "Iteration 12227, loss = 0.00328416\n",
      "Iteration 12228, loss = 0.00328502\n",
      "Iteration 12229, loss = 0.00327829\n",
      "Iteration 12230, loss = 0.00327552\n",
      "Iteration 12231, loss = 0.00327694\n",
      "Iteration 12232, loss = 0.00327733\n",
      "Iteration 12233, loss = 0.00327690\n",
      "Iteration 12234, loss = 0.00327588\n",
      "Iteration 12235, loss = 0.00327433\n",
      "Iteration 12236, loss = 0.00327217\n",
      "Iteration 12237, loss = 0.00326940\n",
      "Iteration 12238, loss = 0.00326620\n",
      "Iteration 12239, loss = 0.00326271\n",
      "Iteration 12240, loss = 0.00326813\n",
      "Iteration 12241, loss = 0.00326802\n",
      "Iteration 12242, loss = 0.00326057\n",
      "Iteration 12243, loss = 0.00326096\n",
      "Iteration 12244, loss = 0.00326215\n",
      "Iteration 12245, loss = 0.00326247\n",
      "Iteration 12246, loss = 0.00326244\n",
      "Iteration 12247, loss = 0.00326185\n",
      "Iteration 12248, loss = 0.00326034\n",
      "Iteration 12249, loss = 0.00325802\n",
      "Iteration 12250, loss = 0.00325535\n",
      "Iteration 12251, loss = 0.00325246\n",
      "Iteration 12252, loss = 0.00324914\n",
      "Iteration 12253, loss = 0.00324526\n",
      "Iteration 12254, loss = 0.00325435\n",
      "Iteration 12255, loss = 0.00325517\n",
      "Iteration 12256, loss = 0.00324851\n",
      "Iteration 12257, loss = 0.00324189\n",
      "Iteration 12258, loss = 0.00324324\n",
      "Iteration 12259, loss = 0.00324362\n",
      "Iteration 12260, loss = 0.00324323\n",
      "Iteration 12261, loss = 0.00324227\n",
      "Iteration 12262, loss = 0.00324075\n",
      "Iteration 12263, loss = 0.00323858\n",
      "Iteration 12264, loss = 0.00323586\n",
      "Iteration 12265, loss = 0.00323275\n",
      "Iteration 12266, loss = 0.00323020\n",
      "Iteration 12267, loss = 0.00322889\n",
      "Iteration 12268, loss = 0.00322962\n",
      "Iteration 12269, loss = 0.00322954\n",
      "Iteration 12270, loss = 0.00322841\n",
      "Iteration 12271, loss = 0.00322700\n",
      "Iteration 12272, loss = 0.00322543\n",
      "Iteration 12273, loss = 0.00322317\n",
      "Iteration 12274, loss = 0.00322092\n",
      "Iteration 12275, loss = 0.00321919\n",
      "Iteration 12276, loss = 0.00321829\n",
      "Iteration 12277, loss = 0.00321740\n",
      "Iteration 12278, loss = 0.00321696\n",
      "Iteration 12279, loss = 0.00321597\n",
      "Iteration 12280, loss = 0.00321453\n",
      "Iteration 12281, loss = 0.00321304\n",
      "Iteration 12282, loss = 0.00321107\n",
      "Iteration 12283, loss = 0.00321473\n",
      "Iteration 12284, loss = 0.00321045\n",
      "Iteration 12285, loss = 0.00320919\n",
      "Iteration 12286, loss = 0.00321042\n",
      "Iteration 12287, loss = 0.00320983\n",
      "Iteration 12288, loss = 0.00320812\n",
      "Iteration 12289, loss = 0.00320649\n",
      "Iteration 12290, loss = 0.00320494\n",
      "Iteration 12291, loss = 0.00320255\n",
      "Iteration 12292, loss = 0.00319912\n",
      "Iteration 12293, loss = 0.00320382\n",
      "Iteration 12294, loss = 0.00320225\n",
      "Iteration 12295, loss = 0.00319661\n",
      "Iteration 12296, loss = 0.00319669\n",
      "Iteration 12297, loss = 0.00319501\n",
      "Iteration 12298, loss = 0.00319326\n",
      "Iteration 12299, loss = 0.00319202\n",
      "Iteration 12300, loss = 0.00319066\n",
      "Iteration 12301, loss = 0.00318913\n",
      "Iteration 12302, loss = 0.00318784\n",
      "Iteration 12303, loss = 0.00318670\n",
      "Iteration 12304, loss = 0.00318494\n",
      "Iteration 12305, loss = 0.00318742\n",
      "Iteration 12306, loss = 0.00318348\n",
      "Iteration 12307, loss = 0.00318288\n",
      "Iteration 12308, loss = 0.00318348\n",
      "Iteration 12309, loss = 0.00318345\n",
      "Iteration 12310, loss = 0.00318276\n",
      "Iteration 12311, loss = 0.00318133\n",
      "Iteration 12312, loss = 0.00317928\n",
      "Iteration 12313, loss = 0.00317679\n",
      "Iteration 12314, loss = 0.00317394\n",
      "Iteration 12315, loss = 0.00317292\n",
      "Iteration 12316, loss = 0.00317136\n",
      "Iteration 12317, loss = 0.00317088\n",
      "Iteration 12318, loss = 0.00317087\n",
      "Iteration 12319, loss = 0.00317000\n",
      "Iteration 12320, loss = 0.00316876\n",
      "Iteration 12321, loss = 0.00316717\n",
      "Iteration 12322, loss = 0.00316492\n",
      "Iteration 12323, loss = 0.00316199\n",
      "Iteration 12324, loss = 0.00316697\n",
      "Iteration 12325, loss = 0.00316482\n",
      "Iteration 12326, loss = 0.00315966\n",
      "Iteration 12327, loss = 0.00315990\n",
      "Iteration 12328, loss = 0.00315877\n",
      "Iteration 12329, loss = 0.00315734\n",
      "Iteration 12330, loss = 0.00315597\n",
      "Iteration 12331, loss = 0.00315403\n",
      "Iteration 12332, loss = 0.00315393\n",
      "Iteration 12333, loss = 0.00315006\n",
      "Iteration 12334, loss = 0.00314921\n",
      "Iteration 12335, loss = 0.00314939\n",
      "Iteration 12336, loss = 0.00314832\n",
      "Iteration 12337, loss = 0.00314734\n",
      "Iteration 12338, loss = 0.00314583\n",
      "Iteration 12339, loss = 0.00314441\n",
      "Iteration 12340, loss = 0.00314262\n",
      "Iteration 12341, loss = 0.00314614\n",
      "Iteration 12342, loss = 0.00314174\n",
      "Iteration 12343, loss = 0.00314067\n",
      "Iteration 12344, loss = 0.00314201\n",
      "Iteration 12345, loss = 0.00314154\n",
      "Iteration 12346, loss = 0.00313980\n",
      "Iteration 12347, loss = 0.00313813\n",
      "Iteration 12348, loss = 0.00313664\n",
      "Iteration 12349, loss = 0.00313435\n",
      "Iteration 12350, loss = 0.00313095\n",
      "Iteration 12351, loss = 0.00313512\n",
      "Iteration 12352, loss = 0.00313355\n",
      "Iteration 12353, loss = 0.00312847\n",
      "Iteration 12354, loss = 0.00312859\n",
      "Iteration 12355, loss = 0.00312687\n",
      "Iteration 12356, loss = 0.00312509\n",
      "Iteration 12357, loss = 0.00312388\n",
      "Iteration 12358, loss = 0.00312254\n",
      "Iteration 12359, loss = 0.00312106\n",
      "Iteration 12360, loss = 0.00311974\n",
      "Iteration 12361, loss = 0.00311860\n",
      "Iteration 12362, loss = 0.00311692\n",
      "Iteration 12363, loss = 0.00311951\n",
      "Iteration 12364, loss = 0.00311566\n",
      "Iteration 12365, loss = 0.00311483\n",
      "Iteration 12366, loss = 0.00311541\n",
      "Iteration 12367, loss = 0.00311539\n",
      "Iteration 12368, loss = 0.00311472\n",
      "Iteration 12369, loss = 0.00311332\n",
      "Iteration 12370, loss = 0.00311128\n",
      "Iteration 12371, loss = 0.00310883\n",
      "Iteration 12372, loss = 0.00310604\n",
      "Iteration 12373, loss = 0.00310555\n",
      "Iteration 12374, loss = 0.00310404\n",
      "Iteration 12375, loss = 0.00310298\n",
      "Iteration 12376, loss = 0.00310298\n",
      "Iteration 12377, loss = 0.00310214\n",
      "Iteration 12378, loss = 0.00310094\n",
      "Iteration 12379, loss = 0.00309936\n",
      "Iteration 12380, loss = 0.00309713\n",
      "Iteration 12381, loss = 0.00309473\n",
      "Iteration 12382, loss = 0.00309358\n",
      "Iteration 12383, loss = 0.00309269\n",
      "Iteration 12384, loss = 0.00309122\n",
      "Iteration 12385, loss = 0.00309126\n",
      "Iteration 12386, loss = 0.00309045\n",
      "Iteration 12387, loss = 0.00308919\n",
      "Iteration 12388, loss = 0.00308773\n",
      "Iteration 12389, loss = 0.00308576\n",
      "Iteration 12390, loss = 0.00308835\n",
      "Iteration 12391, loss = 0.00308435\n",
      "Iteration 12392, loss = 0.00308410\n",
      "Iteration 12393, loss = 0.00308521\n",
      "Iteration 12394, loss = 0.00308460\n",
      "Iteration 12395, loss = 0.00308302\n",
      "Iteration 12396, loss = 0.00308156\n",
      "Iteration 12397, loss = 0.00308004\n",
      "Iteration 12398, loss = 0.00307764\n",
      "Iteration 12399, loss = 0.00307430\n",
      "Iteration 12400, loss = 0.00307810\n",
      "Iteration 12401, loss = 0.00307668\n",
      "Iteration 12402, loss = 0.00307194\n",
      "Iteration 12403, loss = 0.00307193\n",
      "Iteration 12404, loss = 0.00307032\n",
      "Iteration 12405, loss = 0.00306871\n",
      "Iteration 12406, loss = 0.00306755\n",
      "Iteration 12407, loss = 0.00306571\n",
      "Iteration 12408, loss = 0.00306911\n",
      "Iteration 12409, loss = 0.00306490\n",
      "Iteration 12410, loss = 0.00306338\n",
      "Iteration 12411, loss = 0.00306482\n",
      "Iteration 12412, loss = 0.00306401\n",
      "Iteration 12413, loss = 0.00306193\n",
      "Iteration 12414, loss = 0.00306030\n",
      "Iteration 12415, loss = 0.00305901\n",
      "Iteration 12416, loss = 0.00305673\n",
      "Iteration 12417, loss = 0.00305546\n",
      "Iteration 12418, loss = 0.00305195\n",
      "Iteration 12419, loss = 0.00305412\n",
      "Iteration 12420, loss = 0.00305539\n",
      "Iteration 12421, loss = 0.00305408\n",
      "Iteration 12422, loss = 0.00305169\n",
      "Iteration 12423, loss = 0.00305014\n",
      "Iteration 12424, loss = 0.00304895\n",
      "Iteration 12425, loss = 0.00304645\n",
      "Iteration 12426, loss = 0.00304636\n",
      "Iteration 12427, loss = 0.00304311\n",
      "Iteration 12428, loss = 0.00304371\n",
      "Iteration 12429, loss = 0.00304492\n",
      "Iteration 12430, loss = 0.00304334\n",
      "Iteration 12431, loss = 0.00304084\n",
      "Iteration 12432, loss = 0.00303938\n",
      "Iteration 12433, loss = 0.00303825\n",
      "Iteration 12434, loss = 0.00303569\n",
      "Iteration 12435, loss = 0.00303867\n",
      "Iteration 12436, loss = 0.00303550\n",
      "Iteration 12437, loss = 0.00303280\n",
      "Iteration 12438, loss = 0.00303400\n",
      "Iteration 12439, loss = 0.00303244\n",
      "Iteration 12440, loss = 0.00302993\n",
      "Iteration 12441, loss = 0.00302846\n",
      "Iteration 12442, loss = 0.00302733\n",
      "Iteration 12443, loss = 0.00302740\n",
      "Iteration 12444, loss = 0.00302289\n",
      "Iteration 12445, loss = 0.00302160\n",
      "Iteration 12446, loss = 0.00302195\n",
      "Iteration 12447, loss = 0.00302183\n",
      "Iteration 12448, loss = 0.00302075\n",
      "Iteration 12449, loss = 0.00301861\n",
      "Iteration 12450, loss = 0.00301698\n",
      "Iteration 12451, loss = 0.00301641\n",
      "Iteration 12452, loss = 0.00301518\n",
      "Iteration 12453, loss = 0.00301380\n",
      "Iteration 12454, loss = 0.00301237\n",
      "Iteration 12455, loss = 0.00301083\n",
      "Iteration 12456, loss = 0.00301226\n",
      "Iteration 12457, loss = 0.00300833\n",
      "Iteration 12458, loss = 0.00300967\n",
      "Iteration 12459, loss = 0.00301005\n",
      "Iteration 12460, loss = 0.00300993\n",
      "Iteration 12461, loss = 0.00300946\n",
      "Iteration 12462, loss = 0.00300829\n",
      "Iteration 12463, loss = 0.00300628\n",
      "Iteration 12464, loss = 0.00300378\n",
      "Iteration 12465, loss = 0.00300107\n",
      "Iteration 12466, loss = 0.00299808\n",
      "Iteration 12467, loss = 0.00300446\n",
      "Iteration 12468, loss = 0.00300404\n",
      "Iteration 12469, loss = 0.00299676\n",
      "Iteration 12470, loss = 0.00299644\n",
      "Iteration 12471, loss = 0.00299776\n",
      "Iteration 12472, loss = 0.00299832\n",
      "Iteration 12473, loss = 0.00299834\n",
      "Iteration 12474, loss = 0.00299774\n",
      "Iteration 12475, loss = 0.00299636\n",
      "Iteration 12476, loss = 0.00299436\n",
      "Iteration 12477, loss = 0.00299197\n",
      "Iteration 12478, loss = 0.00298925\n",
      "Iteration 12479, loss = 0.00298612\n",
      "Iteration 12480, loss = 0.00298256\n",
      "Iteration 12481, loss = 0.00298958\n",
      "Iteration 12482, loss = 0.00299030\n",
      "Iteration 12483, loss = 0.00298410\n",
      "Iteration 12484, loss = 0.00297970\n",
      "Iteration 12485, loss = 0.00298094\n",
      "Iteration 12486, loss = 0.00298123\n",
      "Iteration 12487, loss = 0.00298087\n",
      "Iteration 12488, loss = 0.00298004\n",
      "Iteration 12489, loss = 0.00297866\n",
      "Iteration 12490, loss = 0.00297661\n",
      "Iteration 12491, loss = 0.00297402\n",
      "Iteration 12492, loss = 0.00297113\n",
      "Iteration 12493, loss = 0.00296796\n",
      "Iteration 12494, loss = 0.00297504\n",
      "Iteration 12495, loss = 0.00297498\n",
      "Iteration 12496, loss = 0.00296810\n",
      "Iteration 12497, loss = 0.00296613\n",
      "Iteration 12498, loss = 0.00296733\n",
      "Iteration 12499, loss = 0.00296775\n",
      "Iteration 12500, loss = 0.00296773\n",
      "Iteration 12501, loss = 0.00296711\n",
      "Iteration 12502, loss = 0.00296573\n",
      "Iteration 12503, loss = 0.00296368\n",
      "Iteration 12504, loss = 0.00296126\n",
      "Iteration 12505, loss = 0.00295856\n",
      "Iteration 12506, loss = 0.00295546\n",
      "Iteration 12507, loss = 0.00295435\n",
      "Iteration 12508, loss = 0.00295414\n",
      "Iteration 12509, loss = 0.00295122\n",
      "Iteration 12510, loss = 0.00295101\n",
      "Iteration 12511, loss = 0.00295009\n",
      "Iteration 12512, loss = 0.00294873\n",
      "Iteration 12513, loss = 0.00294695\n",
      "Iteration 12514, loss = 0.00294613\n",
      "Iteration 12515, loss = 0.00294416\n",
      "Iteration 12516, loss = 0.00294331\n",
      "Iteration 12517, loss = 0.00294191\n",
      "Iteration 12518, loss = 0.00294266\n",
      "Iteration 12519, loss = 0.00293975\n",
      "Iteration 12520, loss = 0.00293894\n",
      "Iteration 12521, loss = 0.00293764\n",
      "Iteration 12522, loss = 0.00293685\n",
      "Iteration 12523, loss = 0.00293600\n",
      "Iteration 12524, loss = 0.00293546\n",
      "Iteration 12525, loss = 0.00293439\n",
      "Iteration 12526, loss = 0.00293274\n",
      "Iteration 12527, loss = 0.00293057\n",
      "Iteration 12528, loss = 0.00293462\n",
      "Iteration 12529, loss = 0.00293161\n",
      "Iteration 12530, loss = 0.00292877\n",
      "Iteration 12531, loss = 0.00292934\n",
      "Iteration 12532, loss = 0.00292903\n",
      "Iteration 12533, loss = 0.00292808\n",
      "Iteration 12534, loss = 0.00292668\n",
      "Iteration 12535, loss = 0.00292483\n",
      "Iteration 12536, loss = 0.00292245\n",
      "Iteration 12537, loss = 0.00292023\n",
      "Iteration 12538, loss = 0.00291888\n",
      "Iteration 12539, loss = 0.00291779\n",
      "Iteration 12540, loss = 0.00291888\n",
      "Iteration 12541, loss = 0.00291607\n",
      "Iteration 12542, loss = 0.00291529\n",
      "Iteration 12543, loss = 0.00291405\n",
      "Iteration 12544, loss = 0.00291252\n",
      "Iteration 12545, loss = 0.00291429\n",
      "Iteration 12546, loss = 0.00291007\n",
      "Iteration 12547, loss = 0.00290925\n",
      "Iteration 12548, loss = 0.00290802\n",
      "Iteration 12549, loss = 0.00290890\n",
      "Iteration 12550, loss = 0.00290626\n",
      "Iteration 12551, loss = 0.00290552\n",
      "Iteration 12552, loss = 0.00290424\n",
      "Iteration 12553, loss = 0.00290261\n",
      "Iteration 12554, loss = 0.00290493\n",
      "Iteration 12555, loss = 0.00290052\n",
      "Iteration 12556, loss = 0.00290184\n",
      "Iteration 12557, loss = 0.00290293\n",
      "Iteration 12558, loss = 0.00290287\n",
      "Iteration 12559, loss = 0.00290185\n",
      "Iteration 12560, loss = 0.00290049\n",
      "Iteration 12561, loss = 0.00289894\n",
      "Iteration 12562, loss = 0.00289685\n",
      "Iteration 12563, loss = 0.00289401\n",
      "Iteration 12564, loss = 0.00289079\n",
      "Iteration 12565, loss = 0.00289616\n",
      "Iteration 12566, loss = 0.00289576\n",
      "Iteration 12567, loss = 0.00288883\n",
      "Iteration 12568, loss = 0.00289027\n",
      "Iteration 12569, loss = 0.00289109\n",
      "Iteration 12570, loss = 0.00289130\n",
      "Iteration 12571, loss = 0.00289157\n",
      "Iteration 12572, loss = 0.00289131\n",
      "Iteration 12573, loss = 0.00288986\n",
      "Iteration 12574, loss = 0.00288761\n",
      "Iteration 12575, loss = 0.00288527\n",
      "Iteration 12576, loss = 0.00288287\n",
      "Iteration 12577, loss = 0.00287993\n",
      "Iteration 12578, loss = 0.00287633\n",
      "Iteration 12579, loss = 0.00288127\n",
      "Iteration 12580, loss = 0.00288213\n",
      "Iteration 12581, loss = 0.00287620\n",
      "Iteration 12582, loss = 0.00287323\n",
      "Iteration 12583, loss = 0.00287450\n",
      "Iteration 12584, loss = 0.00287487\n",
      "Iteration 12585, loss = 0.00287448\n",
      "Iteration 12586, loss = 0.00287357\n",
      "Iteration 12587, loss = 0.00287221\n",
      "Iteration 12588, loss = 0.00287027\n",
      "Iteration 12589, loss = 0.00286780\n",
      "Iteration 12590, loss = 0.00286495\n",
      "Iteration 12591, loss = 0.00286185\n",
      "Iteration 12592, loss = 0.00286796\n",
      "Iteration 12593, loss = 0.00286795\n",
      "Iteration 12594, loss = 0.00286134\n",
      "Iteration 12595, loss = 0.00286023\n",
      "Iteration 12596, loss = 0.00286132\n",
      "Iteration 12597, loss = 0.00286165\n",
      "Iteration 12598, loss = 0.00286165\n",
      "Iteration 12599, loss = 0.00286113\n",
      "Iteration 12600, loss = 0.00285976\n",
      "Iteration 12601, loss = 0.00285770\n",
      "Iteration 12602, loss = 0.00285536\n",
      "Iteration 12603, loss = 0.00285280\n",
      "Iteration 12604, loss = 0.00284981\n",
      "Iteration 12605, loss = 0.00284827\n",
      "Iteration 12606, loss = 0.00284810\n",
      "Iteration 12607, loss = 0.00284561\n",
      "Iteration 12608, loss = 0.00284539\n",
      "Iteration 12609, loss = 0.00284454\n",
      "Iteration 12610, loss = 0.00284325\n",
      "Iteration 12611, loss = 0.00284152\n",
      "Iteration 12612, loss = 0.00284028\n",
      "Iteration 12613, loss = 0.00283883\n",
      "Iteration 12614, loss = 0.00283803\n",
      "Iteration 12615, loss = 0.00283666\n",
      "Iteration 12616, loss = 0.00283707\n",
      "Iteration 12617, loss = 0.00283454\n",
      "Iteration 12618, loss = 0.00283379\n",
      "Iteration 12619, loss = 0.00283255\n",
      "Iteration 12620, loss = 0.00283146\n",
      "Iteration 12621, loss = 0.00283095\n",
      "Iteration 12622, loss = 0.00283046\n",
      "Iteration 12623, loss = 0.00282943\n",
      "Iteration 12624, loss = 0.00282780\n",
      "Iteration 12625, loss = 0.00282570\n",
      "Iteration 12626, loss = 0.00282941\n",
      "Iteration 12627, loss = 0.00282651\n",
      "Iteration 12628, loss = 0.00282399\n",
      "Iteration 12629, loss = 0.00282455\n",
      "Iteration 12630, loss = 0.00282426\n",
      "Iteration 12631, loss = 0.00282331\n",
      "Iteration 12632, loss = 0.00282194\n",
      "Iteration 12633, loss = 0.00282017\n",
      "Iteration 12634, loss = 0.00281789\n",
      "Iteration 12635, loss = 0.00281573\n",
      "Iteration 12636, loss = 0.00281439\n",
      "Iteration 12637, loss = 0.00281334\n",
      "Iteration 12638, loss = 0.00281442\n",
      "Iteration 12639, loss = 0.00281170\n",
      "Iteration 12640, loss = 0.00281094\n",
      "Iteration 12641, loss = 0.00280974\n",
      "Iteration 12642, loss = 0.00280827\n",
      "Iteration 12643, loss = 0.00281010\n",
      "Iteration 12644, loss = 0.00280588\n",
      "Iteration 12645, loss = 0.00280509\n",
      "Iteration 12646, loss = 0.00280393\n",
      "Iteration 12647, loss = 0.00280489\n",
      "Iteration 12648, loss = 0.00280220\n",
      "Iteration 12649, loss = 0.00280149\n",
      "Iteration 12650, loss = 0.00280026\n",
      "Iteration 12651, loss = 0.00279870\n",
      "Iteration 12652, loss = 0.00280106\n",
      "Iteration 12653, loss = 0.00279677\n",
      "Iteration 12654, loss = 0.00279797\n",
      "Iteration 12655, loss = 0.00279905\n",
      "Iteration 12656, loss = 0.00279897\n",
      "Iteration 12657, loss = 0.00279797\n",
      "Iteration 12658, loss = 0.00279670\n",
      "Iteration 12659, loss = 0.00279523\n",
      "Iteration 12660, loss = 0.00279316\n",
      "Iteration 12661, loss = 0.00279039\n",
      "Iteration 12662, loss = 0.00278730\n",
      "Iteration 12663, loss = 0.00279259\n",
      "Iteration 12664, loss = 0.00279224\n",
      "Iteration 12665, loss = 0.00278552\n",
      "Iteration 12666, loss = 0.00278676\n",
      "Iteration 12667, loss = 0.00278757\n",
      "Iteration 12668, loss = 0.00278783\n",
      "Iteration 12669, loss = 0.00278812\n",
      "Iteration 12670, loss = 0.00278784\n",
      "Iteration 12671, loss = 0.00278643\n",
      "Iteration 12672, loss = 0.00278426\n",
      "Iteration 12673, loss = 0.00278203\n",
      "Iteration 12674, loss = 0.00277972\n",
      "Iteration 12675, loss = 0.00277684\n",
      "Iteration 12676, loss = 0.00277334\n",
      "Iteration 12677, loss = 0.00277823\n",
      "Iteration 12678, loss = 0.00277906\n",
      "Iteration 12679, loss = 0.00277332\n",
      "Iteration 12680, loss = 0.00277040\n",
      "Iteration 12681, loss = 0.00277165\n",
      "Iteration 12682, loss = 0.00277199\n",
      "Iteration 12683, loss = 0.00277160\n",
      "Iteration 12684, loss = 0.00277073\n",
      "Iteration 12685, loss = 0.00276943\n",
      "Iteration 12686, loss = 0.00276755\n",
      "Iteration 12687, loss = 0.00276515\n",
      "Iteration 12688, loss = 0.00276240\n",
      "Iteration 12689, loss = 0.00275940\n",
      "Iteration 12690, loss = 0.00276546\n",
      "Iteration 12691, loss = 0.00276545\n",
      "Iteration 12692, loss = 0.00275904\n",
      "Iteration 12693, loss = 0.00275781\n",
      "Iteration 12694, loss = 0.00275889\n",
      "Iteration 12695, loss = 0.00275927\n",
      "Iteration 12696, loss = 0.00275927\n",
      "Iteration 12697, loss = 0.00275874\n",
      "Iteration 12698, loss = 0.00275742\n",
      "Iteration 12699, loss = 0.00275546\n",
      "Iteration 12700, loss = 0.00275320\n",
      "Iteration 12701, loss = 0.00275072\n",
      "Iteration 12702, loss = 0.00274781\n",
      "Iteration 12703, loss = 0.00274645\n",
      "Iteration 12704, loss = 0.00274630\n",
      "Iteration 12705, loss = 0.00274375\n",
      "Iteration 12706, loss = 0.00274356\n",
      "Iteration 12707, loss = 0.00274274\n",
      "Iteration 12708, loss = 0.00274148\n",
      "Iteration 12709, loss = 0.00273979\n",
      "Iteration 12710, loss = 0.00273884\n",
      "Iteration 12711, loss = 0.00273720\n",
      "Iteration 12712, loss = 0.00273644\n",
      "Iteration 12713, loss = 0.00273510\n",
      "Iteration 12714, loss = 0.00273578\n",
      "Iteration 12715, loss = 0.00273305\n",
      "Iteration 12716, loss = 0.00273233\n",
      "Iteration 12717, loss = 0.00273114\n",
      "Iteration 12718, loss = 0.00273038\n",
      "Iteration 12719, loss = 0.00272960\n",
      "Iteration 12720, loss = 0.00272914\n",
      "Iteration 12721, loss = 0.00272814\n",
      "Iteration 12722, loss = 0.00272657\n",
      "Iteration 12723, loss = 0.00272453\n",
      "Iteration 12724, loss = 0.00272846\n",
      "Iteration 12725, loss = 0.00272566\n",
      "Iteration 12726, loss = 0.00272288\n",
      "Iteration 12727, loss = 0.00272343\n",
      "Iteration 12728, loss = 0.00272316\n",
      "Iteration 12729, loss = 0.00272225\n",
      "Iteration 12730, loss = 0.00272094\n",
      "Iteration 12731, loss = 0.00271922\n",
      "Iteration 12732, loss = 0.00271700\n",
      "Iteration 12733, loss = 0.00271520\n",
      "Iteration 12734, loss = 0.00271365\n",
      "Iteration 12735, loss = 0.00271264\n",
      "Iteration 12736, loss = 0.00271399\n",
      "Iteration 12737, loss = 0.00271100\n",
      "Iteration 12738, loss = 0.00271029\n",
      "Iteration 12739, loss = 0.00270915\n",
      "Iteration 12740, loss = 0.00270774\n",
      "Iteration 12741, loss = 0.00270975\n",
      "Iteration 12742, loss = 0.00270544\n",
      "Iteration 12743, loss = 0.00270471\n",
      "Iteration 12744, loss = 0.00270357\n",
      "Iteration 12745, loss = 0.00270474\n",
      "Iteration 12746, loss = 0.00270188\n",
      "Iteration 12747, loss = 0.00270119\n",
      "Iteration 12748, loss = 0.00270003\n",
      "Iteration 12749, loss = 0.00269853\n",
      "Iteration 12750, loss = 0.00270104\n",
      "Iteration 12751, loss = 0.00269689\n",
      "Iteration 12752, loss = 0.00269786\n",
      "Iteration 12753, loss = 0.00269890\n",
      "Iteration 12754, loss = 0.00269880\n",
      "Iteration 12755, loss = 0.00269783\n",
      "Iteration 12756, loss = 0.00269663\n",
      "Iteration 12757, loss = 0.00269522\n",
      "Iteration 12758, loss = 0.00269321\n",
      "Iteration 12759, loss = 0.00269052\n",
      "Iteration 12760, loss = 0.00268754\n",
      "Iteration 12761, loss = 0.00269291\n",
      "Iteration 12762, loss = 0.00269258\n",
      "Iteration 12763, loss = 0.00268605\n",
      "Iteration 12764, loss = 0.00268697\n",
      "Iteration 12765, loss = 0.00268777\n",
      "Iteration 12766, loss = 0.00268810\n",
      "Iteration 12767, loss = 0.00268841\n",
      "Iteration 12768, loss = 0.00268810\n",
      "Iteration 12769, loss = 0.00268670\n",
      "Iteration 12770, loss = 0.00268465\n",
      "Iteration 12771, loss = 0.00268254\n",
      "Iteration 12772, loss = 0.00268029\n",
      "Iteration 12773, loss = 0.00267746\n",
      "Iteration 12774, loss = 0.00267406\n",
      "Iteration 12775, loss = 0.00267907\n",
      "Iteration 12776, loss = 0.00267988\n",
      "Iteration 12777, loss = 0.00267430\n",
      "Iteration 12778, loss = 0.00267130\n",
      "Iteration 12779, loss = 0.00267256\n",
      "Iteration 12780, loss = 0.00267288\n",
      "Iteration 12781, loss = 0.00267247\n",
      "Iteration 12782, loss = 0.00267162\n",
      "Iteration 12783, loss = 0.00267036\n",
      "Iteration 12784, loss = 0.00266856\n",
      "Iteration 12785, loss = 0.00266622\n",
      "Iteration 12786, loss = 0.00266355\n",
      "Iteration 12787, loss = 0.00266066\n",
      "Iteration 12788, loss = 0.00266690\n",
      "Iteration 12789, loss = 0.00266691\n",
      "Iteration 12790, loss = 0.00266072\n",
      "Iteration 12791, loss = 0.00265910\n",
      "Iteration 12792, loss = 0.00266014\n",
      "Iteration 12793, loss = 0.00266053\n",
      "Iteration 12794, loss = 0.00266056\n",
      "Iteration 12795, loss = 0.00266004\n",
      "Iteration 12796, loss = 0.00265873\n",
      "Iteration 12797, loss = 0.00265684\n",
      "Iteration 12798, loss = 0.00265470\n",
      "Iteration 12799, loss = 0.00265229\n",
      "Iteration 12800, loss = 0.00264946\n",
      "Iteration 12801, loss = 0.00264853\n",
      "Iteration 12802, loss = 0.00264839\n",
      "Iteration 12803, loss = 0.00264554\n",
      "Iteration 12804, loss = 0.00264538\n",
      "Iteration 12805, loss = 0.00264460\n",
      "Iteration 12806, loss = 0.00264338\n",
      "Iteration 12807, loss = 0.00264173\n",
      "Iteration 12808, loss = 0.00264113\n",
      "Iteration 12809, loss = 0.00263926\n",
      "Iteration 12810, loss = 0.00263851\n",
      "Iteration 12811, loss = 0.00263721\n",
      "Iteration 12812, loss = 0.00263824\n",
      "Iteration 12813, loss = 0.00263526\n",
      "Iteration 12814, loss = 0.00263458\n",
      "Iteration 12815, loss = 0.00263342\n",
      "Iteration 12816, loss = 0.00263299\n",
      "Iteration 12817, loss = 0.00263194\n",
      "Iteration 12818, loss = 0.00263150\n",
      "Iteration 12819, loss = 0.00263052\n",
      "Iteration 12820, loss = 0.00262903\n",
      "Iteration 12821, loss = 0.00262737\n",
      "Iteration 12822, loss = 0.00262694\n",
      "Iteration 12823, loss = 0.00262622\n",
      "Iteration 12824, loss = 0.00262497\n",
      "Iteration 12825, loss = 0.00262327\n",
      "Iteration 12826, loss = 0.00262627\n",
      "Iteration 12827, loss = 0.00262282\n",
      "Iteration 12828, loss = 0.00262232\n",
      "Iteration 12829, loss = 0.00262306\n",
      "Iteration 12830, loss = 0.00262289\n",
      "Iteration 12831, loss = 0.00262206\n",
      "Iteration 12832, loss = 0.00262091\n",
      "Iteration 12833, loss = 0.00261939\n",
      "Iteration 12834, loss = 0.00261732\n",
      "Iteration 12835, loss = 0.00261475\n",
      "Iteration 12836, loss = 0.00261475\n",
      "Iteration 12837, loss = 0.00261338\n",
      "Iteration 12838, loss = 0.00261263\n",
      "Iteration 12839, loss = 0.00261267\n",
      "Iteration 12840, loss = 0.00261160\n",
      "Iteration 12841, loss = 0.00261041\n",
      "Iteration 12842, loss = 0.00260928\n",
      "Iteration 12843, loss = 0.00260754\n",
      "Iteration 12844, loss = 0.00260624\n",
      "Iteration 12845, loss = 0.00260411\n",
      "Iteration 12846, loss = 0.00260351\n",
      "Iteration 12847, loss = 0.00260318\n",
      "Iteration 12848, loss = 0.00260250\n",
      "Iteration 12849, loss = 0.00260159\n",
      "Iteration 12850, loss = 0.00260045\n",
      "Iteration 12851, loss = 0.00259936\n",
      "Iteration 12852, loss = 0.00259808\n",
      "Iteration 12853, loss = 0.00259716\n",
      "Iteration 12854, loss = 0.00259640\n",
      "Iteration 12855, loss = 0.00259548\n",
      "Iteration 12856, loss = 0.00259391\n",
      "Iteration 12857, loss = 0.00259503\n",
      "Iteration 12858, loss = 0.00259175\n",
      "Iteration 12859, loss = 0.00259250\n",
      "Iteration 12860, loss = 0.00259309\n",
      "Iteration 12861, loss = 0.00259307\n",
      "Iteration 12862, loss = 0.00259249\n",
      "Iteration 12863, loss = 0.00259136\n",
      "Iteration 12864, loss = 0.00258972\n",
      "Iteration 12865, loss = 0.00258767\n",
      "Iteration 12866, loss = 0.00258527\n",
      "Iteration 12867, loss = 0.00258312\n",
      "Iteration 12868, loss = 0.00258182\n",
      "Iteration 12869, loss = 0.00258281\n",
      "Iteration 12870, loss = 0.00258288\n",
      "Iteration 12871, loss = 0.00258213\n",
      "Iteration 12872, loss = 0.00258105\n",
      "Iteration 12873, loss = 0.00257974\n",
      "Iteration 12874, loss = 0.00257790\n",
      "Iteration 12875, loss = 0.00257545\n",
      "Iteration 12876, loss = 0.00257889\n",
      "Iteration 12877, loss = 0.00257719\n",
      "Iteration 12878, loss = 0.00257360\n",
      "Iteration 12879, loss = 0.00257378\n",
      "Iteration 12880, loss = 0.00257276\n",
      "Iteration 12881, loss = 0.00257160\n",
      "Iteration 12882, loss = 0.00257054\n",
      "Iteration 12883, loss = 0.00256891\n",
      "Iteration 12884, loss = 0.00256868\n",
      "Iteration 12885, loss = 0.00256558\n",
      "Iteration 12886, loss = 0.00256499\n",
      "Iteration 12887, loss = 0.00256526\n",
      "Iteration 12888, loss = 0.00256414\n",
      "Iteration 12889, loss = 0.00256325\n",
      "Iteration 12890, loss = 0.00256210\n",
      "Iteration 12891, loss = 0.00256105\n",
      "Iteration 12892, loss = 0.00255988\n",
      "Iteration 12893, loss = 0.00255894\n",
      "Iteration 12894, loss = 0.00255817\n",
      "Iteration 12895, loss = 0.00255728\n",
      "Iteration 12896, loss = 0.00255576\n",
      "Iteration 12897, loss = 0.00255664\n",
      "Iteration 12898, loss = 0.00255337\n",
      "Iteration 12899, loss = 0.00255440\n",
      "Iteration 12900, loss = 0.00255498\n",
      "Iteration 12901, loss = 0.00255496\n",
      "Iteration 12902, loss = 0.00255441\n",
      "Iteration 12903, loss = 0.00255330\n",
      "Iteration 12904, loss = 0.00255170\n",
      "Iteration 12905, loss = 0.00254966\n",
      "Iteration 12906, loss = 0.00254730\n",
      "Iteration 12907, loss = 0.00254474\n",
      "Iteration 12908, loss = 0.00254386\n",
      "Iteration 12909, loss = 0.00254396\n",
      "Iteration 12910, loss = 0.00254291\n",
      "Iteration 12911, loss = 0.00254252\n",
      "Iteration 12912, loss = 0.00254161\n",
      "Iteration 12913, loss = 0.00254037\n",
      "Iteration 12914, loss = 0.00253872\n",
      "Iteration 12915, loss = 0.00253986\n",
      "Iteration 12916, loss = 0.00253652\n",
      "Iteration 12917, loss = 0.00253769\n",
      "Iteration 12918, loss = 0.00253853\n",
      "Iteration 12919, loss = 0.00253806\n",
      "Iteration 12920, loss = 0.00253696\n",
      "Iteration 12921, loss = 0.00253588\n",
      "Iteration 12922, loss = 0.00253455\n",
      "Iteration 12923, loss = 0.00253245\n",
      "Iteration 12924, loss = 0.00252973\n",
      "Iteration 12925, loss = 0.00253049\n",
      "Iteration 12926, loss = 0.00252937\n",
      "Iteration 12927, loss = 0.00252795\n",
      "Iteration 12928, loss = 0.00252783\n",
      "Iteration 12929, loss = 0.00252655\n",
      "Iteration 12930, loss = 0.00252543\n",
      "Iteration 12931, loss = 0.00252450\n",
      "Iteration 12932, loss = 0.00252279\n",
      "Iteration 12933, loss = 0.00252282\n",
      "Iteration 12934, loss = 0.00251946\n",
      "Iteration 12935, loss = 0.00252135\n",
      "Iteration 12936, loss = 0.00252241\n",
      "Iteration 12937, loss = 0.00252142\n",
      "Iteration 12938, loss = 0.00251974\n",
      "Iteration 12939, loss = 0.00251872\n",
      "Iteration 12940, loss = 0.00251772\n",
      "Iteration 12941, loss = 0.00251553\n",
      "Iteration 12942, loss = 0.00251239\n",
      "Iteration 12943, loss = 0.00251608\n",
      "Iteration 12944, loss = 0.00251530\n",
      "Iteration 12945, loss = 0.00251096\n",
      "Iteration 12946, loss = 0.00251056\n",
      "Iteration 12947, loss = 0.00250900\n",
      "Iteration 12948, loss = 0.00250795\n",
      "Iteration 12949, loss = 0.00250724\n",
      "Iteration 12950, loss = 0.00250642\n",
      "Iteration 12951, loss = 0.00250438\n",
      "Iteration 12952, loss = 0.00250359\n",
      "Iteration 12953, loss = 0.00250296\n",
      "Iteration 12954, loss = 0.00250142\n",
      "Iteration 12955, loss = 0.00250408\n",
      "Iteration 12956, loss = 0.00250107\n",
      "Iteration 12957, loss = 0.00249941\n",
      "Iteration 12958, loss = 0.00250000\n",
      "Iteration 12959, loss = 0.00249999\n",
      "Iteration 12960, loss = 0.00249936\n",
      "Iteration 12961, loss = 0.00249820\n",
      "Iteration 12962, loss = 0.00249662\n",
      "Iteration 12963, loss = 0.00249467\n",
      "Iteration 12964, loss = 0.00249233\n",
      "Iteration 12965, loss = 0.00249354\n",
      "Iteration 12966, loss = 0.00249239\n",
      "Iteration 12967, loss = 0.00248968\n",
      "Iteration 12968, loss = 0.00248979\n",
      "Iteration 12969, loss = 0.00248920\n",
      "Iteration 12970, loss = 0.00248818\n",
      "Iteration 12971, loss = 0.00248682\n",
      "Iteration 12972, loss = 0.00248502\n",
      "Iteration 12973, loss = 0.00248530\n",
      "Iteration 12974, loss = 0.00248248\n",
      "Iteration 12975, loss = 0.00248374\n",
      "Iteration 12976, loss = 0.00248440\n",
      "Iteration 12977, loss = 0.00248381\n",
      "Iteration 12978, loss = 0.00248268\n",
      "Iteration 12979, loss = 0.00248160\n",
      "Iteration 12980, loss = 0.00248023\n",
      "Iteration 12981, loss = 0.00247808\n",
      "Iteration 12982, loss = 0.00247534\n",
      "Iteration 12983, loss = 0.00247888\n",
      "Iteration 12984, loss = 0.00247798\n",
      "Iteration 12985, loss = 0.00247346\n",
      "Iteration 12986, loss = 0.00247326\n",
      "Iteration 12987, loss = 0.00247206\n",
      "Iteration 12988, loss = 0.00247101\n",
      "Iteration 12989, loss = 0.00247007\n",
      "Iteration 12990, loss = 0.00246898\n",
      "Iteration 12991, loss = 0.00246750\n",
      "Iteration 12992, loss = 0.00246679\n",
      "Iteration 12993, loss = 0.00246601\n",
      "Iteration 12994, loss = 0.00246440\n",
      "Iteration 12995, loss = 0.00246718\n",
      "Iteration 12996, loss = 0.00246427\n",
      "Iteration 12997, loss = 0.00246268\n",
      "Iteration 12998, loss = 0.00246323\n",
      "Iteration 12999, loss = 0.00246319\n",
      "Iteration 13000, loss = 0.00246258\n",
      "Iteration 13001, loss = 0.00246146\n",
      "Iteration 13002, loss = 0.00245989\n",
      "Iteration 13003, loss = 0.00245794\n",
      "Iteration 13004, loss = 0.00245564\n",
      "Iteration 13005, loss = 0.00245704\n",
      "Iteration 13006, loss = 0.00245589\n",
      "Iteration 13007, loss = 0.00245309\n",
      "Iteration 13008, loss = 0.00245319\n",
      "Iteration 13009, loss = 0.00245256\n",
      "Iteration 13010, loss = 0.00245154\n",
      "Iteration 13011, loss = 0.00245022\n",
      "Iteration 13012, loss = 0.00244845\n",
      "Iteration 13013, loss = 0.00244902\n",
      "Iteration 13014, loss = 0.00244621\n",
      "Iteration 13015, loss = 0.00244719\n",
      "Iteration 13016, loss = 0.00244788\n",
      "Iteration 13017, loss = 0.00244727\n",
      "Iteration 13018, loss = 0.00244611\n",
      "Iteration 13019, loss = 0.00244504\n",
      "Iteration 13020, loss = 0.00244372\n",
      "Iteration 13021, loss = 0.00244161\n",
      "Iteration 13022, loss = 0.00243888\n",
      "Iteration 13023, loss = 0.00244277\n",
      "Iteration 13024, loss = 0.00244188\n",
      "Iteration 13025, loss = 0.00243704\n",
      "Iteration 13026, loss = 0.00243684\n",
      "Iteration 13027, loss = 0.00243566\n",
      "Iteration 13028, loss = 0.00243462\n",
      "Iteration 13029, loss = 0.00243369\n",
      "Iteration 13030, loss = 0.00243301\n",
      "Iteration 13031, loss = 0.00243116\n",
      "Iteration 13032, loss = 0.00243046\n",
      "Iteration 13033, loss = 0.00242969\n",
      "Iteration 13034, loss = 0.00242830\n",
      "Iteration 13035, loss = 0.00242805\n",
      "Iteration 13036, loss = 0.00242716\n",
      "Iteration 13037, loss = 0.00242609\n",
      "Iteration 13038, loss = 0.00242490\n",
      "Iteration 13039, loss = 0.00242589\n",
      "Iteration 13040, loss = 0.00242259\n",
      "Iteration 13041, loss = 0.00242190\n",
      "Iteration 13042, loss = 0.00242095\n",
      "Iteration 13043, loss = 0.00242177\n",
      "Iteration 13044, loss = 0.00241931\n",
      "Iteration 13045, loss = 0.00241857\n",
      "Iteration 13046, loss = 0.00241752\n",
      "Iteration 13047, loss = 0.00241673\n",
      "Iteration 13048, loss = 0.00241626\n",
      "Iteration 13049, loss = 0.00241579\n",
      "Iteration 13050, loss = 0.00241493\n",
      "Iteration 13051, loss = 0.00241362\n",
      "Iteration 13052, loss = 0.00241180\n",
      "Iteration 13053, loss = 0.00241455\n",
      "Iteration 13054, loss = 0.00241212\n",
      "Iteration 13055, loss = 0.00241032\n",
      "Iteration 13056, loss = 0.00241077\n",
      "Iteration 13057, loss = 0.00241055\n",
      "Iteration 13058, loss = 0.00240981\n",
      "Iteration 13059, loss = 0.00240866\n",
      "Iteration 13060, loss = 0.00240709\n",
      "Iteration 13061, loss = 0.00240508\n",
      "Iteration 13062, loss = 0.00240290\n",
      "Iteration 13063, loss = 0.00240217\n",
      "Iteration 13064, loss = 0.00240124\n",
      "Iteration 13065, loss = 0.00240206\n",
      "Iteration 13066, loss = 0.00239972\n",
      "Iteration 13067, loss = 0.00239911\n",
      "Iteration 13068, loss = 0.00239813\n",
      "Iteration 13069, loss = 0.00239684\n",
      "Iteration 13070, loss = 0.00239848\n",
      "Iteration 13071, loss = 0.00239478\n",
      "Iteration 13072, loss = 0.00239418\n",
      "Iteration 13073, loss = 0.00239315\n",
      "Iteration 13074, loss = 0.00239437\n",
      "Iteration 13075, loss = 0.00239158\n",
      "Iteration 13076, loss = 0.00239100\n",
      "Iteration 13077, loss = 0.00238998\n",
      "Iteration 13078, loss = 0.00238890\n",
      "Iteration 13079, loss = 0.00238873\n",
      "Iteration 13080, loss = 0.00238835\n",
      "Iteration 13081, loss = 0.00238750\n",
      "Iteration 13082, loss = 0.00238617\n",
      "Iteration 13083, loss = 0.00238438\n",
      "Iteration 13084, loss = 0.00238674\n",
      "Iteration 13085, loss = 0.00238426\n",
      "Iteration 13086, loss = 0.00238300\n",
      "Iteration 13087, loss = 0.00238348\n",
      "Iteration 13088, loss = 0.00238325\n",
      "Iteration 13089, loss = 0.00238248\n",
      "Iteration 13090, loss = 0.00238134\n",
      "Iteration 13091, loss = 0.00237981\n",
      "Iteration 13092, loss = 0.00237783\n",
      "Iteration 13093, loss = 0.00237545\n",
      "Iteration 13094, loss = 0.00238009\n",
      "Iteration 13095, loss = 0.00237911\n",
      "Iteration 13096, loss = 0.00237318\n",
      "Iteration 13097, loss = 0.00237318\n",
      "Iteration 13098, loss = 0.00237235\n",
      "Iteration 13099, loss = 0.00237131\n",
      "Iteration 13100, loss = 0.00237016\n",
      "Iteration 13101, loss = 0.00237009\n",
      "Iteration 13102, loss = 0.00236796\n",
      "Iteration 13103, loss = 0.00236730\n",
      "Iteration 13104, loss = 0.00236637\n",
      "Iteration 13105, loss = 0.00236593\n",
      "Iteration 13106, loss = 0.00236480\n",
      "Iteration 13107, loss = 0.00236409\n",
      "Iteration 13108, loss = 0.00236305\n",
      "Iteration 13109, loss = 0.00236177\n",
      "Iteration 13110, loss = 0.00236339\n",
      "Iteration 13111, loss = 0.00235971\n",
      "Iteration 13112, loss = 0.00236113\n",
      "Iteration 13113, loss = 0.00236214\n",
      "Iteration 13114, loss = 0.00236200\n",
      "Iteration 13115, loss = 0.00236103\n",
      "Iteration 13116, loss = 0.00235994\n",
      "Iteration 13117, loss = 0.00235876\n",
      "Iteration 13118, loss = 0.00235697\n",
      "Iteration 13119, loss = 0.00235452\n",
      "Iteration 13120, loss = 0.00235185\n",
      "Iteration 13121, loss = 0.00235718\n",
      "Iteration 13122, loss = 0.00235701\n",
      "Iteration 13123, loss = 0.00235126\n",
      "Iteration 13124, loss = 0.00235132\n",
      "Iteration 13125, loss = 0.00235203\n",
      "Iteration 13126, loss = 0.00235243\n",
      "Iteration 13127, loss = 0.00235276\n",
      "Iteration 13128, loss = 0.00235241\n",
      "Iteration 13129, loss = 0.00235111\n",
      "Iteration 13130, loss = 0.00234935\n",
      "Iteration 13131, loss = 0.00234756\n",
      "Iteration 13132, loss = 0.00234556\n",
      "Iteration 13133, loss = 0.00234299\n",
      "Iteration 13134, loss = 0.00233997\n",
      "Iteration 13135, loss = 0.00234557\n",
      "Iteration 13136, loss = 0.00234632\n",
      "Iteration 13137, loss = 0.00234140\n",
      "Iteration 13138, loss = 0.00233761\n",
      "Iteration 13139, loss = 0.00233874\n",
      "Iteration 13140, loss = 0.00233900\n",
      "Iteration 13141, loss = 0.00233862\n",
      "Iteration 13142, loss = 0.00233789\n",
      "Iteration 13143, loss = 0.00233681\n",
      "Iteration 13144, loss = 0.00233523\n",
      "Iteration 13145, loss = 0.00233312\n",
      "Iteration 13146, loss = 0.00233075\n",
      "Iteration 13147, loss = 0.00232927\n",
      "Iteration 13148, loss = 0.00232850\n",
      "Iteration 13149, loss = 0.00232859\n",
      "Iteration 13150, loss = 0.00232842\n",
      "Iteration 13151, loss = 0.00232752\n",
      "Iteration 13152, loss = 0.00232659\n",
      "Iteration 13153, loss = 0.00232549\n",
      "Iteration 13154, loss = 0.00232374\n",
      "Iteration 13155, loss = 0.00232331\n",
      "Iteration 13156, loss = 0.00232080\n",
      "Iteration 13157, loss = 0.00232026\n",
      "Iteration 13158, loss = 0.00232118\n",
      "Iteration 13159, loss = 0.00231908\n",
      "Iteration 13160, loss = 0.00231833\n",
      "Iteration 13161, loss = 0.00231742\n",
      "Iteration 13162, loss = 0.00231642\n",
      "Iteration 13163, loss = 0.00231677\n",
      "Iteration 13164, loss = 0.00231438\n",
      "Iteration 13165, loss = 0.00231380\n",
      "Iteration 13166, loss = 0.00231298\n",
      "Iteration 13167, loss = 0.00231173\n",
      "Iteration 13168, loss = 0.00231156\n",
      "Iteration 13169, loss = 0.00231090\n",
      "Iteration 13170, loss = 0.00230991\n",
      "Iteration 13171, loss = 0.00230871\n",
      "Iteration 13172, loss = 0.00230861\n",
      "Iteration 13173, loss = 0.00230667\n",
      "Iteration 13174, loss = 0.00230606\n",
      "Iteration 13175, loss = 0.00230512\n",
      "Iteration 13176, loss = 0.00230456\n",
      "Iteration 13177, loss = 0.00230361\n",
      "Iteration 13178, loss = 0.00230299\n",
      "Iteration 13179, loss = 0.00230200\n",
      "Iteration 13180, loss = 0.00230073\n",
      "Iteration 13181, loss = 0.00230181\n",
      "Iteration 13182, loss = 0.00229869\n",
      "Iteration 13183, loss = 0.00229808\n",
      "Iteration 13184, loss = 0.00229705\n",
      "Iteration 13185, loss = 0.00229822\n",
      "Iteration 13186, loss = 0.00229553\n",
      "Iteration 13187, loss = 0.00229495\n",
      "Iteration 13188, loss = 0.00229396\n",
      "Iteration 13189, loss = 0.00229320\n",
      "Iteration 13190, loss = 0.00229275\n",
      "Iteration 13191, loss = 0.00229237\n",
      "Iteration 13192, loss = 0.00229156\n",
      "Iteration 13193, loss = 0.00229027\n",
      "Iteration 13194, loss = 0.00228855\n",
      "Iteration 13195, loss = 0.00229132\n",
      "Iteration 13196, loss = 0.00228895\n",
      "Iteration 13197, loss = 0.00228721\n",
      "Iteration 13198, loss = 0.00228769\n",
      "Iteration 13199, loss = 0.00228747\n",
      "Iteration 13200, loss = 0.00228671\n",
      "Iteration 13201, loss = 0.00228562\n",
      "Iteration 13202, loss = 0.00228417\n",
      "Iteration 13203, loss = 0.00228226\n",
      "Iteration 13204, loss = 0.00228035\n",
      "Iteration 13205, loss = 0.00227945\n",
      "Iteration 13206, loss = 0.00227862\n",
      "Iteration 13207, loss = 0.00227954\n",
      "Iteration 13208, loss = 0.00227716\n",
      "Iteration 13209, loss = 0.00227657\n",
      "Iteration 13210, loss = 0.00227564\n",
      "Iteration 13211, loss = 0.00227444\n",
      "Iteration 13212, loss = 0.00227622\n",
      "Iteration 13213, loss = 0.00227256\n",
      "Iteration 13214, loss = 0.00227397\n",
      "Iteration 13215, loss = 0.00227496\n",
      "Iteration 13216, loss = 0.00227483\n",
      "Iteration 13217, loss = 0.00227393\n",
      "Iteration 13218, loss = 0.00227295\n",
      "Iteration 13219, loss = 0.00227184\n",
      "Iteration 13220, loss = 0.00227010\n",
      "Iteration 13221, loss = 0.00226773\n",
      "Iteration 13222, loss = 0.00226520\n",
      "Iteration 13223, loss = 0.00226943\n",
      "Iteration 13224, loss = 0.00226923\n",
      "Iteration 13225, loss = 0.00226366\n",
      "Iteration 13226, loss = 0.00226470\n",
      "Iteration 13227, loss = 0.00226539\n",
      "Iteration 13228, loss = 0.00226580\n",
      "Iteration 13229, loss = 0.00226613\n",
      "Iteration 13230, loss = 0.00226579\n",
      "Iteration 13231, loss = 0.00226454\n",
      "Iteration 13232, loss = 0.00226284\n",
      "Iteration 13233, loss = 0.00226113\n",
      "Iteration 13234, loss = 0.00225919\n",
      "Iteration 13235, loss = 0.00225669\n",
      "Iteration 13236, loss = 0.00225379\n",
      "Iteration 13237, loss = 0.00225826\n",
      "Iteration 13238, loss = 0.00225897\n",
      "Iteration 13239, loss = 0.00225421\n",
      "Iteration 13240, loss = 0.00225155\n",
      "Iteration 13241, loss = 0.00225266\n",
      "Iteration 13242, loss = 0.00225291\n",
      "Iteration 13243, loss = 0.00225252\n",
      "Iteration 13244, loss = 0.00225182\n",
      "Iteration 13245, loss = 0.00225079\n",
      "Iteration 13246, loss = 0.00224923\n",
      "Iteration 13247, loss = 0.00224719\n",
      "Iteration 13248, loss = 0.00224493\n",
      "Iteration 13249, loss = 0.00224274\n",
      "Iteration 13250, loss = 0.00224201\n",
      "Iteration 13251, loss = 0.00224286\n",
      "Iteration 13252, loss = 0.00224266\n",
      "Iteration 13253, loss = 0.00224180\n",
      "Iteration 13254, loss = 0.00224092\n",
      "Iteration 13255, loss = 0.00223987\n",
      "Iteration 13256, loss = 0.00223816\n",
      "Iteration 13257, loss = 0.00223701\n",
      "Iteration 13258, loss = 0.00223535\n",
      "Iteration 13259, loss = 0.00223486\n",
      "Iteration 13260, loss = 0.00223499\n",
      "Iteration 13261, loss = 0.00223366\n",
      "Iteration 13262, loss = 0.00223293\n",
      "Iteration 13263, loss = 0.00223209\n",
      "Iteration 13264, loss = 0.00223114\n",
      "Iteration 13265, loss = 0.00223080\n",
      "Iteration 13266, loss = 0.00222913\n",
      "Iteration 13267, loss = 0.00222860\n",
      "Iteration 13268, loss = 0.00222783\n",
      "Iteration 13269, loss = 0.00222636\n",
      "Iteration 13270, loss = 0.00222863\n",
      "Iteration 13271, loss = 0.00222589\n",
      "Iteration 13272, loss = 0.00222515\n",
      "Iteration 13273, loss = 0.00222568\n",
      "Iteration 13274, loss = 0.00222566\n",
      "Iteration 13275, loss = 0.00222515\n",
      "Iteration 13276, loss = 0.00222417\n",
      "Iteration 13277, loss = 0.00222277\n",
      "Iteration 13278, loss = 0.00222100\n",
      "Iteration 13279, loss = 0.00221892\n",
      "Iteration 13280, loss = 0.00221884\n",
      "Iteration 13281, loss = 0.00221773\n",
      "Iteration 13282, loss = 0.00221677\n",
      "Iteration 13283, loss = 0.00221689\n",
      "Iteration 13284, loss = 0.00221625\n",
      "Iteration 13285, loss = 0.00221531\n",
      "Iteration 13286, loss = 0.00221417\n",
      "Iteration 13287, loss = 0.00221259\n",
      "Iteration 13288, loss = 0.00221166\n",
      "Iteration 13289, loss = 0.00220996\n",
      "Iteration 13290, loss = 0.00220932\n",
      "Iteration 13291, loss = 0.00220934\n",
      "Iteration 13292, loss = 0.00220833\n",
      "Iteration 13293, loss = 0.00220774\n",
      "Iteration 13294, loss = 0.00220683\n",
      "Iteration 13295, loss = 0.00220581\n",
      "Iteration 13296, loss = 0.00220498\n",
      "Iteration 13297, loss = 0.00220397\n",
      "Iteration 13298, loss = 0.00220341\n",
      "Iteration 13299, loss = 0.00220259\n",
      "Iteration 13300, loss = 0.00220117\n",
      "Iteration 13301, loss = 0.00220288\n",
      "Iteration 13302, loss = 0.00220013\n",
      "Iteration 13303, loss = 0.00220008\n",
      "Iteration 13304, loss = 0.00220062\n",
      "Iteration 13305, loss = 0.00220059\n",
      "Iteration 13306, loss = 0.00220007\n",
      "Iteration 13307, loss = 0.00219911\n",
      "Iteration 13308, loss = 0.00219773\n",
      "Iteration 13309, loss = 0.00219597\n",
      "Iteration 13310, loss = 0.00219390\n",
      "Iteration 13311, loss = 0.00219324\n",
      "Iteration 13312, loss = 0.00219216\n",
      "Iteration 13313, loss = 0.00219184\n",
      "Iteration 13314, loss = 0.00219191\n",
      "Iteration 13315, loss = 0.00219124\n",
      "Iteration 13316, loss = 0.00219032\n",
      "Iteration 13317, loss = 0.00218922\n",
      "Iteration 13318, loss = 0.00218766\n",
      "Iteration 13319, loss = 0.00218619\n",
      "Iteration 13320, loss = 0.00218502\n",
      "Iteration 13321, loss = 0.00218443\n",
      "Iteration 13322, loss = 0.00218395\n",
      "Iteration 13323, loss = 0.00218342\n",
      "Iteration 13324, loss = 0.00218279\n",
      "Iteration 13325, loss = 0.00218192\n",
      "Iteration 13326, loss = 0.00218092\n",
      "Iteration 13327, loss = 0.00217972\n",
      "Iteration 13328, loss = 0.00217908\n",
      "Iteration 13329, loss = 0.00217853\n",
      "Iteration 13330, loss = 0.00217772\n",
      "Iteration 13331, loss = 0.00217631\n",
      "Iteration 13332, loss = 0.00217771\n",
      "Iteration 13333, loss = 0.00217502\n",
      "Iteration 13334, loss = 0.00217521\n",
      "Iteration 13335, loss = 0.00217573\n",
      "Iteration 13336, loss = 0.00217569\n",
      "Iteration 13337, loss = 0.00217518\n",
      "Iteration 13338, loss = 0.00217424\n",
      "Iteration 13339, loss = 0.00217286\n",
      "Iteration 13340, loss = 0.00217112\n",
      "Iteration 13341, loss = 0.00216907\n",
      "Iteration 13342, loss = 0.00216835\n",
      "Iteration 13343, loss = 0.00216731\n",
      "Iteration 13344, loss = 0.00216702\n",
      "Iteration 13345, loss = 0.00216709\n",
      "Iteration 13346, loss = 0.00216644\n",
      "Iteration 13347, loss = 0.00216553\n",
      "Iteration 13348, loss = 0.00216443\n",
      "Iteration 13349, loss = 0.00216288\n",
      "Iteration 13350, loss = 0.00216146\n",
      "Iteration 13351, loss = 0.00216028\n",
      "Iteration 13352, loss = 0.00215969\n",
      "Iteration 13353, loss = 0.00215927\n",
      "Iteration 13354, loss = 0.00215868\n",
      "Iteration 13355, loss = 0.00215807\n",
      "Iteration 13356, loss = 0.00215720\n",
      "Iteration 13357, loss = 0.00215621\n",
      "Iteration 13358, loss = 0.00215511\n",
      "Iteration 13359, loss = 0.00215439\n",
      "Iteration 13360, loss = 0.00215386\n",
      "Iteration 13361, loss = 0.00215305\n",
      "Iteration 13362, loss = 0.00215164\n",
      "Iteration 13363, loss = 0.00215315\n",
      "Iteration 13364, loss = 0.00215050\n",
      "Iteration 13365, loss = 0.00215056\n",
      "Iteration 13366, loss = 0.00215107\n",
      "Iteration 13367, loss = 0.00215104\n",
      "Iteration 13368, loss = 0.00215053\n",
      "Iteration 13369, loss = 0.00214958\n",
      "Iteration 13370, loss = 0.00214823\n",
      "Iteration 13371, loss = 0.00214651\n",
      "Iteration 13372, loss = 0.00214448\n",
      "Iteration 13373, loss = 0.00214392\n",
      "Iteration 13374, loss = 0.00214291\n",
      "Iteration 13375, loss = 0.00214245\n",
      "Iteration 13376, loss = 0.00214252\n",
      "Iteration 13377, loss = 0.00214188\n",
      "Iteration 13378, loss = 0.00214097\n",
      "Iteration 13379, loss = 0.00213990\n",
      "Iteration 13380, loss = 0.00213836\n",
      "Iteration 13381, loss = 0.00213714\n",
      "Iteration 13382, loss = 0.00213579\n",
      "Iteration 13383, loss = 0.00213522\n",
      "Iteration 13384, loss = 0.00213500\n",
      "Iteration 13385, loss = 0.00213421\n",
      "Iteration 13386, loss = 0.00213360\n",
      "Iteration 13387, loss = 0.00213276\n",
      "Iteration 13388, loss = 0.00213179\n",
      "Iteration 13389, loss = 0.00213087\n",
      "Iteration 13390, loss = 0.00212998\n",
      "Iteration 13391, loss = 0.00212946\n",
      "Iteration 13392, loss = 0.00212867\n",
      "Iteration 13393, loss = 0.00212727\n",
      "Iteration 13394, loss = 0.00212894\n",
      "Iteration 13395, loss = 0.00212633\n",
      "Iteration 13396, loss = 0.00212619\n",
      "Iteration 13397, loss = 0.00212670\n",
      "Iteration 13398, loss = 0.00212668\n",
      "Iteration 13399, loss = 0.00212618\n",
      "Iteration 13400, loss = 0.00212524\n",
      "Iteration 13401, loss = 0.00212390\n",
      "Iteration 13402, loss = 0.00212221\n",
      "Iteration 13403, loss = 0.00212021\n",
      "Iteration 13404, loss = 0.00211985\n",
      "Iteration 13405, loss = 0.00211882\n",
      "Iteration 13406, loss = 0.00211818\n",
      "Iteration 13407, loss = 0.00211826\n",
      "Iteration 13408, loss = 0.00211763\n",
      "Iteration 13409, loss = 0.00211674\n",
      "Iteration 13410, loss = 0.00211566\n",
      "Iteration 13411, loss = 0.00211414\n",
      "Iteration 13412, loss = 0.00211312\n",
      "Iteration 13413, loss = 0.00211162\n",
      "Iteration 13414, loss = 0.00211104\n",
      "Iteration 13415, loss = 0.00211101\n",
      "Iteration 13416, loss = 0.00211004\n",
      "Iteration 13417, loss = 0.00210945\n",
      "Iteration 13418, loss = 0.00210861\n",
      "Iteration 13419, loss = 0.00210766\n",
      "Iteration 13420, loss = 0.00210691\n",
      "Iteration 13421, loss = 0.00210586\n",
      "Iteration 13422, loss = 0.00210535\n",
      "Iteration 13423, loss = 0.00210457\n",
      "Iteration 13424, loss = 0.00210318\n",
      "Iteration 13425, loss = 0.00210503\n",
      "Iteration 13426, loss = 0.00210246\n",
      "Iteration 13427, loss = 0.00210213\n",
      "Iteration 13428, loss = 0.00210265\n",
      "Iteration 13429, loss = 0.00210262\n",
      "Iteration 13430, loss = 0.00210213\n",
      "Iteration 13431, loss = 0.00210120\n",
      "Iteration 13432, loss = 0.00209987\n",
      "Iteration 13433, loss = 0.00209819\n",
      "Iteration 13434, loss = 0.00209621\n",
      "Iteration 13435, loss = 0.00209608\n",
      "Iteration 13436, loss = 0.00209507\n",
      "Iteration 13437, loss = 0.00209421\n",
      "Iteration 13438, loss = 0.00209430\n",
      "Iteration 13439, loss = 0.00209367\n",
      "Iteration 13440, loss = 0.00209278\n",
      "Iteration 13441, loss = 0.00209172\n",
      "Iteration 13442, loss = 0.00209022\n",
      "Iteration 13443, loss = 0.00208949\n",
      "Iteration 13444, loss = 0.00208770\n",
      "Iteration 13445, loss = 0.00208714\n",
      "Iteration 13446, loss = 0.00208742\n",
      "Iteration 13447, loss = 0.00208615\n",
      "Iteration 13448, loss = 0.00208557\n",
      "Iteration 13449, loss = 0.00208474\n",
      "Iteration 13450, loss = 0.00208379\n",
      "Iteration 13451, loss = 0.00208335\n",
      "Iteration 13452, loss = 0.00208203\n",
      "Iteration 13453, loss = 0.00208153\n",
      "Iteration 13454, loss = 0.00208076\n",
      "Iteration 13455, loss = 0.00207938\n",
      "Iteration 13456, loss = 0.00208148\n",
      "Iteration 13457, loss = 0.00207892\n",
      "Iteration 13458, loss = 0.00207835\n",
      "Iteration 13459, loss = 0.00207886\n",
      "Iteration 13460, loss = 0.00207884\n",
      "Iteration 13461, loss = 0.00207834\n",
      "Iteration 13462, loss = 0.00207742\n",
      "Iteration 13463, loss = 0.00207612\n",
      "Iteration 13464, loss = 0.00207446\n",
      "Iteration 13465, loss = 0.00207250\n",
      "Iteration 13466, loss = 0.00207259\n",
      "Iteration 13467, loss = 0.00207158\n",
      "Iteration 13468, loss = 0.00207053\n",
      "Iteration 13469, loss = 0.00207061\n",
      "Iteration 13470, loss = 0.00206999\n",
      "Iteration 13471, loss = 0.00206912\n",
      "Iteration 13472, loss = 0.00206807\n",
      "Iteration 13473, loss = 0.00206659\n",
      "Iteration 13474, loss = 0.00206605\n",
      "Iteration 13475, loss = 0.00206411\n",
      "Iteration 13476, loss = 0.00206355\n",
      "Iteration 13477, loss = 0.00206401\n",
      "Iteration 13478, loss = 0.00206257\n",
      "Iteration 13479, loss = 0.00206200\n",
      "Iteration 13480, loss = 0.00206121\n",
      "Iteration 13481, loss = 0.00206027\n",
      "Iteration 13482, loss = 0.00205997\n",
      "Iteration 13483, loss = 0.00205852\n",
      "Iteration 13484, loss = 0.00205803\n",
      "Iteration 13485, loss = 0.00205726\n",
      "Iteration 13486, loss = 0.00205590\n",
      "Iteration 13487, loss = 0.00205814\n",
      "Iteration 13488, loss = 0.00205561\n",
      "Iteration 13489, loss = 0.00205488\n",
      "Iteration 13490, loss = 0.00205538\n",
      "Iteration 13491, loss = 0.00205536\n",
      "Iteration 13492, loss = 0.00205487\n",
      "Iteration 13493, loss = 0.00205397\n",
      "Iteration 13494, loss = 0.00205268\n",
      "Iteration 13495, loss = 0.00205104\n",
      "Iteration 13496, loss = 0.00204910\n",
      "Iteration 13497, loss = 0.00204935\n",
      "Iteration 13498, loss = 0.00204837\n",
      "Iteration 13499, loss = 0.00204715\n",
      "Iteration 13500, loss = 0.00204723\n",
      "Iteration 13501, loss = 0.00204662\n",
      "Iteration 13502, loss = 0.00204575\n",
      "Iteration 13503, loss = 0.00204472\n",
      "Iteration 13504, loss = 0.00204325\n",
      "Iteration 13505, loss = 0.00204290\n",
      "Iteration 13506, loss = 0.00204080\n",
      "Iteration 13507, loss = 0.00204025\n",
      "Iteration 13508, loss = 0.00204088\n",
      "Iteration 13509, loss = 0.00203927\n",
      "Iteration 13510, loss = 0.00203870\n",
      "Iteration 13511, loss = 0.00203793\n",
      "Iteration 13512, loss = 0.00203700\n",
      "Iteration 13513, loss = 0.00203686\n",
      "Iteration 13514, loss = 0.00203527\n",
      "Iteration 13515, loss = 0.00203481\n",
      "Iteration 13516, loss = 0.00203405\n",
      "Iteration 13517, loss = 0.00203269\n",
      "Iteration 13518, loss = 0.00203509\n",
      "Iteration 13519, loss = 0.00203259\n",
      "Iteration 13520, loss = 0.00203169\n",
      "Iteration 13521, loss = 0.00203219\n",
      "Iteration 13522, loss = 0.00203217\n",
      "Iteration 13523, loss = 0.00203169\n",
      "Iteration 13524, loss = 0.00203079\n",
      "Iteration 13525, loss = 0.00202951\n",
      "Iteration 13526, loss = 0.00202789\n",
      "Iteration 13527, loss = 0.00202597\n",
      "Iteration 13528, loss = 0.00202643\n",
      "Iteration 13529, loss = 0.00202546\n",
      "Iteration 13530, loss = 0.00202403\n",
      "Iteration 13531, loss = 0.00202412\n",
      "Iteration 13532, loss = 0.00202353\n",
      "Iteration 13533, loss = 0.00202267\n",
      "Iteration 13534, loss = 0.00202164\n",
      "Iteration 13535, loss = 0.00202019\n",
      "Iteration 13536, loss = 0.00202006\n",
      "Iteration 13537, loss = 0.00201777\n",
      "Iteration 13538, loss = 0.00201721\n",
      "Iteration 13539, loss = 0.00201810\n",
      "Iteration 13540, loss = 0.00201624\n",
      "Iteration 13541, loss = 0.00201570\n",
      "Iteration 13542, loss = 0.00201492\n",
      "Iteration 13543, loss = 0.00201400\n",
      "Iteration 13544, loss = 0.00201411\n",
      "Iteration 13545, loss = 0.00201229\n",
      "Iteration 13546, loss = 0.00201184\n",
      "Iteration 13547, loss = 0.00201108\n",
      "Iteration 13548, loss = 0.00200995\n",
      "Iteration 13549, loss = 0.00200979\n",
      "Iteration 13550, loss = 0.00200928\n",
      "Iteration 13551, loss = 0.00200846\n",
      "Iteration 13552, loss = 0.00200739\n",
      "Iteration 13553, loss = 0.00200728\n",
      "Iteration 13554, loss = 0.00200563\n",
      "Iteration 13555, loss = 0.00200515\n",
      "Iteration 13556, loss = 0.00200429\n",
      "Iteration 13557, loss = 0.00200406\n",
      "Iteration 13558, loss = 0.00200290\n",
      "Iteration 13559, loss = 0.00200240\n",
      "Iteration 13560, loss = 0.00200157\n",
      "Iteration 13561, loss = 0.00200044\n",
      "Iteration 13562, loss = 0.00200179\n",
      "Iteration 13563, loss = 0.00199874\n",
      "Iteration 13564, loss = 0.00200006\n",
      "Iteration 13565, loss = 0.00200084\n",
      "Iteration 13566, loss = 0.00200063\n",
      "Iteration 13567, loss = 0.00199991\n",
      "Iteration 13568, loss = 0.00199912\n",
      "Iteration 13569, loss = 0.00199810\n",
      "Iteration 13570, loss = 0.00199647\n",
      "Iteration 13571, loss = 0.00199439\n",
      "Iteration 13572, loss = 0.00199224\n",
      "Iteration 13573, loss = 0.00199677\n",
      "Iteration 13574, loss = 0.00199663\n",
      "Iteration 13575, loss = 0.00199167\n",
      "Iteration 13576, loss = 0.00199158\n",
      "Iteration 13577, loss = 0.00199232\n",
      "Iteration 13578, loss = 0.00199278\n",
      "Iteration 13579, loss = 0.00199302\n",
      "Iteration 13580, loss = 0.00199264\n",
      "Iteration 13581, loss = 0.00199156\n",
      "Iteration 13582, loss = 0.00199018\n",
      "Iteration 13583, loss = 0.00198869\n",
      "Iteration 13584, loss = 0.00198691\n",
      "Iteration 13585, loss = 0.00198467\n",
      "Iteration 13586, loss = 0.00198217\n",
      "Iteration 13587, loss = 0.00198734\n",
      "Iteration 13588, loss = 0.00198794\n",
      "Iteration 13589, loss = 0.00198374\n",
      "Iteration 13590, loss = 0.00198031\n",
      "Iteration 13591, loss = 0.00198129\n",
      "Iteration 13592, loss = 0.00198145\n",
      "Iteration 13593, loss = 0.00198112\n",
      "Iteration 13594, loss = 0.00198056\n",
      "Iteration 13595, loss = 0.00197967\n",
      "Iteration 13596, loss = 0.00197827\n",
      "Iteration 13597, loss = 0.00197646\n",
      "Iteration 13598, loss = 0.00197449\n",
      "Iteration 13599, loss = 0.00197374\n",
      "Iteration 13600, loss = 0.00197314\n",
      "Iteration 13601, loss = 0.00197261\n",
      "Iteration 13602, loss = 0.00197243\n",
      "Iteration 13603, loss = 0.00197175\n",
      "Iteration 13604, loss = 0.00197105\n",
      "Iteration 13605, loss = 0.00197009\n",
      "Iteration 13606, loss = 0.00196852\n",
      "Iteration 13607, loss = 0.00196856\n",
      "Iteration 13608, loss = 0.00196638\n",
      "Iteration 13609, loss = 0.00196779\n",
      "Iteration 13610, loss = 0.00196824\n",
      "Iteration 13611, loss = 0.00196743\n",
      "Iteration 13612, loss = 0.00196651\n",
      "Iteration 13613, loss = 0.00196592\n",
      "Iteration 13614, loss = 0.00196487\n",
      "Iteration 13615, loss = 0.00196292\n",
      "Iteration 13616, loss = 0.00196064\n",
      "Iteration 13617, loss = 0.00196403\n",
      "Iteration 13618, loss = 0.00196352\n",
      "Iteration 13619, loss = 0.00195940\n",
      "Iteration 13620, loss = 0.00195897\n",
      "Iteration 13621, loss = 0.00195810\n",
      "Iteration 13622, loss = 0.00195757\n",
      "Iteration 13623, loss = 0.00195686\n",
      "Iteration 13624, loss = 0.00195592\n",
      "Iteration 13625, loss = 0.00195459\n",
      "Iteration 13626, loss = 0.00195433\n",
      "Iteration 13627, loss = 0.00195376\n",
      "Iteration 13628, loss = 0.00195224\n",
      "Iteration 13629, loss = 0.00195467\n",
      "Iteration 13630, loss = 0.00195255\n",
      "Iteration 13631, loss = 0.00195092\n",
      "Iteration 13632, loss = 0.00195136\n",
      "Iteration 13633, loss = 0.00195131\n",
      "Iteration 13634, loss = 0.00195083\n",
      "Iteration 13635, loss = 0.00194995\n",
      "Iteration 13636, loss = 0.00194871\n",
      "Iteration 13637, loss = 0.00194714\n",
      "Iteration 13638, loss = 0.00194529\n",
      "Iteration 13639, loss = 0.00194730\n",
      "Iteration 13640, loss = 0.00194639\n",
      "Iteration 13641, loss = 0.00194324\n",
      "Iteration 13642, loss = 0.00194340\n",
      "Iteration 13643, loss = 0.00194291\n",
      "Iteration 13644, loss = 0.00194204\n",
      "Iteration 13645, loss = 0.00194098\n",
      "Iteration 13646, loss = 0.00193961\n",
      "Iteration 13647, loss = 0.00194140\n",
      "Iteration 13648, loss = 0.00193920\n",
      "Iteration 13649, loss = 0.00193855\n",
      "Iteration 13650, loss = 0.00193913\n",
      "Iteration 13651, loss = 0.00193866\n",
      "Iteration 13652, loss = 0.00193774\n",
      "Iteration 13653, loss = 0.00193691\n",
      "Iteration 13654, loss = 0.00193585\n",
      "Iteration 13655, loss = 0.00193414\n",
      "Iteration 13656, loss = 0.00193303\n",
      "Iteration 13657, loss = 0.00193157\n",
      "Iteration 13658, loss = 0.00193137\n",
      "Iteration 13659, loss = 0.00193181\n",
      "Iteration 13660, loss = 0.00193128\n",
      "Iteration 13661, loss = 0.00193034\n",
      "Iteration 13662, loss = 0.00192966\n",
      "Iteration 13663, loss = 0.00192876\n",
      "Iteration 13664, loss = 0.00192712\n",
      "Iteration 13665, loss = 0.00192639\n",
      "Iteration 13666, loss = 0.00192603\n",
      "Iteration 13667, loss = 0.00192534\n",
      "Iteration 13668, loss = 0.00192485\n",
      "Iteration 13669, loss = 0.00192350\n",
      "Iteration 13670, loss = 0.00192288\n",
      "Iteration 13671, loss = 0.00192217\n",
      "Iteration 13672, loss = 0.00192187\n",
      "Iteration 13673, loss = 0.00192098\n",
      "Iteration 13674, loss = 0.00192061\n",
      "Iteration 13675, loss = 0.00192002\n",
      "Iteration 13676, loss = 0.00191892\n",
      "Iteration 13677, loss = 0.00191781\n",
      "Iteration 13678, loss = 0.00191719\n",
      "Iteration 13679, loss = 0.00191663\n",
      "Iteration 13680, loss = 0.00191574\n",
      "Iteration 13681, loss = 0.00191546\n",
      "Iteration 13682, loss = 0.00191451\n",
      "Iteration 13683, loss = 0.00191417\n",
      "Iteration 13684, loss = 0.00191341\n",
      "Iteration 13685, loss = 0.00191222\n",
      "Iteration 13686, loss = 0.00191274\n",
      "Iteration 13687, loss = 0.00191057\n",
      "Iteration 13688, loss = 0.00191001\n",
      "Iteration 13689, loss = 0.00190912\n",
      "Iteration 13690, loss = 0.00190946\n",
      "Iteration 13691, loss = 0.00190936\n",
      "Iteration 13692, loss = 0.00190878\n",
      "Iteration 13693, loss = 0.00190780\n",
      "Iteration 13694, loss = 0.00190653\n",
      "Iteration 13695, loss = 0.00190518\n",
      "Iteration 13696, loss = 0.00190468\n",
      "Iteration 13697, loss = 0.00190406\n",
      "Iteration 13698, loss = 0.00190307\n",
      "Iteration 13699, loss = 0.00190394\n",
      "Iteration 13700, loss = 0.00190164\n",
      "Iteration 13701, loss = 0.00190113\n",
      "Iteration 13702, loss = 0.00190026\n",
      "Iteration 13703, loss = 0.00190074\n",
      "Iteration 13704, loss = 0.00189915\n",
      "Iteration 13705, loss = 0.00189882\n",
      "Iteration 13706, loss = 0.00189809\n",
      "Iteration 13707, loss = 0.00189696\n",
      "Iteration 13708, loss = 0.00189749\n",
      "Iteration 13709, loss = 0.00189543\n",
      "Iteration 13710, loss = 0.00189490\n",
      "Iteration 13711, loss = 0.00189398\n",
      "Iteration 13712, loss = 0.00189488\n",
      "Iteration 13713, loss = 0.00189280\n",
      "Iteration 13714, loss = 0.00189243\n",
      "Iteration 13715, loss = 0.00189164\n",
      "Iteration 13716, loss = 0.00189049\n",
      "Iteration 13717, loss = 0.00189243\n",
      "Iteration 13718, loss = 0.00188974\n",
      "Iteration 13719, loss = 0.00189009\n",
      "Iteration 13720, loss = 0.00189070\n",
      "Iteration 13721, loss = 0.00189062\n",
      "Iteration 13722, loss = 0.00189010\n",
      "Iteration 13723, loss = 0.00188934\n",
      "Iteration 13724, loss = 0.00188825\n",
      "Iteration 13725, loss = 0.00188674\n",
      "Iteration 13726, loss = 0.00188489\n",
      "Iteration 13727, loss = 0.00188323\n",
      "Iteration 13728, loss = 0.00188246\n",
      "Iteration 13729, loss = 0.00188276\n",
      "Iteration 13730, loss = 0.00188187\n",
      "Iteration 13731, loss = 0.00188143\n",
      "Iteration 13732, loss = 0.00188074\n",
      "Iteration 13733, loss = 0.00187994\n",
      "Iteration 13734, loss = 0.00187877\n",
      "Iteration 13735, loss = 0.00187993\n",
      "Iteration 13736, loss = 0.00187733\n",
      "Iteration 13737, loss = 0.00187811\n",
      "Iteration 13738, loss = 0.00187882\n",
      "Iteration 13739, loss = 0.00187830\n",
      "Iteration 13740, loss = 0.00187739\n",
      "Iteration 13741, loss = 0.00187672\n",
      "Iteration 13742, loss = 0.00187581\n",
      "Iteration 13743, loss = 0.00187413\n",
      "Iteration 13744, loss = 0.00187198\n",
      "Iteration 13745, loss = 0.00187344\n",
      "Iteration 13746, loss = 0.00187282\n",
      "Iteration 13747, loss = 0.00187087\n",
      "Iteration 13748, loss = 0.00187056\n",
      "Iteration 13749, loss = 0.00186961\n",
      "Iteration 13750, loss = 0.00186902\n",
      "Iteration 13751, loss = 0.00186840\n",
      "Iteration 13752, loss = 0.00186692\n",
      "Iteration 13753, loss = 0.00186777\n",
      "Iteration 13754, loss = 0.00186548\n",
      "Iteration 13755, loss = 0.00186628\n",
      "Iteration 13756, loss = 0.00186685\n",
      "Iteration 13757, loss = 0.00186586\n",
      "Iteration 13758, loss = 0.00186480\n",
      "Iteration 13759, loss = 0.00186437\n",
      "Iteration 13760, loss = 0.00186353\n",
      "Iteration 13761, loss = 0.00186158\n",
      "Iteration 13762, loss = 0.00185939\n",
      "Iteration 13763, loss = 0.00185912\n",
      "Iteration 13764, loss = 0.00185895\n",
      "Iteration 13765, loss = 0.00185916\n",
      "Iteration 13766, loss = 0.00185711\n",
      "Iteration 13767, loss = 0.00185644\n",
      "Iteration 13768, loss = 0.00185600\n",
      "Iteration 13769, loss = 0.00185529\n",
      "Iteration 13770, loss = 0.00185496\n",
      "Iteration 13771, loss = 0.00185454\n",
      "Iteration 13772, loss = 0.00185412\n",
      "Iteration 13773, loss = 0.00185320\n",
      "Iteration 13774, loss = 0.00185163\n",
      "Iteration 13775, loss = 0.00185281\n",
      "Iteration 13776, loss = 0.00185113\n",
      "Iteration 13777, loss = 0.00185032\n",
      "Iteration 13778, loss = 0.00185067\n",
      "Iteration 13779, loss = 0.00185057\n",
      "Iteration 13780, loss = 0.00185003\n",
      "Iteration 13781, loss = 0.00184910\n",
      "Iteration 13782, loss = 0.00184786\n",
      "Iteration 13783, loss = 0.00184634\n",
      "Iteration 13784, loss = 0.00184454\n",
      "Iteration 13785, loss = 0.00184843\n",
      "Iteration 13786, loss = 0.00184768\n",
      "Iteration 13787, loss = 0.00184281\n",
      "Iteration 13788, loss = 0.00184455\n",
      "Iteration 13789, loss = 0.00184545\n",
      "Iteration 13790, loss = 0.00184564\n",
      "Iteration 13791, loss = 0.00184571\n",
      "Iteration 13792, loss = 0.00184558\n",
      "Iteration 13793, loss = 0.00184482\n",
      "Iteration 13794, loss = 0.00184345\n",
      "Iteration 13795, loss = 0.00184191\n",
      "Iteration 13796, loss = 0.00184031\n",
      "Iteration 13797, loss = 0.00183842\n",
      "Iteration 13798, loss = 0.00183610\n",
      "Iteration 13799, loss = 0.00183757\n",
      "Iteration 13800, loss = 0.00183816\n",
      "Iteration 13801, loss = 0.00183434\n",
      "Iteration 13802, loss = 0.00183428\n",
      "Iteration 13803, loss = 0.00183513\n",
      "Iteration 13804, loss = 0.00183534\n",
      "Iteration 13805, loss = 0.00183511\n",
      "Iteration 13806, loss = 0.00183459\n",
      "Iteration 13807, loss = 0.00183373\n",
      "Iteration 13808, loss = 0.00183243\n",
      "Iteration 13809, loss = 0.00183079\n",
      "Iteration 13810, loss = 0.00182896\n",
      "Iteration 13811, loss = 0.00182697\n",
      "Iteration 13812, loss = 0.00183004\n",
      "Iteration 13813, loss = 0.00183015\n",
      "Iteration 13814, loss = 0.00182594\n",
      "Iteration 13815, loss = 0.00182582\n",
      "Iteration 13816, loss = 0.00182661\n",
      "Iteration 13817, loss = 0.00182692\n",
      "Iteration 13818, loss = 0.00182693\n",
      "Iteration 13819, loss = 0.00182654\n",
      "Iteration 13820, loss = 0.00182565\n",
      "Iteration 13821, loss = 0.00182437\n",
      "Iteration 13822, loss = 0.00182287\n",
      "Iteration 13823, loss = 0.00182117\n",
      "Iteration 13824, loss = 0.00181918\n",
      "Iteration 13825, loss = 0.00181876\n",
      "Iteration 13826, loss = 0.00181876\n",
      "Iteration 13827, loss = 0.00181649\n",
      "Iteration 13828, loss = 0.00181639\n",
      "Iteration 13829, loss = 0.00181585\n",
      "Iteration 13830, loss = 0.00181499\n",
      "Iteration 13831, loss = 0.00181386\n",
      "Iteration 13832, loss = 0.00181436\n",
      "Iteration 13833, loss = 0.00181213\n",
      "Iteration 13834, loss = 0.00181162\n",
      "Iteration 13835, loss = 0.00181126\n",
      "Iteration 13836, loss = 0.00181108\n",
      "Iteration 13837, loss = 0.00181082\n",
      "Iteration 13838, loss = 0.00181016\n",
      "Iteration 13839, loss = 0.00180931\n",
      "Iteration 13840, loss = 0.00180816\n",
      "Iteration 13841, loss = 0.00180844\n",
      "Iteration 13842, loss = 0.00180626\n",
      "Iteration 13843, loss = 0.00180576\n",
      "Iteration 13844, loss = 0.00180573\n",
      "Iteration 13845, loss = 0.00180517\n",
      "Iteration 13846, loss = 0.00180480\n",
      "Iteration 13847, loss = 0.00180410\n",
      "Iteration 13848, loss = 0.00180328\n",
      "Iteration 13849, loss = 0.00180214\n",
      "Iteration 13850, loss = 0.00180344\n",
      "Iteration 13851, loss = 0.00180104\n",
      "Iteration 13852, loss = 0.00180145\n",
      "Iteration 13853, loss = 0.00180210\n",
      "Iteration 13854, loss = 0.00180167\n",
      "Iteration 13855, loss = 0.00180082\n",
      "Iteration 13856, loss = 0.00180014\n",
      "Iteration 13857, loss = 0.00179925\n",
      "Iteration 13858, loss = 0.00179766\n",
      "Iteration 13859, loss = 0.00179564\n",
      "Iteration 13860, loss = 0.00179742\n",
      "Iteration 13861, loss = 0.00179681\n",
      "Iteration 13862, loss = 0.00179453\n",
      "Iteration 13863, loss = 0.00179427\n",
      "Iteration 13864, loss = 0.00179341\n",
      "Iteration 13865, loss = 0.00179283\n",
      "Iteration 13866, loss = 0.00179221\n",
      "Iteration 13867, loss = 0.00179082\n",
      "Iteration 13868, loss = 0.00179192\n",
      "Iteration 13869, loss = 0.00178972\n",
      "Iteration 13870, loss = 0.00179018\n",
      "Iteration 13871, loss = 0.00179074\n",
      "Iteration 13872, loss = 0.00178983\n",
      "Iteration 13873, loss = 0.00178885\n",
      "Iteration 13874, loss = 0.00178841\n",
      "Iteration 13875, loss = 0.00178760\n",
      "Iteration 13876, loss = 0.00178576\n",
      "Iteration 13877, loss = 0.00178381\n",
      "Iteration 13878, loss = 0.00178342\n",
      "Iteration 13879, loss = 0.00178323\n",
      "Iteration 13880, loss = 0.00178358\n",
      "Iteration 13881, loss = 0.00178151\n",
      "Iteration 13882, loss = 0.00178087\n",
      "Iteration 13883, loss = 0.00178044\n",
      "Iteration 13884, loss = 0.00177984\n",
      "Iteration 13885, loss = 0.00177946\n",
      "Iteration 13886, loss = 0.00177906\n",
      "Iteration 13887, loss = 0.00177864\n",
      "Iteration 13888, loss = 0.00177777\n",
      "Iteration 13889, loss = 0.00177628\n",
      "Iteration 13890, loss = 0.00177750\n",
      "Iteration 13891, loss = 0.00177588\n",
      "Iteration 13892, loss = 0.00177505\n",
      "Iteration 13893, loss = 0.00177538\n",
      "Iteration 13894, loss = 0.00177529\n",
      "Iteration 13895, loss = 0.00177477\n",
      "Iteration 13896, loss = 0.00177389\n",
      "Iteration 13897, loss = 0.00177270\n",
      "Iteration 13898, loss = 0.00177124\n",
      "Iteration 13899, loss = 0.00176953\n",
      "Iteration 13900, loss = 0.00177328\n",
      "Iteration 13901, loss = 0.00177258\n",
      "Iteration 13902, loss = 0.00176790\n",
      "Iteration 13903, loss = 0.00176955\n",
      "Iteration 13904, loss = 0.00177040\n",
      "Iteration 13905, loss = 0.00177060\n",
      "Iteration 13906, loss = 0.00177069\n",
      "Iteration 13907, loss = 0.00177058\n",
      "Iteration 13908, loss = 0.00176983\n",
      "Iteration 13909, loss = 0.00176852\n",
      "Iteration 13910, loss = 0.00176704\n",
      "Iteration 13911, loss = 0.00176552\n",
      "Iteration 13912, loss = 0.00176369\n",
      "Iteration 13913, loss = 0.00176148\n",
      "Iteration 13914, loss = 0.00176291\n",
      "Iteration 13915, loss = 0.00176348\n",
      "Iteration 13916, loss = 0.00175982\n",
      "Iteration 13917, loss = 0.00175975\n",
      "Iteration 13918, loss = 0.00176059\n",
      "Iteration 13919, loss = 0.00176079\n",
      "Iteration 13920, loss = 0.00176056\n",
      "Iteration 13921, loss = 0.00176006\n",
      "Iteration 13922, loss = 0.00175923\n",
      "Iteration 13923, loss = 0.00175798\n",
      "Iteration 13924, loss = 0.00175640\n",
      "Iteration 13925, loss = 0.00175465\n",
      "Iteration 13926, loss = 0.00175275\n",
      "Iteration 13927, loss = 0.00175581\n",
      "Iteration 13928, loss = 0.00175592\n",
      "Iteration 13929, loss = 0.00175189\n",
      "Iteration 13930, loss = 0.00175164\n",
      "Iteration 13931, loss = 0.00175240\n",
      "Iteration 13932, loss = 0.00175270\n",
      "Iteration 13933, loss = 0.00175271\n",
      "Iteration 13934, loss = 0.00175233\n",
      "Iteration 13935, loss = 0.00175147\n",
      "Iteration 13936, loss = 0.00175024\n",
      "Iteration 13937, loss = 0.00174882\n",
      "Iteration 13938, loss = 0.00174720\n",
      "Iteration 13939, loss = 0.00174528\n",
      "Iteration 13940, loss = 0.00174508\n",
      "Iteration 13941, loss = 0.00174510\n",
      "Iteration 13942, loss = 0.00174272\n",
      "Iteration 13943, loss = 0.00174262\n",
      "Iteration 13944, loss = 0.00174210\n",
      "Iteration 13945, loss = 0.00174129\n",
      "Iteration 13946, loss = 0.00174020\n",
      "Iteration 13947, loss = 0.00174091\n",
      "Iteration 13948, loss = 0.00173855\n",
      "Iteration 13949, loss = 0.00173806\n",
      "Iteration 13950, loss = 0.00173797\n",
      "Iteration 13951, loss = 0.00173754\n",
      "Iteration 13952, loss = 0.00173730\n",
      "Iteration 13953, loss = 0.00173666\n",
      "Iteration 13954, loss = 0.00173584\n",
      "Iteration 13955, loss = 0.00173474\n",
      "Iteration 13956, loss = 0.00173529\n",
      "Iteration 13957, loss = 0.00173298\n",
      "Iteration 13958, loss = 0.00173413\n",
      "Iteration 13959, loss = 0.00173474\n",
      "Iteration 13960, loss = 0.00173440\n",
      "Iteration 13961, loss = 0.00173364\n",
      "Iteration 13962, loss = 0.00173295\n",
      "Iteration 13963, loss = 0.00173207\n",
      "Iteration 13964, loss = 0.00173057\n",
      "Iteration 13965, loss = 0.00172867\n",
      "Iteration 13966, loss = 0.00172931\n",
      "Iteration 13967, loss = 0.00172867\n",
      "Iteration 13968, loss = 0.00172758\n",
      "Iteration 13969, loss = 0.00172737\n",
      "Iteration 13970, loss = 0.00172654\n",
      "Iteration 13971, loss = 0.00172593\n",
      "Iteration 13972, loss = 0.00172533\n",
      "Iteration 13973, loss = 0.00172403\n",
      "Iteration 13974, loss = 0.00172418\n",
      "Iteration 13975, loss = 0.00172203\n",
      "Iteration 13976, loss = 0.00172340\n",
      "Iteration 13977, loss = 0.00172396\n",
      "Iteration 13978, loss = 0.00172309\n",
      "Iteration 13979, loss = 0.00172211\n",
      "Iteration 13980, loss = 0.00172167\n",
      "Iteration 13981, loss = 0.00172090\n",
      "Iteration 13982, loss = 0.00171915\n",
      "Iteration 13983, loss = 0.00171702\n",
      "Iteration 13984, loss = 0.00172003\n",
      "Iteration 13985, loss = 0.00171969\n",
      "Iteration 13986, loss = 0.00171620\n",
      "Iteration 13987, loss = 0.00171571\n",
      "Iteration 13988, loss = 0.00171483\n",
      "Iteration 13989, loss = 0.00171444\n",
      "Iteration 13990, loss = 0.00171391\n",
      "Iteration 13991, loss = 0.00171323\n",
      "Iteration 13992, loss = 0.00171180\n",
      "Iteration 13993, loss = 0.00171158\n",
      "Iteration 13994, loss = 0.00171117\n",
      "Iteration 13995, loss = 0.00170997\n",
      "Iteration 13996, loss = 0.00170962\n",
      "Iteration 13997, loss = 0.00170904\n",
      "Iteration 13998, loss = 0.00170851\n",
      "Iteration 13999, loss = 0.00170770\n",
      "Iteration 14000, loss = 0.00170831\n",
      "Iteration 14001, loss = 0.00170594\n",
      "Iteration 14002, loss = 0.00170566\n",
      "Iteration 14003, loss = 0.00170501\n",
      "Iteration 14004, loss = 0.00170566\n",
      "Iteration 14005, loss = 0.00170359\n",
      "Iteration 14006, loss = 0.00170314\n",
      "Iteration 14007, loss = 0.00170253\n",
      "Iteration 14008, loss = 0.00170237\n",
      "Iteration 14009, loss = 0.00170157\n",
      "Iteration 14010, loss = 0.00170132\n",
      "Iteration 14011, loss = 0.00170079\n",
      "Iteration 14012, loss = 0.00169983\n",
      "Iteration 14013, loss = 0.00169855\n",
      "Iteration 14014, loss = 0.00169840\n",
      "Iteration 14015, loss = 0.00169793\n",
      "Iteration 14016, loss = 0.00169714\n",
      "Iteration 14017, loss = 0.00169628\n",
      "Iteration 14018, loss = 0.00169608\n",
      "Iteration 14019, loss = 0.00169582\n",
      "Iteration 14020, loss = 0.00169514\n",
      "Iteration 14021, loss = 0.00169408\n",
      "Iteration 14022, loss = 0.00169396\n",
      "Iteration 14023, loss = 0.00169264\n",
      "Iteration 14024, loss = 0.00169216\n",
      "Iteration 14025, loss = 0.00169133\n",
      "Iteration 14026, loss = 0.00169198\n",
      "Iteration 14027, loss = 0.00169027\n",
      "Iteration 14028, loss = 0.00168993\n",
      "Iteration 14029, loss = 0.00168920\n",
      "Iteration 14030, loss = 0.00168827\n",
      "Iteration 14031, loss = 0.00168830\n",
      "Iteration 14032, loss = 0.00168802\n",
      "Iteration 14033, loss = 0.00168739\n",
      "Iteration 14034, loss = 0.00168644\n",
      "Iteration 14035, loss = 0.00168521\n",
      "Iteration 14036, loss = 0.00168763\n",
      "Iteration 14037, loss = 0.00168585\n",
      "Iteration 14038, loss = 0.00168435\n",
      "Iteration 14039, loss = 0.00168475\n",
      "Iteration 14040, loss = 0.00168450\n",
      "Iteration 14041, loss = 0.00168390\n",
      "Iteration 14042, loss = 0.00168315\n",
      "Iteration 14043, loss = 0.00168212\n",
      "Iteration 14044, loss = 0.00168067\n",
      "Iteration 14045, loss = 0.00168009\n",
      "Iteration 14046, loss = 0.00167866\n",
      "Iteration 14047, loss = 0.00167977\n",
      "Iteration 14048, loss = 0.00168004\n",
      "Iteration 14049, loss = 0.00167937\n",
      "Iteration 14050, loss = 0.00167859\n",
      "Iteration 14051, loss = 0.00167798\n",
      "Iteration 14052, loss = 0.00167697\n",
      "Iteration 14053, loss = 0.00167528\n",
      "Iteration 14054, loss = 0.00167524\n",
      "Iteration 14055, loss = 0.00167402\n",
      "Iteration 14056, loss = 0.00167453\n",
      "Iteration 14057, loss = 0.00167464\n",
      "Iteration 14058, loss = 0.00167368\n",
      "Iteration 14059, loss = 0.00167291\n",
      "Iteration 14060, loss = 0.00167247\n",
      "Iteration 14061, loss = 0.00167146\n",
      "Iteration 14062, loss = 0.00166960\n",
      "Iteration 14063, loss = 0.00167109\n",
      "Iteration 14064, loss = 0.00167001\n",
      "Iteration 14065, loss = 0.00166900\n",
      "Iteration 14066, loss = 0.00166899\n",
      "Iteration 14067, loss = 0.00166791\n",
      "Iteration 14068, loss = 0.00166720\n",
      "Iteration 14069, loss = 0.00166688\n",
      "Iteration 14070, loss = 0.00166585\n",
      "Iteration 14071, loss = 0.00166456\n",
      "Iteration 14072, loss = 0.00166333\n",
      "Iteration 14073, loss = 0.00166329\n",
      "Iteration 14074, loss = 0.00166259\n",
      "Iteration 14075, loss = 0.00166401\n",
      "Iteration 14076, loss = 0.00166154\n",
      "Iteration 14077, loss = 0.00166136\n",
      "Iteration 14078, loss = 0.00166193\n",
      "Iteration 14079, loss = 0.00166207\n",
      "Iteration 14080, loss = 0.00166170\n",
      "Iteration 14081, loss = 0.00166097\n",
      "Iteration 14082, loss = 0.00166000\n",
      "Iteration 14083, loss = 0.00165878\n",
      "Iteration 14084, loss = 0.00165725\n",
      "Iteration 14085, loss = 0.00165547\n",
      "Iteration 14086, loss = 0.00165903\n",
      "Iteration 14087, loss = 0.00165886\n",
      "Iteration 14088, loss = 0.00165482\n",
      "Iteration 14089, loss = 0.00165478\n",
      "Iteration 14090, loss = 0.00165568\n",
      "Iteration 14091, loss = 0.00165592\n",
      "Iteration 14092, loss = 0.00165580\n",
      "Iteration 14093, loss = 0.00165550\n",
      "Iteration 14094, loss = 0.00165482\n",
      "Iteration 14095, loss = 0.00165368\n",
      "Iteration 14096, loss = 0.00165223\n",
      "Iteration 14097, loss = 0.00165067\n",
      "Iteration 14098, loss = 0.00164894\n",
      "Iteration 14099, loss = 0.00164777\n",
      "Iteration 14100, loss = 0.00164777\n",
      "Iteration 14101, loss = 0.00164661\n",
      "Iteration 14102, loss = 0.00164646\n",
      "Iteration 14103, loss = 0.00164595\n",
      "Iteration 14104, loss = 0.00164524\n",
      "Iteration 14105, loss = 0.00164424\n",
      "Iteration 14106, loss = 0.00164386\n",
      "Iteration 14107, loss = 0.00164263\n",
      "Iteration 14108, loss = 0.00164224\n",
      "Iteration 14109, loss = 0.00164144\n",
      "Iteration 14110, loss = 0.00164241\n",
      "Iteration 14111, loss = 0.00164013\n",
      "Iteration 14112, loss = 0.00163971\n",
      "Iteration 14113, loss = 0.00163903\n",
      "Iteration 14114, loss = 0.00163962\n",
      "Iteration 14115, loss = 0.00163809\n",
      "Iteration 14116, loss = 0.00163784\n",
      "Iteration 14117, loss = 0.00163727\n",
      "Iteration 14118, loss = 0.00163630\n",
      "Iteration 14119, loss = 0.00163660\n",
      "Iteration 14120, loss = 0.00163495\n",
      "Iteration 14121, loss = 0.00163450\n",
      "Iteration 14122, loss = 0.00163373\n",
      "Iteration 14123, loss = 0.00163444\n",
      "Iteration 14124, loss = 0.00163274\n",
      "Iteration 14125, loss = 0.00163245\n",
      "Iteration 14126, loss = 0.00163178\n",
      "Iteration 14127, loss = 0.00163078\n",
      "Iteration 14128, loss = 0.00163234\n",
      "Iteration 14129, loss = 0.00163008\n",
      "Iteration 14130, loss = 0.00163042\n",
      "Iteration 14131, loss = 0.00163094\n",
      "Iteration 14132, loss = 0.00163091\n",
      "Iteration 14133, loss = 0.00163048\n",
      "Iteration 14134, loss = 0.00162981\n",
      "Iteration 14135, loss = 0.00162887\n",
      "Iteration 14136, loss = 0.00162758\n",
      "Iteration 14137, loss = 0.00162601\n",
      "Iteration 14138, loss = 0.00162468\n",
      "Iteration 14139, loss = 0.00162389\n",
      "Iteration 14140, loss = 0.00162430\n",
      "Iteration 14141, loss = 0.00162343\n",
      "Iteration 14142, loss = 0.00162307\n",
      "Iteration 14143, loss = 0.00162246\n",
      "Iteration 14144, loss = 0.00162177\n",
      "Iteration 14145, loss = 0.00162076\n",
      "Iteration 14146, loss = 0.00162210\n",
      "Iteration 14147, loss = 0.00161990\n",
      "Iteration 14148, loss = 0.00162017\n",
      "Iteration 14149, loss = 0.00162081\n",
      "Iteration 14150, loss = 0.00162040\n",
      "Iteration 14151, loss = 0.00161961\n",
      "Iteration 14152, loss = 0.00161899\n",
      "Iteration 14153, loss = 0.00161821\n",
      "Iteration 14154, loss = 0.00161680\n",
      "Iteration 14155, loss = 0.00161496\n",
      "Iteration 14156, loss = 0.00161684\n",
      "Iteration 14157, loss = 0.00161631\n",
      "Iteration 14158, loss = 0.00161399\n",
      "Iteration 14159, loss = 0.00161376\n",
      "Iteration 14160, loss = 0.00161295\n",
      "Iteration 14161, loss = 0.00161240\n",
      "Iteration 14162, loss = 0.00161187\n",
      "Iteration 14163, loss = 0.00161064\n",
      "Iteration 14164, loss = 0.00161221\n",
      "Iteration 14165, loss = 0.00161023\n",
      "Iteration 14166, loss = 0.00161001\n",
      "Iteration 14167, loss = 0.00161055\n",
      "Iteration 14168, loss = 0.00160975\n",
      "Iteration 14169, loss = 0.00160883\n",
      "Iteration 14170, loss = 0.00160841\n",
      "Iteration 14171, loss = 0.00160770\n",
      "Iteration 14172, loss = 0.00160608\n",
      "Iteration 14173, loss = 0.00160527\n",
      "Iteration 14174, loss = 0.00160391\n",
      "Iteration 14175, loss = 0.00160415\n",
      "Iteration 14176, loss = 0.00160436\n",
      "Iteration 14177, loss = 0.00160365\n",
      "Iteration 14178, loss = 0.00160278\n",
      "Iteration 14179, loss = 0.00160241\n",
      "Iteration 14180, loss = 0.00160178\n",
      "Iteration 14181, loss = 0.00160070\n",
      "Iteration 14182, loss = 0.00159956\n",
      "Iteration 14183, loss = 0.00159937\n",
      "Iteration 14184, loss = 0.00159893\n",
      "Iteration 14185, loss = 0.00159880\n",
      "Iteration 14186, loss = 0.00159723\n",
      "Iteration 14187, loss = 0.00159669\n",
      "Iteration 14188, loss = 0.00159619\n",
      "Iteration 14189, loss = 0.00159657\n",
      "Iteration 14190, loss = 0.00159516\n",
      "Iteration 14191, loss = 0.00159483\n",
      "Iteration 14192, loss = 0.00159439\n",
      "Iteration 14193, loss = 0.00159351\n",
      "Iteration 14194, loss = 0.00159323\n",
      "Iteration 14195, loss = 0.00159201\n",
      "Iteration 14196, loss = 0.00159156\n",
      "Iteration 14197, loss = 0.00159084\n",
      "Iteration 14198, loss = 0.00159151\n",
      "Iteration 14199, loss = 0.00158981\n",
      "Iteration 14200, loss = 0.00158953\n",
      "Iteration 14201, loss = 0.00158893\n",
      "Iteration 14202, loss = 0.00158794\n",
      "Iteration 14203, loss = 0.00158937\n",
      "Iteration 14204, loss = 0.00158726\n",
      "Iteration 14205, loss = 0.00158749\n",
      "Iteration 14206, loss = 0.00158795\n",
      "Iteration 14207, loss = 0.00158795\n",
      "Iteration 14208, loss = 0.00158758\n",
      "Iteration 14209, loss = 0.00158692\n",
      "Iteration 14210, loss = 0.00158597\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 21.06079407\n",
      "Iteration 2, loss = 24.70735246\n",
      "Iteration 3, loss = 23.83532825\n",
      "Iteration 4, loss = 9.08408871\n",
      "Iteration 5, loss = 1.12277970\n",
      "Iteration 6, loss = 1.12272309\n",
      "Iteration 7, loss = 1.12265798\n",
      "Iteration 8, loss = 1.12258516\n",
      "Iteration 9, loss = 1.12250541\n",
      "Iteration 10, loss = 1.12241942\n",
      "Iteration 11, loss = 1.12232784\n",
      "Iteration 12, loss = 1.12223125\n",
      "Iteration 13, loss = 1.12213019\n",
      "Iteration 14, loss = 1.12202517\n",
      "Iteration 15, loss = 1.12191664\n",
      "Iteration 16, loss = 1.12180499\n",
      "Iteration 17, loss = 1.12169060\n",
      "Iteration 18, loss = 1.12157382\n",
      "Iteration 19, loss = 1.12145495\n",
      "Iteration 20, loss = 1.12133427\n",
      "Iteration 21, loss = 1.12121203\n",
      "Iteration 22, loss = 1.12108845\n",
      "Iteration 23, loss = 1.12096374\n",
      "Iteration 24, loss = 1.12083809\n",
      "Iteration 25, loss = 1.12071167\n",
      "Iteration 26, loss = 1.12058462\n",
      "Iteration 27, loss = 1.12045708\n",
      "Iteration 28, loss = 1.12032918\n",
      "Iteration 29, loss = 1.12020102\n",
      "Iteration 30, loss = 1.12007269\n",
      "Iteration 31, loss = 1.11994430\n",
      "Iteration 32, loss = 1.11981593\n",
      "Iteration 33, loss = 1.11968763\n",
      "Iteration 34, loss = 1.11955948\n",
      "Iteration 35, loss = 1.11943153\n",
      "Iteration 36, loss = 1.11930383\n",
      "Iteration 37, loss = 1.11917644\n",
      "Iteration 38, loss = 1.11904939\n",
      "Iteration 39, loss = 1.11892272\n",
      "Iteration 40, loss = 1.11879645\n",
      "Iteration 41, loss = 1.11867063\n",
      "Iteration 42, loss = 1.11854527\n",
      "Iteration 43, loss = 1.11842040\n",
      "Iteration 44, loss = 1.11829604\n",
      "Iteration 45, loss = 1.11817221\n",
      "Iteration 46, loss = 1.11804892\n",
      "Iteration 47, loss = 1.11792618\n",
      "Iteration 48, loss = 1.11780402\n",
      "Iteration 49, loss = 1.11768243\n",
      "Iteration 50, loss = 1.11756143\n",
      "Iteration 51, loss = 1.11744102\n",
      "Iteration 52, loss = 1.11732122\n",
      "Iteration 53, loss = 1.11720202\n",
      "Iteration 54, loss = 1.11708344\n",
      "Iteration 55, loss = 1.11696548\n",
      "Iteration 56, loss = 1.11684814\n",
      "Iteration 57, loss = 1.11673142\n",
      "Iteration 58, loss = 1.11661532\n",
      "Iteration 59, loss = 1.11649985\n",
      "Iteration 60, loss = 1.11638501\n",
      "Iteration 61, loss = 1.11627080\n",
      "Iteration 62, loss = 1.11615721\n",
      "Iteration 63, loss = 1.11604426\n",
      "Iteration 64, loss = 1.11593193\n",
      "Iteration 65, loss = 1.11582023\n",
      "Iteration 66, loss = 1.11570915\n",
      "Iteration 67, loss = 1.11559870\n",
      "Iteration 68, loss = 1.11548888\n",
      "Iteration 69, loss = 1.11537967\n",
      "Iteration 70, loss = 1.11527109\n",
      "Iteration 71, loss = 1.11516312\n",
      "Iteration 72, loss = 1.11505577\n",
      "Iteration 73, loss = 1.11494904\n",
      "Iteration 74, loss = 1.11484291\n",
      "Iteration 75, loss = 1.11473740\n",
      "Iteration 76, loss = 1.11463249\n",
      "Iteration 77, loss = 1.11452819\n",
      "Iteration 78, loss = 1.11442449\n",
      "Iteration 79, loss = 1.11432139\n",
      "Iteration 80, loss = 1.11421889\n",
      "Iteration 81, loss = 1.11411698\n",
      "Iteration 82, loss = 1.11401566\n",
      "Iteration 83, loss = 1.11391493\n",
      "Iteration 84, loss = 1.11381479\n",
      "Iteration 85, loss = 1.11371524\n",
      "Iteration 86, loss = 1.11361626\n",
      "Iteration 87, loss = 1.11351786\n",
      "Iteration 88, loss = 1.11342004\n",
      "Iteration 89, loss = 1.11332279\n",
      "Iteration 90, loss = 1.11322611\n",
      "Iteration 91, loss = 1.11312999\n",
      "Iteration 92, loss = 1.11303444\n",
      "Iteration 93, loss = 1.11293945\n",
      "Iteration 94, loss = 1.11284502\n",
      "Iteration 95, loss = 1.11275114\n",
      "Iteration 96, loss = 1.11265782\n",
      "Iteration 97, loss = 1.11256504\n",
      "Iteration 98, loss = 1.11247281\n",
      "Iteration 99, loss = 1.11238113\n",
      "Iteration 100, loss = 1.11228999\n",
      "Iteration 101, loss = 1.11219938\n",
      "Iteration 102, loss = 1.11210931\n",
      "Iteration 103, loss = 1.11201978\n",
      "Iteration 104, loss = 1.11193077\n",
      "Iteration 105, loss = 1.11184229\n",
      "Iteration 106, loss = 1.11175433\n",
      "Iteration 107, loss = 1.11166690\n",
      "Iteration 108, loss = 1.11157999\n",
      "Iteration 109, loss = 1.11149359\n",
      "Iteration 110, loss = 1.11140770\n",
      "Iteration 111, loss = 1.11132232\n",
      "Iteration 112, loss = 1.11123746\n",
      "Iteration 113, loss = 1.11115309\n",
      "Iteration 114, loss = 1.11106923\n",
      "Iteration 115, loss = 1.11098587\n",
      "Iteration 116, loss = 1.11090300\n",
      "Iteration 117, loss = 1.11082063\n",
      "Iteration 118, loss = 1.11073875\n",
      "Iteration 119, loss = 1.11065736\n",
      "Iteration 120, loss = 1.11057646\n",
      "Iteration 121, loss = 1.11049604\n",
      "Iteration 122, loss = 1.11041610\n",
      "Iteration 123, loss = 1.11033664\n",
      "Iteration 124, loss = 1.11025765\n",
      "Iteration 125, loss = 1.11017914\n",
      "Iteration 126, loss = 1.11010110\n",
      "Iteration 127, loss = 1.11002353\n",
      "Iteration 128, loss = 1.10994642\n",
      "Iteration 129, loss = 1.10986977\n",
      "Iteration 130, loss = 1.10979359\n",
      "Iteration 131, loss = 1.10971786\n",
      "Iteration 132, loss = 1.10964259\n",
      "Iteration 133, loss = 1.10956777\n",
      "Iteration 134, loss = 1.10949340\n",
      "Iteration 135, loss = 1.10941948\n",
      "Iteration 136, loss = 1.10934600\n",
      "Iteration 137, loss = 1.10927297\n",
      "Iteration 138, loss = 1.10920038\n",
      "Iteration 139, loss = 1.10912822\n",
      "Iteration 140, loss = 1.10905650\n",
      "Iteration 141, loss = 1.10898522\n",
      "Iteration 142, loss = 1.10891436\n",
      "Iteration 143, loss = 1.10884394\n",
      "Iteration 144, loss = 1.10877393\n",
      "Iteration 145, loss = 1.10870436\n",
      "Iteration 146, loss = 1.10863520\n",
      "Iteration 147, loss = 1.10856647\n",
      "Iteration 148, loss = 1.10849815\n",
      "Iteration 149, loss = 1.10843024\n",
      "Iteration 150, loss = 1.10836275\n",
      "Iteration 151, loss = 1.10829566\n",
      "Iteration 152, loss = 1.10822898\n",
      "Iteration 153, loss = 1.10816271\n",
      "Iteration 154, loss = 1.10809684\n",
      "Iteration 155, loss = 1.10803137\n",
      "Iteration 156, loss = 1.10796630\n",
      "Iteration 157, loss = 1.10790163\n",
      "Iteration 158, loss = 1.10783735\n",
      "Iteration 159, loss = 1.10777346\n",
      "Iteration 160, loss = 1.10770996\n",
      "Iteration 161, loss = 1.10764685\n",
      "Iteration 162, loss = 1.10758412\n",
      "Iteration 163, loss = 1.10752178\n",
      "Iteration 164, loss = 1.10745982\n",
      "Iteration 165, loss = 1.10739823\n",
      "Iteration 166, loss = 1.10733702\n",
      "Iteration 167, loss = 1.10727619\n",
      "Iteration 168, loss = 1.10721573\n",
      "Iteration 169, loss = 1.10715564\n",
      "Iteration 170, loss = 1.10709591\n",
      "Iteration 171, loss = 1.10703655\n",
      "Iteration 172, loss = 1.10697756\n",
      "Iteration 173, loss = 1.10691893\n",
      "Iteration 174, loss = 1.10686065\n",
      "Iteration 175, loss = 1.10680274\n",
      "Iteration 176, loss = 1.10674518\n",
      "Iteration 177, loss = 1.10668797\n",
      "Iteration 178, loss = 1.10663112\n",
      "Iteration 179, loss = 1.10657461\n",
      "Iteration 180, loss = 1.10651845\n",
      "Iteration 181, loss = 1.10646264\n",
      "Iteration 182, loss = 1.10640717\n",
      "Iteration 183, loss = 1.10635205\n",
      "Iteration 184, loss = 1.10629726\n",
      "Iteration 185, loss = 1.10624281\n",
      "Iteration 186, loss = 1.10618869\n",
      "Iteration 187, loss = 1.10613491\n",
      "Iteration 188, loss = 1.10608146\n",
      "Iteration 189, loss = 1.10602835\n",
      "Iteration 190, loss = 1.10597555\n",
      "Iteration 191, loss = 1.10592309\n",
      "Iteration 192, loss = 1.10587095\n",
      "Iteration 193, loss = 1.10581913\n",
      "Iteration 194, loss = 1.10576763\n",
      "Iteration 195, loss = 1.10571646\n",
      "Iteration 196, loss = 1.10566559\n",
      "Iteration 197, loss = 1.10561505\n",
      "Iteration 198, loss = 1.10556481\n",
      "Iteration 199, loss = 1.10551489\n",
      "Iteration 200, loss = 1.10546528\n",
      "Iteration 201, loss = 1.10541597\n",
      "Iteration 202, loss = 1.10536697\n",
      "Iteration 203, loss = 1.10531828\n",
      "Iteration 204, loss = 1.10526989\n",
      "Iteration 205, loss = 1.10522180\n",
      "Iteration 206, loss = 1.10517400\n",
      "Iteration 207, loss = 1.10512651\n",
      "Iteration 208, loss = 1.10507931\n",
      "Iteration 209, loss = 1.10503240\n",
      "Iteration 210, loss = 1.10498578\n",
      "Iteration 211, loss = 1.10493946\n",
      "Iteration 212, loss = 1.10489342\n",
      "Iteration 213, loss = 1.10484768\n",
      "Iteration 214, loss = 1.10480221\n",
      "Iteration 215, loss = 1.10475703\n",
      "Iteration 216, loss = 1.10471213\n",
      "Iteration 217, loss = 1.10466751\n",
      "Iteration 218, loss = 1.10462317\n",
      "Iteration 219, loss = 1.10457911\n",
      "Iteration 220, loss = 1.10453532\n",
      "Iteration 221, loss = 1.10449181\n",
      "Iteration 222, loss = 1.10444857\n",
      "Iteration 223, loss = 1.10440560\n",
      "Iteration 224, loss = 1.10436290\n",
      "Iteration 225, loss = 1.10432046\n",
      "Iteration 226, loss = 1.10427829\n",
      "Iteration 227, loss = 1.10423639\n",
      "Iteration 228, loss = 1.10419474\n",
      "Iteration 229, loss = 1.10415336\n",
      "Iteration 230, loss = 1.10411224\n",
      "Iteration 231, loss = 1.10407137\n",
      "Iteration 232, loss = 1.10403077\n",
      "Iteration 233, loss = 1.10399041\n",
      "Iteration 234, loss = 1.10395031\n",
      "Iteration 235, loss = 1.10391047\n",
      "Iteration 236, loss = 1.10387087\n",
      "Iteration 237, loss = 1.10383152\n",
      "Iteration 238, loss = 1.10379242\n",
      "Iteration 239, loss = 1.10375356\n",
      "Iteration 240, loss = 1.10371495\n",
      "Iteration 241, loss = 1.10367659\n",
      "Iteration 242, loss = 1.10363846\n",
      "Iteration 243, loss = 1.10360057\n",
      "Iteration 244, loss = 1.10356293\n",
      "Iteration 245, loss = 1.10352552\n",
      "Iteration 246, loss = 1.10348834\n",
      "Iteration 247, loss = 1.10345140\n",
      "Iteration 248, loss = 1.10341469\n",
      "Iteration 249, loss = 1.10337822\n",
      "Iteration 250, loss = 1.10334197\n",
      "Iteration 251, loss = 1.10330596\n",
      "Iteration 252, loss = 1.10327017\n",
      "Iteration 253, loss = 1.10323461\n",
      "Iteration 254, loss = 1.10319927\n",
      "Iteration 255, loss = 1.10316416\n",
      "Iteration 256, loss = 1.10312926\n",
      "Iteration 257, loss = 1.10309459\n",
      "Iteration 258, loss = 1.10306014\n",
      "Iteration 259, loss = 1.10302591\n",
      "Iteration 260, loss = 1.10299189\n",
      "Iteration 261, loss = 1.10295809\n",
      "Iteration 262, loss = 1.10292450\n",
      "Iteration 263, loss = 1.10289113\n",
      "Iteration 264, loss = 1.10285796\n",
      "Iteration 265, loss = 1.10282501\n",
      "Iteration 266, loss = 1.10279227\n",
      "Iteration 267, loss = 1.10275973\n",
      "Iteration 268, loss = 1.10272740\n",
      "Iteration 269, loss = 1.10269528\n",
      "Iteration 270, loss = 1.10266336\n",
      "Iteration 271, loss = 1.10263164\n",
      "Iteration 272, loss = 1.10260013\n",
      "Iteration 273, loss = 1.10256881\n",
      "Iteration 274, loss = 1.10253770\n",
      "Iteration 275, loss = 1.10250678\n",
      "Iteration 276, loss = 1.10247606\n",
      "Iteration 277, loss = 1.10244553\n",
      "Iteration 278, loss = 1.10241520\n",
      "Iteration 279, loss = 1.10238506\n",
      "Iteration 280, loss = 1.10235512\n",
      "Iteration 281, loss = 1.10232536\n",
      "Iteration 282, loss = 1.10229579\n",
      "Iteration 283, loss = 1.10226642\n",
      "Iteration 284, loss = 1.10223723\n",
      "Iteration 285, loss = 1.10220822\n",
      "Iteration 286, loss = 1.10217941\n",
      "Iteration 287, loss = 1.10215077\n",
      "Iteration 288, loss = 1.10212232\n",
      "Iteration 289, loss = 1.10209405\n",
      "Iteration 290, loss = 1.10206596\n",
      "Iteration 291, loss = 1.10203805\n",
      "Iteration 292, loss = 1.10201032\n",
      "Iteration 293, loss = 1.10198276\n",
      "Iteration 294, loss = 1.10195539\n",
      "Iteration 295, loss = 1.10192818\n",
      "Iteration 296, loss = 1.10190116\n",
      "Iteration 297, loss = 1.10187430\n",
      "Iteration 298, loss = 1.10184762\n",
      "Iteration 299, loss = 1.10182110\n",
      "Iteration 300, loss = 1.10179476\n",
      "Iteration 301, loss = 1.10176859\n",
      "Iteration 302, loss = 1.10174258\n",
      "Iteration 303, loss = 1.10171674\n",
      "Iteration 304, loss = 1.10169107\n",
      "Iteration 305, loss = 1.10166556\n",
      "Iteration 306, loss = 1.10164021\n",
      "Iteration 307, loss = 1.10161503\n",
      "Iteration 308, loss = 1.10159001\n",
      "Iteration 309, loss = 1.10156515\n",
      "Iteration 310, loss = 1.10154045\n",
      "Iteration 311, loss = 1.10151590\n",
      "Iteration 312, loss = 1.10149152\n",
      "Iteration 313, loss = 1.10146729\n",
      "Iteration 314, loss = 1.10144322\n",
      "Iteration 315, loss = 1.10141930\n",
      "Iteration 316, loss = 1.10139554\n",
      "Iteration 317, loss = 1.10137193\n",
      "Iteration 318, loss = 1.10134847\n",
      "Iteration 319, loss = 1.10132516\n",
      "Iteration 320, loss = 1.10130200\n",
      "Iteration 321, loss = 1.10127900\n",
      "Iteration 322, loss = 1.10125614\n",
      "Iteration 323, loss = 1.10123342\n",
      "Iteration 324, loss = 1.10121086\n",
      "Iteration 325, loss = 1.10118843\n",
      "Iteration 326, loss = 1.10116616\n",
      "Iteration 327, loss = 1.10114403\n",
      "Iteration 328, loss = 1.10112204\n",
      "Iteration 329, loss = 1.10110019\n",
      "Iteration 330, loss = 1.10107848\n",
      "Iteration 331, loss = 1.10105691\n",
      "Iteration 332, loss = 1.10103549\n",
      "Iteration 333, loss = 1.10101420\n",
      "Iteration 334, loss = 1.10099305\n",
      "Iteration 335, loss = 1.10097203\n",
      "Iteration 336, loss = 1.10095115\n",
      "Iteration 337, loss = 1.10093041\n",
      "Iteration 338, loss = 1.10090980\n",
      "Iteration 339, loss = 1.10088932\n",
      "Iteration 340, loss = 1.10086898\n",
      "Iteration 341, loss = 1.10084876\n",
      "Iteration 342, loss = 1.10082868\n",
      "Iteration 343, loss = 1.10080873\n",
      "Iteration 344, loss = 1.10078891\n",
      "Iteration 345, loss = 1.10076921\n",
      "Iteration 346, loss = 1.10074965\n",
      "Iteration 347, loss = 1.10073021\n",
      "Iteration 348, loss = 1.10071089\n",
      "Iteration 349, loss = 1.10069170\n",
      "Iteration 350, loss = 1.10067264\n",
      "Iteration 351, loss = 1.10065370\n",
      "Iteration 352, loss = 1.10063488\n",
      "Iteration 353, loss = 1.10061619\n",
      "Iteration 354, loss = 1.10059761\n",
      "Iteration 355, loss = 1.10057916\n",
      "Iteration 356, loss = 1.10056082\n",
      "Iteration 357, loss = 1.10054261\n",
      "Iteration 358, loss = 1.10052451\n",
      "Iteration 359, loss = 1.10050653\n",
      "Iteration 360, loss = 1.10048867\n",
      "Iteration 361, loss = 1.10047092\n",
      "Iteration 362, loss = 1.10045329\n",
      "Iteration 363, loss = 1.10043578\n",
      "Iteration 364, loss = 1.10041837\n",
      "Iteration 365, loss = 1.10040109\n",
      "Iteration 366, loss = 1.10038391\n",
      "Iteration 367, loss = 1.10036684\n",
      "Iteration 368, loss = 1.10034989\n",
      "Iteration 369, loss = 1.10033305\n",
      "Iteration 370, loss = 1.10031631\n",
      "Iteration 371, loss = 1.10029969\n",
      "Iteration 372, loss = 1.10028317\n",
      "Iteration 373, loss = 1.10026676\n",
      "Iteration 374, loss = 1.10025046\n",
      "Iteration 375, loss = 1.10023427\n",
      "Iteration 376, loss = 1.10021818\n",
      "Iteration 377, loss = 1.10020219\n",
      "Iteration 378, loss = 1.10018631\n",
      "Iteration 379, loss = 1.10017053\n",
      "Iteration 380, loss = 1.10015486\n",
      "Iteration 381, loss = 1.10013929\n",
      "Iteration 382, loss = 1.10012381\n",
      "Iteration 383, loss = 1.10010844\n",
      "Iteration 384, loss = 1.10009318\n",
      "Iteration 385, loss = 1.10007801\n",
      "Iteration 386, loss = 1.10006294\n",
      "Iteration 387, loss = 1.10004796\n",
      "Iteration 388, loss = 1.10003309\n",
      "Iteration 389, loss = 1.10001831\n",
      "Iteration 390, loss = 1.10000363\n",
      "Iteration 391, loss = 1.09998905\n",
      "Iteration 392, loss = 1.09997456\n",
      "Iteration 393, loss = 1.09996016\n",
      "Iteration 394, loss = 1.09994586\n",
      "Iteration 395, loss = 1.09993166\n",
      "Iteration 396, loss = 1.09991754\n",
      "Iteration 397, loss = 1.09990352\n",
      "Iteration 398, loss = 1.09988959\n",
      "Iteration 399, loss = 1.09987576\n",
      "Iteration 400, loss = 1.09986201\n",
      "Iteration 401, loss = 1.09984835\n",
      "Iteration 402, loss = 1.09983478\n",
      "Iteration 403, loss = 1.09982130\n",
      "Iteration 404, loss = 1.09980791\n",
      "Iteration 405, loss = 1.09979461\n",
      "Iteration 406, loss = 1.09978139\n",
      "Iteration 407, loss = 1.09976826\n",
      "Iteration 408, loss = 1.09975522\n",
      "Iteration 409, loss = 1.09974226\n",
      "Iteration 410, loss = 1.09972939\n",
      "Iteration 411, loss = 1.09971660\n",
      "Iteration 412, loss = 1.09970390\n",
      "Iteration 413, loss = 1.09969128\n",
      "Iteration 414, loss = 1.09967874\n",
      "Iteration 415, loss = 1.09966628\n",
      "Iteration 416, loss = 1.09965391\n",
      "Iteration 417, loss = 1.09964162\n",
      "Iteration 418, loss = 1.09962940\n",
      "Iteration 419, loss = 1.09961727\n",
      "Iteration 420, loss = 1.09960522\n",
      "Iteration 421, loss = 1.09959325\n",
      "Iteration 422, loss = 1.09958135\n",
      "Iteration 423, loss = 1.09956954\n",
      "Iteration 424, loss = 1.09955780\n",
      "Iteration 425, loss = 1.09954614\n",
      "Iteration 426, loss = 1.09953455\n",
      "Iteration 427, loss = 1.09952305\n",
      "Iteration 428, loss = 1.09951161\n",
      "Iteration 429, loss = 1.09950026\n",
      "Iteration 430, loss = 1.09948898\n",
      "Iteration 431, loss = 1.09947777\n",
      "Iteration 432, loss = 1.09946663\n",
      "Iteration 433, loss = 1.09945557\n",
      "Iteration 434, loss = 1.09944458\n",
      "Iteration 435, loss = 1.09943367\n",
      "Iteration 436, loss = 1.09942282\n",
      "Iteration 437, loss = 1.09941205\n",
      "Iteration 438, loss = 1.09940135\n",
      "Iteration 439, loss = 1.09939072\n",
      "Iteration 440, loss = 1.09938016\n",
      "Iteration 441, loss = 1.09936967\n",
      "Iteration 442, loss = 1.09935925\n",
      "Iteration 443, loss = 1.09934889\n",
      "Iteration 444, loss = 1.09933861\n",
      "Iteration 445, loss = 1.09932839\n",
      "Iteration 446, loss = 1.09931824\n",
      "Iteration 447, loss = 1.09930816\n",
      "Iteration 448, loss = 1.09929814\n",
      "Iteration 449, loss = 1.09928819\n",
      "Iteration 450, loss = 1.09927831\n",
      "Iteration 451, loss = 1.09926849\n",
      "Iteration 452, loss = 1.09925873\n",
      "Iteration 453, loss = 1.09924904\n",
      "Iteration 454, loss = 1.09923942\n",
      "Iteration 455, loss = 1.09922986\n",
      "Iteration 456, loss = 1.09922036\n",
      "Iteration 457, loss = 1.09921092\n",
      "Iteration 458, loss = 1.09920155\n",
      "Iteration 459, loss = 1.09919223\n",
      "Iteration 460, loss = 1.09918298\n",
      "Iteration 461, loss = 1.09917379\n",
      "Iteration 462, loss = 1.09916467\n",
      "Iteration 463, loss = 1.09915560\n",
      "Iteration 464, loss = 1.09914659\n",
      "Iteration 465, loss = 1.09913764\n",
      "Iteration 466, loss = 1.09912875\n",
      "Iteration 467, loss = 1.09911992\n",
      "Iteration 468, loss = 1.09911115\n",
      "Iteration 469, loss = 1.09910244\n",
      "Iteration 470, loss = 1.09909378\n",
      "Iteration 471, loss = 1.09908518\n",
      "Iteration 472, loss = 1.09907664\n",
      "Iteration 473, loss = 1.09906815\n",
      "Iteration 474, loss = 1.09905972\n",
      "Iteration 475, loss = 1.09905135\n",
      "Iteration 476, loss = 1.09904303\n",
      "Iteration 477, loss = 1.09903477\n",
      "Iteration 478, loss = 1.09902656\n",
      "Iteration 479, loss = 1.09901841\n",
      "Iteration 480, loss = 1.09901031\n",
      "Iteration 481, loss = 1.09900226\n",
      "Iteration 482, loss = 1.09899427\n",
      "Iteration 483, loss = 1.09898633\n",
      "Iteration 484, loss = 1.09897844\n",
      "Iteration 485, loss = 1.09897061\n",
      "Iteration 486, loss = 1.09896283\n",
      "Iteration 487, loss = 1.09895510\n",
      "Iteration 488, loss = 1.09894742\n",
      "Iteration 489, loss = 1.09893979\n",
      "Iteration 490, loss = 1.09893221\n",
      "Iteration 491, loss = 1.09892468\n",
      "Iteration 492, loss = 1.09891720\n",
      "Iteration 493, loss = 1.09890978\n",
      "Iteration 494, loss = 1.09890240\n",
      "Iteration 495, loss = 1.09889507\n",
      "Iteration 496, loss = 1.09888779\n",
      "Iteration 497, loss = 1.09888055\n",
      "Iteration 498, loss = 1.09887337\n",
      "Iteration 499, loss = 1.09886623\n",
      "Iteration 500, loss = 1.09885914\n",
      "Iteration 501, loss = 1.09885210\n",
      "Iteration 502, loss = 1.09884511\n",
      "Iteration 503, loss = 1.09883816\n",
      "Iteration 504, loss = 1.09883125\n",
      "Iteration 505, loss = 1.09882440\n",
      "Iteration 506, loss = 1.09881759\n",
      "Iteration 507, loss = 1.09881082\n",
      "Iteration 508, loss = 1.09880410\n",
      "Iteration 509, loss = 1.09879742\n",
      "Iteration 510, loss = 1.09879079\n",
      "Iteration 511, loss = 1.09878420\n",
      "Iteration 512, loss = 1.09877766\n",
      "Iteration 513, loss = 1.09877116\n",
      "Iteration 514, loss = 1.09876470\n",
      "Iteration 515, loss = 1.09875829\n",
      "Iteration 516, loss = 1.09875192\n",
      "Iteration 517, loss = 1.09874559\n",
      "Iteration 518, loss = 1.09873930\n",
      "Iteration 519, loss = 1.09873306\n",
      "Iteration 520, loss = 1.09872685\n",
      "Iteration 521, loss = 1.09872069\n",
      "Iteration 522, loss = 1.09871457\n",
      "Iteration 523, loss = 1.09870849\n",
      "Iteration 524, loss = 1.09870245\n",
      "Iteration 525, loss = 1.09869645\n",
      "Iteration 526, loss = 1.09869049\n",
      "Iteration 527, loss = 1.09868457\n",
      "Iteration 528, loss = 1.09867869\n",
      "Iteration 529, loss = 1.09867285\n",
      "Iteration 530, loss = 1.09866705\n",
      "Iteration 531, loss = 1.09866129\n",
      "Iteration 532, loss = 1.09865556\n",
      "Iteration 533, loss = 1.09864988\n",
      "Iteration 534, loss = 1.09864423\n",
      "Iteration 535, loss = 1.09863862\n",
      "Iteration 536, loss = 1.09863304\n",
      "Iteration 537, loss = 1.09862751\n",
      "Iteration 538, loss = 1.09862201\n",
      "Iteration 539, loss = 1.09861655\n",
      "Iteration 540, loss = 1.09861112\n",
      "Iteration 541, loss = 1.09860573\n",
      "Iteration 542, loss = 1.09860038\n",
      "Iteration 543, loss = 1.09859506\n",
      "Iteration 544, loss = 1.09858977\n",
      "Iteration 545, loss = 1.09858453\n",
      "Iteration 546, loss = 1.09857931\n",
      "Iteration 547, loss = 1.09857414\n",
      "Iteration 548, loss = 1.09856899\n",
      "Iteration 549, loss = 1.09856389\n",
      "Iteration 550, loss = 1.09855881\n",
      "Iteration 551, loss = 1.09855377\n",
      "Iteration 552, loss = 1.09854876\n",
      "Iteration 553, loss = 1.09854379\n",
      "Iteration 554, loss = 1.09853885\n",
      "Iteration 555, loss = 1.09853394\n",
      "Iteration 556, loss = 1.09852907\n",
      "Iteration 557, loss = 1.09852423\n",
      "Iteration 558, loss = 1.09851942\n",
      "Iteration 559, loss = 1.09851464\n",
      "Iteration 560, loss = 1.09850990\n",
      "Iteration 561, loss = 1.09850518\n",
      "Iteration 562, loss = 1.09850050\n",
      "Iteration 563, loss = 1.09849585\n",
      "Iteration 564, loss = 1.09849123\n",
      "Iteration 565, loss = 1.09848664\n",
      "Iteration 566, loss = 1.09848208\n",
      "Iteration 567, loss = 1.09847756\n",
      "Iteration 568, loss = 1.09847306\n",
      "Iteration 569, loss = 1.09846859\n",
      "Iteration 570, loss = 1.09846416\n",
      "Iteration 571, loss = 1.09845975\n",
      "Iteration 572, loss = 1.09845537\n",
      "Iteration 573, loss = 1.09845102\n",
      "Iteration 574, loss = 1.09844670\n",
      "Iteration 575, loss = 1.09844241\n",
      "Iteration 576, loss = 1.09843815\n",
      "Iteration 577, loss = 1.09843392\n",
      "Iteration 578, loss = 1.09842971\n",
      "Iteration 579, loss = 1.09842554\n",
      "Iteration 580, loss = 1.09842139\n",
      "Iteration 581, loss = 1.09841727\n",
      "Iteration 582, loss = 1.09841318\n",
      "Iteration 583, loss = 1.09840911\n",
      "Iteration 584, loss = 1.09840507\n",
      "Iteration 585, loss = 1.09840106\n",
      "Iteration 586, loss = 1.09839708\n",
      "Iteration 587, loss = 1.09839312\n",
      "Iteration 588, loss = 1.09838919\n",
      "Iteration 589, loss = 1.09838528\n",
      "Iteration 590, loss = 1.09838140\n",
      "Iteration 591, loss = 1.09837755\n",
      "Iteration 592, loss = 1.09837372\n",
      "Iteration 593, loss = 1.09836992\n",
      "Iteration 594, loss = 1.09836615\n",
      "Iteration 595, loss = 1.09836240\n",
      "Iteration 596, loss = 1.09835867\n",
      "Iteration 597, loss = 1.09835497\n",
      "Iteration 598, loss = 1.09835130\n",
      "Iteration 599, loss = 1.09834765\n",
      "Iteration 600, loss = 1.09834402\n",
      "Iteration 601, loss = 1.09834042\n",
      "Iteration 602, loss = 1.09833684\n",
      "Iteration 603, loss = 1.09833329\n",
      "Iteration 604, loss = 1.09832976\n",
      "Iteration 605, loss = 1.09832625\n",
      "Iteration 606, loss = 1.09832277\n",
      "Iteration 607, loss = 1.09831931\n",
      "Iteration 608, loss = 1.09831587\n",
      "Iteration 609, loss = 1.09831246\n",
      "Iteration 610, loss = 1.09830907\n",
      "Iteration 611, loss = 1.09830571\n",
      "Iteration 612, loss = 1.09830236\n",
      "Iteration 613, loss = 1.09829904\n",
      "Iteration 614, loss = 1.09829574\n",
      "Iteration 615, loss = 1.09829246\n",
      "Iteration 616, loss = 1.09828921\n",
      "Iteration 617, loss = 1.09828597\n",
      "Iteration 618, loss = 1.09828276\n",
      "Iteration 619, loss = 1.09827957\n",
      "Iteration 620, loss = 1.09827640\n",
      "Iteration 621, loss = 1.09827326\n",
      "Iteration 622, loss = 1.09827013\n",
      "Iteration 623, loss = 1.09826702\n",
      "Iteration 624, loss = 1.09826394\n",
      "Iteration 625, loss = 1.09826088\n",
      "Iteration 626, loss = 1.09825783\n",
      "Iteration 627, loss = 1.09825481\n",
      "Iteration 628, loss = 1.09825181\n",
      "Iteration 629, loss = 1.09824883\n",
      "Iteration 630, loss = 1.09824586\n",
      "Iteration 631, loss = 1.09824292\n",
      "Iteration 632, loss = 1.09824000\n",
      "Iteration 633, loss = 1.09823710\n",
      "Iteration 634, loss = 1.09823421\n",
      "Iteration 635, loss = 1.09823135\n",
      "Iteration 636, loss = 1.09822851\n",
      "Iteration 637, loss = 1.09822568\n",
      "Iteration 638, loss = 1.09822288\n",
      "Iteration 639, loss = 1.09822009\n",
      "Iteration 640, loss = 1.09821732\n",
      "Iteration 641, loss = 1.09821457\n",
      "Iteration 642, loss = 1.09821184\n",
      "Iteration 643, loss = 1.09820913\n",
      "Iteration 644, loss = 1.09820643\n",
      "Iteration 645, loss = 1.09820376\n",
      "Iteration 646, loss = 1.09820110\n",
      "Iteration 647, loss = 1.09819846\n",
      "Iteration 648, loss = 1.09819583\n",
      "Iteration 649, loss = 1.09819323\n",
      "Iteration 650, loss = 1.09819064\n",
      "Iteration 651, loss = 1.09818807\n",
      "Iteration 652, loss = 1.09818552\n",
      "Iteration 653, loss = 1.09818298\n",
      "Iteration 654, loss = 1.09818046\n",
      "Iteration 655, loss = 1.09817796\n",
      "Iteration 656, loss = 1.09817548\n",
      "Iteration 657, loss = 1.09817301\n",
      "Iteration 658, loss = 1.09817056\n",
      "Iteration 659, loss = 1.09816812\n",
      "Iteration 660, loss = 1.09816571\n",
      "Iteration 661, loss = 1.09816330\n",
      "Iteration 662, loss = 1.09816092\n",
      "Iteration 663, loss = 1.09815855\n",
      "Iteration 664, loss = 1.09815619\n",
      "Iteration 665, loss = 1.09815386\n",
      "Iteration 666, loss = 1.09815153\n",
      "Iteration 667, loss = 1.09814923\n",
      "Iteration 668, loss = 1.09814694\n",
      "Iteration 669, loss = 1.09814466\n",
      "Iteration 670, loss = 1.09814240\n",
      "Iteration 671, loss = 1.09814016\n",
      "Iteration 672, loss = 1.09813793\n",
      "Iteration 673, loss = 1.09813571\n",
      "Iteration 674, loss = 1.09813351\n",
      "Iteration 675, loss = 1.09813133\n",
      "Iteration 676, loss = 1.09812916\n",
      "Iteration 677, loss = 1.09812700\n",
      "Iteration 678, loss = 1.09812486\n",
      "Iteration 679, loss = 1.09812274\n",
      "Iteration 680, loss = 1.09812062\n",
      "Iteration 681, loss = 1.09811853\n",
      "Iteration 682, loss = 1.09811644\n",
      "Iteration 683, loss = 1.09811437\n",
      "Iteration 684, loss = 1.09811232\n",
      "Iteration 685, loss = 1.09811027\n",
      "Iteration 686, loss = 1.09810825\n",
      "Iteration 687, loss = 1.09810623\n",
      "Iteration 688, loss = 1.09810423\n",
      "Iteration 689, loss = 1.09810224\n",
      "Iteration 690, loss = 1.09810027\n",
      "Iteration 691, loss = 1.09809831\n",
      "Iteration 692, loss = 1.09809636\n",
      "Iteration 693, loss = 1.09809443\n",
      "Iteration 694, loss = 1.09809251\n",
      "Iteration 695, loss = 1.09809060\n",
      "Iteration 696, loss = 1.09808871\n",
      "Iteration 697, loss = 1.09808682\n",
      "Iteration 698, loss = 1.09808495\n",
      "Iteration 699, loss = 1.09808310\n",
      "Iteration 700, loss = 1.09808125\n",
      "Iteration 701, loss = 1.09807942\n",
      "Iteration 702, loss = 1.09807760\n",
      "Iteration 703, loss = 1.09807579\n",
      "Iteration 704, loss = 1.09807400\n",
      "Iteration 705, loss = 1.09807222\n",
      "Iteration 706, loss = 1.09807045\n",
      "Iteration 707, loss = 1.09806869\n",
      "Iteration 708, loss = 1.09806694\n",
      "Iteration 709, loss = 1.09806520\n",
      "Iteration 710, loss = 1.09806348\n",
      "Iteration 711, loss = 1.09806177\n",
      "Iteration 712, loss = 1.09806007\n",
      "Iteration 713, loss = 1.09805838\n",
      "Iteration 714, loss = 1.09805670\n",
      "Iteration 715, loss = 1.09805504\n",
      "Iteration 716, loss = 1.09805338\n",
      "Iteration 717, loss = 1.09805174\n",
      "Iteration 718, loss = 1.09805011\n",
      "Iteration 719, loss = 1.09804849\n",
      "Iteration 720, loss = 1.09804688\n",
      "Iteration 721, loss = 1.09804528\n",
      "Iteration 722, loss = 1.09804369\n",
      "Iteration 723, loss = 1.09804211\n",
      "Iteration 724, loss = 1.09804054\n",
      "Iteration 725, loss = 1.09803899\n",
      "Iteration 726, loss = 1.09803744\n",
      "Iteration 727, loss = 1.09803591\n",
      "Iteration 728, loss = 1.09803438\n",
      "Iteration 729, loss = 1.09803287\n",
      "Iteration 730, loss = 1.09803136\n",
      "Iteration 731, loss = 1.09802987\n",
      "Iteration 732, loss = 1.09802838\n",
      "Iteration 733, loss = 1.09802691\n",
      "Iteration 734, loss = 1.09802544\n",
      "Iteration 735, loss = 1.09802399\n",
      "Iteration 736, loss = 1.09802254\n",
      "Iteration 737, loss = 1.09802111\n",
      "Iteration 738, loss = 1.09801969\n",
      "Iteration 739, loss = 1.09801827\n",
      "Iteration 740, loss = 1.09801686\n",
      "Iteration 741, loss = 1.09801547\n",
      "Iteration 742, loss = 1.09801408\n",
      "Iteration 743, loss = 1.09801270\n",
      "Iteration 744, loss = 1.09801134\n",
      "Iteration 745, loss = 1.09800998\n",
      "Iteration 746, loss = 1.09800863\n",
      "Iteration 747, loss = 1.09800729\n",
      "Iteration 748, loss = 1.09800596\n",
      "Iteration 749, loss = 1.09800463\n",
      "Iteration 750, loss = 1.09800332\n",
      "Iteration 751, loss = 1.09800202\n",
      "Iteration 752, loss = 1.09800072\n",
      "Iteration 753, loss = 1.09799943\n",
      "Iteration 754, loss = 1.09799815\n",
      "Iteration 755, loss = 1.09799689\n",
      "Iteration 756, loss = 1.09799562\n",
      "Iteration 757, loss = 1.09799437\n",
      "Iteration 758, loss = 1.09799313\n",
      "Iteration 759, loss = 1.09799189\n",
      "Iteration 760, loss = 1.09799067\n",
      "Iteration 761, loss = 1.09798945\n",
      "Iteration 762, loss = 1.09798824\n",
      "Iteration 763, loss = 1.09798703\n",
      "Iteration 764, loss = 1.09798584\n",
      "Iteration 765, loss = 1.09798465\n",
      "Iteration 766, loss = 1.09798348\n",
      "Iteration 767, loss = 1.09798231\n",
      "Iteration 768, loss = 1.09798114\n",
      "Iteration 769, loss = 1.09797999\n",
      "Iteration 770, loss = 1.09797884\n",
      "Iteration 771, loss = 1.09797771\n",
      "Iteration 772, loss = 1.09797657\n",
      "Iteration 773, loss = 1.09797545\n",
      "Iteration 774, loss = 1.09797434\n",
      "Iteration 775, loss = 1.09797323\n",
      "Iteration 776, loss = 1.09797213\n",
      "Iteration 777, loss = 1.09797103\n",
      "Iteration 778, loss = 1.09796995\n",
      "Iteration 779, loss = 1.09796887\n",
      "Iteration 780, loss = 1.09796780\n",
      "Iteration 781, loss = 1.09796674\n",
      "Iteration 782, loss = 1.09796568\n",
      "Iteration 783, loss = 1.09796463\n",
      "Iteration 784, loss = 1.09796359\n",
      "Iteration 785, loss = 1.09796255\n",
      "Iteration 786, loss = 1.09796152\n",
      "Iteration 787, loss = 1.09796050\n",
      "Iteration 788, loss = 1.09795949\n",
      "Iteration 789, loss = 1.09795848\n",
      "Iteration 790, loss = 1.09795748\n",
      "Iteration 791, loss = 1.09795649\n",
      "Iteration 792, loss = 1.09795550\n",
      "Iteration 793, loss = 1.09795452\n",
      "Iteration 794, loss = 1.09795355\n",
      "Iteration 795, loss = 1.09795258\n",
      "Iteration 796, loss = 1.09795162\n",
      "Iteration 797, loss = 1.09795067\n",
      "Iteration 798, loss = 1.09794972\n",
      "Iteration 799, loss = 1.09794878\n",
      "Iteration 800, loss = 1.09794784\n",
      "Iteration 801, loss = 1.09794692\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88375410\n",
      "Iteration 2, loss = 0.87908772\n",
      "Iteration 3, loss = 0.87248832\n",
      "Iteration 4, loss = 0.86420373\n",
      "Iteration 5, loss = 0.85447079\n",
      "Iteration 6, loss = 0.84350843\n",
      "Iteration 7, loss = 0.83151974\n",
      "Iteration 8, loss = 0.81869410\n",
      "Iteration 9, loss = 0.80521670\n",
      "Iteration 10, loss = 0.79122696\n",
      "Iteration 11, loss = 0.77686178\n",
      "Iteration 12, loss = 0.76224193\n",
      "Iteration 13, loss = 0.74746646\n",
      "Iteration 14, loss = 0.73266122\n",
      "Iteration 15, loss = 0.71789654\n",
      "Iteration 16, loss = 0.70325024\n",
      "Iteration 17, loss = 0.68880686\n",
      "Iteration 18, loss = 0.67460658\n",
      "Iteration 19, loss = 0.66064637\n",
      "Iteration 20, loss = 0.64696920\n",
      "Iteration 21, loss = 0.63359347\n",
      "Iteration 22, loss = 0.62054714\n",
      "Iteration 23, loss = 0.60785579\n",
      "Iteration 24, loss = 0.59552713\n",
      "Iteration 25, loss = 0.58357711\n",
      "Iteration 26, loss = 0.57200262\n",
      "Iteration 27, loss = 0.56080709\n",
      "Iteration 28, loss = 0.54998930\n",
      "Iteration 29, loss = 0.53956771\n",
      "Iteration 30, loss = 0.52951515\n",
      "Iteration 31, loss = 0.51983371\n",
      "Iteration 32, loss = 0.51052726\n",
      "Iteration 33, loss = 0.50158932\n",
      "Iteration 34, loss = 0.49300344\n",
      "Iteration 35, loss = 0.48476075\n",
      "Iteration 36, loss = 0.47684768\n",
      "Iteration 37, loss = 0.46925475\n",
      "Iteration 38, loss = 0.46194944\n",
      "Iteration 39, loss = 0.45493519\n",
      "Iteration 40, loss = 0.44820198\n",
      "Iteration 41, loss = 0.44172347\n",
      "Iteration 42, loss = 0.43547542\n",
      "Iteration 43, loss = 0.42945776\n",
      "Iteration 44, loss = 0.42366277\n",
      "Iteration 45, loss = 0.41808836\n",
      "Iteration 46, loss = 0.41271625\n",
      "Iteration 47, loss = 0.40753684\n",
      "Iteration 48, loss = 0.40252750\n",
      "Iteration 49, loss = 0.39768573\n",
      "Iteration 50, loss = 0.39301105\n",
      "Iteration 51, loss = 0.38848890\n",
      "Iteration 52, loss = 0.38411487\n",
      "Iteration 53, loss = 0.37988009\n",
      "Iteration 54, loss = 0.37578362\n",
      "Iteration 55, loss = 0.37181728\n",
      "Iteration 56, loss = 0.36797443\n",
      "Iteration 57, loss = 0.36425543\n",
      "Iteration 58, loss = 0.36064882\n",
      "Iteration 59, loss = 0.35715395\n",
      "Iteration 60, loss = 0.35376638\n",
      "Iteration 61, loss = 0.35048442\n",
      "Iteration 62, loss = 0.34729397\n",
      "Iteration 63, loss = 0.34419341\n",
      "Iteration 64, loss = 0.34117833\n",
      "Iteration 65, loss = 0.33824483\n",
      "Iteration 66, loss = 0.33538574\n",
      "Iteration 67, loss = 0.33259592\n",
      "Iteration 68, loss = 0.32986525\n",
      "Iteration 69, loss = 0.32720078\n",
      "Iteration 70, loss = 0.32459961\n",
      "Iteration 71, loss = 0.32205941\n",
      "Iteration 72, loss = 0.31957822\n",
      "Iteration 73, loss = 0.31715452\n",
      "Iteration 74, loss = 0.31478510\n",
      "Iteration 75, loss = 0.31246794\n",
      "Iteration 76, loss = 0.31020133\n",
      "Iteration 77, loss = 0.30798570\n",
      "Iteration 78, loss = 0.30582495\n",
      "Iteration 79, loss = 0.30371360\n",
      "Iteration 80, loss = 0.30164766\n",
      "Iteration 81, loss = 0.29962556\n",
      "Iteration 82, loss = 0.29764768\n",
      "Iteration 83, loss = 0.29571029\n",
      "Iteration 84, loss = 0.29381218\n",
      "Iteration 85, loss = 0.29195156\n",
      "Iteration 86, loss = 0.29012624\n",
      "Iteration 87, loss = 0.28833473\n",
      "Iteration 88, loss = 0.28657651\n",
      "Iteration 89, loss = 0.28485076\n",
      "Iteration 90, loss = 0.28315605\n",
      "Iteration 91, loss = 0.28149142\n",
      "Iteration 92, loss = 0.27985637\n",
      "Iteration 93, loss = 0.27825014\n",
      "Iteration 94, loss = 0.27667385\n",
      "Iteration 95, loss = 0.27512491\n",
      "Iteration 96, loss = 0.27360197\n",
      "Iteration 97, loss = 0.27210442\n",
      "Iteration 98, loss = 0.27063185\n",
      "Iteration 99, loss = 0.26918381\n",
      "Iteration 100, loss = 0.26775893\n",
      "Iteration 101, loss = 0.26635659\n",
      "Iteration 102, loss = 0.26497622\n",
      "Iteration 103, loss = 0.26361828\n",
      "Iteration 104, loss = 0.26228302\n",
      "Iteration 105, loss = 0.26096949\n",
      "Iteration 106, loss = 0.25967598\n",
      "Iteration 107, loss = 0.25840173\n",
      "Iteration 108, loss = 0.25714644\n",
      "Iteration 109, loss = 0.25590947\n",
      "Iteration 110, loss = 0.25469035\n",
      "Iteration 111, loss = 0.25348909\n",
      "Iteration 112, loss = 0.25230542\n",
      "Iteration 113, loss = 0.25113843\n",
      "Iteration 114, loss = 0.24998767\n",
      "Iteration 115, loss = 0.24885325\n",
      "Iteration 116, loss = 0.24773430\n",
      "Iteration 117, loss = 0.24663051\n",
      "Iteration 118, loss = 0.24554089\n",
      "Iteration 119, loss = 0.24446553\n",
      "Iteration 120, loss = 0.24340427\n",
      "Iteration 121, loss = 0.24235597\n",
      "Iteration 122, loss = 0.24132014\n",
      "Iteration 123, loss = 0.24029773\n",
      "Iteration 124, loss = 0.23928821\n",
      "Iteration 125, loss = 0.23829230\n",
      "Iteration 126, loss = 0.23731017\n",
      "Iteration 127, loss = 0.23634037\n",
      "Iteration 128, loss = 0.23538252\n",
      "Iteration 129, loss = 0.23443637\n",
      "Iteration 130, loss = 0.23350170\n",
      "Iteration 131, loss = 0.23257831\n",
      "Iteration 132, loss = 0.23166610\n",
      "Iteration 133, loss = 0.23076502\n",
      "Iteration 134, loss = 0.22987478\n",
      "Iteration 135, loss = 0.22899499\n",
      "Iteration 136, loss = 0.22812558\n",
      "Iteration 137, loss = 0.22726636\n",
      "Iteration 138, loss = 0.22641711\n",
      "Iteration 139, loss = 0.22557765\n",
      "Iteration 140, loss = 0.22474815\n",
      "Iteration 141, loss = 0.22392805\n",
      "Iteration 142, loss = 0.22311716\n",
      "Iteration 143, loss = 0.22231433\n",
      "Iteration 144, loss = 0.22151297\n",
      "Iteration 145, loss = 0.22072006\n",
      "Iteration 146, loss = 0.21993547\n",
      "Iteration 147, loss = 0.21915890\n",
      "Iteration 148, loss = 0.21839032\n",
      "Iteration 149, loss = 0.21762957\n",
      "Iteration 150, loss = 0.21687656\n",
      "Iteration 151, loss = 0.21613117\n",
      "Iteration 152, loss = 0.21539332\n",
      "Iteration 153, loss = 0.21466295\n",
      "Iteration 154, loss = 0.21393992\n",
      "Iteration 155, loss = 0.21322402\n",
      "Iteration 156, loss = 0.21251524\n",
      "Iteration 157, loss = 0.21181332\n",
      "Iteration 158, loss = 0.21111835\n",
      "Iteration 159, loss = 0.21043029\n",
      "Iteration 160, loss = 0.20974917\n",
      "Iteration 161, loss = 0.20907480\n",
      "Iteration 162, loss = 0.20840701\n",
      "Iteration 163, loss = 0.20774592\n",
      "Iteration 164, loss = 0.20709134\n",
      "Iteration 165, loss = 0.20644333\n",
      "Iteration 166, loss = 0.20580179\n",
      "Iteration 167, loss = 0.20516652\n",
      "Iteration 168, loss = 0.20453753\n",
      "Iteration 169, loss = 0.20391517\n",
      "Iteration 170, loss = 0.20329879\n",
      "Iteration 171, loss = 0.20268848\n",
      "Iteration 172, loss = 0.20208409\n",
      "Iteration 173, loss = 0.20148544\n",
      "Iteration 174, loss = 0.20089242\n",
      "Iteration 175, loss = 0.20030494\n",
      "Iteration 176, loss = 0.19972294\n",
      "Iteration 177, loss = 0.19914635\n",
      "Iteration 178, loss = 0.19857506\n",
      "Iteration 179, loss = 0.19800901\n",
      "Iteration 180, loss = 0.19744812\n",
      "Iteration 181, loss = 0.19689232\n",
      "Iteration 182, loss = 0.19633574\n",
      "Iteration 183, loss = 0.19578239\n",
      "Iteration 184, loss = 0.19523342\n",
      "Iteration 185, loss = 0.19468882\n",
      "Iteration 186, loss = 0.19414858\n",
      "Iteration 187, loss = 0.19361268\n",
      "Iteration 188, loss = 0.19308111\n",
      "Iteration 189, loss = 0.19255385\n",
      "Iteration 190, loss = 0.19203088\n",
      "Iteration 191, loss = 0.19151219\n",
      "Iteration 192, loss = 0.19099775\n",
      "Iteration 193, loss = 0.19048752\n",
      "Iteration 194, loss = 0.18998145\n",
      "Iteration 195, loss = 0.18947952\n",
      "Iteration 196, loss = 0.18898186\n",
      "Iteration 197, loss = 0.18848831\n",
      "Iteration 198, loss = 0.18799880\n",
      "Iteration 199, loss = 0.18751328\n",
      "Iteration 200, loss = 0.18703173\n",
      "Iteration 201, loss = 0.18655409\n",
      "Iteration 202, loss = 0.18608039\n",
      "Iteration 203, loss = 0.18561061\n",
      "Iteration 204, loss = 0.18514466\n",
      "Iteration 205, loss = 0.18468246\n",
      "Iteration 206, loss = 0.18422397\n",
      "Iteration 207, loss = 0.18376917\n",
      "Iteration 208, loss = 0.18331800\n",
      "Iteration 209, loss = 0.18287081\n",
      "Iteration 210, loss = 0.18242773\n",
      "Iteration 211, loss = 0.18198821\n",
      "Iteration 212, loss = 0.18155227\n",
      "Iteration 213, loss = 0.18111990\n",
      "Iteration 214, loss = 0.18069099\n",
      "Iteration 215, loss = 0.18026549\n",
      "Iteration 216, loss = 0.17984341\n",
      "Iteration 217, loss = 0.17942470\n",
      "Iteration 218, loss = 0.17900930\n",
      "Iteration 219, loss = 0.17859719\n",
      "Iteration 220, loss = 0.17818829\n",
      "Iteration 221, loss = 0.17778255\n",
      "Iteration 222, loss = 0.17737994\n",
      "Iteration 223, loss = 0.17698041\n",
      "Iteration 224, loss = 0.17658394\n",
      "Iteration 225, loss = 0.17619047\n",
      "Iteration 226, loss = 0.17579999\n",
      "Iteration 227, loss = 0.17541246\n",
      "Iteration 228, loss = 0.17502783\n",
      "Iteration 229, loss = 0.17464608\n",
      "Iteration 230, loss = 0.17426718\n",
      "Iteration 231, loss = 0.17389109\n",
      "Iteration 232, loss = 0.17351779\n",
      "Iteration 233, loss = 0.17314723\n",
      "Iteration 234, loss = 0.17277940\n",
      "Iteration 235, loss = 0.17241427\n",
      "Iteration 236, loss = 0.17205185\n",
      "Iteration 237, loss = 0.17169210\n",
      "Iteration 238, loss = 0.17133497\n",
      "Iteration 239, loss = 0.17098085\n",
      "Iteration 240, loss = 0.17062980\n",
      "Iteration 241, loss = 0.17028134\n",
      "Iteration 242, loss = 0.16993543\n",
      "Iteration 243, loss = 0.16959202\n",
      "Iteration 244, loss = 0.16925109\n",
      "Iteration 245, loss = 0.16891261\n",
      "Iteration 246, loss = 0.16857654\n",
      "Iteration 247, loss = 0.16824286\n",
      "Iteration 248, loss = 0.16791154\n",
      "Iteration 249, loss = 0.16758256\n",
      "Iteration 250, loss = 0.16725588\n",
      "Iteration 251, loss = 0.16693149\n",
      "Iteration 252, loss = 0.16660937\n",
      "Iteration 253, loss = 0.16629303\n",
      "Iteration 254, loss = 0.16598056\n",
      "Iteration 255, loss = 0.16567056\n",
      "Iteration 256, loss = 0.16536300\n",
      "Iteration 257, loss = 0.16505782\n",
      "Iteration 258, loss = 0.16475497\n",
      "Iteration 259, loss = 0.16445439\n",
      "Iteration 260, loss = 0.16415604\n",
      "Iteration 261, loss = 0.16385989\n",
      "Iteration 262, loss = 0.16356589\n",
      "Iteration 263, loss = 0.16327400\n",
      "Iteration 264, loss = 0.16298421\n",
      "Iteration 265, loss = 0.16269648\n",
      "Iteration 266, loss = 0.16241079\n",
      "Iteration 267, loss = 0.16212709\n",
      "Iteration 268, loss = 0.16184536\n",
      "Iteration 269, loss = 0.16156556\n",
      "Iteration 270, loss = 0.16128783\n",
      "Iteration 271, loss = 0.16101222\n",
      "Iteration 272, loss = 0.16073848\n",
      "Iteration 273, loss = 0.16046660\n",
      "Iteration 274, loss = 0.16019655\n",
      "Iteration 275, loss = 0.15992830\n",
      "Iteration 276, loss = 0.15966184\n",
      "Iteration 277, loss = 0.15939715\n",
      "Iteration 278, loss = 0.15913420\n",
      "Iteration 279, loss = 0.15887297\n",
      "Iteration 280, loss = 0.15861344\n",
      "Iteration 281, loss = 0.15835561\n",
      "Iteration 282, loss = 0.15809948\n",
      "Iteration 283, loss = 0.15784503\n",
      "Iteration 284, loss = 0.15759224\n",
      "Iteration 285, loss = 0.15734111\n",
      "Iteration 286, loss = 0.15709154\n",
      "Iteration 287, loss = 0.15684355\n",
      "Iteration 288, loss = 0.15659713\n",
      "Iteration 289, loss = 0.15635227\n",
      "Iteration 290, loss = 0.15610894\n",
      "Iteration 291, loss = 0.15586714\n",
      "Iteration 292, loss = 0.15562686\n",
      "Iteration 293, loss = 0.15538807\n",
      "Iteration 294, loss = 0.15515075\n",
      "Iteration 295, loss = 0.15491491\n",
      "Iteration 296, loss = 0.15468051\n",
      "Iteration 297, loss = 0.15444757\n",
      "Iteration 298, loss = 0.15421607\n",
      "Iteration 299, loss = 0.15398598\n",
      "Iteration 300, loss = 0.15375729\n",
      "Iteration 301, loss = 0.15352997\n",
      "Iteration 302, loss = 0.15330402\n",
      "Iteration 303, loss = 0.15307936\n",
      "Iteration 304, loss = 0.15285585\n",
      "Iteration 305, loss = 0.15263364\n",
      "Iteration 306, loss = 0.15241274\n",
      "Iteration 307, loss = 0.15219318\n",
      "Iteration 308, loss = 0.15197491\n",
      "Iteration 309, loss = 0.15175793\n",
      "Iteration 310, loss = 0.15154221\n",
      "Iteration 311, loss = 0.15132775\n",
      "Iteration 312, loss = 0.15111457\n",
      "Iteration 313, loss = 0.15090262\n",
      "Iteration 314, loss = 0.15069191\n",
      "Iteration 315, loss = 0.15048242\n",
      "Iteration 316, loss = 0.15027416\n",
      "Iteration 317, loss = 0.15006710\n",
      "Iteration 318, loss = 0.14986124\n",
      "Iteration 319, loss = 0.14965665\n",
      "Iteration 320, loss = 0.14945331\n",
      "Iteration 321, loss = 0.14925114\n",
      "Iteration 322, loss = 0.14905013\n",
      "Iteration 323, loss = 0.14885027\n",
      "Iteration 324, loss = 0.14865155\n",
      "Iteration 325, loss = 0.14845396\n",
      "Iteration 326, loss = 0.14825749\n",
      "Iteration 327, loss = 0.14806214\n",
      "Iteration 328, loss = 0.14786788\n",
      "Iteration 329, loss = 0.14767473\n",
      "Iteration 330, loss = 0.14748265\n",
      "Iteration 331, loss = 0.14729164\n",
      "Iteration 332, loss = 0.14710170\n",
      "Iteration 333, loss = 0.14691281\n",
      "Iteration 334, loss = 0.14672498\n",
      "Iteration 335, loss = 0.14653817\n",
      "Iteration 336, loss = 0.14635238\n",
      "Iteration 337, loss = 0.14616763\n",
      "Iteration 338, loss = 0.14598389\n",
      "Iteration 339, loss = 0.14580116\n",
      "Iteration 340, loss = 0.14561944\n",
      "Iteration 341, loss = 0.14543872\n",
      "Iteration 342, loss = 0.14525898\n",
      "Iteration 343, loss = 0.14508024\n",
      "Iteration 344, loss = 0.14490247\n",
      "Iteration 345, loss = 0.14472566\n",
      "Iteration 346, loss = 0.14454981\n",
      "Iteration 347, loss = 0.14437491\n",
      "Iteration 348, loss = 0.14420095\n",
      "Iteration 349, loss = 0.14402792\n",
      "Iteration 350, loss = 0.14385584\n",
      "Iteration 351, loss = 0.14368464\n",
      "Iteration 352, loss = 0.14351434\n",
      "Iteration 353, loss = 0.14334494\n",
      "Iteration 354, loss = 0.14317644\n",
      "Iteration 355, loss = 0.14300882\n",
      "Iteration 356, loss = 0.14284209\n",
      "Iteration 357, loss = 0.14267623\n",
      "Iteration 358, loss = 0.14251125\n",
      "Iteration 359, loss = 0.14234712\n",
      "Iteration 360, loss = 0.14218387\n",
      "Iteration 361, loss = 0.14202148\n",
      "Iteration 362, loss = 0.14185993\n",
      "Iteration 363, loss = 0.14169923\n",
      "Iteration 364, loss = 0.14153936\n",
      "Iteration 365, loss = 0.14138032\n",
      "Iteration 366, loss = 0.14122211\n",
      "Iteration 367, loss = 0.14106470\n",
      "Iteration 368, loss = 0.14090811\n",
      "Iteration 369, loss = 0.14075232\n",
      "Iteration 370, loss = 0.14059732\n",
      "Iteration 371, loss = 0.14044312\n",
      "Iteration 372, loss = 0.14028970\n",
      "Iteration 373, loss = 0.14013706\n",
      "Iteration 374, loss = 0.13998519\n",
      "Iteration 375, loss = 0.13983409\n",
      "Iteration 376, loss = 0.13968375\n",
      "Iteration 377, loss = 0.13953417\n",
      "Iteration 378, loss = 0.13938534\n",
      "Iteration 379, loss = 0.13923726\n",
      "Iteration 380, loss = 0.13908991\n",
      "Iteration 381, loss = 0.13894330\n",
      "Iteration 382, loss = 0.13879742\n",
      "Iteration 383, loss = 0.13865226\n",
      "Iteration 384, loss = 0.13850782\n",
      "Iteration 385, loss = 0.13836409\n",
      "Iteration 386, loss = 0.13822108\n",
      "Iteration 387, loss = 0.13807876\n",
      "Iteration 388, loss = 0.13793715\n",
      "Iteration 389, loss = 0.13779623\n",
      "Iteration 390, loss = 0.13765599\n",
      "Iteration 391, loss = 0.13751645\n",
      "Iteration 392, loss = 0.13737758\n",
      "Iteration 393, loss = 0.13723938\n",
      "Iteration 394, loss = 0.13710186\n",
      "Iteration 395, loss = 0.13696501\n",
      "Iteration 396, loss = 0.13682883\n",
      "Iteration 397, loss = 0.13669331\n",
      "Iteration 398, loss = 0.13655843\n",
      "Iteration 399, loss = 0.13642420\n",
      "Iteration 400, loss = 0.13629062\n",
      "Iteration 401, loss = 0.13615767\n",
      "Iteration 402, loss = 0.13602536\n",
      "Iteration 403, loss = 0.13589368\n",
      "Iteration 404, loss = 0.13576262\n",
      "Iteration 405, loss = 0.13563218\n",
      "Iteration 406, loss = 0.13550239\n",
      "Iteration 407, loss = 0.13537323\n",
      "Iteration 408, loss = 0.13524469\n",
      "Iteration 409, loss = 0.13511676\n",
      "Iteration 410, loss = 0.13498942\n",
      "Iteration 411, loss = 0.13486269\n",
      "Iteration 412, loss = 0.13473655\n",
      "Iteration 413, loss = 0.13461100\n",
      "Iteration 414, loss = 0.13448607\n",
      "Iteration 415, loss = 0.13436171\n",
      "Iteration 416, loss = 0.13423793\n",
      "Iteration 417, loss = 0.13411472\n",
      "Iteration 418, loss = 0.13399208\n",
      "Iteration 419, loss = 0.13387003\n",
      "Iteration 420, loss = 0.13374857\n",
      "Iteration 421, loss = 0.13362767\n",
      "Iteration 422, loss = 0.13350732\n",
      "Iteration 423, loss = 0.13338753\n",
      "Iteration 424, loss = 0.13326828\n",
      "Iteration 425, loss = 0.13314958\n",
      "Iteration 426, loss = 0.13303142\n",
      "Iteration 427, loss = 0.13291379\n",
      "Iteration 428, loss = 0.13279670\n",
      "Iteration 429, loss = 0.13268013\n",
      "Iteration 430, loss = 0.13256409\n",
      "Iteration 431, loss = 0.13244864\n",
      "Iteration 432, loss = 0.13233383\n",
      "Iteration 433, loss = 0.13221955\n",
      "Iteration 434, loss = 0.13210578\n",
      "Iteration 435, loss = 0.13199252\n",
      "Iteration 436, loss = 0.13187978\n",
      "Iteration 437, loss = 0.13176754\n",
      "Iteration 438, loss = 0.13165580\n",
      "Iteration 439, loss = 0.13154456\n",
      "Iteration 440, loss = 0.13143382\n",
      "Iteration 441, loss = 0.13132357\n",
      "Iteration 442, loss = 0.13121380\n",
      "Iteration 443, loss = 0.13110452\n",
      "Iteration 444, loss = 0.13099572\n",
      "Iteration 445, loss = 0.13088739\n",
      "Iteration 446, loss = 0.13077955\n",
      "Iteration 447, loss = 0.13067218\n",
      "Iteration 448, loss = 0.13056528\n",
      "Iteration 449, loss = 0.13045884\n",
      "Iteration 450, loss = 0.13035287\n",
      "Iteration 451, loss = 0.13024737\n",
      "Iteration 452, loss = 0.13014230\n",
      "Iteration 453, loss = 0.13003767\n",
      "Iteration 454, loss = 0.12993349\n",
      "Iteration 455, loss = 0.12982976\n",
      "Iteration 456, loss = 0.12972646\n",
      "Iteration 457, loss = 0.12962361\n",
      "Iteration 458, loss = 0.12952119\n",
      "Iteration 459, loss = 0.12941921\n",
      "Iteration 460, loss = 0.12931766\n",
      "Iteration 461, loss = 0.12921653\n",
      "Iteration 462, loss = 0.12911584\n",
      "Iteration 463, loss = 0.12901557\n",
      "Iteration 464, loss = 0.12891572\n",
      "Iteration 465, loss = 0.12881629\n",
      "Iteration 466, loss = 0.12871728\n",
      "Iteration 467, loss = 0.12861868\n",
      "Iteration 468, loss = 0.12852049\n",
      "Iteration 469, loss = 0.12842271\n",
      "Iteration 470, loss = 0.12832534\n",
      "Iteration 471, loss = 0.12822838\n",
      "Iteration 472, loss = 0.12813182\n",
      "Iteration 473, loss = 0.12803568\n",
      "Iteration 474, loss = 0.12793995\n",
      "Iteration 475, loss = 0.12784462\n",
      "Iteration 476, loss = 0.12774969\n",
      "Iteration 477, loss = 0.12765515\n",
      "Iteration 478, loss = 0.12756099\n",
      "Iteration 479, loss = 0.12746722\n",
      "Iteration 480, loss = 0.12737383\n",
      "Iteration 481, loss = 0.12728082\n",
      "Iteration 482, loss = 0.12718818\n",
      "Iteration 483, loss = 0.12709593\n",
      "Iteration 484, loss = 0.12700404\n",
      "Iteration 485, loss = 0.12691253\n",
      "Iteration 486, loss = 0.12682139\n",
      "Iteration 487, loss = 0.12673061\n",
      "Iteration 488, loss = 0.12664020\n",
      "Iteration 489, loss = 0.12655016\n",
      "Iteration 490, loss = 0.12646047\n",
      "Iteration 491, loss = 0.12637114\n",
      "Iteration 492, loss = 0.12628217\n",
      "Iteration 493, loss = 0.12619356\n",
      "Iteration 494, loss = 0.12610529\n",
      "Iteration 495, loss = 0.12601738\n",
      "Iteration 496, loss = 0.12592981\n",
      "Iteration 497, loss = 0.12584259\n",
      "Iteration 498, loss = 0.12575572\n",
      "Iteration 499, loss = 0.12566919\n",
      "Iteration 500, loss = 0.12558300\n",
      "Iteration 501, loss = 0.12549714\n",
      "Iteration 502, loss = 0.12541163\n",
      "Iteration 503, loss = 0.12532645\n",
      "Iteration 504, loss = 0.12524160\n",
      "Iteration 505, loss = 0.12515708\n",
      "Iteration 506, loss = 0.12507289\n",
      "Iteration 507, loss = 0.12498903\n",
      "Iteration 508, loss = 0.12490553\n",
      "Iteration 509, loss = 0.12482238\n",
      "Iteration 510, loss = 0.12473956\n",
      "Iteration 511, loss = 0.12465707\n",
      "Iteration 512, loss = 0.12457490\n",
      "Iteration 513, loss = 0.12449305\n",
      "Iteration 514, loss = 0.12441152\n",
      "Iteration 515, loss = 0.12433031\n",
      "Iteration 516, loss = 0.12424941\n",
      "Iteration 517, loss = 0.12416883\n",
      "Iteration 518, loss = 0.12408855\n",
      "Iteration 519, loss = 0.12400858\n",
      "Iteration 520, loss = 0.12392892\n",
      "Iteration 521, loss = 0.12384956\n",
      "Iteration 522, loss = 0.12377050\n",
      "Iteration 523, loss = 0.12369175\n",
      "Iteration 524, loss = 0.12361332\n",
      "Iteration 525, loss = 0.12353518\n",
      "Iteration 526, loss = 0.12345734\n",
      "Iteration 527, loss = 0.12337980\n",
      "Iteration 528, loss = 0.12330254\n",
      "Iteration 529, loss = 0.12322558\n",
      "Iteration 530, loss = 0.12314890\n",
      "Iteration 531, loss = 0.12307252\n",
      "Iteration 532, loss = 0.12299641\n",
      "Iteration 533, loss = 0.12292059\n",
      "Iteration 534, loss = 0.12284505\n",
      "Iteration 535, loss = 0.12276979\n",
      "Iteration 536, loss = 0.12269481\n",
      "Iteration 537, loss = 0.12262011\n",
      "Iteration 538, loss = 0.12254570\n",
      "Iteration 539, loss = 0.12247155\n",
      "Iteration 540, loss = 0.12239768\n",
      "Iteration 541, loss = 0.12232409\n",
      "Iteration 542, loss = 0.12225076\n",
      "Iteration 543, loss = 0.12217770\n",
      "Iteration 544, loss = 0.12210490\n",
      "Iteration 545, loss = 0.12203238\n",
      "Iteration 546, loss = 0.12196012\n",
      "Iteration 547, loss = 0.12188812\n",
      "Iteration 548, loss = 0.12181639\n",
      "Iteration 549, loss = 0.12174491\n",
      "Iteration 550, loss = 0.12167370\n",
      "Iteration 551, loss = 0.12160274\n",
      "Iteration 552, loss = 0.12153203\n",
      "Iteration 553, loss = 0.12146158\n",
      "Iteration 554, loss = 0.12139139\n",
      "Iteration 555, loss = 0.12132144\n",
      "Iteration 556, loss = 0.12125174\n",
      "Iteration 557, loss = 0.12118230\n",
      "Iteration 558, loss = 0.12111310\n",
      "Iteration 559, loss = 0.12104415\n",
      "Iteration 560, loss = 0.12097544\n",
      "Iteration 561, loss = 0.12090697\n",
      "Iteration 562, loss = 0.12083875\n",
      "Iteration 563, loss = 0.12077077\n",
      "Iteration 564, loss = 0.12070303\n",
      "Iteration 565, loss = 0.12063553\n",
      "Iteration 566, loss = 0.12056826\n",
      "Iteration 567, loss = 0.12050123\n",
      "Iteration 568, loss = 0.12043444\n",
      "Iteration 569, loss = 0.12036787\n",
      "Iteration 570, loss = 0.12030154\n",
      "Iteration 571, loss = 0.12023544\n",
      "Iteration 572, loss = 0.12016957\n",
      "Iteration 573, loss = 0.12010392\n",
      "Iteration 574, loss = 0.12003850\n",
      "Iteration 575, loss = 0.11997331\n",
      "Iteration 576, loss = 0.11990835\n",
      "Iteration 577, loss = 0.11984361\n",
      "Iteration 578, loss = 0.11977909\n",
      "Iteration 579, loss = 0.11971480\n",
      "Iteration 580, loss = 0.11965072\n",
      "Iteration 581, loss = 0.11958687\n",
      "Iteration 582, loss = 0.11952323\n",
      "Iteration 583, loss = 0.11945981\n",
      "Iteration 584, loss = 0.11939661\n",
      "Iteration 585, loss = 0.11933363\n",
      "Iteration 586, loss = 0.11927085\n",
      "Iteration 587, loss = 0.11920829\n",
      "Iteration 588, loss = 0.11914594\n",
      "Iteration 589, loss = 0.11908380\n",
      "Iteration 590, loss = 0.11902187\n",
      "Iteration 591, loss = 0.11896015\n",
      "Iteration 592, loss = 0.11889864\n",
      "Iteration 593, loss = 0.11883733\n",
      "Iteration 594, loss = 0.11877623\n",
      "Iteration 595, loss = 0.11871534\n",
      "Iteration 596, loss = 0.11865464\n",
      "Iteration 597, loss = 0.11859415\n",
      "Iteration 598, loss = 0.11853387\n",
      "Iteration 599, loss = 0.11847378\n",
      "Iteration 600, loss = 0.11841389\n",
      "Iteration 601, loss = 0.11835420\n",
      "Iteration 602, loss = 0.11829471\n",
      "Iteration 603, loss = 0.11823542\n",
      "Iteration 604, loss = 0.11817632\n",
      "Iteration 605, loss = 0.11811742\n",
      "Iteration 606, loss = 0.11805871\n",
      "Iteration 607, loss = 0.11800020\n",
      "Iteration 608, loss = 0.11794189\n",
      "Iteration 609, loss = 0.11788377\n",
      "Iteration 610, loss = 0.11782584\n",
      "Iteration 611, loss = 0.11776812\n",
      "Iteration 612, loss = 0.11771060\n",
      "Iteration 613, loss = 0.11765326\n",
      "Iteration 614, loss = 0.11759611\n",
      "Iteration 615, loss = 0.11753915\n",
      "Iteration 616, loss = 0.11748237\n",
      "Iteration 617, loss = 0.11742578\n",
      "Iteration 618, loss = 0.11736937\n",
      "Iteration 619, loss = 0.11731315\n",
      "Iteration 620, loss = 0.11725710\n",
      "Iteration 621, loss = 0.11720124\n",
      "Iteration 622, loss = 0.11714556\n",
      "Iteration 623, loss = 0.11709005\n",
      "Iteration 624, loss = 0.11703473\n",
      "Iteration 625, loss = 0.11697958\n",
      "Iteration 626, loss = 0.11692460\n",
      "Iteration 627, loss = 0.11686981\n",
      "Iteration 628, loss = 0.11681519\n",
      "Iteration 629, loss = 0.11676074\n",
      "Iteration 630, loss = 0.11670646\n",
      "Iteration 631, loss = 0.11665236\n",
      "Iteration 632, loss = 0.11659843\n",
      "Iteration 633, loss = 0.11654466\n",
      "Iteration 634, loss = 0.11649107\n",
      "Iteration 635, loss = 0.11643765\n",
      "Iteration 636, loss = 0.11638439\n",
      "Iteration 637, loss = 0.11633131\n",
      "Iteration 638, loss = 0.11627838\n",
      "Iteration 639, loss = 0.11622563\n",
      "Iteration 640, loss = 0.11617304\n",
      "Iteration 641, loss = 0.11612061\n",
      "Iteration 642, loss = 0.11606835\n",
      "Iteration 643, loss = 0.11601625\n",
      "Iteration 644, loss = 0.11596431\n",
      "Iteration 645, loss = 0.11591253\n",
      "Iteration 646, loss = 0.11586091\n",
      "Iteration 647, loss = 0.11580945\n",
      "Iteration 648, loss = 0.11575815\n",
      "Iteration 649, loss = 0.11570701\n",
      "Iteration 650, loss = 0.11565603\n",
      "Iteration 651, loss = 0.11560522\n",
      "Iteration 652, loss = 0.11555456\n",
      "Iteration 653, loss = 0.11550406\n",
      "Iteration 654, loss = 0.11545372\n",
      "Iteration 655, loss = 0.11540353\n",
      "Iteration 656, loss = 0.11535349\n",
      "Iteration 657, loss = 0.11530360\n",
      "Iteration 658, loss = 0.11525387\n",
      "Iteration 659, loss = 0.11520429\n",
      "Iteration 660, loss = 0.11515486\n",
      "Iteration 661, loss = 0.11510557\n",
      "Iteration 662, loss = 0.11505644\n",
      "Iteration 663, loss = 0.11500746\n",
      "Iteration 664, loss = 0.11495862\n",
      "Iteration 665, loss = 0.11490993\n",
      "Iteration 666, loss = 0.11486139\n",
      "Iteration 667, loss = 0.11481299\n",
      "Iteration 668, loss = 0.11476474\n",
      "Iteration 669, loss = 0.11471662\n",
      "Iteration 670, loss = 0.11466864\n",
      "Iteration 671, loss = 0.11462080\n",
      "Iteration 672, loss = 0.11457310\n",
      "Iteration 673, loss = 0.11452555\n",
      "Iteration 674, loss = 0.11447814\n",
      "Iteration 675, loss = 0.11443086\n",
      "Iteration 676, loss = 0.11438372\n",
      "Iteration 677, loss = 0.11433673\n",
      "Iteration 678, loss = 0.11428987\n",
      "Iteration 679, loss = 0.11424315\n",
      "Iteration 680, loss = 0.11419656\n",
      "Iteration 681, loss = 0.11415012\n",
      "Iteration 682, loss = 0.11410380\n",
      "Iteration 683, loss = 0.11405763\n",
      "Iteration 684, loss = 0.11401158\n",
      "Iteration 685, loss = 0.11396568\n",
      "Iteration 686, loss = 0.11391990\n",
      "Iteration 687, loss = 0.11387426\n",
      "Iteration 688, loss = 0.11382875\n",
      "Iteration 689, loss = 0.11378354\n",
      "Iteration 690, loss = 0.11373846\n",
      "Iteration 691, loss = 0.11369352\n",
      "Iteration 692, loss = 0.11364872\n",
      "Iteration 693, loss = 0.11360405\n",
      "Iteration 694, loss = 0.11355951\n",
      "Iteration 695, loss = 0.11351511\n",
      "Iteration 696, loss = 0.11347084\n",
      "Iteration 697, loss = 0.11342669\n",
      "Iteration 698, loss = 0.11338268\n",
      "Iteration 699, loss = 0.11333879\n",
      "Iteration 700, loss = 0.11329504\n",
      "Iteration 701, loss = 0.11325140\n",
      "Iteration 702, loss = 0.11320790\n",
      "Iteration 703, loss = 0.11316452\n",
      "Iteration 704, loss = 0.11312126\n",
      "Iteration 705, loss = 0.11307813\n",
      "Iteration 706, loss = 0.11303512\n",
      "Iteration 707, loss = 0.11299223\n",
      "Iteration 708, loss = 0.11294947\n",
      "Iteration 709, loss = 0.11290682\n",
      "Iteration 710, loss = 0.11286430\n",
      "Iteration 711, loss = 0.11282192\n",
      "Iteration 712, loss = 0.11277968\n",
      "Iteration 713, loss = 0.11273756\n",
      "Iteration 714, loss = 0.11269556\n",
      "Iteration 715, loss = 0.11265368\n",
      "Iteration 716, loss = 0.11261192\n",
      "Iteration 717, loss = 0.11257027\n",
      "Iteration 718, loss = 0.11252875\n",
      "Iteration 719, loss = 0.11248734\n",
      "Iteration 720, loss = 0.11244604\n",
      "Iteration 721, loss = 0.11240486\n",
      "Iteration 722, loss = 0.11236380\n",
      "Iteration 723, loss = 0.11232285\n",
      "Iteration 724, loss = 0.11228201\n",
      "Iteration 725, loss = 0.11224128\n",
      "Iteration 726, loss = 0.11220067\n",
      "Iteration 727, loss = 0.11216017\n",
      "Iteration 728, loss = 0.11211978\n",
      "Iteration 729, loss = 0.11207950\n",
      "Iteration 730, loss = 0.11203933\n",
      "Iteration 731, loss = 0.11199927\n",
      "Iteration 732, loss = 0.11195932\n",
      "Iteration 733, loss = 0.11191948\n",
      "Iteration 734, loss = 0.11187975\n",
      "Iteration 735, loss = 0.11184012\n",
      "Iteration 736, loss = 0.11180061\n",
      "Iteration 737, loss = 0.11176119\n",
      "Iteration 738, loss = 0.11172189\n",
      "Iteration 739, loss = 0.11168269\n",
      "Iteration 740, loss = 0.11164359\n",
      "Iteration 741, loss = 0.11160460\n",
      "Iteration 742, loss = 0.11156573\n",
      "Iteration 743, loss = 0.11152696\n",
      "Iteration 744, loss = 0.11148830\n",
      "Iteration 745, loss = 0.11144974\n",
      "Iteration 746, loss = 0.11141128\n",
      "Iteration 747, loss = 0.11137293\n",
      "Iteration 748, loss = 0.11133472\n",
      "Iteration 749, loss = 0.11129669\n",
      "Iteration 750, loss = 0.11125876\n",
      "Iteration 751, loss = 0.11122094\n",
      "Iteration 752, loss = 0.11118322\n",
      "Iteration 753, loss = 0.11114561\n",
      "Iteration 754, loss = 0.11110810\n",
      "Iteration 755, loss = 0.11107069\n",
      "Iteration 756, loss = 0.11103339\n",
      "Iteration 757, loss = 0.11099619\n",
      "Iteration 758, loss = 0.11095909\n",
      "Iteration 759, loss = 0.11092209\n",
      "Iteration 760, loss = 0.11088519\n",
      "Iteration 761, loss = 0.11084839\n",
      "Iteration 762, loss = 0.11081168\n",
      "Iteration 763, loss = 0.11077508\n",
      "Iteration 764, loss = 0.11073857\n",
      "Iteration 765, loss = 0.11070215\n",
      "Iteration 766, loss = 0.11066583\n",
      "Iteration 767, loss = 0.11062961\n",
      "Iteration 768, loss = 0.11059348\n",
      "Iteration 769, loss = 0.11055744\n",
      "Iteration 770, loss = 0.11052150\n",
      "Iteration 771, loss = 0.11048565\n",
      "Iteration 772, loss = 0.11044989\n",
      "Iteration 773, loss = 0.11041423\n",
      "Iteration 774, loss = 0.11037865\n",
      "Iteration 775, loss = 0.11034317\n",
      "Iteration 776, loss = 0.11030778\n",
      "Iteration 777, loss = 0.11027247\n",
      "Iteration 778, loss = 0.11023726\n",
      "Iteration 779, loss = 0.11020214\n",
      "Iteration 780, loss = 0.11016711\n",
      "Iteration 781, loss = 0.11013216\n",
      "Iteration 782, loss = 0.11009731\n",
      "Iteration 783, loss = 0.11006254\n",
      "Iteration 784, loss = 0.11002786\n",
      "Iteration 785, loss = 0.10999327\n",
      "Iteration 786, loss = 0.10995877\n",
      "Iteration 787, loss = 0.10992435\n",
      "Iteration 788, loss = 0.10989001\n",
      "Iteration 789, loss = 0.10985577\n",
      "Iteration 790, loss = 0.10982161\n",
      "Iteration 791, loss = 0.10978753\n",
      "Iteration 792, loss = 0.10975354\n",
      "Iteration 793, loss = 0.10971963\n",
      "Iteration 794, loss = 0.10968581\n",
      "Iteration 795, loss = 0.10965207\n",
      "Iteration 796, loss = 0.10961841\n",
      "Iteration 797, loss = 0.10958484\n",
      "Iteration 798, loss = 0.10955135\n",
      "Iteration 799, loss = 0.10951794\n",
      "Iteration 800, loss = 0.10948462\n",
      "Iteration 801, loss = 0.10945137\n",
      "Iteration 802, loss = 0.10941821\n",
      "Iteration 803, loss = 0.10938513\n",
      "Iteration 804, loss = 0.10935213\n",
      "Iteration 805, loss = 0.10931921\n",
      "Iteration 806, loss = 0.10928638\n",
      "Iteration 807, loss = 0.10925362\n",
      "Iteration 808, loss = 0.10922094\n",
      "Iteration 809, loss = 0.10918834\n",
      "Iteration 810, loss = 0.10915582\n",
      "Iteration 811, loss = 0.10912338\n",
      "Iteration 812, loss = 0.10909101\n",
      "Iteration 813, loss = 0.10905873\n",
      "Iteration 814, loss = 0.10902652\n",
      "Iteration 815, loss = 0.10899439\n",
      "Iteration 816, loss = 0.10896233\n",
      "Iteration 817, loss = 0.10893036\n",
      "Iteration 818, loss = 0.10889847\n",
      "Iteration 819, loss = 0.10886665\n",
      "Iteration 820, loss = 0.10883491\n",
      "Iteration 821, loss = 0.10880324\n",
      "Iteration 822, loss = 0.10877165\n",
      "Iteration 823, loss = 0.10874014\n",
      "Iteration 824, loss = 0.10870870\n",
      "Iteration 825, loss = 0.10867733\n",
      "Iteration 826, loss = 0.10864604\n",
      "Iteration 827, loss = 0.10861483\n",
      "Iteration 828, loss = 0.10858368\n",
      "Iteration 829, loss = 0.10855261\n",
      "Iteration 830, loss = 0.10852162\n",
      "Iteration 831, loss = 0.10849069\n",
      "Iteration 832, loss = 0.10845984\n",
      "Iteration 833, loss = 0.10842906\n",
      "Iteration 834, loss = 0.10839836\n",
      "Iteration 835, loss = 0.10836773\n",
      "Iteration 836, loss = 0.10833717\n",
      "Iteration 837, loss = 0.10830668\n",
      "Iteration 838, loss = 0.10827626\n",
      "Iteration 839, loss = 0.10824592\n",
      "Iteration 840, loss = 0.10821564\n",
      "Iteration 841, loss = 0.10818544\n",
      "Iteration 842, loss = 0.10815531\n",
      "Iteration 843, loss = 0.10812525\n",
      "Iteration 844, loss = 0.10809526\n",
      "Iteration 845, loss = 0.10806534\n",
      "Iteration 846, loss = 0.10803549\n",
      "Iteration 847, loss = 0.10800570\n",
      "Iteration 848, loss = 0.10797599\n",
      "Iteration 849, loss = 0.10794635\n",
      "Iteration 850, loss = 0.10791677\n",
      "Iteration 851, loss = 0.10788726\n",
      "Iteration 852, loss = 0.10785782\n",
      "Iteration 853, loss = 0.10782845\n",
      "Iteration 854, loss = 0.10779914\n",
      "Iteration 855, loss = 0.10776990\n",
      "Iteration 856, loss = 0.10774073\n",
      "Iteration 857, loss = 0.10771167\n",
      "Iteration 858, loss = 0.10768328\n",
      "Iteration 859, loss = 0.10765498\n",
      "Iteration 860, loss = 0.10762676\n",
      "Iteration 861, loss = 0.10759862\n",
      "Iteration 862, loss = 0.10757055\n",
      "Iteration 863, loss = 0.10754257\n",
      "Iteration 864, loss = 0.10751465\n",
      "Iteration 865, loss = 0.10748681\n",
      "Iteration 866, loss = 0.10745905\n",
      "Iteration 867, loss = 0.10743135\n",
      "Iteration 868, loss = 0.10740372\n",
      "Iteration 869, loss = 0.10737616\n",
      "Iteration 870, loss = 0.10734866\n",
      "Iteration 871, loss = 0.10732123\n",
      "Iteration 872, loss = 0.10729387\n",
      "Iteration 873, loss = 0.10726657\n",
      "Iteration 874, loss = 0.10723934\n",
      "Iteration 875, loss = 0.10721217\n",
      "Iteration 876, loss = 0.10718506\n",
      "Iteration 877, loss = 0.10715801\n",
      "Iteration 878, loss = 0.10713102\n",
      "Iteration 879, loss = 0.10710410\n",
      "Iteration 880, loss = 0.10707723\n",
      "Iteration 881, loss = 0.10705043\n",
      "Iteration 882, loss = 0.10702368\n",
      "Iteration 883, loss = 0.10699700\n",
      "Iteration 884, loss = 0.10697037\n",
      "Iteration 885, loss = 0.10694380\n",
      "Iteration 886, loss = 0.10691729\n",
      "Iteration 887, loss = 0.10689083\n",
      "Iteration 888, loss = 0.10686444\n",
      "Iteration 889, loss = 0.10683810\n",
      "Iteration 890, loss = 0.10681182\n",
      "Iteration 891, loss = 0.10678559\n",
      "Iteration 892, loss = 0.10675942\n",
      "Iteration 893, loss = 0.10673331\n",
      "Iteration 894, loss = 0.10670725\n",
      "Iteration 895, loss = 0.10668125\n",
      "Iteration 896, loss = 0.10665531\n",
      "Iteration 897, loss = 0.10662942\n",
      "Iteration 898, loss = 0.10660358\n",
      "Iteration 899, loss = 0.10657780\n",
      "Iteration 900, loss = 0.10655207\n",
      "Iteration 901, loss = 0.10652640\n",
      "Iteration 902, loss = 0.10650078\n",
      "Iteration 903, loss = 0.10647521\n",
      "Iteration 904, loss = 0.10644970\n",
      "Iteration 905, loss = 0.10642424\n",
      "Iteration 906, loss = 0.10639884\n",
      "Iteration 907, loss = 0.10637348\n",
      "Iteration 908, loss = 0.10634818\n",
      "Iteration 909, loss = 0.10632293\n",
      "Iteration 910, loss = 0.10629773\n",
      "Iteration 911, loss = 0.10627259\n",
      "Iteration 912, loss = 0.10624749\n",
      "Iteration 913, loss = 0.10622245\n",
      "Iteration 914, loss = 0.10619746\n",
      "Iteration 915, loss = 0.10617253\n",
      "Iteration 916, loss = 0.10614764\n",
      "Iteration 917, loss = 0.10612280\n",
      "Iteration 918, loss = 0.10609802\n",
      "Iteration 919, loss = 0.10607328\n",
      "Iteration 920, loss = 0.10604860\n",
      "Iteration 921, loss = 0.10602397\n",
      "Iteration 922, loss = 0.10599938\n",
      "Iteration 923, loss = 0.10597485\n",
      "Iteration 924, loss = 0.10595037\n",
      "Iteration 925, loss = 0.10592593\n",
      "Iteration 926, loss = 0.10590155\n",
      "Iteration 927, loss = 0.10587722\n",
      "Iteration 928, loss = 0.10585293\n",
      "Iteration 929, loss = 0.10582869\n",
      "Iteration 930, loss = 0.10580451\n",
      "Iteration 931, loss = 0.10578037\n",
      "Iteration 932, loss = 0.10575627\n",
      "Iteration 933, loss = 0.10573223\n",
      "Iteration 934, loss = 0.10570824\n",
      "Iteration 935, loss = 0.10568429\n",
      "Iteration 936, loss = 0.10566039\n",
      "Iteration 937, loss = 0.10563654\n",
      "Iteration 938, loss = 0.10561274\n",
      "Iteration 939, loss = 0.10558897\n",
      "Iteration 940, loss = 0.10556526\n",
      "Iteration 941, loss = 0.10554159\n",
      "Iteration 942, loss = 0.10551797\n",
      "Iteration 943, loss = 0.10549439\n",
      "Iteration 944, loss = 0.10547086\n",
      "Iteration 945, loss = 0.10544738\n",
      "Iteration 946, loss = 0.10542394\n",
      "Iteration 947, loss = 0.10540055\n",
      "Iteration 948, loss = 0.10537720\n",
      "Iteration 949, loss = 0.10535390\n",
      "Iteration 950, loss = 0.10533065\n",
      "Iteration 951, loss = 0.10530744\n",
      "Iteration 952, loss = 0.10528427\n",
      "Iteration 953, loss = 0.10526115\n",
      "Iteration 954, loss = 0.10523808\n",
      "Iteration 955, loss = 0.10521505\n",
      "Iteration 956, loss = 0.10519207\n",
      "Iteration 957, loss = 0.10516913\n",
      "Iteration 958, loss = 0.10514623\n",
      "Iteration 959, loss = 0.10512338\n",
      "Iteration 960, loss = 0.10510057\n",
      "Iteration 961, loss = 0.10507781\n",
      "Iteration 962, loss = 0.10505509\n",
      "Iteration 963, loss = 0.10503241\n",
      "Iteration 964, loss = 0.10500978\n",
      "Iteration 965, loss = 0.10498719\n",
      "Iteration 966, loss = 0.10496465\n",
      "Iteration 967, loss = 0.10494215\n",
      "Iteration 968, loss = 0.10491969\n",
      "Iteration 969, loss = 0.10489727\n",
      "Iteration 970, loss = 0.10487490\n",
      "Iteration 971, loss = 0.10485257\n",
      "Iteration 972, loss = 0.10483028\n",
      "Iteration 973, loss = 0.10480803\n",
      "Iteration 974, loss = 0.10478583\n",
      "Iteration 975, loss = 0.10476367\n",
      "Iteration 976, loss = 0.10474155\n",
      "Iteration 977, loss = 0.10471947\n",
      "Iteration 978, loss = 0.10469743\n",
      "Iteration 979, loss = 0.10467543\n",
      "Iteration 980, loss = 0.10465348\n",
      "Iteration 981, loss = 0.10463156\n",
      "Iteration 982, loss = 0.10460969\n",
      "Iteration 983, loss = 0.10458786\n",
      "Iteration 984, loss = 0.10456607\n",
      "Iteration 985, loss = 0.10454432\n",
      "Iteration 986, loss = 0.10452261\n",
      "Iteration 987, loss = 0.10450094\n",
      "Iteration 988, loss = 0.10447931\n",
      "Iteration 989, loss = 0.10445772\n",
      "Iteration 990, loss = 0.10443618\n",
      "Iteration 991, loss = 0.10441467\n",
      "Iteration 992, loss = 0.10439320\n",
      "Iteration 993, loss = 0.10437177\n",
      "Iteration 994, loss = 0.10435038\n",
      "Iteration 995, loss = 0.10432903\n",
      "Iteration 996, loss = 0.10430772\n",
      "Iteration 997, loss = 0.10428645\n",
      "Iteration 998, loss = 0.10426522\n",
      "Iteration 999, loss = 0.10424403\n",
      "Iteration 1000, loss = 0.10422287\n",
      "Iteration 1001, loss = 0.10420176\n",
      "Iteration 1002, loss = 0.10418068\n",
      "Iteration 1003, loss = 0.10415964\n",
      "Iteration 1004, loss = 0.10413865\n",
      "Iteration 1005, loss = 0.10411768\n",
      "Iteration 1006, loss = 0.10409676\n",
      "Iteration 1007, loss = 0.10407588\n",
      "Iteration 1008, loss = 0.10405505\n",
      "Iteration 1009, loss = 0.10403427\n",
      "Iteration 1010, loss = 0.10401352\n",
      "Iteration 1011, loss = 0.10399281\n",
      "Iteration 1012, loss = 0.10397214\n",
      "Iteration 1013, loss = 0.10395151\n",
      "Iteration 1014, loss = 0.10393091\n",
      "Iteration 1015, loss = 0.10391035\n",
      "Iteration 1016, loss = 0.10388984\n",
      "Iteration 1017, loss = 0.10386935\n",
      "Iteration 1018, loss = 0.10384891\n",
      "Iteration 1019, loss = 0.10382850\n",
      "Iteration 1020, loss = 0.10380813\n",
      "Iteration 1021, loss = 0.10378780\n",
      "Iteration 1022, loss = 0.10376750\n",
      "Iteration 1023, loss = 0.10374724\n",
      "Iteration 1024, loss = 0.10372701\n",
      "Iteration 1025, loss = 0.10370682\n",
      "Iteration 1026, loss = 0.10368667\n",
      "Iteration 1027, loss = 0.10366655\n",
      "Iteration 1028, loss = 0.10364647\n",
      "Iteration 1029, loss = 0.10362643\n",
      "Iteration 1030, loss = 0.10360642\n",
      "Iteration 1031, loss = 0.10358644\n",
      "Iteration 1032, loss = 0.10356650\n",
      "Iteration 1033, loss = 0.10354660\n",
      "Iteration 1034, loss = 0.10352673\n",
      "Iteration 1035, loss = 0.10350689\n",
      "Iteration 1036, loss = 0.10348709\n",
      "Iteration 1037, loss = 0.10346733\n",
      "Iteration 1038, loss = 0.10344760\n",
      "Iteration 1039, loss = 0.10342790\n",
      "Iteration 1040, loss = 0.10340824\n",
      "Iteration 1041, loss = 0.10338861\n",
      "Iteration 1042, loss = 0.10336902\n",
      "Iteration 1043, loss = 0.10334946\n",
      "Iteration 1044, loss = 0.10332993\n",
      "Iteration 1045, loss = 0.10331044\n",
      "Iteration 1046, loss = 0.10329098\n",
      "Iteration 1047, loss = 0.10327155\n",
      "Iteration 1048, loss = 0.10325216\n",
      "Iteration 1049, loss = 0.10323280\n",
      "Iteration 1050, loss = 0.10321348\n",
      "Iteration 1051, loss = 0.10319418\n",
      "Iteration 1052, loss = 0.10317492\n",
      "Iteration 1053, loss = 0.10315570\n",
      "Iteration 1054, loss = 0.10313650\n",
      "Iteration 1055, loss = 0.10311734\n",
      "Iteration 1056, loss = 0.10309821\n",
      "Iteration 1057, loss = 0.10307912\n",
      "Iteration 1058, loss = 0.10306005\n",
      "Iteration 1059, loss = 0.10304102\n",
      "Iteration 1060, loss = 0.10302202\n",
      "Iteration 1061, loss = 0.10300306\n",
      "Iteration 1062, loss = 0.10298412\n",
      "Iteration 1063, loss = 0.10296519\n",
      "Iteration 1064, loss = 0.10294630\n",
      "Iteration 1065, loss = 0.10292744\n",
      "Iteration 1066, loss = 0.10290861\n",
      "Iteration 1067, loss = 0.10288981\n",
      "Iteration 1068, loss = 0.10287104\n",
      "Iteration 1069, loss = 0.10285230\n",
      "Iteration 1070, loss = 0.10283359\n",
      "Iteration 1071, loss = 0.10281491\n",
      "Iteration 1072, loss = 0.10279626\n",
      "Iteration 1073, loss = 0.10277764\n",
      "Iteration 1074, loss = 0.10275905\n",
      "Iteration 1075, loss = 0.10274049\n",
      "Iteration 1076, loss = 0.10272196\n",
      "Iteration 1077, loss = 0.10270346\n",
      "Iteration 1078, loss = 0.10268499\n",
      "Iteration 1079, loss = 0.10266656\n",
      "Iteration 1080, loss = 0.10264815\n",
      "Iteration 1081, loss = 0.10262977\n",
      "Iteration 1082, loss = 0.10261142\n",
      "Iteration 1083, loss = 0.10259310\n",
      "Iteration 1084, loss = 0.10257481\n",
      "Iteration 1085, loss = 0.10255655\n",
      "Iteration 1086, loss = 0.10253832\n",
      "Iteration 1087, loss = 0.10252012\n",
      "Iteration 1088, loss = 0.10250196\n",
      "Iteration 1089, loss = 0.10248382\n",
      "Iteration 1090, loss = 0.10246573\n",
      "Iteration 1091, loss = 0.10244767\n",
      "Iteration 1092, loss = 0.10242964\n",
      "Iteration 1093, loss = 0.10241163\n",
      "Iteration 1094, loss = 0.10239366\n",
      "Iteration 1095, loss = 0.10237572\n",
      "Iteration 1096, loss = 0.10235780\n",
      "Iteration 1097, loss = 0.10233992\n",
      "Iteration 1098, loss = 0.10232206\n",
      "Iteration 1099, loss = 0.10230423\n",
      "Iteration 1100, loss = 0.10228643\n",
      "Iteration 1101, loss = 0.10226866\n",
      "Iteration 1102, loss = 0.10225092\n",
      "Iteration 1103, loss = 0.10223321\n",
      "Iteration 1104, loss = 0.10221553\n",
      "Iteration 1105, loss = 0.10219787\n",
      "Iteration 1106, loss = 0.10218024\n",
      "Iteration 1107, loss = 0.10216264\n",
      "Iteration 1108, loss = 0.10214507\n",
      "Iteration 1109, loss = 0.10212753\n",
      "Iteration 1110, loss = 0.10211001\n",
      "Iteration 1111, loss = 0.10209253\n",
      "Iteration 1112, loss = 0.10207507\n",
      "Iteration 1113, loss = 0.10205763\n",
      "Iteration 1114, loss = 0.10204023\n",
      "Iteration 1115, loss = 0.10202285\n",
      "Iteration 1116, loss = 0.10200550\n",
      "Iteration 1117, loss = 0.10198818\n",
      "Iteration 1118, loss = 0.10197088\n",
      "Iteration 1119, loss = 0.10195361\n",
      "Iteration 1120, loss = 0.10193637\n",
      "Iteration 1121, loss = 0.10191916\n",
      "Iteration 1122, loss = 0.10190197\n",
      "Iteration 1123, loss = 0.10188481\n",
      "Iteration 1124, loss = 0.10186768\n",
      "Iteration 1125, loss = 0.10185057\n",
      "Iteration 1126, loss = 0.10183349\n",
      "Iteration 1127, loss = 0.10181643\n",
      "Iteration 1128, loss = 0.10179941\n",
      "Iteration 1129, loss = 0.10178240\n",
      "Iteration 1130, loss = 0.10176543\n",
      "Iteration 1131, loss = 0.10174848\n",
      "Iteration 1132, loss = 0.10173156\n",
      "Iteration 1133, loss = 0.10171466\n",
      "Iteration 1134, loss = 0.10169779\n",
      "Iteration 1135, loss = 0.10168094\n",
      "Iteration 1136, loss = 0.10166413\n",
      "Iteration 1137, loss = 0.10164733\n",
      "Iteration 1138, loss = 0.10163056\n",
      "Iteration 1139, loss = 0.10161382\n",
      "Iteration 1140, loss = 0.10159711\n",
      "Iteration 1141, loss = 0.10158041\n",
      "Iteration 1142, loss = 0.10156375\n",
      "Iteration 1143, loss = 0.10154711\n",
      "Iteration 1144, loss = 0.10153050\n",
      "Iteration 1145, loss = 0.10151391\n",
      "Iteration 1146, loss = 0.10149735\n",
      "Iteration 1147, loss = 0.10148082\n",
      "Iteration 1148, loss = 0.10146431\n",
      "Iteration 1149, loss = 0.10144782\n",
      "Iteration 1150, loss = 0.10143136\n",
      "Iteration 1151, loss = 0.10141492\n",
      "Iteration 1152, loss = 0.10139851\n",
      "Iteration 1153, loss = 0.10138213\n",
      "Iteration 1154, loss = 0.10136576\n",
      "Iteration 1155, loss = 0.10134943\n",
      "Iteration 1156, loss = 0.10133311\n",
      "Iteration 1157, loss = 0.10131683\n",
      "Iteration 1158, loss = 0.10130056\n",
      "Iteration 1159, loss = 0.10128432\n",
      "Iteration 1160, loss = 0.10126811\n",
      "Iteration 1161, loss = 0.10125192\n",
      "Iteration 1162, loss = 0.10123575\n",
      "Iteration 1163, loss = 0.10121960\n",
      "Iteration 1164, loss = 0.10120349\n",
      "Iteration 1165, loss = 0.10118739\n",
      "Iteration 1166, loss = 0.10117132\n",
      "Iteration 1167, loss = 0.10115527\n",
      "Iteration 1168, loss = 0.10113924\n",
      "Iteration 1169, loss = 0.10112324\n",
      "Iteration 1170, loss = 0.10110727\n",
      "Iteration 1171, loss = 0.10109131\n",
      "Iteration 1172, loss = 0.10107538\n",
      "Iteration 1173, loss = 0.10105947\n",
      "Iteration 1174, loss = 0.10104359\n",
      "Iteration 1175, loss = 0.10102773\n",
      "Iteration 1176, loss = 0.10101189\n",
      "Iteration 1177, loss = 0.10099607\n",
      "Iteration 1178, loss = 0.10098028\n",
      "Iteration 1179, loss = 0.10096451\n",
      "Iteration 1180, loss = 0.10094876\n",
      "Iteration 1181, loss = 0.10093304\n",
      "Iteration 1182, loss = 0.10091734\n",
      "Iteration 1183, loss = 0.10090166\n",
      "Iteration 1184, loss = 0.10088600\n",
      "Iteration 1185, loss = 0.10087037\n",
      "Iteration 1186, loss = 0.10085476\n",
      "Iteration 1187, loss = 0.10083917\n",
      "Iteration 1188, loss = 0.10082360\n",
      "Iteration 1189, loss = 0.10080806\n",
      "Iteration 1190, loss = 0.10079253\n",
      "Iteration 1191, loss = 0.10077703\n",
      "Iteration 1192, loss = 0.10076156\n",
      "Iteration 1193, loss = 0.10074610\n",
      "Iteration 1194, loss = 0.10073067\n",
      "Iteration 1195, loss = 0.10071526\n",
      "Iteration 1196, loss = 0.10069987\n",
      "Iteration 1197, loss = 0.10068450\n",
      "Iteration 1198, loss = 0.10066915\n",
      "Iteration 1199, loss = 0.10065383\n",
      "Iteration 1200, loss = 0.10063853\n",
      "Iteration 1201, loss = 0.10062324\n",
      "Iteration 1202, loss = 0.10060799\n",
      "Iteration 1203, loss = 0.10059275\n",
      "Iteration 1204, loss = 0.10057753\n",
      "Iteration 1205, loss = 0.10056234\n",
      "Iteration 1206, loss = 0.10054716\n",
      "Iteration 1207, loss = 0.10053201\n",
      "Iteration 1208, loss = 0.10051688\n",
      "Iteration 1209, loss = 0.10050177\n",
      "Iteration 1210, loss = 0.10048668\n",
      "Iteration 1211, loss = 0.10047161\n",
      "Iteration 1212, loss = 0.10045657\n",
      "Iteration 1213, loss = 0.10044154\n",
      "Iteration 1214, loss = 0.10042653\n",
      "Iteration 1215, loss = 0.10041155\n",
      "Iteration 1216, loss = 0.10039659\n",
      "Iteration 1217, loss = 0.10038164\n",
      "Iteration 1218, loss = 0.10036672\n",
      "Iteration 1219, loss = 0.10035182\n",
      "Iteration 1220, loss = 0.10033694\n",
      "Iteration 1221, loss = 0.10032208\n",
      "Iteration 1222, loss = 0.10030724\n",
      "Iteration 1223, loss = 0.10029242\n",
      "Iteration 1224, loss = 0.10027762\n",
      "Iteration 1225, loss = 0.10026284\n",
      "Iteration 1226, loss = 0.10024808\n",
      "Iteration 1227, loss = 0.10023335\n",
      "Iteration 1228, loss = 0.10021863\n",
      "Iteration 1229, loss = 0.10020393\n",
      "Iteration 1230, loss = 0.10018925\n",
      "Iteration 1231, loss = 0.10017460\n",
      "Iteration 1232, loss = 0.10015996\n",
      "Iteration 1233, loss = 0.10014535\n",
      "Iteration 1234, loss = 0.10013075\n",
      "Iteration 1235, loss = 0.10011618\n",
      "Iteration 1236, loss = 0.10010162\n",
      "Iteration 1237, loss = 0.10008708\n",
      "Iteration 1238, loss = 0.10007257\n",
      "Iteration 1239, loss = 0.10005807\n",
      "Iteration 1240, loss = 0.10004359\n",
      "Iteration 1241, loss = 0.10002914\n",
      "Iteration 1242, loss = 0.10001470\n",
      "Iteration 1243, loss = 0.10000028\n",
      "Iteration 1244, loss = 0.09998588\n",
      "Iteration 1245, loss = 0.09997150\n",
      "Iteration 1246, loss = 0.09995714\n",
      "Iteration 1247, loss = 0.09994280\n",
      "Iteration 1248, loss = 0.09992847\n",
      "Iteration 1249, loss = 0.09991417\n",
      "Iteration 1250, loss = 0.09989988\n",
      "Iteration 1251, loss = 0.09988562\n",
      "Iteration 1252, loss = 0.09987137\n",
      "Iteration 1253, loss = 0.09985714\n",
      "Iteration 1254, loss = 0.09984294\n",
      "Iteration 1255, loss = 0.09982875\n",
      "Iteration 1256, loss = 0.09981457\n",
      "Iteration 1257, loss = 0.09980042\n",
      "Iteration 1258, loss = 0.09978629\n",
      "Iteration 1259, loss = 0.09977217\n",
      "Iteration 1260, loss = 0.09975808\n",
      "Iteration 1261, loss = 0.09974400\n",
      "Iteration 1262, loss = 0.09972994\n",
      "Iteration 1263, loss = 0.09971590\n",
      "Iteration 1264, loss = 0.09970187\n",
      "Iteration 1265, loss = 0.09968787\n",
      "Iteration 1266, loss = 0.09967388\n",
      "Iteration 1267, loss = 0.09965992\n",
      "Iteration 1268, loss = 0.09964597\n",
      "Iteration 1269, loss = 0.09963204\n",
      "Iteration 1270, loss = 0.09961813\n",
      "Iteration 1271, loss = 0.09960423\n",
      "Iteration 1272, loss = 0.09959036\n",
      "Iteration 1273, loss = 0.09957650\n",
      "Iteration 1274, loss = 0.09956266\n",
      "Iteration 1275, loss = 0.09954884\n",
      "Iteration 1276, loss = 0.09953503\n",
      "Iteration 1277, loss = 0.09952125\n",
      "Iteration 1278, loss = 0.09950748\n",
      "Iteration 1279, loss = 0.09949373\n",
      "Iteration 1280, loss = 0.09948000\n",
      "Iteration 1281, loss = 0.09946628\n",
      "Iteration 1282, loss = 0.09945258\n",
      "Iteration 1283, loss = 0.09943891\n",
      "Iteration 1284, loss = 0.09942524\n",
      "Iteration 1285, loss = 0.09941160\n",
      "Iteration 1286, loss = 0.09939797\n",
      "Iteration 1287, loss = 0.09938436\n",
      "Iteration 1288, loss = 0.09937077\n",
      "Iteration 1289, loss = 0.09935719\n",
      "Iteration 1290, loss = 0.09934364\n",
      "Iteration 1291, loss = 0.09933010\n",
      "Iteration 1292, loss = 0.09931657\n",
      "Iteration 1293, loss = 0.09930307\n",
      "Iteration 1294, loss = 0.09928958\n",
      "Iteration 1295, loss = 0.09927610\n",
      "Iteration 1296, loss = 0.09926265\n",
      "Iteration 1297, loss = 0.09924921\n",
      "Iteration 1298, loss = 0.09923579\n",
      "Iteration 1299, loss = 0.09922238\n",
      "Iteration 1300, loss = 0.09920900\n",
      "Iteration 1301, loss = 0.09919564\n",
      "Iteration 1302, loss = 0.09918229\n",
      "Iteration 1303, loss = 0.09916897\n",
      "Iteration 1304, loss = 0.09915565\n",
      "Iteration 1305, loss = 0.09914236\n",
      "Iteration 1306, loss = 0.09912908\n",
      "Iteration 1307, loss = 0.09911582\n",
      "Iteration 1308, loss = 0.09910257\n",
      "Iteration 1309, loss = 0.09908934\n",
      "Iteration 1310, loss = 0.09907613\n",
      "Iteration 1311, loss = 0.09906294\n",
      "Iteration 1312, loss = 0.09904976\n",
      "Iteration 1313, loss = 0.09903659\n",
      "Iteration 1314, loss = 0.09902345\n",
      "Iteration 1315, loss = 0.09901032\n",
      "Iteration 1316, loss = 0.09899720\n",
      "Iteration 1317, loss = 0.09898410\n",
      "Iteration 1318, loss = 0.09897102\n",
      "Iteration 1319, loss = 0.09895796\n",
      "Iteration 1320, loss = 0.09894491\n",
      "Iteration 1321, loss = 0.09893187\n",
      "Iteration 1322, loss = 0.09891885\n",
      "Iteration 1323, loss = 0.09890585\n",
      "Iteration 1324, loss = 0.09889286\n",
      "Iteration 1325, loss = 0.09887989\n",
      "Iteration 1326, loss = 0.09886694\n",
      "Iteration 1327, loss = 0.09885400\n",
      "Iteration 1328, loss = 0.09884107\n",
      "Iteration 1329, loss = 0.09882817\n",
      "Iteration 1330, loss = 0.09881527\n",
      "Iteration 1331, loss = 0.09880240\n",
      "Iteration 1332, loss = 0.09878953\n",
      "Iteration 1333, loss = 0.09877669\n",
      "Iteration 1334, loss = 0.09876386\n",
      "Iteration 1335, loss = 0.09875104\n",
      "Iteration 1336, loss = 0.09873824\n",
      "Iteration 1337, loss = 0.09872546\n",
      "Iteration 1338, loss = 0.09871269\n",
      "Iteration 1339, loss = 0.09869993\n",
      "Iteration 1340, loss = 0.09868719\n",
      "Iteration 1341, loss = 0.09867447\n",
      "Iteration 1342, loss = 0.09866176\n",
      "Iteration 1343, loss = 0.09864907\n",
      "Iteration 1344, loss = 0.09863639\n",
      "Iteration 1345, loss = 0.09862372\n",
      "Iteration 1346, loss = 0.09861107\n",
      "Iteration 1347, loss = 0.09859844\n",
      "Iteration 1348, loss = 0.09858582\n",
      "Iteration 1349, loss = 0.09857322\n",
      "Iteration 1350, loss = 0.09856063\n",
      "Iteration 1351, loss = 0.09854805\n",
      "Iteration 1352, loss = 0.09853549\n",
      "Iteration 1353, loss = 0.09852295\n",
      "Iteration 1354, loss = 0.09851042\n",
      "Iteration 1355, loss = 0.09849790\n",
      "Iteration 1356, loss = 0.09848540\n",
      "Iteration 1357, loss = 0.09847291\n",
      "Iteration 1358, loss = 0.09846044\n",
      "Iteration 1359, loss = 0.09844798\n",
      "Iteration 1360, loss = 0.09843554\n",
      "Iteration 1361, loss = 0.09842312\n",
      "Iteration 1362, loss = 0.09841071\n",
      "Iteration 1363, loss = 0.09839831\n",
      "Iteration 1364, loss = 0.09838593\n",
      "Iteration 1365, loss = 0.09837357\n",
      "Iteration 1366, loss = 0.09836121\n",
      "Iteration 1367, loss = 0.09834888\n",
      "Iteration 1368, loss = 0.09833655\n",
      "Iteration 1369, loss = 0.09832424\n",
      "Iteration 1370, loss = 0.09831195\n",
      "Iteration 1371, loss = 0.09829967\n",
      "Iteration 1372, loss = 0.09828740\n",
      "Iteration 1373, loss = 0.09827515\n",
      "Iteration 1374, loss = 0.09826291\n",
      "Iteration 1375, loss = 0.09825069\n",
      "Iteration 1376, loss = 0.09823848\n",
      "Iteration 1377, loss = 0.09822628\n",
      "Iteration 1378, loss = 0.09821410\n",
      "Iteration 1379, loss = 0.09820193\n",
      "Iteration 1380, loss = 0.09818977\n",
      "Iteration 1381, loss = 0.09817763\n",
      "Iteration 1382, loss = 0.09816550\n",
      "Iteration 1383, loss = 0.09815339\n",
      "Iteration 1384, loss = 0.09814128\n",
      "Iteration 1385, loss = 0.09812918\n",
      "Iteration 1386, loss = 0.09811711\n",
      "Iteration 1387, loss = 0.09810504\n",
      "Iteration 1388, loss = 0.09809299\n",
      "Iteration 1389, loss = 0.09808096\n",
      "Iteration 1390, loss = 0.09806893\n",
      "Iteration 1391, loss = 0.09805692\n",
      "Iteration 1392, loss = 0.09804493\n",
      "Iteration 1393, loss = 0.09803294\n",
      "Iteration 1394, loss = 0.09802097\n",
      "Iteration 1395, loss = 0.09800902\n",
      "Iteration 1396, loss = 0.09799707\n",
      "Iteration 1397, loss = 0.09798514\n",
      "Iteration 1398, loss = 0.09797323\n",
      "Iteration 1399, loss = 0.09796132\n",
      "Iteration 1400, loss = 0.09794943\n",
      "Iteration 1401, loss = 0.09793755\n",
      "Iteration 1402, loss = 0.09792569\n",
      "Iteration 1403, loss = 0.09791384\n",
      "Iteration 1404, loss = 0.09790200\n",
      "Iteration 1405, loss = 0.09789017\n",
      "Iteration 1406, loss = 0.09787836\n",
      "Iteration 1407, loss = 0.09786656\n",
      "Iteration 1408, loss = 0.09785477\n",
      "Iteration 1409, loss = 0.09784300\n",
      "Iteration 1410, loss = 0.09783124\n",
      "Iteration 1411, loss = 0.09781949\n",
      "Iteration 1412, loss = 0.09780775\n",
      "Iteration 1413, loss = 0.09779603\n",
      "Iteration 1414, loss = 0.09778432\n",
      "Iteration 1415, loss = 0.09777262\n",
      "Iteration 1416, loss = 0.09776093\n",
      "Iteration 1417, loss = 0.09774926\n",
      "Iteration 1418, loss = 0.09773760\n",
      "Iteration 1419, loss = 0.09772595\n",
      "Iteration 1420, loss = 0.09771431\n",
      "Iteration 1421, loss = 0.09770269\n",
      "Iteration 1422, loss = 0.09769108\n",
      "Iteration 1423, loss = 0.09767948\n",
      "Iteration 1424, loss = 0.09766789\n",
      "Iteration 1425, loss = 0.09765632\n",
      "Iteration 1426, loss = 0.09764476\n",
      "Iteration 1427, loss = 0.09763321\n",
      "Iteration 1428, loss = 0.09762167\n",
      "Iteration 1429, loss = 0.09761015\n",
      "Iteration 1430, loss = 0.09759863\n",
      "Iteration 1431, loss = 0.09758713\n",
      "Iteration 1432, loss = 0.09757565\n",
      "Iteration 1433, loss = 0.09756417\n",
      "Iteration 1434, loss = 0.09755271\n",
      "Iteration 1435, loss = 0.09754126\n",
      "Iteration 1436, loss = 0.09752982\n",
      "Iteration 1437, loss = 0.09751839\n",
      "Iteration 1438, loss = 0.09750697\n",
      "Iteration 1439, loss = 0.09749557\n",
      "Iteration 1440, loss = 0.09748418\n",
      "Iteration 1441, loss = 0.09747280\n",
      "Iteration 1442, loss = 0.09746143\n",
      "Iteration 1443, loss = 0.09745008\n",
      "Iteration 1444, loss = 0.09743873\n",
      "Iteration 1445, loss = 0.09742740\n",
      "Iteration 1446, loss = 0.09741608\n",
      "Iteration 1447, loss = 0.09740477\n",
      "Iteration 1448, loss = 0.09739348\n",
      "Iteration 1449, loss = 0.09738219\n",
      "Iteration 1450, loss = 0.09737092\n",
      "Iteration 1451, loss = 0.09735966\n",
      "Iteration 1452, loss = 0.09734841\n",
      "Iteration 1453, loss = 0.09733717\n",
      "Iteration 1454, loss = 0.09732594\n",
      "Iteration 1455, loss = 0.09731473\n",
      "Iteration 1456, loss = 0.09730353\n",
      "Iteration 1457, loss = 0.09729233\n",
      "Iteration 1458, loss = 0.09728115\n",
      "Iteration 1459, loss = 0.09726999\n",
      "Iteration 1460, loss = 0.09725883\n",
      "Iteration 1461, loss = 0.09724768\n",
      "Iteration 1462, loss = 0.09723655\n",
      "Iteration 1463, loss = 0.09722543\n",
      "Iteration 1464, loss = 0.09721432\n",
      "Iteration 1465, loss = 0.09720322\n",
      "Iteration 1466, loss = 0.09719214\n",
      "Iteration 1467, loss = 0.09718107\n",
      "Iteration 1468, loss = 0.09717001\n",
      "Iteration 1469, loss = 0.09715896\n",
      "Iteration 1470, loss = 0.09714793\n",
      "Iteration 1471, loss = 0.09713690\n",
      "Iteration 1472, loss = 0.09712589\n",
      "Iteration 1473, loss = 0.09711489\n",
      "Iteration 1474, loss = 0.09710389\n",
      "Iteration 1475, loss = 0.09709291\n",
      "Iteration 1476, loss = 0.09708195\n",
      "Iteration 1477, loss = 0.09707099\n",
      "Iteration 1478, loss = 0.09706004\n",
      "Iteration 1479, loss = 0.09704911\n",
      "Iteration 1480, loss = 0.09703818\n",
      "Iteration 1481, loss = 0.09702727\n",
      "Iteration 1482, loss = 0.09701637\n",
      "Iteration 1483, loss = 0.09700548\n",
      "Iteration 1484, loss = 0.09699460\n",
      "Iteration 1485, loss = 0.09698373\n",
      "Iteration 1486, loss = 0.09697287\n",
      "Iteration 1487, loss = 0.09696202\n",
      "Iteration 1488, loss = 0.09695118\n",
      "Iteration 1489, loss = 0.09694036\n",
      "Iteration 1490, loss = 0.09692954\n",
      "Iteration 1491, loss = 0.09691874\n",
      "Iteration 1492, loss = 0.09690794\n",
      "Iteration 1493, loss = 0.09689716\n",
      "Iteration 1494, loss = 0.09688639\n",
      "Iteration 1495, loss = 0.09687563\n",
      "Iteration 1496, loss = 0.09686489\n",
      "Iteration 1497, loss = 0.09685415\n",
      "Iteration 1498, loss = 0.09684342\n",
      "Iteration 1499, loss = 0.09683271\n",
      "Iteration 1500, loss = 0.09682200\n",
      "Iteration 1501, loss = 0.09681131\n",
      "Iteration 1502, loss = 0.09680062\n",
      "Iteration 1503, loss = 0.09678995\n",
      "Iteration 1504, loss = 0.09677929\n",
      "Iteration 1505, loss = 0.09676864\n",
      "Iteration 1506, loss = 0.09675800\n",
      "Iteration 1507, loss = 0.09674737\n",
      "Iteration 1508, loss = 0.09673674\n",
      "Iteration 1509, loss = 0.09672613\n",
      "Iteration 1510, loss = 0.09671554\n",
      "Iteration 1511, loss = 0.09670495\n",
      "Iteration 1512, loss = 0.09669437\n",
      "Iteration 1513, loss = 0.09668380\n",
      "Iteration 1514, loss = 0.09667324\n",
      "Iteration 1515, loss = 0.09666269\n",
      "Iteration 1516, loss = 0.09665215\n",
      "Iteration 1517, loss = 0.09664162\n",
      "Iteration 1518, loss = 0.09663111\n",
      "Iteration 1519, loss = 0.09662060\n",
      "Iteration 1520, loss = 0.09661010\n",
      "Iteration 1521, loss = 0.09659961\n",
      "Iteration 1522, loss = 0.09658913\n",
      "Iteration 1523, loss = 0.09657867\n",
      "Iteration 1524, loss = 0.09656821\n",
      "Iteration 1525, loss = 0.09655776\n",
      "Iteration 1526, loss = 0.09654732\n",
      "Iteration 1527, loss = 0.09653690\n",
      "Iteration 1528, loss = 0.09652648\n",
      "Iteration 1529, loss = 0.09651606\n",
      "Iteration 1530, loss = 0.09650566\n",
      "Iteration 1531, loss = 0.09649527\n",
      "Iteration 1532, loss = 0.09648488\n",
      "Iteration 1533, loss = 0.09647451\n",
      "Iteration 1534, loss = 0.09646415\n",
      "Iteration 1535, loss = 0.09645379\n",
      "Iteration 1536, loss = 0.09644345\n",
      "Iteration 1537, loss = 0.09643311\n",
      "Iteration 1538, loss = 0.09642279\n",
      "Iteration 1539, loss = 0.09641247\n",
      "Iteration 1540, loss = 0.09640216\n",
      "Iteration 1541, loss = 0.09639186\n",
      "Iteration 1542, loss = 0.09638158\n",
      "Iteration 1543, loss = 0.09637130\n",
      "Iteration 1544, loss = 0.09636103\n",
      "Iteration 1545, loss = 0.09635077\n",
      "Iteration 1546, loss = 0.09634052\n",
      "Iteration 1547, loss = 0.09633028\n",
      "Iteration 1548, loss = 0.09632005\n",
      "Iteration 1549, loss = 0.09630983\n",
      "Iteration 1550, loss = 0.09629962\n",
      "Iteration 1551, loss = 0.09628942\n",
      "Iteration 1552, loss = 0.09627922\n",
      "Iteration 1553, loss = 0.09626904\n",
      "Iteration 1554, loss = 0.09625887\n",
      "Iteration 1555, loss = 0.09624870\n",
      "Iteration 1556, loss = 0.09623855\n",
      "Iteration 1557, loss = 0.09622840\n",
      "Iteration 1558, loss = 0.09621826\n",
      "Iteration 1559, loss = 0.09620813\n",
      "Iteration 1560, loss = 0.09619802\n",
      "Iteration 1561, loss = 0.09618791\n",
      "Iteration 1562, loss = 0.09617781\n",
      "Iteration 1563, loss = 0.09616772\n",
      "Iteration 1564, loss = 0.09615763\n",
      "Iteration 1565, loss = 0.09614756\n",
      "Iteration 1566, loss = 0.09613750\n",
      "Iteration 1567, loss = 0.09612744\n",
      "Iteration 1568, loss = 0.09611740\n",
      "Iteration 1569, loss = 0.09610736\n",
      "Iteration 1570, loss = 0.09609733\n",
      "Iteration 1571, loss = 0.09608732\n",
      "Iteration 1572, loss = 0.09607731\n",
      "Iteration 1573, loss = 0.09606731\n",
      "Iteration 1574, loss = 0.09605731\n",
      "Iteration 1575, loss = 0.09604733\n",
      "Iteration 1576, loss = 0.09603736\n",
      "Iteration 1577, loss = 0.09602739\n",
      "Iteration 1578, loss = 0.09601744\n",
      "Iteration 1579, loss = 0.09600749\n",
      "Iteration 1580, loss = 0.09599756\n",
      "Iteration 1581, loss = 0.09598763\n",
      "Iteration 1582, loss = 0.09597771\n",
      "Iteration 1583, loss = 0.09596780\n",
      "Iteration 1584, loss = 0.09595789\n",
      "Iteration 1585, loss = 0.09594800\n",
      "Iteration 1586, loss = 0.09593812\n",
      "Iteration 1587, loss = 0.09592824\n",
      "Iteration 1588, loss = 0.09591837\n",
      "Iteration 1589, loss = 0.09590851\n",
      "Iteration 1590, loss = 0.09589866\n",
      "Iteration 1591, loss = 0.09588882\n",
      "Iteration 1592, loss = 0.09587899\n",
      "Iteration 1593, loss = 0.09586917\n",
      "Iteration 1594, loss = 0.09585935\n",
      "Iteration 1595, loss = 0.09584955\n",
      "Iteration 1596, loss = 0.09583975\n",
      "Iteration 1597, loss = 0.09582996\n",
      "Iteration 1598, loss = 0.09582018\n",
      "Iteration 1599, loss = 0.09581041\n",
      "Iteration 1600, loss = 0.09580064\n",
      "Iteration 1601, loss = 0.09579089\n",
      "Iteration 1602, loss = 0.09578114\n",
      "Iteration 1603, loss = 0.09577141\n",
      "Iteration 1604, loss = 0.09576168\n",
      "Iteration 1605, loss = 0.09575196\n",
      "Iteration 1606, loss = 0.09574225\n",
      "Iteration 1607, loss = 0.09573255\n",
      "Iteration 1608, loss = 0.09572285\n",
      "Iteration 1609, loss = 0.09571317\n",
      "Iteration 1610, loss = 0.09570349\n",
      "Iteration 1611, loss = 0.09569383\n",
      "Iteration 1612, loss = 0.09568417\n",
      "Iteration 1613, loss = 0.09567452\n",
      "Iteration 1614, loss = 0.09566488\n",
      "Iteration 1615, loss = 0.09565525\n",
      "Iteration 1616, loss = 0.09564562\n",
      "Iteration 1617, loss = 0.09563601\n",
      "Iteration 1618, loss = 0.09562640\n",
      "Iteration 1619, loss = 0.09561680\n",
      "Iteration 1620, loss = 0.09560721\n",
      "Iteration 1621, loss = 0.09559763\n",
      "Iteration 1622, loss = 0.09558806\n",
      "Iteration 1623, loss = 0.09557849\n",
      "Iteration 1624, loss = 0.09556893\n",
      "Iteration 1625, loss = 0.09555938\n",
      "Iteration 1626, loss = 0.09554984\n",
      "Iteration 1627, loss = 0.09554031\n",
      "Iteration 1628, loss = 0.09553078\n",
      "Iteration 1629, loss = 0.09552127\n",
      "Iteration 1630, loss = 0.09551176\n",
      "Iteration 1631, loss = 0.09550226\n",
      "Iteration 1632, loss = 0.09549276\n",
      "Iteration 1633, loss = 0.09548328\n",
      "Iteration 1634, loss = 0.09547380\n",
      "Iteration 1635, loss = 0.09546433\n",
      "Iteration 1636, loss = 0.09545487\n",
      "Iteration 1637, loss = 0.09544542\n",
      "Iteration 1638, loss = 0.09543597\n",
      "Iteration 1639, loss = 0.09542653\n",
      "Iteration 1640, loss = 0.09541710\n",
      "Iteration 1641, loss = 0.09540768\n",
      "Iteration 1642, loss = 0.09539827\n",
      "Iteration 1643, loss = 0.09538886\n",
      "Iteration 1644, loss = 0.09537946\n",
      "Iteration 1645, loss = 0.09537007\n",
      "Iteration 1646, loss = 0.09536069\n",
      "Iteration 1647, loss = 0.09535131\n",
      "Iteration 1648, loss = 0.09534194\n",
      "Iteration 1649, loss = 0.09533258\n",
      "Iteration 1650, loss = 0.09532323\n",
      "Iteration 1651, loss = 0.09531389\n",
      "Iteration 1652, loss = 0.09530456\n",
      "Iteration 1653, loss = 0.09529523\n",
      "Iteration 1654, loss = 0.09528591\n",
      "Iteration 1655, loss = 0.09527660\n",
      "Iteration 1656, loss = 0.09526730\n",
      "Iteration 1657, loss = 0.09525800\n",
      "Iteration 1658, loss = 0.09524872\n",
      "Iteration 1659, loss = 0.09523944\n",
      "Iteration 1660, loss = 0.09523016\n",
      "Iteration 1661, loss = 0.09522090\n",
      "Iteration 1662, loss = 0.09521164\n",
      "Iteration 1663, loss = 0.09520239\n",
      "Iteration 1664, loss = 0.09519315\n",
      "Iteration 1665, loss = 0.09518391\n",
      "Iteration 1666, loss = 0.09517469\n",
      "Iteration 1667, loss = 0.09516547\n",
      "Iteration 1668, loss = 0.09515625\n",
      "Iteration 1669, loss = 0.09514705\n",
      "Iteration 1670, loss = 0.09513785\n",
      "Iteration 1671, loss = 0.09512866\n",
      "Iteration 1672, loss = 0.09511948\n",
      "Iteration 1673, loss = 0.09511030\n",
      "Iteration 1674, loss = 0.09510113\n",
      "Iteration 1675, loss = 0.09509197\n",
      "Iteration 1676, loss = 0.09508282\n",
      "Iteration 1677, loss = 0.09507367\n",
      "Iteration 1678, loss = 0.09506453\n",
      "Iteration 1679, loss = 0.09505540\n",
      "Iteration 1680, loss = 0.09504628\n",
      "Iteration 1681, loss = 0.09503716\n",
      "Iteration 1682, loss = 0.09502805\n",
      "Iteration 1683, loss = 0.09501895\n",
      "Iteration 1684, loss = 0.09500985\n",
      "Iteration 1685, loss = 0.09500076\n",
      "Iteration 1686, loss = 0.09499168\n",
      "Iteration 1687, loss = 0.09498261\n",
      "Iteration 1688, loss = 0.09497354\n",
      "Iteration 1689, loss = 0.09496448\n",
      "Iteration 1690, loss = 0.09495543\n",
      "Iteration 1691, loss = 0.09494638\n",
      "Iteration 1692, loss = 0.09493734\n",
      "Iteration 1693, loss = 0.09492831\n",
      "Iteration 1694, loss = 0.09491928\n",
      "Iteration 1695, loss = 0.09491027\n",
      "Iteration 1696, loss = 0.09490126\n",
      "Iteration 1697, loss = 0.09489225\n",
      "Iteration 1698, loss = 0.09488324\n",
      "Iteration 1699, loss = 0.09487423\n",
      "Iteration 1700, loss = 0.09486523\n",
      "Iteration 1701, loss = 0.09485623\n",
      "Iteration 1702, loss = 0.09484724\n",
      "Iteration 1703, loss = 0.09483825\n",
      "Iteration 1704, loss = 0.09482927\n",
      "Iteration 1705, loss = 0.09482030\n",
      "Iteration 1706, loss = 0.09481133\n",
      "Iteration 1707, loss = 0.09480237\n",
      "Iteration 1708, loss = 0.09479341\n",
      "Iteration 1709, loss = 0.09478446\n",
      "Iteration 1710, loss = 0.09477552\n",
      "Iteration 1711, loss = 0.09476658\n",
      "Iteration 1712, loss = 0.09475765\n",
      "Iteration 1713, loss = 0.09474873\n",
      "Iteration 1714, loss = 0.09473981\n",
      "Iteration 1715, loss = 0.09473089\n",
      "Iteration 1716, loss = 0.09472196\n",
      "Iteration 1717, loss = 0.09471304\n",
      "Iteration 1718, loss = 0.09470413\n",
      "Iteration 1719, loss = 0.09469522\n",
      "Iteration 1720, loss = 0.09468631\n",
      "Iteration 1721, loss = 0.09467741\n",
      "Iteration 1722, loss = 0.09466852\n",
      "Iteration 1723, loss = 0.09465963\n",
      "Iteration 1724, loss = 0.09465075\n",
      "Iteration 1725, loss = 0.09464187\n",
      "Iteration 1726, loss = 0.09463300\n",
      "Iteration 1727, loss = 0.09462414\n",
      "Iteration 1728, loss = 0.09461528\n",
      "Iteration 1729, loss = 0.09460643\n",
      "Iteration 1730, loss = 0.09459758\n",
      "Iteration 1731, loss = 0.09458874\n",
      "Iteration 1732, loss = 0.09457991\n",
      "Iteration 1733, loss = 0.09457108\n",
      "Iteration 1734, loss = 0.09456226\n",
      "Iteration 1735, loss = 0.09455344\n",
      "Iteration 1736, loss = 0.09454464\n",
      "Iteration 1737, loss = 0.09453583\n",
      "Iteration 1738, loss = 0.09452704\n",
      "Iteration 1739, loss = 0.09451825\n",
      "Iteration 1740, loss = 0.09450946\n",
      "Iteration 1741, loss = 0.09450069\n",
      "Iteration 1742, loss = 0.09449192\n",
      "Iteration 1743, loss = 0.09448315\n",
      "Iteration 1744, loss = 0.09447439\n",
      "Iteration 1745, loss = 0.09446564\n",
      "Iteration 1746, loss = 0.09445690\n",
      "Iteration 1747, loss = 0.09444816\n",
      "Iteration 1748, loss = 0.09443943\n",
      "Iteration 1749, loss = 0.09443070\n",
      "Iteration 1750, loss = 0.09442198\n",
      "Iteration 1751, loss = 0.09441327\n",
      "Iteration 1752, loss = 0.09440456\n",
      "Iteration 1753, loss = 0.09439586\n",
      "Iteration 1754, loss = 0.09438717\n",
      "Iteration 1755, loss = 0.09437848\n",
      "Iteration 1756, loss = 0.09436980\n",
      "Iteration 1757, loss = 0.09436112\n",
      "Iteration 1758, loss = 0.09435245\n",
      "Iteration 1759, loss = 0.09434379\n",
      "Iteration 1760, loss = 0.09433513\n",
      "Iteration 1761, loss = 0.09432648\n",
      "Iteration 1762, loss = 0.09431784\n",
      "Iteration 1763, loss = 0.09430920\n",
      "Iteration 1764, loss = 0.09430057\n",
      "Iteration 1765, loss = 0.09429194\n",
      "Iteration 1766, loss = 0.09428332\n",
      "Iteration 1767, loss = 0.09427471\n",
      "Iteration 1768, loss = 0.09426610\n",
      "Iteration 1769, loss = 0.09425750\n",
      "Iteration 1770, loss = 0.09424891\n",
      "Iteration 1771, loss = 0.09424032\n",
      "Iteration 1772, loss = 0.09423174\n",
      "Iteration 1773, loss = 0.09422316\n",
      "Iteration 1774, loss = 0.09421459\n",
      "Iteration 1775, loss = 0.09420603\n",
      "Iteration 1776, loss = 0.09419747\n",
      "Iteration 1777, loss = 0.09418892\n",
      "Iteration 1778, loss = 0.09418037\n",
      "Iteration 1779, loss = 0.09417183\n",
      "Iteration 1780, loss = 0.09416330\n",
      "Iteration 1781, loss = 0.09415477\n",
      "Iteration 1782, loss = 0.09414625\n",
      "Iteration 1783, loss = 0.09413773\n",
      "Iteration 1784, loss = 0.09412922\n",
      "Iteration 1785, loss = 0.09412072\n",
      "Iteration 1786, loss = 0.09411222\n",
      "Iteration 1787, loss = 0.09410374\n",
      "Iteration 1788, loss = 0.09409525\n",
      "Iteration 1789, loss = 0.09408678\n",
      "Iteration 1790, loss = 0.09407831\n",
      "Iteration 1791, loss = 0.09406984\n",
      "Iteration 1792, loss = 0.09406138\n",
      "Iteration 1793, loss = 0.09405293\n",
      "Iteration 1794, loss = 0.09404448\n",
      "Iteration 1795, loss = 0.09403604\n",
      "Iteration 1796, loss = 0.09402761\n",
      "Iteration 1797, loss = 0.09401918\n",
      "Iteration 1798, loss = 0.09401076\n",
      "Iteration 1799, loss = 0.09400234\n",
      "Iteration 1800, loss = 0.09399393\n",
      "Iteration 1801, loss = 0.09398552\n",
      "Iteration 1802, loss = 0.09397712\n",
      "Iteration 1803, loss = 0.09396873\n",
      "Iteration 1804, loss = 0.09396034\n",
      "Iteration 1805, loss = 0.09395197\n",
      "Iteration 1806, loss = 0.09394363\n",
      "Iteration 1807, loss = 0.09393528\n",
      "Iteration 1808, loss = 0.09392695\n",
      "Iteration 1809, loss = 0.09391862\n",
      "Iteration 1810, loss = 0.09391031\n",
      "Iteration 1811, loss = 0.09390199\n",
      "Iteration 1812, loss = 0.09389369\n",
      "Iteration 1813, loss = 0.09388539\n",
      "Iteration 1814, loss = 0.09387710\n",
      "Iteration 1815, loss = 0.09386881\n",
      "Iteration 1816, loss = 0.09386053\n",
      "Iteration 1817, loss = 0.09385226\n",
      "Iteration 1818, loss = 0.09384399\n",
      "Iteration 1819, loss = 0.09383573\n",
      "Iteration 1820, loss = 0.09382747\n",
      "Iteration 1821, loss = 0.09381923\n",
      "Iteration 1822, loss = 0.09381098\n",
      "Iteration 1823, loss = 0.09380274\n",
      "Iteration 1824, loss = 0.09379451\n",
      "Iteration 1825, loss = 0.09378629\n",
      "Iteration 1826, loss = 0.09377807\n",
      "Iteration 1827, loss = 0.09376985\n",
      "Iteration 1828, loss = 0.09376164\n",
      "Iteration 1829, loss = 0.09375344\n",
      "Iteration 1830, loss = 0.09374524\n",
      "Iteration 1831, loss = 0.09373705\n",
      "Iteration 1832, loss = 0.09372887\n",
      "Iteration 1833, loss = 0.09372069\n",
      "Iteration 1834, loss = 0.09371251\n",
      "Iteration 1835, loss = 0.09370434\n",
      "Iteration 1836, loss = 0.09369618\n",
      "Iteration 1837, loss = 0.09368802\n",
      "Iteration 1838, loss = 0.09367987\n",
      "Iteration 1839, loss = 0.09367172\n",
      "Iteration 1840, loss = 0.09366358\n",
      "Iteration 1841, loss = 0.09365544\n",
      "Iteration 1842, loss = 0.09364732\n",
      "Iteration 1843, loss = 0.09363920\n",
      "Iteration 1844, loss = 0.09363108\n",
      "Iteration 1845, loss = 0.09362297\n",
      "Iteration 1846, loss = 0.09361487\n",
      "Iteration 1847, loss = 0.09360677\n",
      "Iteration 1848, loss = 0.09359868\n",
      "Iteration 1849, loss = 0.09359060\n",
      "Iteration 1850, loss = 0.09358252\n",
      "Iteration 1851, loss = 0.09357444\n",
      "Iteration 1852, loss = 0.09356637\n",
      "Iteration 1853, loss = 0.09355831\n",
      "Iteration 1854, loss = 0.09355025\n",
      "Iteration 1855, loss = 0.09354220\n",
      "Iteration 1856, loss = 0.09353415\n",
      "Iteration 1857, loss = 0.09352611\n",
      "Iteration 1858, loss = 0.09351808\n",
      "Iteration 1859, loss = 0.09351005\n",
      "Iteration 1860, loss = 0.09350200\n",
      "Iteration 1861, loss = 0.09349397\n",
      "Iteration 1862, loss = 0.09348593\n",
      "Iteration 1863, loss = 0.09347791\n",
      "Iteration 1864, loss = 0.09346988\n",
      "Iteration 1865, loss = 0.09346186\n",
      "Iteration 1866, loss = 0.09345384\n",
      "Iteration 1867, loss = 0.09344583\n",
      "Iteration 1868, loss = 0.09343783\n",
      "Iteration 1869, loss = 0.09342983\n",
      "Iteration 1870, loss = 0.09342183\n",
      "Iteration 1871, loss = 0.09341384\n",
      "Iteration 1872, loss = 0.09340585\n",
      "Iteration 1873, loss = 0.09339786\n",
      "Iteration 1874, loss = 0.09338989\n",
      "Iteration 1875, loss = 0.09338191\n",
      "Iteration 1876, loss = 0.09337395\n",
      "Iteration 1877, loss = 0.09336598\n",
      "Iteration 1878, loss = 0.09335802\n",
      "Iteration 1879, loss = 0.09335007\n",
      "Iteration 1880, loss = 0.09334212\n",
      "Iteration 1881, loss = 0.09333418\n",
      "Iteration 1882, loss = 0.09332624\n",
      "Iteration 1883, loss = 0.09331831\n",
      "Iteration 1884, loss = 0.09331038\n",
      "Iteration 1885, loss = 0.09330245\n",
      "Iteration 1886, loss = 0.09329454\n",
      "Iteration 1887, loss = 0.09328662\n",
      "Iteration 1888, loss = 0.09327871\n",
      "Iteration 1889, loss = 0.09327081\n",
      "Iteration 1890, loss = 0.09326291\n",
      "Iteration 1891, loss = 0.09325502\n",
      "Iteration 1892, loss = 0.09324713\n",
      "Iteration 1893, loss = 0.09323924\n",
      "Iteration 1894, loss = 0.09323136\n",
      "Iteration 1895, loss = 0.09322349\n",
      "Iteration 1896, loss = 0.09321562\n",
      "Iteration 1897, loss = 0.09320776\n",
      "Iteration 1898, loss = 0.09319989\n",
      "Iteration 1899, loss = 0.09319204\n",
      "Iteration 1900, loss = 0.09318419\n",
      "Iteration 1901, loss = 0.09317634\n",
      "Iteration 1902, loss = 0.09316850\n",
      "Iteration 1903, loss = 0.09316067\n",
      "Iteration 1904, loss = 0.09315284\n",
      "Iteration 1905, loss = 0.09314501\n",
      "Iteration 1906, loss = 0.09313719\n",
      "Iteration 1907, loss = 0.09312937\n",
      "Iteration 1908, loss = 0.09312156\n",
      "Iteration 1909, loss = 0.09311376\n",
      "Iteration 1910, loss = 0.09310595\n",
      "Iteration 1911, loss = 0.09309816\n",
      "Iteration 1912, loss = 0.09309036\n",
      "Iteration 1913, loss = 0.09308257\n",
      "Iteration 1914, loss = 0.09307479\n",
      "Iteration 1915, loss = 0.09306701\n",
      "Iteration 1916, loss = 0.09305924\n",
      "Iteration 1917, loss = 0.09305147\n",
      "Iteration 1918, loss = 0.09304371\n",
      "Iteration 1919, loss = 0.09303595\n",
      "Iteration 1920, loss = 0.09302819\n",
      "Iteration 1921, loss = 0.09302044\n",
      "Iteration 1922, loss = 0.09301270\n",
      "Iteration 1923, loss = 0.09300496\n",
      "Iteration 1924, loss = 0.09299723\n",
      "Iteration 1925, loss = 0.09298951\n",
      "Iteration 1926, loss = 0.09298179\n",
      "Iteration 1927, loss = 0.09297407\n",
      "Iteration 1928, loss = 0.09296636\n",
      "Iteration 1929, loss = 0.09295865\n",
      "Iteration 1930, loss = 0.09295095\n",
      "Iteration 1931, loss = 0.09294325\n",
      "Iteration 1932, loss = 0.09293557\n",
      "Iteration 1933, loss = 0.09292788\n",
      "Iteration 1934, loss = 0.09292019\n",
      "Iteration 1935, loss = 0.09291251\n",
      "Iteration 1936, loss = 0.09290484\n",
      "Iteration 1937, loss = 0.09289717\n",
      "Iteration 1938, loss = 0.09288951\n",
      "Iteration 1939, loss = 0.09288185\n",
      "Iteration 1940, loss = 0.09287419\n",
      "Iteration 1941, loss = 0.09286654\n",
      "Iteration 1942, loss = 0.09285890\n",
      "Iteration 1943, loss = 0.09285126\n",
      "Iteration 1944, loss = 0.09284362\n",
      "Iteration 1945, loss = 0.09283599\n",
      "Iteration 1946, loss = 0.09282836\n",
      "Iteration 1947, loss = 0.09282074\n",
      "Iteration 1948, loss = 0.09281312\n",
      "Iteration 1949, loss = 0.09280551\n",
      "Iteration 1950, loss = 0.09279790\n",
      "Iteration 1951, loss = 0.09279030\n",
      "Iteration 1952, loss = 0.09278270\n",
      "Iteration 1953, loss = 0.09277510\n",
      "Iteration 1954, loss = 0.09276751\n",
      "Iteration 1955, loss = 0.09275993\n",
      "Iteration 1956, loss = 0.09275235\n",
      "Iteration 1957, loss = 0.09274477\n",
      "Iteration 1958, loss = 0.09273720\n",
      "Iteration 1959, loss = 0.09272963\n",
      "Iteration 1960, loss = 0.09272206\n",
      "Iteration 1961, loss = 0.09271451\n",
      "Iteration 1962, loss = 0.09270695\n",
      "Iteration 1963, loss = 0.09269940\n",
      "Iteration 1964, loss = 0.09269185\n",
      "Iteration 1965, loss = 0.09268431\n",
      "Iteration 1966, loss = 0.09267678\n",
      "Iteration 1967, loss = 0.09266924\n",
      "Iteration 1968, loss = 0.09266171\n",
      "Iteration 1969, loss = 0.09265419\n",
      "Iteration 1970, loss = 0.09264667\n",
      "Iteration 1971, loss = 0.09263915\n",
      "Iteration 1972, loss = 0.09263164\n",
      "Iteration 1973, loss = 0.09262414\n",
      "Iteration 1974, loss = 0.09261663\n",
      "Iteration 1975, loss = 0.09260914\n",
      "Iteration 1976, loss = 0.09260164\n",
      "Iteration 1977, loss = 0.09259415\n",
      "Iteration 1978, loss = 0.09258667\n",
      "Iteration 1979, loss = 0.09257919\n",
      "Iteration 1980, loss = 0.09257171\n",
      "Iteration 1981, loss = 0.09256424\n",
      "Iteration 1982, loss = 0.09255677\n",
      "Iteration 1983, loss = 0.09254931\n",
      "Iteration 1984, loss = 0.09254185\n",
      "Iteration 1985, loss = 0.09253439\n",
      "Iteration 1986, loss = 0.09252694\n",
      "Iteration 1987, loss = 0.09251949\n",
      "Iteration 1988, loss = 0.09251205\n",
      "Iteration 1989, loss = 0.09250461\n",
      "Iteration 1990, loss = 0.09249718\n",
      "Iteration 1991, loss = 0.09248975\n",
      "Iteration 1992, loss = 0.09248232\n",
      "Iteration 1993, loss = 0.09247490\n",
      "Iteration 1994, loss = 0.09246748\n",
      "Iteration 1995, loss = 0.09246007\n",
      "Iteration 1996, loss = 0.09245266\n",
      "Iteration 1997, loss = 0.09244526\n",
      "Iteration 1998, loss = 0.09243786\n",
      "Iteration 1999, loss = 0.09243046\n",
      "Iteration 2000, loss = 0.09242307\n",
      "Iteration 2001, loss = 0.09241568\n",
      "Iteration 2002, loss = 0.09240830\n",
      "Iteration 2003, loss = 0.09240092\n",
      "Iteration 2004, loss = 0.09239354\n",
      "Iteration 2005, loss = 0.09238618\n",
      "Iteration 2006, loss = 0.09237883\n",
      "Iteration 2007, loss = 0.09237148\n",
      "Iteration 2008, loss = 0.09236414\n",
      "Iteration 2009, loss = 0.09235680\n",
      "Iteration 2010, loss = 0.09234946\n",
      "Iteration 2011, loss = 0.09234213\n",
      "Iteration 2012, loss = 0.09233481\n",
      "Iteration 2013, loss = 0.09232749\n",
      "Iteration 2014, loss = 0.09232017\n",
      "Iteration 2015, loss = 0.09231286\n",
      "Iteration 2016, loss = 0.09230555\n",
      "Iteration 2017, loss = 0.09229825\n",
      "Iteration 2018, loss = 0.09229095\n",
      "Iteration 2019, loss = 0.09228365\n",
      "Iteration 2020, loss = 0.09227636\n",
      "Iteration 2021, loss = 0.09226906\n",
      "Iteration 2022, loss = 0.09226176\n",
      "Iteration 2023, loss = 0.09225447\n",
      "Iteration 2024, loss = 0.09224718\n",
      "Iteration 2025, loss = 0.09223989\n",
      "Iteration 2026, loss = 0.09223268\n",
      "Iteration 2027, loss = 0.09222552\n",
      "Iteration 2028, loss = 0.09221833\n",
      "Iteration 2029, loss = 0.09221114\n",
      "Iteration 2030, loss = 0.09220394\n",
      "Iteration 2031, loss = 0.09219673\n",
      "Iteration 2032, loss = 0.09218951\n",
      "Iteration 2033, loss = 0.09218228\n",
      "Iteration 2034, loss = 0.09217506\n",
      "Iteration 2035, loss = 0.09216785\n",
      "Iteration 2036, loss = 0.09216069\n",
      "Iteration 2037, loss = 0.09215352\n",
      "Iteration 2038, loss = 0.09214634\n",
      "Iteration 2039, loss = 0.09213917\n",
      "Iteration 2040, loss = 0.09213199\n",
      "Iteration 2041, loss = 0.09212486\n",
      "Iteration 2042, loss = 0.09211770\n",
      "Iteration 2043, loss = 0.09211053\n",
      "Iteration 2044, loss = 0.09210341\n",
      "Iteration 2045, loss = 0.09209626\n",
      "Iteration 2046, loss = 0.09208911\n",
      "Iteration 2047, loss = 0.09208198\n",
      "Iteration 2048, loss = 0.09207485\n",
      "Iteration 2049, loss = 0.09206773\n",
      "Iteration 2050, loss = 0.09206060\n",
      "Iteration 2051, loss = 0.09205349\n",
      "Iteration 2052, loss = 0.09204638\n",
      "Iteration 2053, loss = 0.09203926\n",
      "Iteration 2054, loss = 0.09203217\n",
      "Iteration 2055, loss = 0.09202507\n",
      "Iteration 2056, loss = 0.09201797\n",
      "Iteration 2057, loss = 0.09201088\n",
      "Iteration 2058, loss = 0.09200379\n",
      "Iteration 2059, loss = 0.09199671\n",
      "Iteration 2060, loss = 0.09198963\n",
      "Iteration 2061, loss = 0.09198256\n",
      "Iteration 2062, loss = 0.09197548\n",
      "Iteration 2063, loss = 0.09196843\n",
      "Iteration 2064, loss = 0.09196135\n",
      "Iteration 2065, loss = 0.09195430\n",
      "Iteration 2066, loss = 0.09194724\n",
      "Iteration 2067, loss = 0.09194018\n",
      "Iteration 2068, loss = 0.09193315\n",
      "Iteration 2069, loss = 0.09192609\n",
      "Iteration 2070, loss = 0.09191907\n",
      "Iteration 2071, loss = 0.09191203\n",
      "Iteration 2072, loss = 0.09190500\n",
      "Iteration 2073, loss = 0.09189796\n",
      "Iteration 2074, loss = 0.09189095\n",
      "Iteration 2075, loss = 0.09188392\n",
      "Iteration 2076, loss = 0.09187692\n",
      "Iteration 2077, loss = 0.09186991\n",
      "Iteration 2078, loss = 0.09186289\n",
      "Iteration 2079, loss = 0.09185590\n",
      "Iteration 2080, loss = 0.09184889\n",
      "Iteration 2081, loss = 0.09184189\n",
      "Iteration 2082, loss = 0.09183490\n",
      "Iteration 2083, loss = 0.09182792\n",
      "Iteration 2084, loss = 0.09182093\n",
      "Iteration 2085, loss = 0.09181396\n",
      "Iteration 2086, loss = 0.09180698\n",
      "Iteration 2087, loss = 0.09180001\n",
      "Iteration 2088, loss = 0.09179303\n",
      "Iteration 2089, loss = 0.09178608\n",
      "Iteration 2090, loss = 0.09177911\n",
      "Iteration 2091, loss = 0.09177216\n",
      "Iteration 2092, loss = 0.09176521\n",
      "Iteration 2093, loss = 0.09175825\n",
      "Iteration 2094, loss = 0.09175132\n",
      "Iteration 2095, loss = 0.09174437\n",
      "Iteration 2096, loss = 0.09173745\n",
      "Iteration 2097, loss = 0.09173050\n",
      "Iteration 2098, loss = 0.09172359\n",
      "Iteration 2099, loss = 0.09171666\n",
      "Iteration 2100, loss = 0.09170973\n",
      "Iteration 2101, loss = 0.09170282\n",
      "Iteration 2102, loss = 0.09169590\n",
      "Iteration 2103, loss = 0.09168899\n",
      "Iteration 2104, loss = 0.09168209\n",
      "Iteration 2105, loss = 0.09167519\n",
      "Iteration 2106, loss = 0.09166829\n",
      "Iteration 2107, loss = 0.09166140\n",
      "Iteration 2108, loss = 0.09165450\n",
      "Iteration 2109, loss = 0.09164763\n",
      "Iteration 2110, loss = 0.09164073\n",
      "Iteration 2111, loss = 0.09163387\n",
      "Iteration 2112, loss = 0.09162698\n",
      "Iteration 2113, loss = 0.09162012\n",
      "Iteration 2114, loss = 0.09161324\n",
      "Iteration 2115, loss = 0.09160639\n",
      "Iteration 2116, loss = 0.09159952\n",
      "Iteration 2117, loss = 0.09159268\n",
      "Iteration 2118, loss = 0.09158582\n",
      "Iteration 2119, loss = 0.09157898\n",
      "Iteration 2120, loss = 0.09157215\n",
      "Iteration 2121, loss = 0.09156532\n",
      "Iteration 2122, loss = 0.09155849\n",
      "Iteration 2123, loss = 0.09155168\n",
      "Iteration 2124, loss = 0.09154486\n",
      "Iteration 2125, loss = 0.09153805\n",
      "Iteration 2126, loss = 0.09153124\n",
      "Iteration 2127, loss = 0.09152444\n",
      "Iteration 2128, loss = 0.09151763\n",
      "Iteration 2129, loss = 0.09151084\n",
      "Iteration 2130, loss = 0.09150405\n",
      "Iteration 2131, loss = 0.09149726\n",
      "Iteration 2132, loss = 0.09149048\n",
      "Iteration 2133, loss = 0.09148369\n",
      "Iteration 2134, loss = 0.09147692\n",
      "Iteration 2135, loss = 0.09147014\n",
      "Iteration 2136, loss = 0.09146339\n",
      "Iteration 2137, loss = 0.09145661\n",
      "Iteration 2138, loss = 0.09144985\n",
      "Iteration 2139, loss = 0.09144309\n",
      "Iteration 2140, loss = 0.09143634\n",
      "Iteration 2141, loss = 0.09142958\n",
      "Iteration 2142, loss = 0.09142285\n",
      "Iteration 2143, loss = 0.09141609\n",
      "Iteration 2144, loss = 0.09140936\n",
      "Iteration 2145, loss = 0.09140261\n",
      "Iteration 2146, loss = 0.09139590\n",
      "Iteration 2147, loss = 0.09138916\n",
      "Iteration 2148, loss = 0.09138243\n",
      "Iteration 2149, loss = 0.09137570\n",
      "Iteration 2150, loss = 0.09136900\n",
      "Iteration 2151, loss = 0.09136228\n",
      "Iteration 2152, loss = 0.09135557\n",
      "Iteration 2153, loss = 0.09134885\n",
      "Iteration 2154, loss = 0.09134216\n",
      "Iteration 2155, loss = 0.09133545\n",
      "Iteration 2156, loss = 0.09132876\n",
      "Iteration 2157, loss = 0.09132206\n",
      "Iteration 2158, loss = 0.09131538\n",
      "Iteration 2159, loss = 0.09130868\n",
      "Iteration 2160, loss = 0.09130201\n",
      "Iteration 2161, loss = 0.09129532\n",
      "Iteration 2162, loss = 0.09128866\n",
      "Iteration 2163, loss = 0.09128198\n",
      "Iteration 2164, loss = 0.09127531\n",
      "Iteration 2165, loss = 0.09126864\n",
      "Iteration 2166, loss = 0.09126200\n",
      "Iteration 2167, loss = 0.09125533\n",
      "Iteration 2168, loss = 0.09124868\n",
      "Iteration 2169, loss = 0.09124202\n",
      "Iteration 2170, loss = 0.09123539\n",
      "Iteration 2171, loss = 0.09122874\n",
      "Iteration 2172, loss = 0.09122211\n",
      "Iteration 2173, loss = 0.09121547\n",
      "Iteration 2174, loss = 0.09120884\n",
      "Iteration 2175, loss = 0.09120221\n",
      "Iteration 2176, loss = 0.09119560\n",
      "Iteration 2177, loss = 0.09118896\n",
      "Iteration 2178, loss = 0.09118236\n",
      "Iteration 2179, loss = 0.09117575\n",
      "Iteration 2180, loss = 0.09116913\n",
      "Iteration 2181, loss = 0.09116252\n",
      "Iteration 2182, loss = 0.09115592\n",
      "Iteration 2183, loss = 0.09114932\n",
      "Iteration 2184, loss = 0.09114272\n",
      "Iteration 2185, loss = 0.09113613\n",
      "Iteration 2186, loss = 0.09112954\n",
      "Iteration 2187, loss = 0.09112296\n",
      "Iteration 2188, loss = 0.09111637\n",
      "Iteration 2189, loss = 0.09110980\n",
      "Iteration 2190, loss = 0.09110322\n",
      "Iteration 2191, loss = 0.09109665\n",
      "Iteration 2192, loss = 0.09109008\n",
      "Iteration 2193, loss = 0.09108352\n",
      "Iteration 2194, loss = 0.09107695\n",
      "Iteration 2195, loss = 0.09107041\n",
      "Iteration 2196, loss = 0.09106384\n",
      "Iteration 2197, loss = 0.09105729\n",
      "Iteration 2198, loss = 0.09105074\n",
      "Iteration 2199, loss = 0.09104421\n",
      "Iteration 2200, loss = 0.09103766\n",
      "Iteration 2201, loss = 0.09103112\n",
      "Iteration 2202, loss = 0.09102458\n",
      "Iteration 2203, loss = 0.09101806\n",
      "Iteration 2204, loss = 0.09101152\n",
      "Iteration 2205, loss = 0.09100501\n",
      "Iteration 2206, loss = 0.09099847\n",
      "Iteration 2207, loss = 0.09099197\n",
      "Iteration 2208, loss = 0.09098545\n",
      "Iteration 2209, loss = 0.09097892\n",
      "Iteration 2210, loss = 0.09097243\n",
      "Iteration 2211, loss = 0.09096591\n",
      "Iteration 2212, loss = 0.09095942\n",
      "Iteration 2213, loss = 0.09095291\n",
      "Iteration 2214, loss = 0.09094643\n",
      "Iteration 2215, loss = 0.09093993\n",
      "Iteration 2216, loss = 0.09093344\n",
      "Iteration 2217, loss = 0.09092695\n",
      "Iteration 2218, loss = 0.09092048\n",
      "Iteration 2219, loss = 0.09091400\n",
      "Iteration 2220, loss = 0.09090752\n",
      "Iteration 2221, loss = 0.09090104\n",
      "Iteration 2222, loss = 0.09089458\n",
      "Iteration 2223, loss = 0.09088810\n",
      "Iteration 2224, loss = 0.09088165\n",
      "Iteration 2225, loss = 0.09087518\n",
      "Iteration 2226, loss = 0.09086874\n",
      "Iteration 2227, loss = 0.09086227\n",
      "Iteration 2228, loss = 0.09085583\n",
      "Iteration 2229, loss = 0.09084938\n",
      "Iteration 2230, loss = 0.09084293\n",
      "Iteration 2231, loss = 0.09083650\n",
      "Iteration 2232, loss = 0.09083005\n",
      "Iteration 2233, loss = 0.09082362\n",
      "Iteration 2234, loss = 0.09081718\n",
      "Iteration 2235, loss = 0.09081077\n",
      "Iteration 2236, loss = 0.09080434\n",
      "Iteration 2237, loss = 0.09079790\n",
      "Iteration 2238, loss = 0.09079149\n",
      "Iteration 2239, loss = 0.09078507\n",
      "Iteration 2240, loss = 0.09077866\n",
      "Iteration 2241, loss = 0.09077224\n",
      "Iteration 2242, loss = 0.09076584\n",
      "Iteration 2243, loss = 0.09075943\n",
      "Iteration 2244, loss = 0.09075304\n",
      "Iteration 2245, loss = 0.09074663\n",
      "Iteration 2246, loss = 0.09074024\n",
      "Iteration 2247, loss = 0.09073385\n",
      "Iteration 2248, loss = 0.09072745\n",
      "Iteration 2249, loss = 0.09072107\n",
      "Iteration 2250, loss = 0.09071469\n",
      "Iteration 2251, loss = 0.09070831\n",
      "Iteration 2252, loss = 0.09070193\n",
      "Iteration 2253, loss = 0.09069556\n",
      "Iteration 2254, loss = 0.09068918\n",
      "Iteration 2255, loss = 0.09068283\n",
      "Iteration 2256, loss = 0.09067645\n",
      "Iteration 2257, loss = 0.09067010\n",
      "Iteration 2258, loss = 0.09066373\n",
      "Iteration 2259, loss = 0.09065738\n",
      "Iteration 2260, loss = 0.09065102\n",
      "Iteration 2261, loss = 0.09064467\n",
      "Iteration 2262, loss = 0.09063833\n",
      "Iteration 2263, loss = 0.09063198\n",
      "Iteration 2264, loss = 0.09062565\n",
      "Iteration 2265, loss = 0.09061931\n",
      "Iteration 2266, loss = 0.09061298\n",
      "Iteration 2267, loss = 0.09060663\n",
      "Iteration 2268, loss = 0.09060032\n",
      "Iteration 2269, loss = 0.09059398\n",
      "Iteration 2270, loss = 0.09058767\n",
      "Iteration 2271, loss = 0.09058135\n",
      "Iteration 2272, loss = 0.09057504\n",
      "Iteration 2273, loss = 0.09056873\n",
      "Iteration 2274, loss = 0.09056242\n",
      "Iteration 2275, loss = 0.09055613\n",
      "Iteration 2276, loss = 0.09054982\n",
      "Iteration 2277, loss = 0.09054354\n",
      "Iteration 2278, loss = 0.09053724\n",
      "Iteration 2279, loss = 0.09053093\n",
      "Iteration 2280, loss = 0.09052465\n",
      "Iteration 2281, loss = 0.09051834\n",
      "Iteration 2282, loss = 0.09051207\n",
      "Iteration 2283, loss = 0.09050577\n",
      "Iteration 2284, loss = 0.09049949\n",
      "Iteration 2285, loss = 0.09049320\n",
      "Iteration 2286, loss = 0.09048692\n",
      "Iteration 2287, loss = 0.09048064\n",
      "Iteration 2288, loss = 0.09047436\n",
      "Iteration 2289, loss = 0.09046809\n",
      "Iteration 2290, loss = 0.09046182\n",
      "Iteration 2291, loss = 0.09045556\n",
      "Iteration 2292, loss = 0.09044928\n",
      "Iteration 2293, loss = 0.09044303\n",
      "Iteration 2294, loss = 0.09043677\n",
      "Iteration 2295, loss = 0.09043050\n",
      "Iteration 2296, loss = 0.09042427\n",
      "Iteration 2297, loss = 0.09041801\n",
      "Iteration 2298, loss = 0.09041176\n",
      "Iteration 2299, loss = 0.09040550\n",
      "Iteration 2300, loss = 0.09039928\n",
      "Iteration 2301, loss = 0.09039302\n",
      "Iteration 2302, loss = 0.09038681\n",
      "Iteration 2303, loss = 0.09038057\n",
      "Iteration 2304, loss = 0.09037433\n",
      "Iteration 2305, loss = 0.09036811\n",
      "Iteration 2306, loss = 0.09036187\n",
      "Iteration 2307, loss = 0.09035567\n",
      "Iteration 2308, loss = 0.09034944\n",
      "Iteration 2309, loss = 0.09034322\n",
      "Iteration 2310, loss = 0.09033701\n",
      "Iteration 2311, loss = 0.09033079\n",
      "Iteration 2312, loss = 0.09032460\n",
      "Iteration 2313, loss = 0.09031838\n",
      "Iteration 2314, loss = 0.09031241\n",
      "Iteration 2315, loss = 0.09030656\n",
      "Iteration 2316, loss = 0.09030069\n",
      "Iteration 2317, loss = 0.09029484\n",
      "Iteration 2318, loss = 0.09028897\n",
      "Iteration 2319, loss = 0.09028312\n",
      "Iteration 2320, loss = 0.09027724\n",
      "Iteration 2321, loss = 0.09027138\n",
      "Iteration 2322, loss = 0.09026550\n",
      "Iteration 2323, loss = 0.09025962\n",
      "Iteration 2324, loss = 0.09025377\n",
      "Iteration 2325, loss = 0.09024788\n",
      "Iteration 2326, loss = 0.09024201\n",
      "Iteration 2327, loss = 0.09023613\n",
      "Iteration 2328, loss = 0.09023027\n",
      "Iteration 2329, loss = 0.09022439\n",
      "Iteration 2330, loss = 0.09021852\n",
      "Iteration 2331, loss = 0.09021266\n",
      "Iteration 2332, loss = 0.09020679\n",
      "Iteration 2333, loss = 0.09020093\n",
      "Iteration 2334, loss = 0.09019506\n",
      "Iteration 2335, loss = 0.09018920\n",
      "Iteration 2336, loss = 0.09018334\n",
      "Iteration 2337, loss = 0.09017747\n",
      "Iteration 2338, loss = 0.09017163\n",
      "Iteration 2339, loss = 0.09016577\n",
      "Iteration 2340, loss = 0.09015992\n",
      "Iteration 2341, loss = 0.09015407\n",
      "Iteration 2342, loss = 0.09014822\n",
      "Iteration 2343, loss = 0.09014239\n",
      "Iteration 2344, loss = 0.09013654\n",
      "Iteration 2345, loss = 0.09013070\n",
      "Iteration 2346, loss = 0.09012486\n",
      "Iteration 2347, loss = 0.09011902\n",
      "Iteration 2348, loss = 0.09011320\n",
      "Iteration 2349, loss = 0.09010736\n",
      "Iteration 2350, loss = 0.09010154\n",
      "Iteration 2351, loss = 0.09009570\n",
      "Iteration 2352, loss = 0.09008989\n",
      "Iteration 2353, loss = 0.09008407\n",
      "Iteration 2354, loss = 0.09007825\n",
      "Iteration 2355, loss = 0.09007252\n",
      "Iteration 2356, loss = 0.09006673\n",
      "Iteration 2357, loss = 0.09006091\n",
      "Iteration 2358, loss = 0.09005509\n",
      "Iteration 2359, loss = 0.09004930\n",
      "Iteration 2360, loss = 0.09004354\n",
      "Iteration 2361, loss = 0.09003774\n",
      "Iteration 2362, loss = 0.09003198\n",
      "Iteration 2363, loss = 0.09002621\n",
      "Iteration 2364, loss = 0.09002043\n",
      "Iteration 2365, loss = 0.09001468\n",
      "Iteration 2366, loss = 0.09000889\n",
      "Iteration 2367, loss = 0.09000314\n",
      "Iteration 2368, loss = 0.08999740\n",
      "Iteration 2369, loss = 0.08999161\n",
      "Iteration 2370, loss = 0.08998588\n",
      "Iteration 2371, loss = 0.08998011\n",
      "Iteration 2372, loss = 0.08997439\n",
      "Iteration 2373, loss = 0.08996863\n",
      "Iteration 2374, loss = 0.08996288\n",
      "Iteration 2375, loss = 0.08995716\n",
      "Iteration 2376, loss = 0.08995140\n",
      "Iteration 2377, loss = 0.08994569\n",
      "Iteration 2378, loss = 0.08993995\n",
      "Iteration 2379, loss = 0.08993422\n",
      "Iteration 2380, loss = 0.08992851\n",
      "Iteration 2381, loss = 0.08992277\n",
      "Iteration 2382, loss = 0.08991708\n",
      "Iteration 2383, loss = 0.08991134\n",
      "Iteration 2384, loss = 0.08990562\n",
      "Iteration 2385, loss = 0.08989992\n",
      "Iteration 2386, loss = 0.08989420\n",
      "Iteration 2387, loss = 0.08988851\n",
      "Iteration 2388, loss = 0.08988280\n",
      "Iteration 2389, loss = 0.08987709\n",
      "Iteration 2390, loss = 0.08987140\n",
      "Iteration 2391, loss = 0.08986568\n",
      "Iteration 2392, loss = 0.08986002\n",
      "Iteration 2393, loss = 0.08985431\n",
      "Iteration 2394, loss = 0.08984861\n",
      "Iteration 2395, loss = 0.08984293\n",
      "Iteration 2396, loss = 0.08983724\n",
      "Iteration 2397, loss = 0.08983157\n",
      "Iteration 2398, loss = 0.08982588\n",
      "Iteration 2399, loss = 0.08982020\n",
      "Iteration 2400, loss = 0.08981453\n",
      "Iteration 2401, loss = 0.08980885\n",
      "Iteration 2402, loss = 0.08980320\n",
      "Iteration 2403, loss = 0.08979752\n",
      "Iteration 2404, loss = 0.08979184\n",
      "Iteration 2405, loss = 0.08978619\n",
      "Iteration 2406, loss = 0.08978053\n",
      "Iteration 2407, loss = 0.08977488\n",
      "Iteration 2408, loss = 0.08976923\n",
      "Iteration 2409, loss = 0.08976356\n",
      "Iteration 2410, loss = 0.08975791\n",
      "Iteration 2411, loss = 0.08975228\n",
      "Iteration 2412, loss = 0.08974662\n",
      "Iteration 2413, loss = 0.08974099\n",
      "Iteration 2414, loss = 0.08973534\n",
      "Iteration 2415, loss = 0.08972970\n",
      "Iteration 2416, loss = 0.08972408\n",
      "Iteration 2417, loss = 0.08971844\n",
      "Iteration 2418, loss = 0.08971282\n",
      "Iteration 2419, loss = 0.08970718\n",
      "Iteration 2420, loss = 0.08970156\n",
      "Iteration 2421, loss = 0.08969595\n",
      "Iteration 2422, loss = 0.08969032\n",
      "Iteration 2423, loss = 0.08968471\n",
      "Iteration 2424, loss = 0.08967909\n",
      "Iteration 2425, loss = 0.08967347\n",
      "Iteration 2426, loss = 0.08966788\n",
      "Iteration 2427, loss = 0.08966227\n",
      "Iteration 2428, loss = 0.08965665\n",
      "Iteration 2429, loss = 0.08965107\n",
      "Iteration 2430, loss = 0.08964546\n",
      "Iteration 2431, loss = 0.08963987\n",
      "Iteration 2432, loss = 0.08963427\n",
      "Iteration 2433, loss = 0.08962867\n",
      "Iteration 2434, loss = 0.08962309\n",
      "Iteration 2435, loss = 0.08961750\n",
      "Iteration 2436, loss = 0.08961192\n",
      "Iteration 2437, loss = 0.08960634\n",
      "Iteration 2438, loss = 0.08960075\n",
      "Iteration 2439, loss = 0.08959517\n",
      "Iteration 2440, loss = 0.08958961\n",
      "Iteration 2441, loss = 0.08958403\n",
      "Iteration 2442, loss = 0.08957847\n",
      "Iteration 2443, loss = 0.08957289\n",
      "Iteration 2444, loss = 0.08956733\n",
      "Iteration 2445, loss = 0.08956179\n",
      "Iteration 2446, loss = 0.08955621\n",
      "Iteration 2447, loss = 0.08955064\n",
      "Iteration 2448, loss = 0.08954509\n",
      "Iteration 2449, loss = 0.08953955\n",
      "Iteration 2450, loss = 0.08953400\n",
      "Iteration 2451, loss = 0.08952844\n",
      "Iteration 2452, loss = 0.08952291\n",
      "Iteration 2453, loss = 0.08951736\n",
      "Iteration 2454, loss = 0.08951183\n",
      "Iteration 2455, loss = 0.08950628\n",
      "Iteration 2456, loss = 0.08950075\n",
      "Iteration 2457, loss = 0.08949521\n",
      "Iteration 2458, loss = 0.08948969\n",
      "Iteration 2459, loss = 0.08948416\n",
      "Iteration 2460, loss = 0.08947863\n",
      "Iteration 2461, loss = 0.08947311\n",
      "Iteration 2462, loss = 0.08946758\n",
      "Iteration 2463, loss = 0.08946207\n",
      "Iteration 2464, loss = 0.08945657\n",
      "Iteration 2465, loss = 0.08945104\n",
      "Iteration 2466, loss = 0.08944554\n",
      "Iteration 2467, loss = 0.08944002\n",
      "Iteration 2468, loss = 0.08943452\n",
      "Iteration 2469, loss = 0.08942902\n",
      "Iteration 2470, loss = 0.08942352\n",
      "Iteration 2471, loss = 0.08941801\n",
      "Iteration 2472, loss = 0.08941253\n",
      "Iteration 2473, loss = 0.08940702\n",
      "Iteration 2474, loss = 0.08940154\n",
      "Iteration 2475, loss = 0.08939605\n",
      "Iteration 2476, loss = 0.08939056\n",
      "Iteration 2477, loss = 0.08938509\n",
      "Iteration 2478, loss = 0.08937963\n",
      "Iteration 2479, loss = 0.08937416\n",
      "Iteration 2480, loss = 0.08936871\n",
      "Iteration 2481, loss = 0.08936324\n",
      "Iteration 2482, loss = 0.08935777\n",
      "Iteration 2483, loss = 0.08935234\n",
      "Iteration 2484, loss = 0.08934688\n",
      "Iteration 2485, loss = 0.08934144\n",
      "Iteration 2486, loss = 0.08933599\n",
      "Iteration 2487, loss = 0.08933054\n",
      "Iteration 2488, loss = 0.08932509\n",
      "Iteration 2489, loss = 0.08931967\n",
      "Iteration 2490, loss = 0.08931423\n",
      "Iteration 2491, loss = 0.08930880\n",
      "Iteration 2492, loss = 0.08930336\n",
      "Iteration 2493, loss = 0.08929794\n",
      "Iteration 2494, loss = 0.08929252\n",
      "Iteration 2495, loss = 0.08928709\n",
      "Iteration 2496, loss = 0.08928166\n",
      "Iteration 2497, loss = 0.08927626\n",
      "Iteration 2498, loss = 0.08927084\n",
      "Iteration 2499, loss = 0.08926543\n",
      "Iteration 2500, loss = 0.08926002\n",
      "Iteration 2501, loss = 0.08925461\n",
      "Iteration 2502, loss = 0.08924919\n",
      "Iteration 2503, loss = 0.08924382\n",
      "Iteration 2504, loss = 0.08923839\n",
      "Iteration 2505, loss = 0.08923300\n",
      "Iteration 2506, loss = 0.08922760\n",
      "Iteration 2507, loss = 0.08922220\n",
      "Iteration 2508, loss = 0.08921684\n",
      "Iteration 2509, loss = 0.08921143\n",
      "Iteration 2510, loss = 0.08920605\n",
      "Iteration 2511, loss = 0.08920066\n",
      "Iteration 2512, loss = 0.08919527\n",
      "Iteration 2513, loss = 0.08918991\n",
      "Iteration 2514, loss = 0.08918453\n",
      "Iteration 2515, loss = 0.08917915\n",
      "Iteration 2516, loss = 0.08917376\n",
      "Iteration 2517, loss = 0.08916841\n",
      "Iteration 2518, loss = 0.08916304\n",
      "Iteration 2519, loss = 0.08915767\n",
      "Iteration 2520, loss = 0.08915231\n",
      "Iteration 2521, loss = 0.08914694\n",
      "Iteration 2522, loss = 0.08914158\n",
      "Iteration 2523, loss = 0.08913624\n",
      "Iteration 2524, loss = 0.08913087\n",
      "Iteration 2525, loss = 0.08912553\n",
      "Iteration 2526, loss = 0.08912017\n",
      "Iteration 2527, loss = 0.08911481\n",
      "Iteration 2528, loss = 0.08910949\n",
      "Iteration 2529, loss = 0.08910415\n",
      "Iteration 2530, loss = 0.08909879\n",
      "Iteration 2531, loss = 0.08909346\n",
      "Iteration 2532, loss = 0.08908811\n",
      "Iteration 2533, loss = 0.08908279\n",
      "Iteration 2534, loss = 0.08907746\n",
      "Iteration 2535, loss = 0.08907212\n",
      "Iteration 2536, loss = 0.08906679\n",
      "Iteration 2537, loss = 0.08906148\n",
      "Iteration 2538, loss = 0.08905614\n",
      "Iteration 2539, loss = 0.08905083\n",
      "Iteration 2540, loss = 0.08904550\n",
      "Iteration 2541, loss = 0.08904018\n",
      "Iteration 2542, loss = 0.08903488\n",
      "Iteration 2543, loss = 0.08902956\n",
      "Iteration 2544, loss = 0.08902425\n",
      "Iteration 2545, loss = 0.08901893\n",
      "Iteration 2546, loss = 0.08901363\n",
      "Iteration 2547, loss = 0.08900834\n",
      "Iteration 2548, loss = 0.08900302\n",
      "Iteration 2549, loss = 0.08899773\n",
      "Iteration 2550, loss = 0.08899243\n",
      "Iteration 2551, loss = 0.08898713\n",
      "Iteration 2552, loss = 0.08898185\n",
      "Iteration 2553, loss = 0.08897656\n",
      "Iteration 2554, loss = 0.08897126\n",
      "Iteration 2555, loss = 0.08896598\n",
      "Iteration 2556, loss = 0.08896068\n",
      "Iteration 2557, loss = 0.08895541\n",
      "Iteration 2558, loss = 0.08895013\n",
      "Iteration 2559, loss = 0.08894485\n",
      "Iteration 2560, loss = 0.08893957\n",
      "Iteration 2561, loss = 0.08893430\n",
      "Iteration 2562, loss = 0.08892903\n",
      "Iteration 2563, loss = 0.08892376\n",
      "Iteration 2564, loss = 0.08891849\n",
      "Iteration 2565, loss = 0.08891322\n",
      "Iteration 2566, loss = 0.08890796\n",
      "Iteration 2567, loss = 0.08890271\n",
      "Iteration 2568, loss = 0.08889743\n",
      "Iteration 2569, loss = 0.08889218\n",
      "Iteration 2570, loss = 0.08888692\n",
      "Iteration 2571, loss = 0.08888167\n",
      "Iteration 2572, loss = 0.08887643\n",
      "Iteration 2573, loss = 0.08887117\n",
      "Iteration 2574, loss = 0.08886592\n",
      "Iteration 2575, loss = 0.08886069\n",
      "Iteration 2576, loss = 0.08885544\n",
      "Iteration 2577, loss = 0.08885020\n",
      "Iteration 2578, loss = 0.08884497\n",
      "Iteration 2579, loss = 0.08883972\n",
      "Iteration 2580, loss = 0.08883448\n",
      "Iteration 2581, loss = 0.08882926\n",
      "Iteration 2582, loss = 0.08882403\n",
      "Iteration 2583, loss = 0.08881880\n",
      "Iteration 2584, loss = 0.08881359\n",
      "Iteration 2585, loss = 0.08880837\n",
      "Iteration 2586, loss = 0.08880315\n",
      "Iteration 2587, loss = 0.08879796\n",
      "Iteration 2588, loss = 0.08879275\n",
      "Iteration 2589, loss = 0.08878754\n",
      "Iteration 2590, loss = 0.08878233\n",
      "Iteration 2591, loss = 0.08877714\n",
      "Iteration 2592, loss = 0.08877194\n",
      "Iteration 2593, loss = 0.08876674\n",
      "Iteration 2594, loss = 0.08876155\n",
      "Iteration 2595, loss = 0.08875635\n",
      "Iteration 2596, loss = 0.08875116\n",
      "Iteration 2597, loss = 0.08874600\n",
      "Iteration 2598, loss = 0.08874079\n",
      "Iteration 2599, loss = 0.08873562\n",
      "Iteration 2600, loss = 0.08873043\n",
      "Iteration 2601, loss = 0.08872526\n",
      "Iteration 2602, loss = 0.08872007\n",
      "Iteration 2603, loss = 0.08871491\n",
      "Iteration 2604, loss = 0.08870973\n",
      "Iteration 2605, loss = 0.08870455\n",
      "Iteration 2606, loss = 0.08869941\n",
      "Iteration 2607, loss = 0.08869422\n",
      "Iteration 2608, loss = 0.08868906\n",
      "Iteration 2609, loss = 0.08868391\n",
      "Iteration 2610, loss = 0.08867874\n",
      "Iteration 2611, loss = 0.08867360\n",
      "Iteration 2612, loss = 0.08866843\n",
      "Iteration 2613, loss = 0.08866327\n",
      "Iteration 2614, loss = 0.08865812\n",
      "Iteration 2615, loss = 0.08865297\n",
      "Iteration 2616, loss = 0.08864783\n",
      "Iteration 2617, loss = 0.08864268\n",
      "Iteration 2618, loss = 0.08863754\n",
      "Iteration 2619, loss = 0.08863240\n",
      "Iteration 2620, loss = 0.08862725\n",
      "Iteration 2621, loss = 0.08862212\n",
      "Iteration 2622, loss = 0.08861699\n",
      "Iteration 2623, loss = 0.08861185\n",
      "Iteration 2624, loss = 0.08860671\n",
      "Iteration 2625, loss = 0.08860159\n",
      "Iteration 2626, loss = 0.08859648\n",
      "Iteration 2627, loss = 0.08859133\n",
      "Iteration 2628, loss = 0.08858622\n",
      "Iteration 2629, loss = 0.08858108\n",
      "Iteration 2630, loss = 0.08857597\n",
      "Iteration 2631, loss = 0.08857086\n",
      "Iteration 2632, loss = 0.08856574\n",
      "Iteration 2633, loss = 0.08856062\n",
      "Iteration 2634, loss = 0.08855551\n",
      "Iteration 2635, loss = 0.08855040\n",
      "Iteration 2636, loss = 0.08854530\n",
      "Iteration 2637, loss = 0.08854019\n",
      "Iteration 2638, loss = 0.08853508\n",
      "Iteration 2639, loss = 0.08852998\n",
      "Iteration 2640, loss = 0.08852488\n",
      "Iteration 2641, loss = 0.08851979\n",
      "Iteration 2642, loss = 0.08851469\n",
      "Iteration 2643, loss = 0.08850959\n",
      "Iteration 2644, loss = 0.08850451\n",
      "Iteration 2645, loss = 0.08849941\n",
      "Iteration 2646, loss = 0.08849433\n",
      "Iteration 2647, loss = 0.08848924\n",
      "Iteration 2648, loss = 0.08848415\n",
      "Iteration 2649, loss = 0.08847906\n",
      "Iteration 2650, loss = 0.08847400\n",
      "Iteration 2651, loss = 0.08846891\n",
      "Iteration 2652, loss = 0.08846383\n",
      "Iteration 2653, loss = 0.08845877\n",
      "Iteration 2654, loss = 0.08845369\n",
      "Iteration 2655, loss = 0.08844862\n",
      "Iteration 2656, loss = 0.08844356\n",
      "Iteration 2657, loss = 0.08843849\n",
      "Iteration 2658, loss = 0.08843341\n",
      "Iteration 2659, loss = 0.08842835\n",
      "Iteration 2660, loss = 0.08842330\n",
      "Iteration 2661, loss = 0.08841823\n",
      "Iteration 2662, loss = 0.08841317\n",
      "Iteration 2663, loss = 0.08840813\n",
      "Iteration 2664, loss = 0.08840306\n",
      "Iteration 2665, loss = 0.08839800\n",
      "Iteration 2666, loss = 0.08839298\n",
      "Iteration 2667, loss = 0.08838792\n",
      "Iteration 2668, loss = 0.08838287\n",
      "Iteration 2669, loss = 0.08837783\n",
      "Iteration 2670, loss = 0.08837278\n",
      "Iteration 2671, loss = 0.08836776\n",
      "Iteration 2672, loss = 0.08836270\n",
      "Iteration 2673, loss = 0.08835768\n",
      "Iteration 2674, loss = 0.08835263\n",
      "Iteration 2675, loss = 0.08834762\n",
      "Iteration 2676, loss = 0.08834259\n",
      "Iteration 2677, loss = 0.08833757\n",
      "Iteration 2678, loss = 0.08833254\n",
      "Iteration 2679, loss = 0.08832751\n",
      "Iteration 2680, loss = 0.08832251\n",
      "Iteration 2681, loss = 0.08831748\n",
      "Iteration 2682, loss = 0.08831248\n",
      "Iteration 2683, loss = 0.08830746\n",
      "Iteration 2684, loss = 0.08830244\n",
      "Iteration 2685, loss = 0.08829743\n",
      "Iteration 2686, loss = 0.08829242\n",
      "Iteration 2687, loss = 0.08828742\n",
      "Iteration 2688, loss = 0.08828241\n",
      "Iteration 2689, loss = 0.08827740\n",
      "Iteration 2690, loss = 0.08827241\n",
      "Iteration 2691, loss = 0.08826743\n",
      "Iteration 2692, loss = 0.08826241\n",
      "Iteration 2693, loss = 0.08825743\n",
      "Iteration 2694, loss = 0.08825242\n",
      "Iteration 2695, loss = 0.08824744\n",
      "Iteration 2696, loss = 0.08824246\n",
      "Iteration 2697, loss = 0.08823748\n",
      "Iteration 2698, loss = 0.08823248\n",
      "Iteration 2699, loss = 0.08822749\n",
      "Iteration 2700, loss = 0.08822251\n",
      "Iteration 2701, loss = 0.08821755\n",
      "Iteration 2702, loss = 0.08821255\n",
      "Iteration 2703, loss = 0.08820759\n",
      "Iteration 2704, loss = 0.08820260\n",
      "Iteration 2705, loss = 0.08819764\n",
      "Iteration 2706, loss = 0.08819267\n",
      "Iteration 2707, loss = 0.08818770\n",
      "Iteration 2708, loss = 0.08818273\n",
      "Iteration 2709, loss = 0.08817775\n",
      "Iteration 2710, loss = 0.08817281\n",
      "Iteration 2711, loss = 0.08816784\n",
      "Iteration 2712, loss = 0.08816288\n",
      "Iteration 2713, loss = 0.08815793\n",
      "Iteration 2714, loss = 0.08815297\n",
      "Iteration 2715, loss = 0.08814801\n",
      "Iteration 2716, loss = 0.08814306\n",
      "Iteration 2717, loss = 0.08813811\n",
      "Iteration 2718, loss = 0.08813316\n",
      "Iteration 2719, loss = 0.08812820\n",
      "Iteration 2720, loss = 0.08812328\n",
      "Iteration 2721, loss = 0.08811832\n",
      "Iteration 2722, loss = 0.08811338\n",
      "Iteration 2723, loss = 0.08810845\n",
      "Iteration 2724, loss = 0.08810351\n",
      "Iteration 2725, loss = 0.08809857\n",
      "Iteration 2726, loss = 0.08809363\n",
      "Iteration 2727, loss = 0.08808872\n",
      "Iteration 2728, loss = 0.08808377\n",
      "Iteration 2729, loss = 0.08807884\n",
      "Iteration 2730, loss = 0.08807391\n",
      "Iteration 2731, loss = 0.08806900\n",
      "Iteration 2732, loss = 0.08806407\n",
      "Iteration 2733, loss = 0.08805914\n",
      "Iteration 2734, loss = 0.08805424\n",
      "Iteration 2735, loss = 0.08804929\n",
      "Iteration 2736, loss = 0.08804440\n",
      "Iteration 2737, loss = 0.08803948\n",
      "Iteration 2738, loss = 0.08803457\n",
      "Iteration 2739, loss = 0.08802965\n",
      "Iteration 2740, loss = 0.08802475\n",
      "Iteration 2741, loss = 0.08801991\n",
      "Iteration 2742, loss = 0.08801500\n",
      "Iteration 2743, loss = 0.08801012\n",
      "Iteration 2744, loss = 0.08800524\n",
      "Iteration 2745, loss = 0.08800036\n",
      "Iteration 2746, loss = 0.08799548\n",
      "Iteration 2747, loss = 0.08799061\n",
      "Iteration 2748, loss = 0.08798572\n",
      "Iteration 2749, loss = 0.08798083\n",
      "Iteration 2750, loss = 0.08797594\n",
      "Iteration 2751, loss = 0.08797110\n",
      "Iteration 2752, loss = 0.08796619\n",
      "Iteration 2753, loss = 0.08796132\n",
      "Iteration 2754, loss = 0.08795647\n",
      "Iteration 2755, loss = 0.08795161\n",
      "Iteration 2756, loss = 0.08794673\n",
      "Iteration 2757, loss = 0.08794187\n",
      "Iteration 2758, loss = 0.08793702\n",
      "Iteration 2759, loss = 0.08793216\n",
      "Iteration 2760, loss = 0.08792730\n",
      "Iteration 2761, loss = 0.08792245\n",
      "Iteration 2762, loss = 0.08791761\n",
      "Iteration 2763, loss = 0.08791274\n",
      "Iteration 2764, loss = 0.08790789\n",
      "Iteration 2765, loss = 0.08790305\n",
      "Iteration 2766, loss = 0.08789821\n",
      "Iteration 2767, loss = 0.08789336\n",
      "Iteration 2768, loss = 0.08788851\n",
      "Iteration 2769, loss = 0.08788369\n",
      "Iteration 2770, loss = 0.08787882\n",
      "Iteration 2771, loss = 0.08787401\n",
      "Iteration 2772, loss = 0.08786917\n",
      "Iteration 2773, loss = 0.08786433\n",
      "Iteration 2774, loss = 0.08785949\n",
      "Iteration 2775, loss = 0.08785467\n",
      "Iteration 2776, loss = 0.08784985\n",
      "Iteration 2777, loss = 0.08784502\n",
      "Iteration 2778, loss = 0.08784019\n",
      "Iteration 2779, loss = 0.08783537\n",
      "Iteration 2780, loss = 0.08783054\n",
      "Iteration 2781, loss = 0.08782573\n",
      "Iteration 2782, loss = 0.08782090\n",
      "Iteration 2783, loss = 0.08781609\n",
      "Iteration 2784, loss = 0.08781126\n",
      "Iteration 2785, loss = 0.08780645\n",
      "Iteration 2786, loss = 0.08780165\n",
      "Iteration 2787, loss = 0.08779684\n",
      "Iteration 2788, loss = 0.08779203\n",
      "Iteration 2789, loss = 0.08778721\n",
      "Iteration 2790, loss = 0.08778241\n",
      "Iteration 2791, loss = 0.08777761\n",
      "Iteration 2792, loss = 0.08777281\n",
      "Iteration 2793, loss = 0.08776800\n",
      "Iteration 2794, loss = 0.08776322\n",
      "Iteration 2795, loss = 0.08775840\n",
      "Iteration 2796, loss = 0.08775363\n",
      "Iteration 2797, loss = 0.08774882\n",
      "Iteration 2798, loss = 0.08774403\n",
      "Iteration 2799, loss = 0.08773923\n",
      "Iteration 2800, loss = 0.08773446\n",
      "Iteration 2801, loss = 0.08772967\n",
      "Iteration 2802, loss = 0.08772488\n",
      "Iteration 2803, loss = 0.08772009\n",
      "Iteration 2804, loss = 0.08771530\n",
      "Iteration 2805, loss = 0.08771053\n",
      "Iteration 2806, loss = 0.08770576\n",
      "Iteration 2807, loss = 0.08770097\n",
      "Iteration 2808, loss = 0.08769620\n",
      "Iteration 2809, loss = 0.08769142\n",
      "Iteration 2810, loss = 0.08768665\n",
      "Iteration 2811, loss = 0.08768189\n",
      "Iteration 2812, loss = 0.08767711\n",
      "Iteration 2813, loss = 0.08767234\n",
      "Iteration 2814, loss = 0.08766757\n",
      "Iteration 2815, loss = 0.08766281\n",
      "Iteration 2816, loss = 0.08765806\n",
      "Iteration 2817, loss = 0.08765329\n",
      "Iteration 2818, loss = 0.08764853\n",
      "Iteration 2819, loss = 0.08764377\n",
      "Iteration 2820, loss = 0.08763902\n",
      "Iteration 2821, loss = 0.08763428\n",
      "Iteration 2822, loss = 0.08762951\n",
      "Iteration 2823, loss = 0.08762475\n",
      "Iteration 2824, loss = 0.08762002\n",
      "Iteration 2825, loss = 0.08761525\n",
      "Iteration 2826, loss = 0.08761051\n",
      "Iteration 2827, loss = 0.08760577\n",
      "Iteration 2828, loss = 0.08760103\n",
      "Iteration 2829, loss = 0.08759628\n",
      "Iteration 2830, loss = 0.08759153\n",
      "Iteration 2831, loss = 0.08758681\n",
      "Iteration 2832, loss = 0.08758207\n",
      "Iteration 2833, loss = 0.08757733\n",
      "Iteration 2834, loss = 0.08757258\n",
      "Iteration 2835, loss = 0.08756787\n",
      "Iteration 2836, loss = 0.08756313\n",
      "Iteration 2837, loss = 0.08755840\n",
      "Iteration 2838, loss = 0.08755368\n",
      "Iteration 2839, loss = 0.08754895\n",
      "Iteration 2840, loss = 0.08754422\n",
      "Iteration 2841, loss = 0.08753951\n",
      "Iteration 2842, loss = 0.08753477\n",
      "Iteration 2843, loss = 0.08753006\n",
      "Iteration 2844, loss = 0.08752534\n",
      "Iteration 2845, loss = 0.08752061\n",
      "Iteration 2846, loss = 0.08751592\n",
      "Iteration 2847, loss = 0.08751120\n",
      "Iteration 2848, loss = 0.08750648\n",
      "Iteration 2849, loss = 0.08750176\n",
      "Iteration 2850, loss = 0.08749706\n",
      "Iteration 2851, loss = 0.08749236\n",
      "Iteration 2852, loss = 0.08748765\n",
      "Iteration 2853, loss = 0.08748294\n",
      "Iteration 2854, loss = 0.08747825\n",
      "Iteration 2855, loss = 0.08747354\n",
      "Iteration 2856, loss = 0.08746884\n",
      "Iteration 2857, loss = 0.08746414\n",
      "Iteration 2858, loss = 0.08745945\n",
      "Iteration 2859, loss = 0.08745476\n",
      "Iteration 2860, loss = 0.08745006\n",
      "Iteration 2861, loss = 0.08744537\n",
      "Iteration 2862, loss = 0.08744068\n",
      "Iteration 2863, loss = 0.08743598\n",
      "Iteration 2864, loss = 0.08743129\n",
      "Iteration 2865, loss = 0.08742661\n",
      "Iteration 2866, loss = 0.08742194\n",
      "Iteration 2867, loss = 0.08741725\n",
      "Iteration 2868, loss = 0.08741256\n",
      "Iteration 2869, loss = 0.08740788\n",
      "Iteration 2870, loss = 0.08740322\n",
      "Iteration 2871, loss = 0.08739854\n",
      "Iteration 2872, loss = 0.08739385\n",
      "Iteration 2873, loss = 0.08738919\n",
      "Iteration 2874, loss = 0.08738452\n",
      "Iteration 2875, loss = 0.08737984\n",
      "Iteration 2876, loss = 0.08737518\n",
      "Iteration 2877, loss = 0.08737050\n",
      "Iteration 2878, loss = 0.08736585\n",
      "Iteration 2879, loss = 0.08736117\n",
      "Iteration 2880, loss = 0.08735650\n",
      "Iteration 2881, loss = 0.08735187\n",
      "Iteration 2882, loss = 0.08734720\n",
      "Iteration 2883, loss = 0.08734254\n",
      "Iteration 2884, loss = 0.08733788\n",
      "Iteration 2885, loss = 0.08733322\n",
      "Iteration 2886, loss = 0.08732858\n",
      "Iteration 2887, loss = 0.08732392\n",
      "Iteration 2888, loss = 0.08731927\n",
      "Iteration 2889, loss = 0.08731463\n",
      "Iteration 2890, loss = 0.08730998\n",
      "Iteration 2891, loss = 0.08730533\n",
      "Iteration 2892, loss = 0.08730069\n",
      "Iteration 2893, loss = 0.08729605\n",
      "Iteration 2894, loss = 0.08729141\n",
      "Iteration 2895, loss = 0.08728677\n",
      "Iteration 2896, loss = 0.08728213\n",
      "Iteration 2897, loss = 0.08727750\n",
      "Iteration 2898, loss = 0.08727287\n",
      "Iteration 2899, loss = 0.08726823\n",
      "Iteration 2900, loss = 0.08726359\n",
      "Iteration 2901, loss = 0.08725896\n",
      "Iteration 2902, loss = 0.08725434\n",
      "Iteration 2903, loss = 0.08724972\n",
      "Iteration 2904, loss = 0.08724509\n",
      "Iteration 2905, loss = 0.08724046\n",
      "Iteration 2906, loss = 0.08723585\n",
      "Iteration 2907, loss = 0.08723123\n",
      "Iteration 2908, loss = 0.08722659\n",
      "Iteration 2909, loss = 0.08722199\n",
      "Iteration 2910, loss = 0.08721737\n",
      "Iteration 2911, loss = 0.08721277\n",
      "Iteration 2912, loss = 0.08720814\n",
      "Iteration 2913, loss = 0.08720353\n",
      "Iteration 2914, loss = 0.08719892\n",
      "Iteration 2915, loss = 0.08719430\n",
      "Iteration 2916, loss = 0.08718969\n",
      "Iteration 2917, loss = 0.08718509\n",
      "Iteration 2918, loss = 0.08718050\n",
      "Iteration 2919, loss = 0.08717589\n",
      "Iteration 2920, loss = 0.08717128\n",
      "Iteration 2921, loss = 0.08716667\n",
      "Iteration 2922, loss = 0.08716209\n",
      "Iteration 2923, loss = 0.08715748\n",
      "Iteration 2924, loss = 0.08715288\n",
      "Iteration 2925, loss = 0.08714828\n",
      "Iteration 2926, loss = 0.08714370\n",
      "Iteration 2927, loss = 0.08713912\n",
      "Iteration 2928, loss = 0.08713452\n",
      "Iteration 2929, loss = 0.08712993\n",
      "Iteration 2930, loss = 0.08712534\n",
      "Iteration 2931, loss = 0.08712075\n",
      "Iteration 2932, loss = 0.08711618\n",
      "Iteration 2933, loss = 0.08711158\n",
      "Iteration 2934, loss = 0.08710701\n",
      "Iteration 2935, loss = 0.08710242\n",
      "Iteration 2936, loss = 0.08709784\n",
      "Iteration 2937, loss = 0.08709327\n",
      "Iteration 2938, loss = 0.08708870\n",
      "Iteration 2939, loss = 0.08708412\n",
      "Iteration 2940, loss = 0.08707955\n",
      "Iteration 2941, loss = 0.08707497\n",
      "Iteration 2942, loss = 0.08707040\n",
      "Iteration 2943, loss = 0.08706585\n",
      "Iteration 2944, loss = 0.08706126\n",
      "Iteration 2945, loss = 0.08705669\n",
      "Iteration 2946, loss = 0.08705213\n",
      "Iteration 2947, loss = 0.08704758\n",
      "Iteration 2948, loss = 0.08704302\n",
      "Iteration 2949, loss = 0.08703845\n",
      "Iteration 2950, loss = 0.08703389\n",
      "Iteration 2951, loss = 0.08702933\n",
      "Iteration 2952, loss = 0.08702479\n",
      "Iteration 2953, loss = 0.08702023\n",
      "Iteration 2954, loss = 0.08701567\n",
      "Iteration 2955, loss = 0.08701112\n",
      "Iteration 2956, loss = 0.08700656\n",
      "Iteration 2957, loss = 0.08700203\n",
      "Iteration 2958, loss = 0.08699747\n",
      "Iteration 2959, loss = 0.08699292\n",
      "Iteration 2960, loss = 0.08698838\n",
      "Iteration 2961, loss = 0.08698384\n",
      "Iteration 2962, loss = 0.08697931\n",
      "Iteration 2963, loss = 0.08697476\n",
      "Iteration 2964, loss = 0.08697022\n",
      "Iteration 2965, loss = 0.08696568\n",
      "Iteration 2966, loss = 0.08696114\n",
      "Iteration 2967, loss = 0.08695661\n",
      "Iteration 2968, loss = 0.08695208\n",
      "Iteration 2969, loss = 0.08694755\n",
      "Iteration 2970, loss = 0.08694302\n",
      "Iteration 2971, loss = 0.08693848\n",
      "Iteration 2972, loss = 0.08693396\n",
      "Iteration 2973, loss = 0.08692944\n",
      "Iteration 2974, loss = 0.08692491\n",
      "Iteration 2975, loss = 0.08692039\n",
      "Iteration 2976, loss = 0.08691586\n",
      "Iteration 2977, loss = 0.08691135\n",
      "Iteration 2978, loss = 0.08690683\n",
      "Iteration 2979, loss = 0.08690231\n",
      "Iteration 2980, loss = 0.08689779\n",
      "Iteration 2981, loss = 0.08689328\n",
      "Iteration 2982, loss = 0.08688876\n",
      "Iteration 2983, loss = 0.08688426\n",
      "Iteration 2984, loss = 0.08687974\n",
      "Iteration 2985, loss = 0.08687524\n",
      "Iteration 2986, loss = 0.08687073\n",
      "Iteration 2987, loss = 0.08686624\n",
      "Iteration 2988, loss = 0.08686174\n",
      "Iteration 2989, loss = 0.08685723\n",
      "Iteration 2990, loss = 0.08685274\n",
      "Iteration 2991, loss = 0.08684824\n",
      "Iteration 2992, loss = 0.08684374\n",
      "Iteration 2993, loss = 0.08683926\n",
      "Iteration 2994, loss = 0.08683476\n",
      "Iteration 2995, loss = 0.08683027\n",
      "Iteration 2996, loss = 0.08682578\n",
      "Iteration 2997, loss = 0.08682129\n",
      "Iteration 2998, loss = 0.08681681\n",
      "Iteration 2999, loss = 0.08681233\n",
      "Iteration 3000, loss = 0.08680784\n",
      "Iteration 3001, loss = 0.08680335\n",
      "Iteration 3002, loss = 0.08679886\n",
      "Iteration 3003, loss = 0.08679441\n",
      "Iteration 3004, loss = 0.08678992\n",
      "Iteration 3005, loss = 0.08678544\n",
      "Iteration 3006, loss = 0.08678096\n",
      "Iteration 3007, loss = 0.08677649\n",
      "Iteration 3008, loss = 0.08677204\n",
      "Iteration 3009, loss = 0.08676756\n",
      "Iteration 3010, loss = 0.08676309\n",
      "Iteration 3011, loss = 0.08675862\n",
      "Iteration 3012, loss = 0.08675414\n",
      "Iteration 3013, loss = 0.08674970\n",
      "Iteration 3014, loss = 0.08674524\n",
      "Iteration 3015, loss = 0.08674077\n",
      "Iteration 3016, loss = 0.08673631\n",
      "Iteration 3017, loss = 0.08673186\n",
      "Iteration 3018, loss = 0.08672738\n",
      "Iteration 3019, loss = 0.08672294\n",
      "Iteration 3020, loss = 0.08671848\n",
      "Iteration 3021, loss = 0.08671404\n",
      "Iteration 3022, loss = 0.08670957\n",
      "Iteration 3023, loss = 0.08670513\n",
      "Iteration 3024, loss = 0.08670068\n",
      "Iteration 3025, loss = 0.08669623\n",
      "Iteration 3026, loss = 0.08669179\n",
      "Iteration 3027, loss = 0.08668734\n",
      "Iteration 3028, loss = 0.08668290\n",
      "Iteration 3029, loss = 0.08667845\n",
      "Iteration 3030, loss = 0.08667401\n",
      "Iteration 3031, loss = 0.08666959\n",
      "Iteration 3032, loss = 0.08666513\n",
      "Iteration 3033, loss = 0.08666069\n",
      "Iteration 3034, loss = 0.08665627\n",
      "Iteration 3035, loss = 0.08665184\n",
      "Iteration 3036, loss = 0.08664740\n",
      "Iteration 3037, loss = 0.08664297\n",
      "Iteration 3038, loss = 0.08663853\n",
      "Iteration 3039, loss = 0.08663412\n",
      "Iteration 3040, loss = 0.08662969\n",
      "Iteration 3041, loss = 0.08662525\n",
      "Iteration 3042, loss = 0.08662082\n",
      "Iteration 3043, loss = 0.08661642\n",
      "Iteration 3044, loss = 0.08661200\n",
      "Iteration 3045, loss = 0.08660758\n",
      "Iteration 3046, loss = 0.08660316\n",
      "Iteration 3047, loss = 0.08659874\n",
      "Iteration 3048, loss = 0.08659432\n",
      "Iteration 3049, loss = 0.08658992\n",
      "Iteration 3050, loss = 0.08658551\n",
      "Iteration 3051, loss = 0.08658110\n",
      "Iteration 3052, loss = 0.08657668\n",
      "Iteration 3053, loss = 0.08657228\n",
      "Iteration 3054, loss = 0.08656788\n",
      "Iteration 3055, loss = 0.08656347\n",
      "Iteration 3056, loss = 0.08655906\n",
      "Iteration 3057, loss = 0.08655466\n",
      "Iteration 3058, loss = 0.08655026\n",
      "Iteration 3059, loss = 0.08654587\n",
      "Iteration 3060, loss = 0.08654147\n",
      "Iteration 3061, loss = 0.08653707\n",
      "Iteration 3062, loss = 0.08653267\n",
      "Iteration 3063, loss = 0.08652829\n",
      "Iteration 3064, loss = 0.08652389\n",
      "Iteration 3065, loss = 0.08651950\n",
      "Iteration 3066, loss = 0.08651510\n",
      "Iteration 3067, loss = 0.08651072\n",
      "Iteration 3068, loss = 0.08650633\n",
      "Iteration 3069, loss = 0.08650196\n",
      "Iteration 3070, loss = 0.08649756\n",
      "Iteration 3071, loss = 0.08649317\n",
      "Iteration 3072, loss = 0.08648879\n",
      "Iteration 3073, loss = 0.08648441\n",
      "Iteration 3074, loss = 0.08648005\n",
      "Iteration 3075, loss = 0.08647566\n",
      "Iteration 3076, loss = 0.08647128\n",
      "Iteration 3077, loss = 0.08646690\n",
      "Iteration 3078, loss = 0.08646252\n",
      "Iteration 3079, loss = 0.08645816\n",
      "Iteration 3080, loss = 0.08645378\n",
      "Iteration 3081, loss = 0.08644941\n",
      "Iteration 3082, loss = 0.08644505\n",
      "Iteration 3083, loss = 0.08644067\n",
      "Iteration 3084, loss = 0.08643631\n",
      "Iteration 3085, loss = 0.08643194\n",
      "Iteration 3086, loss = 0.08642758\n",
      "Iteration 3087, loss = 0.08642322\n",
      "Iteration 3088, loss = 0.08641885\n",
      "Iteration 3089, loss = 0.08641449\n",
      "Iteration 3090, loss = 0.08641013\n",
      "Iteration 3091, loss = 0.08640578\n",
      "Iteration 3092, loss = 0.08640142\n",
      "Iteration 3093, loss = 0.08639707\n",
      "Iteration 3094, loss = 0.08639271\n",
      "Iteration 3095, loss = 0.08638837\n",
      "Iteration 3096, loss = 0.08638401\n",
      "Iteration 3097, loss = 0.08637967\n",
      "Iteration 3098, loss = 0.08637532\n",
      "Iteration 3099, loss = 0.08637096\n",
      "Iteration 3100, loss = 0.08636663\n",
      "Iteration 3101, loss = 0.08636228\n",
      "Iteration 3102, loss = 0.08635794\n",
      "Iteration 3103, loss = 0.08635360\n",
      "Iteration 3104, loss = 0.08634926\n",
      "Iteration 3105, loss = 0.08634492\n",
      "Iteration 3106, loss = 0.08634059\n",
      "Iteration 3107, loss = 0.08633625\n",
      "Iteration 3108, loss = 0.08633191\n",
      "Iteration 3109, loss = 0.08632757\n",
      "Iteration 3110, loss = 0.08632326\n",
      "Iteration 3111, loss = 0.08631893\n",
      "Iteration 3112, loss = 0.08631459\n",
      "Iteration 3113, loss = 0.08631026\n",
      "Iteration 3114, loss = 0.08630593\n",
      "Iteration 3115, loss = 0.08630161\n",
      "Iteration 3116, loss = 0.08629729\n",
      "Iteration 3117, loss = 0.08629297\n",
      "Iteration 3118, loss = 0.08628864\n",
      "Iteration 3119, loss = 0.08628431\n",
      "Iteration 3120, loss = 0.08628000\n",
      "Iteration 3121, loss = 0.08627569\n",
      "Iteration 3122, loss = 0.08627137\n",
      "Iteration 3123, loss = 0.08626705\n",
      "Iteration 3124, loss = 0.08626272\n",
      "Iteration 3125, loss = 0.08625842\n",
      "Iteration 3126, loss = 0.08625411\n",
      "Iteration 3127, loss = 0.08624980\n",
      "Iteration 3128, loss = 0.08624549\n",
      "Iteration 3129, loss = 0.08624117\n",
      "Iteration 3130, loss = 0.08623687\n",
      "Iteration 3131, loss = 0.08623257\n",
      "Iteration 3132, loss = 0.08622826\n",
      "Iteration 3133, loss = 0.08622396\n",
      "Iteration 3134, loss = 0.08621965\n",
      "Iteration 3135, loss = 0.08621535\n",
      "Iteration 3136, loss = 0.08621106\n",
      "Iteration 3137, loss = 0.08620676\n",
      "Iteration 3138, loss = 0.08620246\n",
      "Iteration 3139, loss = 0.08619816\n",
      "Iteration 3140, loss = 0.08619386\n",
      "Iteration 3141, loss = 0.08618958\n",
      "Iteration 3142, loss = 0.08618529\n",
      "Iteration 3143, loss = 0.08618099\n",
      "Iteration 3144, loss = 0.08617670\n",
      "Iteration 3145, loss = 0.08617240\n",
      "Iteration 3146, loss = 0.08616814\n",
      "Iteration 3147, loss = 0.08616384\n",
      "Iteration 3148, loss = 0.08615955\n",
      "Iteration 3149, loss = 0.08615526\n",
      "Iteration 3150, loss = 0.08615098\n",
      "Iteration 3151, loss = 0.08614672\n",
      "Iteration 3152, loss = 0.08614242\n",
      "Iteration 3153, loss = 0.08613814\n",
      "Iteration 3154, loss = 0.08613386\n",
      "Iteration 3155, loss = 0.08612959\n",
      "Iteration 3156, loss = 0.08612532\n",
      "Iteration 3157, loss = 0.08612106\n",
      "Iteration 3158, loss = 0.08611679\n",
      "Iteration 3159, loss = 0.08611253\n",
      "Iteration 3160, loss = 0.08610826\n",
      "Iteration 3161, loss = 0.08610401\n",
      "Iteration 3162, loss = 0.08609974\n",
      "Iteration 3163, loss = 0.08609549\n",
      "Iteration 3164, loss = 0.08609123\n",
      "Iteration 3165, loss = 0.08608697\n",
      "Iteration 3166, loss = 0.08608272\n",
      "Iteration 3167, loss = 0.08607848\n",
      "Iteration 3168, loss = 0.08607423\n",
      "Iteration 3169, loss = 0.08606997\n",
      "Iteration 3170, loss = 0.08606572\n",
      "Iteration 3171, loss = 0.08606147\n",
      "Iteration 3172, loss = 0.08605725\n",
      "Iteration 3173, loss = 0.08605299\n",
      "Iteration 3174, loss = 0.08604874\n",
      "Iteration 3175, loss = 0.08604449\n",
      "Iteration 3176, loss = 0.08604027\n",
      "Iteration 3177, loss = 0.08603603\n",
      "Iteration 3178, loss = 0.08603179\n",
      "Iteration 3179, loss = 0.08602755\n",
      "Iteration 3180, loss = 0.08602331\n",
      "Iteration 3181, loss = 0.08601908\n",
      "Iteration 3182, loss = 0.08601486\n",
      "Iteration 3183, loss = 0.08601062\n",
      "Iteration 3184, loss = 0.08600639\n",
      "Iteration 3185, loss = 0.08600215\n",
      "Iteration 3186, loss = 0.08599793\n",
      "Iteration 3187, loss = 0.08599371\n",
      "Iteration 3188, loss = 0.08598948\n",
      "Iteration 3189, loss = 0.08598526\n",
      "Iteration 3190, loss = 0.08598103\n",
      "Iteration 3191, loss = 0.08597681\n",
      "Iteration 3192, loss = 0.08597259\n",
      "Iteration 3193, loss = 0.08596838\n",
      "Iteration 3194, loss = 0.08596416\n",
      "Iteration 3195, loss = 0.08595994\n",
      "Iteration 3196, loss = 0.08595571\n",
      "Iteration 3197, loss = 0.08595152\n",
      "Iteration 3198, loss = 0.08594730\n",
      "Iteration 3199, loss = 0.08594308\n",
      "Iteration 3200, loss = 0.08593886\n",
      "Iteration 3201, loss = 0.08593466\n",
      "Iteration 3202, loss = 0.08593045\n",
      "Iteration 3203, loss = 0.08592626\n",
      "Iteration 3204, loss = 0.08592204\n",
      "Iteration 3205, loss = 0.08591783\n",
      "Iteration 3206, loss = 0.08591362\n",
      "Iteration 3207, loss = 0.08590943\n",
      "Iteration 3208, loss = 0.08590523\n",
      "Iteration 3209, loss = 0.08590103\n",
      "Iteration 3210, loss = 0.08589683\n",
      "Iteration 3211, loss = 0.08589263\n",
      "Iteration 3212, loss = 0.08588844\n",
      "Iteration 3213, loss = 0.08588424\n",
      "Iteration 3214, loss = 0.08588006\n",
      "Iteration 3215, loss = 0.08587585\n",
      "Iteration 3216, loss = 0.08587166\n",
      "Iteration 3217, loss = 0.08586747\n",
      "Iteration 3218, loss = 0.08586328\n",
      "Iteration 3219, loss = 0.08585910\n",
      "Iteration 3220, loss = 0.08585490\n",
      "Iteration 3221, loss = 0.08585071\n",
      "Iteration 3222, loss = 0.08584654\n",
      "Iteration 3223, loss = 0.08584234\n",
      "Iteration 3224, loss = 0.08583817\n",
      "Iteration 3225, loss = 0.08583398\n",
      "Iteration 3226, loss = 0.08582980\n",
      "Iteration 3227, loss = 0.08582563\n",
      "Iteration 3228, loss = 0.08582144\n",
      "Iteration 3229, loss = 0.08581727\n",
      "Iteration 3230, loss = 0.08581309\n",
      "Iteration 3231, loss = 0.08580892\n",
      "Iteration 3232, loss = 0.08580475\n",
      "Iteration 3233, loss = 0.08580057\n",
      "Iteration 3234, loss = 0.08579640\n",
      "Iteration 3235, loss = 0.08579223\n",
      "Iteration 3236, loss = 0.08578806\n",
      "Iteration 3237, loss = 0.08578389\n",
      "Iteration 3238, loss = 0.08577973\n",
      "Iteration 3239, loss = 0.08577556\n",
      "Iteration 3240, loss = 0.08577139\n",
      "Iteration 3241, loss = 0.08576723\n",
      "Iteration 3242, loss = 0.08576306\n",
      "Iteration 3243, loss = 0.08575892\n",
      "Iteration 3244, loss = 0.08575474\n",
      "Iteration 3245, loss = 0.08575058\n",
      "Iteration 3246, loss = 0.08574642\n",
      "Iteration 3247, loss = 0.08574228\n",
      "Iteration 3248, loss = 0.08573812\n",
      "Iteration 3249, loss = 0.08573396\n",
      "Iteration 3250, loss = 0.08572981\n",
      "Iteration 3251, loss = 0.08572565\n",
      "Iteration 3252, loss = 0.08572150\n",
      "Iteration 3253, loss = 0.08571737\n",
      "Iteration 3254, loss = 0.08571320\n",
      "Iteration 3255, loss = 0.08570906\n",
      "Iteration 3256, loss = 0.08570490\n",
      "Iteration 3257, loss = 0.08570076\n",
      "Iteration 3258, loss = 0.08569662\n",
      "Iteration 3259, loss = 0.08569248\n",
      "Iteration 3260, loss = 0.08568834\n",
      "Iteration 3261, loss = 0.08568419\n",
      "Iteration 3262, loss = 0.08568004\n",
      "Iteration 3263, loss = 0.08567591\n",
      "Iteration 3264, loss = 0.08567178\n",
      "Iteration 3265, loss = 0.08566764\n",
      "Iteration 3266, loss = 0.08566350\n",
      "Iteration 3267, loss = 0.08565936\n",
      "Iteration 3268, loss = 0.08565524\n",
      "Iteration 3269, loss = 0.08565110\n",
      "Iteration 3270, loss = 0.08564697\n",
      "Iteration 3271, loss = 0.08564283\n",
      "Iteration 3272, loss = 0.08563870\n",
      "Iteration 3273, loss = 0.08563458\n",
      "Iteration 3274, loss = 0.08563045\n",
      "Iteration 3275, loss = 0.08562633\n",
      "Iteration 3276, loss = 0.08562220\n",
      "Iteration 3277, loss = 0.08561807\n",
      "Iteration 3278, loss = 0.08561395\n",
      "Iteration 3279, loss = 0.08560983\n",
      "Iteration 3280, loss = 0.08560571\n",
      "Iteration 3281, loss = 0.08560159\n",
      "Iteration 3282, loss = 0.08559747\n",
      "Iteration 3283, loss = 0.08559335\n",
      "Iteration 3284, loss = 0.08558923\n",
      "Iteration 3285, loss = 0.08558512\n",
      "Iteration 3286, loss = 0.08558101\n",
      "Iteration 3287, loss = 0.08557689\n",
      "Iteration 3288, loss = 0.08557277\n",
      "Iteration 3289, loss = 0.08556868\n",
      "Iteration 3290, loss = 0.08556455\n",
      "Iteration 3291, loss = 0.08556044\n",
      "Iteration 3292, loss = 0.08555634\n",
      "Iteration 3293, loss = 0.08555225\n",
      "Iteration 3294, loss = 0.08554813\n",
      "Iteration 3295, loss = 0.08554403\n",
      "Iteration 3296, loss = 0.08553992\n",
      "Iteration 3297, loss = 0.08553581\n",
      "Iteration 3298, loss = 0.08553172\n",
      "Iteration 3299, loss = 0.08552763\n",
      "Iteration 3300, loss = 0.08552351\n",
      "Iteration 3301, loss = 0.08551941\n",
      "Iteration 3302, loss = 0.08551531\n",
      "Iteration 3303, loss = 0.08551123\n",
      "Iteration 3304, loss = 0.08550714\n",
      "Iteration 3305, loss = 0.08550304\n",
      "Iteration 3306, loss = 0.08549895\n",
      "Iteration 3307, loss = 0.08549485\n",
      "Iteration 3308, loss = 0.08549075\n",
      "Iteration 3309, loss = 0.08548667\n",
      "Iteration 3310, loss = 0.08548258\n",
      "Iteration 3311, loss = 0.08547850\n",
      "Iteration 3312, loss = 0.08547440\n",
      "Iteration 3313, loss = 0.08547032\n",
      "Iteration 3314, loss = 0.08546624\n",
      "Iteration 3315, loss = 0.08546216\n",
      "Iteration 3316, loss = 0.08545807\n",
      "Iteration 3317, loss = 0.08545399\n",
      "Iteration 3318, loss = 0.08544991\n",
      "Iteration 3319, loss = 0.08544584\n",
      "Iteration 3320, loss = 0.08544175\n",
      "Iteration 3321, loss = 0.08543769\n",
      "Iteration 3322, loss = 0.08543360\n",
      "Iteration 3323, loss = 0.08542952\n",
      "Iteration 3324, loss = 0.08542547\n",
      "Iteration 3325, loss = 0.08542138\n",
      "Iteration 3326, loss = 0.08541732\n",
      "Iteration 3327, loss = 0.08541325\n",
      "Iteration 3328, loss = 0.08540918\n",
      "Iteration 3329, loss = 0.08540511\n",
      "Iteration 3330, loss = 0.08540105\n",
      "Iteration 3331, loss = 0.08539698\n",
      "Iteration 3332, loss = 0.08539291\n",
      "Iteration 3333, loss = 0.08538886\n",
      "Iteration 3334, loss = 0.08538478\n",
      "Iteration 3335, loss = 0.08538074\n",
      "Iteration 3336, loss = 0.08537667\n",
      "Iteration 3337, loss = 0.08537261\n",
      "Iteration 3338, loss = 0.08536855\n",
      "Iteration 3339, loss = 0.08536449\n",
      "Iteration 3340, loss = 0.08536046\n",
      "Iteration 3341, loss = 0.08535639\n",
      "Iteration 3342, loss = 0.08535233\n",
      "Iteration 3343, loss = 0.08534827\n",
      "Iteration 3344, loss = 0.08534422\n",
      "Iteration 3345, loss = 0.08534018\n",
      "Iteration 3346, loss = 0.08533613\n",
      "Iteration 3347, loss = 0.08533208\n",
      "Iteration 3348, loss = 0.08532803\n",
      "Iteration 3349, loss = 0.08532397\n",
      "Iteration 3350, loss = 0.08531994\n",
      "Iteration 3351, loss = 0.08531590\n",
      "Iteration 3352, loss = 0.08531185\n",
      "Iteration 3353, loss = 0.08530780\n",
      "Iteration 3354, loss = 0.08530375\n",
      "Iteration 3355, loss = 0.08529974\n",
      "Iteration 3356, loss = 0.08529568\n",
      "Iteration 3357, loss = 0.08529165\n",
      "Iteration 3358, loss = 0.08528761\n",
      "Iteration 3359, loss = 0.08528357\n",
      "Iteration 3360, loss = 0.08527954\n",
      "Iteration 3361, loss = 0.08527551\n",
      "Iteration 3362, loss = 0.08527147\n",
      "Iteration 3363, loss = 0.08526744\n",
      "Iteration 3364, loss = 0.08526340\n",
      "Iteration 3365, loss = 0.08525937\n",
      "Iteration 3366, loss = 0.08525535\n",
      "Iteration 3367, loss = 0.08525132\n",
      "Iteration 3368, loss = 0.08524729\n",
      "Iteration 3369, loss = 0.08524325\n",
      "Iteration 3370, loss = 0.08523924\n",
      "Iteration 3371, loss = 0.08523522\n",
      "Iteration 3372, loss = 0.08523119\n",
      "Iteration 3373, loss = 0.08522717\n",
      "Iteration 3374, loss = 0.08522314\n",
      "Iteration 3375, loss = 0.08521911\n",
      "Iteration 3376, loss = 0.08521512\n",
      "Iteration 3377, loss = 0.08521109\n",
      "Iteration 3378, loss = 0.08520707\n",
      "Iteration 3379, loss = 0.08520305\n",
      "Iteration 3380, loss = 0.08519903\n",
      "Iteration 3381, loss = 0.08519504\n",
      "Iteration 3382, loss = 0.08519101\n",
      "Iteration 3383, loss = 0.08518700\n",
      "Iteration 3384, loss = 0.08518298\n",
      "Iteration 3385, loss = 0.08517899\n",
      "Iteration 3386, loss = 0.08517497\n",
      "Iteration 3387, loss = 0.08517096\n",
      "Iteration 3388, loss = 0.08516695\n",
      "Iteration 3389, loss = 0.08516295\n",
      "Iteration 3390, loss = 0.08515894\n",
      "Iteration 3391, loss = 0.08515495\n",
      "Iteration 3392, loss = 0.08515093\n",
      "Iteration 3393, loss = 0.08514693\n",
      "Iteration 3394, loss = 0.08514292\n",
      "Iteration 3395, loss = 0.08513893\n",
      "Iteration 3396, loss = 0.08513493\n",
      "Iteration 3397, loss = 0.08513093\n",
      "Iteration 3398, loss = 0.08512693\n",
      "Iteration 3399, loss = 0.08512293\n",
      "Iteration 3400, loss = 0.08511892\n",
      "Iteration 3401, loss = 0.08511494\n",
      "Iteration 3402, loss = 0.08511094\n",
      "Iteration 3403, loss = 0.08510695\n",
      "Iteration 3404, loss = 0.08510295\n",
      "Iteration 3405, loss = 0.08509895\n",
      "Iteration 3406, loss = 0.08509497\n",
      "Iteration 3407, loss = 0.08509098\n",
      "Iteration 3408, loss = 0.08508698\n",
      "Iteration 3409, loss = 0.08508299\n",
      "Iteration 3410, loss = 0.08507901\n",
      "Iteration 3411, loss = 0.08507506\n",
      "Iteration 3412, loss = 0.08507107\n",
      "Iteration 3413, loss = 0.08506710\n",
      "Iteration 3414, loss = 0.08506314\n",
      "Iteration 3415, loss = 0.08505916\n",
      "Iteration 3416, loss = 0.08505520\n",
      "Iteration 3417, loss = 0.08505123\n",
      "Iteration 3418, loss = 0.08504725\n",
      "Iteration 3419, loss = 0.08504328\n",
      "Iteration 3420, loss = 0.08503931\n",
      "Iteration 3421, loss = 0.08503535\n",
      "Iteration 3422, loss = 0.08503138\n",
      "Iteration 3423, loss = 0.08502742\n",
      "Iteration 3424, loss = 0.08502345\n",
      "Iteration 3425, loss = 0.08501947\n",
      "Iteration 3426, loss = 0.08501551\n",
      "Iteration 3427, loss = 0.08501156\n",
      "Iteration 3428, loss = 0.08500760\n",
      "Iteration 3429, loss = 0.08500364\n",
      "Iteration 3430, loss = 0.08499968\n",
      "Iteration 3431, loss = 0.08499571\n",
      "Iteration 3432, loss = 0.08499177\n",
      "Iteration 3433, loss = 0.08498781\n",
      "Iteration 3434, loss = 0.08498385\n",
      "Iteration 3435, loss = 0.08497990\n",
      "Iteration 3436, loss = 0.08497595\n",
      "Iteration 3437, loss = 0.08497200\n",
      "Iteration 3438, loss = 0.08496805\n",
      "Iteration 3439, loss = 0.08496410\n",
      "Iteration 3440, loss = 0.08496016\n",
      "Iteration 3441, loss = 0.08495619\n",
      "Iteration 3442, loss = 0.08495227\n",
      "Iteration 3443, loss = 0.08494831\n",
      "Iteration 3444, loss = 0.08494437\n",
      "Iteration 3445, loss = 0.08494042\n",
      "Iteration 3446, loss = 0.08493648\n",
      "Iteration 3447, loss = 0.08493255\n",
      "Iteration 3448, loss = 0.08492861\n",
      "Iteration 3449, loss = 0.08492467\n",
      "Iteration 3450, loss = 0.08492073\n",
      "Iteration 3451, loss = 0.08491678\n",
      "Iteration 3452, loss = 0.08491285\n",
      "Iteration 3453, loss = 0.08490891\n",
      "Iteration 3454, loss = 0.08490499\n",
      "Iteration 3455, loss = 0.08490105\n",
      "Iteration 3456, loss = 0.08489711\n",
      "Iteration 3457, loss = 0.08489319\n",
      "Iteration 3458, loss = 0.08488925\n",
      "Iteration 3459, loss = 0.08488532\n",
      "Iteration 3460, loss = 0.08488140\n",
      "Iteration 3461, loss = 0.08487747\n",
      "Iteration 3462, loss = 0.08487355\n",
      "Iteration 3463, loss = 0.08486962\n",
      "Iteration 3464, loss = 0.08486569\n",
      "Iteration 3465, loss = 0.08486177\n",
      "Iteration 3466, loss = 0.08485784\n",
      "Iteration 3467, loss = 0.08485393\n",
      "Iteration 3468, loss = 0.08485001\n",
      "Iteration 3469, loss = 0.08484609\n",
      "Iteration 3470, loss = 0.08484217\n",
      "Iteration 3471, loss = 0.08483825\n",
      "Iteration 3472, loss = 0.08483433\n",
      "Iteration 3473, loss = 0.08483042\n",
      "Iteration 3474, loss = 0.08482652\n",
      "Iteration 3475, loss = 0.08482260\n",
      "Iteration 3476, loss = 0.08481868\n",
      "Iteration 3477, loss = 0.08481477\n",
      "Iteration 3478, loss = 0.08481088\n",
      "Iteration 3479, loss = 0.08480695\n",
      "Iteration 3480, loss = 0.08480304\n",
      "Iteration 3481, loss = 0.08479913\n",
      "Iteration 3482, loss = 0.08479524\n",
      "Iteration 3483, loss = 0.08479133\n",
      "Iteration 3484, loss = 0.08478742\n",
      "Iteration 3485, loss = 0.08478352\n",
      "Iteration 3486, loss = 0.08477961\n",
      "Iteration 3487, loss = 0.08477570\n",
      "Iteration 3488, loss = 0.08477183\n",
      "Iteration 3489, loss = 0.08476791\n",
      "Iteration 3490, loss = 0.08476401\n",
      "Iteration 3491, loss = 0.08476011\n",
      "Iteration 3492, loss = 0.08475622\n",
      "Iteration 3493, loss = 0.08475232\n",
      "Iteration 3494, loss = 0.08474843\n",
      "Iteration 3495, loss = 0.08474454\n",
      "Iteration 3496, loss = 0.08474064\n",
      "Iteration 3497, loss = 0.08473675\n",
      "Iteration 3498, loss = 0.08473287\n",
      "Iteration 3499, loss = 0.08472897\n",
      "Iteration 3500, loss = 0.08472508\n",
      "Iteration 3501, loss = 0.08472118\n",
      "Iteration 3502, loss = 0.08471730\n",
      "Iteration 3503, loss = 0.08471343\n",
      "Iteration 3504, loss = 0.08470953\n",
      "Iteration 3505, loss = 0.08470565\n",
      "Iteration 3506, loss = 0.08470176\n",
      "Iteration 3507, loss = 0.08469787\n",
      "Iteration 3508, loss = 0.08469400\n",
      "Iteration 3509, loss = 0.08469012\n",
      "Iteration 3510, loss = 0.08468624\n",
      "Iteration 3511, loss = 0.08468235\n",
      "Iteration 3512, loss = 0.08467847\n",
      "Iteration 3513, loss = 0.08467461\n",
      "Iteration 3514, loss = 0.08467073\n",
      "Iteration 3515, loss = 0.08466684\n",
      "Iteration 3516, loss = 0.08466298\n",
      "Iteration 3517, loss = 0.08465909\n",
      "Iteration 3518, loss = 0.08465523\n",
      "Iteration 3519, loss = 0.08465136\n",
      "Iteration 3520, loss = 0.08464749\n",
      "Iteration 3521, loss = 0.08464361\n",
      "Iteration 3522, loss = 0.08463974\n",
      "Iteration 3523, loss = 0.08463588\n",
      "Iteration 3524, loss = 0.08463202\n",
      "Iteration 3525, loss = 0.08462815\n",
      "Iteration 3526, loss = 0.08462428\n",
      "Iteration 3527, loss = 0.08462042\n",
      "Iteration 3528, loss = 0.08461654\n",
      "Iteration 3529, loss = 0.08461270\n",
      "Iteration 3530, loss = 0.08460882\n",
      "Iteration 3531, loss = 0.08460497\n",
      "Iteration 3532, loss = 0.08460110\n",
      "Iteration 3533, loss = 0.08459725\n",
      "Iteration 3534, loss = 0.08459339\n",
      "Iteration 3535, loss = 0.08458953\n",
      "Iteration 3536, loss = 0.08458568\n",
      "Iteration 3537, loss = 0.08458182\n",
      "Iteration 3538, loss = 0.08457796\n",
      "Iteration 3539, loss = 0.08457413\n",
      "Iteration 3540, loss = 0.08457026\n",
      "Iteration 3541, loss = 0.08456641\n",
      "Iteration 3542, loss = 0.08456256\n",
      "Iteration 3543, loss = 0.08455871\n",
      "Iteration 3544, loss = 0.08455486\n",
      "Iteration 3545, loss = 0.08455101\n",
      "Iteration 3546, loss = 0.08454718\n",
      "Iteration 3547, loss = 0.08454332\n",
      "Iteration 3548, loss = 0.08453947\n",
      "Iteration 3549, loss = 0.08453563\n",
      "Iteration 3550, loss = 0.08453179\n",
      "Iteration 3551, loss = 0.08452794\n",
      "Iteration 3552, loss = 0.08452410\n",
      "Iteration 3553, loss = 0.08452026\n",
      "Iteration 3554, loss = 0.08451644\n",
      "Iteration 3555, loss = 0.08451259\n",
      "Iteration 3556, loss = 0.08450875\n",
      "Iteration 3557, loss = 0.08450491\n",
      "Iteration 3558, loss = 0.08450107\n",
      "Iteration 3559, loss = 0.08449724\n",
      "Iteration 3560, loss = 0.08449340\n",
      "Iteration 3561, loss = 0.08448958\n",
      "Iteration 3562, loss = 0.08448573\n",
      "Iteration 3563, loss = 0.08448190\n",
      "Iteration 3564, loss = 0.08447808\n",
      "Iteration 3565, loss = 0.08447424\n",
      "Iteration 3566, loss = 0.08447041\n",
      "Iteration 3567, loss = 0.08446658\n",
      "Iteration 3568, loss = 0.08446277\n",
      "Iteration 3569, loss = 0.08445893\n",
      "Iteration 3570, loss = 0.08445511\n",
      "Iteration 3571, loss = 0.08445128\n",
      "Iteration 3572, loss = 0.08444746\n",
      "Iteration 3573, loss = 0.08444363\n",
      "Iteration 3574, loss = 0.08443981\n",
      "Iteration 3575, loss = 0.08443599\n",
      "Iteration 3576, loss = 0.08443218\n",
      "Iteration 3577, loss = 0.08442836\n",
      "Iteration 3578, loss = 0.08442454\n",
      "Iteration 3579, loss = 0.08442072\n",
      "Iteration 3580, loss = 0.08441692\n",
      "Iteration 3581, loss = 0.08441309\n",
      "Iteration 3582, loss = 0.08440927\n",
      "Iteration 3583, loss = 0.08440545\n",
      "Iteration 3584, loss = 0.08440166\n",
      "Iteration 3585, loss = 0.08439783\n",
      "Iteration 3586, loss = 0.08439402\n",
      "Iteration 3587, loss = 0.08439021\n",
      "Iteration 3588, loss = 0.08438640\n",
      "Iteration 3589, loss = 0.08438258\n",
      "Iteration 3590, loss = 0.08437880\n",
      "Iteration 3591, loss = 0.08437498\n",
      "Iteration 3592, loss = 0.08437117\n",
      "Iteration 3593, loss = 0.08436736\n",
      "Iteration 3594, loss = 0.08436357\n",
      "Iteration 3595, loss = 0.08435976\n",
      "Iteration 3596, loss = 0.08435596\n",
      "Iteration 3597, loss = 0.08435216\n",
      "Iteration 3598, loss = 0.08434835\n",
      "Iteration 3599, loss = 0.08434455\n",
      "Iteration 3600, loss = 0.08434076\n",
      "Iteration 3601, loss = 0.08433696\n",
      "Iteration 3602, loss = 0.08433316\n",
      "Iteration 3603, loss = 0.08432936\n",
      "Iteration 3604, loss = 0.08432556\n",
      "Iteration 3605, loss = 0.08432179\n",
      "Iteration 3606, loss = 0.08431797\n",
      "Iteration 3607, loss = 0.08431419\n",
      "Iteration 3608, loss = 0.08431039\n",
      "Iteration 3609, loss = 0.08430660\n",
      "Iteration 3610, loss = 0.08430283\n",
      "Iteration 3611, loss = 0.08429903\n",
      "Iteration 3612, loss = 0.08429523\n",
      "Iteration 3613, loss = 0.08429144\n",
      "Iteration 3614, loss = 0.08428765\n",
      "Iteration 3615, loss = 0.08428389\n",
      "Iteration 3616, loss = 0.08428009\n",
      "Iteration 3617, loss = 0.08427631\n",
      "Iteration 3618, loss = 0.08427252\n",
      "Iteration 3619, loss = 0.08426874\n",
      "Iteration 3620, loss = 0.08426496\n",
      "Iteration 3621, loss = 0.08426118\n",
      "Iteration 3622, loss = 0.08425739\n",
      "Iteration 3623, loss = 0.08425362\n",
      "Iteration 3624, loss = 0.08424983\n",
      "Iteration 3625, loss = 0.08424607\n",
      "Iteration 3626, loss = 0.08424229\n",
      "Iteration 3627, loss = 0.08423851\n",
      "Iteration 3628, loss = 0.08423474\n",
      "Iteration 3629, loss = 0.08423096\n",
      "Iteration 3630, loss = 0.08422718\n",
      "Iteration 3631, loss = 0.08422341\n",
      "Iteration 3632, loss = 0.08421966\n",
      "Iteration 3633, loss = 0.08421587\n",
      "Iteration 3634, loss = 0.08421210\n",
      "Iteration 3635, loss = 0.08420833\n",
      "Iteration 3636, loss = 0.08420458\n",
      "Iteration 3637, loss = 0.08420079\n",
      "Iteration 3638, loss = 0.08419703\n",
      "Iteration 3639, loss = 0.08419326\n",
      "Iteration 3640, loss = 0.08418951\n",
      "Iteration 3641, loss = 0.08418574\n",
      "Iteration 3642, loss = 0.08418198\n",
      "Iteration 3643, loss = 0.08417822\n",
      "Iteration 3644, loss = 0.08417445\n",
      "Iteration 3645, loss = 0.08417069\n",
      "Iteration 3646, loss = 0.08416695\n",
      "Iteration 3647, loss = 0.08416317\n",
      "Iteration 3648, loss = 0.08415941\n",
      "Iteration 3649, loss = 0.08415565\n",
      "Iteration 3650, loss = 0.08415191\n",
      "Iteration 3651, loss = 0.08414815\n",
      "Iteration 3652, loss = 0.08414440\n",
      "Iteration 3653, loss = 0.08414064\n",
      "Iteration 3654, loss = 0.08413689\n",
      "Iteration 3655, loss = 0.08413315\n",
      "Iteration 3656, loss = 0.08412945\n",
      "Iteration 3657, loss = 0.08412575\n",
      "Iteration 3658, loss = 0.08412203\n",
      "Iteration 3659, loss = 0.08411832\n",
      "Iteration 3660, loss = 0.08411462\n",
      "Iteration 3661, loss = 0.08411094\n",
      "Iteration 3662, loss = 0.08410723\n",
      "Iteration 3663, loss = 0.08410353\n",
      "Iteration 3664, loss = 0.08409984\n",
      "Iteration 3665, loss = 0.08409616\n",
      "Iteration 3666, loss = 0.08409248\n",
      "Iteration 3667, loss = 0.08408878\n",
      "Iteration 3668, loss = 0.08408509\n",
      "Iteration 3669, loss = 0.08408141\n",
      "Iteration 3670, loss = 0.08407772\n",
      "Iteration 3671, loss = 0.08407406\n",
      "Iteration 3672, loss = 0.08407036\n",
      "Iteration 3673, loss = 0.08406668\n",
      "Iteration 3674, loss = 0.08406301\n",
      "Iteration 3675, loss = 0.08405933\n",
      "Iteration 3676, loss = 0.08405567\n",
      "Iteration 3677, loss = 0.08405198\n",
      "Iteration 3678, loss = 0.08404831\n",
      "Iteration 3679, loss = 0.08404463\n",
      "Iteration 3680, loss = 0.08404096\n",
      "Iteration 3681, loss = 0.08403729\n",
      "Iteration 3682, loss = 0.08403362\n",
      "Iteration 3683, loss = 0.08402997\n",
      "Iteration 3684, loss = 0.08402628\n",
      "Iteration 3685, loss = 0.08402261\n",
      "Iteration 3686, loss = 0.08401895\n",
      "Iteration 3687, loss = 0.08401529\n",
      "Iteration 3688, loss = 0.08401163\n",
      "Iteration 3689, loss = 0.08400796\n",
      "Iteration 3690, loss = 0.08400429\n",
      "Iteration 3691, loss = 0.08400063\n",
      "Iteration 3692, loss = 0.08399698\n",
      "Iteration 3693, loss = 0.08399333\n",
      "Iteration 3694, loss = 0.08398967\n",
      "Iteration 3695, loss = 0.08398601\n",
      "Iteration 3696, loss = 0.08398234\n",
      "Iteration 3697, loss = 0.08397870\n",
      "Iteration 3698, loss = 0.08397503\n",
      "Iteration 3699, loss = 0.08397138\n",
      "Iteration 3700, loss = 0.08396772\n",
      "Iteration 3701, loss = 0.08396408\n",
      "Iteration 3702, loss = 0.08396043\n",
      "Iteration 3703, loss = 0.08395677\n",
      "Iteration 3704, loss = 0.08395313\n",
      "Iteration 3705, loss = 0.08394947\n",
      "Iteration 3706, loss = 0.08394582\n",
      "Iteration 3707, loss = 0.08394219\n",
      "Iteration 3708, loss = 0.08393852\n",
      "Iteration 3709, loss = 0.08393489\n",
      "Iteration 3710, loss = 0.08393123\n",
      "Iteration 3711, loss = 0.08392760\n",
      "Iteration 3712, loss = 0.08392396\n",
      "Iteration 3713, loss = 0.08392032\n",
      "Iteration 3714, loss = 0.08391667\n",
      "Iteration 3715, loss = 0.08391303\n",
      "Iteration 3716, loss = 0.08390938\n",
      "Iteration 3717, loss = 0.08390575\n",
      "Iteration 3718, loss = 0.08390212\n",
      "Iteration 3719, loss = 0.08389847\n",
      "Iteration 3720, loss = 0.08389483\n",
      "Iteration 3721, loss = 0.08389119\n",
      "Iteration 3722, loss = 0.08388758\n",
      "Iteration 3723, loss = 0.08388393\n",
      "Iteration 3724, loss = 0.08388029\n",
      "Iteration 3725, loss = 0.08387665\n",
      "Iteration 3726, loss = 0.08387302\n",
      "Iteration 3727, loss = 0.08386941\n",
      "Iteration 3728, loss = 0.08386577\n",
      "Iteration 3729, loss = 0.08386214\n",
      "Iteration 3730, loss = 0.08385851\n",
      "Iteration 3731, loss = 0.08385488\n",
      "Iteration 3732, loss = 0.08385124\n",
      "Iteration 3733, loss = 0.08384763\n",
      "Iteration 3734, loss = 0.08384400\n",
      "Iteration 3735, loss = 0.08384038\n",
      "Iteration 3736, loss = 0.08383676\n",
      "Iteration 3737, loss = 0.08383313\n",
      "Iteration 3738, loss = 0.08382951\n",
      "Iteration 3739, loss = 0.08382589\n",
      "Iteration 3740, loss = 0.08382227\n",
      "Iteration 3741, loss = 0.08381864\n",
      "Iteration 3742, loss = 0.08381502\n",
      "Iteration 3743, loss = 0.08381141\n",
      "Iteration 3744, loss = 0.08380779\n",
      "Iteration 3745, loss = 0.08380418\n",
      "Iteration 3746, loss = 0.08380056\n",
      "Iteration 3747, loss = 0.08379695\n",
      "Iteration 3748, loss = 0.08379334\n",
      "Iteration 3749, loss = 0.08378973\n",
      "Iteration 3750, loss = 0.08378612\n",
      "Iteration 3751, loss = 0.08378251\n",
      "Iteration 3752, loss = 0.08377889\n",
      "Iteration 3753, loss = 0.08377530\n",
      "Iteration 3754, loss = 0.08377169\n",
      "Iteration 3755, loss = 0.08376809\n",
      "Iteration 3756, loss = 0.08376447\n",
      "Iteration 3757, loss = 0.08376087\n",
      "Iteration 3758, loss = 0.08375729\n",
      "Iteration 3759, loss = 0.08375367\n",
      "Iteration 3760, loss = 0.08375006\n",
      "Iteration 3761, loss = 0.08374646\n",
      "Iteration 3762, loss = 0.08374287\n",
      "Iteration 3763, loss = 0.08373927\n",
      "Iteration 3764, loss = 0.08373568\n",
      "Iteration 3765, loss = 0.08373207\n",
      "Iteration 3766, loss = 0.08372847\n",
      "Iteration 3767, loss = 0.08372488\n",
      "Iteration 3768, loss = 0.08372130\n",
      "Iteration 3769, loss = 0.08371769\n",
      "Iteration 3770, loss = 0.08371410\n",
      "Iteration 3771, loss = 0.08371050\n",
      "Iteration 3772, loss = 0.08370692\n",
      "Iteration 3773, loss = 0.08370332\n",
      "Iteration 3774, loss = 0.08369975\n",
      "Iteration 3775, loss = 0.08369615\n",
      "Iteration 3776, loss = 0.08369256\n",
      "Iteration 3777, loss = 0.08368897\n",
      "Iteration 3778, loss = 0.08368540\n",
      "Iteration 3779, loss = 0.08368180\n",
      "Iteration 3780, loss = 0.08367822\n",
      "Iteration 3781, loss = 0.08367463\n",
      "Iteration 3782, loss = 0.08367105\n",
      "Iteration 3783, loss = 0.08366748\n",
      "Iteration 3784, loss = 0.08366390\n",
      "Iteration 3785, loss = 0.08366032\n",
      "Iteration 3786, loss = 0.08365674\n",
      "Iteration 3787, loss = 0.08365316\n",
      "Iteration 3788, loss = 0.08364958\n",
      "Iteration 3789, loss = 0.08364600\n",
      "Iteration 3790, loss = 0.08364243\n",
      "Iteration 3791, loss = 0.08363885\n",
      "Iteration 3792, loss = 0.08363527\n",
      "Iteration 3793, loss = 0.08363172\n",
      "Iteration 3794, loss = 0.08362814\n",
      "Iteration 3795, loss = 0.08362456\n",
      "Iteration 3796, loss = 0.08362099\n",
      "Iteration 3797, loss = 0.08361742\n",
      "Iteration 3798, loss = 0.08361387\n",
      "Iteration 3799, loss = 0.08361029\n",
      "Iteration 3800, loss = 0.08360672\n",
      "Iteration 3801, loss = 0.08360315\n",
      "Iteration 3802, loss = 0.08359957\n",
      "Iteration 3803, loss = 0.08359603\n",
      "Iteration 3804, loss = 0.08359245\n",
      "Iteration 3805, loss = 0.08358889\n",
      "Iteration 3806, loss = 0.08358533\n",
      "Iteration 3807, loss = 0.08358176\n",
      "Iteration 3808, loss = 0.08357822\n",
      "Iteration 3809, loss = 0.08357465\n",
      "Iteration 3810, loss = 0.08357108\n",
      "Iteration 3811, loss = 0.08356752\n",
      "Iteration 3812, loss = 0.08356396\n",
      "Iteration 3813, loss = 0.08356042\n",
      "Iteration 3814, loss = 0.08355685\n",
      "Iteration 3815, loss = 0.08355330\n",
      "Iteration 3816, loss = 0.08354974\n",
      "Iteration 3817, loss = 0.08354619\n",
      "Iteration 3818, loss = 0.08354264\n",
      "Iteration 3819, loss = 0.08353908\n",
      "Iteration 3820, loss = 0.08353553\n",
      "Iteration 3821, loss = 0.08353198\n",
      "Iteration 3822, loss = 0.08352842\n",
      "Iteration 3823, loss = 0.08352488\n",
      "Iteration 3824, loss = 0.08352133\n",
      "Iteration 3825, loss = 0.08351778\n",
      "Iteration 3826, loss = 0.08351424\n",
      "Iteration 3827, loss = 0.08351068\n",
      "Iteration 3828, loss = 0.08350713\n",
      "Iteration 3829, loss = 0.08350361\n",
      "Iteration 3830, loss = 0.08350004\n",
      "Iteration 3831, loss = 0.08349650\n",
      "Iteration 3832, loss = 0.08349295\n",
      "Iteration 3833, loss = 0.08348943\n",
      "Iteration 3834, loss = 0.08348588\n",
      "Iteration 3835, loss = 0.08348234\n",
      "Iteration 3836, loss = 0.08347880\n",
      "Iteration 3837, loss = 0.08347525\n",
      "Iteration 3838, loss = 0.08347172\n",
      "Iteration 3839, loss = 0.08346818\n",
      "Iteration 3840, loss = 0.08346465\n",
      "Iteration 3841, loss = 0.08346110\n",
      "Iteration 3842, loss = 0.08345757\n",
      "Iteration 3843, loss = 0.08345403\n",
      "Iteration 3844, loss = 0.08345052\n",
      "Iteration 3845, loss = 0.08344697\n",
      "Iteration 3846, loss = 0.08344344\n",
      "Iteration 3847, loss = 0.08343991\n",
      "Iteration 3848, loss = 0.08343638\n",
      "Iteration 3849, loss = 0.08343285\n",
      "Iteration 3850, loss = 0.08342933\n",
      "Iteration 3851, loss = 0.08342580\n",
      "Iteration 3852, loss = 0.08342227\n",
      "Iteration 3853, loss = 0.08341874\n",
      "Iteration 3854, loss = 0.08341524\n",
      "Iteration 3855, loss = 0.08341170\n",
      "Iteration 3856, loss = 0.08340817\n",
      "Iteration 3857, loss = 0.08340465\n",
      "Iteration 3858, loss = 0.08340113\n",
      "Iteration 3859, loss = 0.08339762\n",
      "Iteration 3860, loss = 0.08339410\n",
      "Iteration 3861, loss = 0.08339057\n",
      "Iteration 3862, loss = 0.08338705\n",
      "Iteration 3863, loss = 0.08338353\n",
      "Iteration 3864, loss = 0.08338002\n",
      "Iteration 3865, loss = 0.08337651\n",
      "Iteration 3866, loss = 0.08337298\n",
      "Iteration 3867, loss = 0.08336947\n",
      "Iteration 3868, loss = 0.08336595\n",
      "Iteration 3869, loss = 0.08336246\n",
      "Iteration 3870, loss = 0.08335894\n",
      "Iteration 3871, loss = 0.08335542\n",
      "Iteration 3872, loss = 0.08335191\n",
      "Iteration 3873, loss = 0.08334839\n",
      "Iteration 3874, loss = 0.08334489\n",
      "Iteration 3875, loss = 0.08334139\n",
      "Iteration 3876, loss = 0.08333788\n",
      "Iteration 3877, loss = 0.08333437\n",
      "Iteration 3878, loss = 0.08333086\n",
      "Iteration 3879, loss = 0.08332736\n",
      "Iteration 3880, loss = 0.08332385\n",
      "Iteration 3881, loss = 0.08332035\n",
      "Iteration 3882, loss = 0.08331684\n",
      "Iteration 3883, loss = 0.08331334\n",
      "Iteration 3884, loss = 0.08330984\n",
      "Iteration 3885, loss = 0.08330633\n",
      "Iteration 3886, loss = 0.08330283\n",
      "Iteration 3887, loss = 0.08329933\n",
      "Iteration 3888, loss = 0.08329584\n",
      "Iteration 3889, loss = 0.08329235\n",
      "Iteration 3890, loss = 0.08328885\n",
      "Iteration 3891, loss = 0.08328535\n",
      "Iteration 3892, loss = 0.08328184\n",
      "Iteration 3893, loss = 0.08327835\n",
      "Iteration 3894, loss = 0.08327486\n",
      "Iteration 3895, loss = 0.08327136\n",
      "Iteration 3896, loss = 0.08326787\n",
      "Iteration 3897, loss = 0.08326437\n",
      "Iteration 3898, loss = 0.08326088\n",
      "Iteration 3899, loss = 0.08325740\n",
      "Iteration 3900, loss = 0.08325391\n",
      "Iteration 3901, loss = 0.08325042\n",
      "Iteration 3902, loss = 0.08324693\n",
      "Iteration 3903, loss = 0.08324344\n",
      "Iteration 3904, loss = 0.08323996\n",
      "Iteration 3905, loss = 0.08323647\n",
      "Iteration 3906, loss = 0.08323298\n",
      "Iteration 3907, loss = 0.08322949\n",
      "Iteration 3908, loss = 0.08322600\n",
      "Iteration 3909, loss = 0.08322253\n",
      "Iteration 3910, loss = 0.08321904\n",
      "Iteration 3911, loss = 0.08321556\n",
      "Iteration 3912, loss = 0.08321208\n",
      "Iteration 3913, loss = 0.08320859\n",
      "Iteration 3914, loss = 0.08320511\n",
      "Iteration 3915, loss = 0.08320164\n",
      "Iteration 3916, loss = 0.08319816\n",
      "Iteration 3917, loss = 0.08319468\n",
      "Iteration 3918, loss = 0.08319120\n",
      "Iteration 3919, loss = 0.08318772\n",
      "Iteration 3920, loss = 0.08318427\n",
      "Iteration 3921, loss = 0.08318077\n",
      "Iteration 3922, loss = 0.08317730\n",
      "Iteration 3923, loss = 0.08317382\n",
      "Iteration 3924, loss = 0.08317036\n",
      "Iteration 3925, loss = 0.08316689\n",
      "Iteration 3926, loss = 0.08316342\n",
      "Iteration 3927, loss = 0.08315995\n",
      "Iteration 3928, loss = 0.08315647\n",
      "Iteration 3929, loss = 0.08315300\n",
      "Iteration 3930, loss = 0.08314954\n",
      "Iteration 3931, loss = 0.08314606\n",
      "Iteration 3932, loss = 0.08314261\n",
      "Iteration 3933, loss = 0.08313914\n",
      "Iteration 3934, loss = 0.08313568\n",
      "Iteration 3935, loss = 0.08313222\n",
      "Iteration 3936, loss = 0.08312876\n",
      "Iteration 3937, loss = 0.08312530\n",
      "Iteration 3938, loss = 0.08312184\n",
      "Iteration 3939, loss = 0.08311837\n",
      "Iteration 3940, loss = 0.08311493\n",
      "Iteration 3941, loss = 0.08311146\n",
      "Iteration 3942, loss = 0.08310800\n",
      "Iteration 3943, loss = 0.08310455\n",
      "Iteration 3944, loss = 0.08310111\n",
      "Iteration 3945, loss = 0.08309765\n",
      "Iteration 3946, loss = 0.08309420\n",
      "Iteration 3947, loss = 0.08309074\n",
      "Iteration 3948, loss = 0.08308730\n",
      "Iteration 3949, loss = 0.08308386\n",
      "Iteration 3950, loss = 0.08308044\n",
      "Iteration 3951, loss = 0.08307699\n",
      "Iteration 3952, loss = 0.08307356\n",
      "Iteration 3953, loss = 0.08307012\n",
      "Iteration 3954, loss = 0.08306669\n",
      "Iteration 3955, loss = 0.08306328\n",
      "Iteration 3956, loss = 0.08305984\n",
      "Iteration 3957, loss = 0.08305641\n",
      "Iteration 3958, loss = 0.08305298\n",
      "Iteration 3959, loss = 0.08304955\n",
      "Iteration 3960, loss = 0.08304614\n",
      "Iteration 3961, loss = 0.08304271\n",
      "Iteration 3962, loss = 0.08303928\n",
      "Iteration 3963, loss = 0.08303586\n",
      "Iteration 3964, loss = 0.08303244\n",
      "Iteration 3965, loss = 0.08302902\n",
      "Iteration 3966, loss = 0.08302561\n",
      "Iteration 3967, loss = 0.08302218\n",
      "Iteration 3968, loss = 0.08301876\n",
      "Iteration 3969, loss = 0.08301535\n",
      "Iteration 3970, loss = 0.08301194\n",
      "Iteration 3971, loss = 0.08300852\n",
      "Iteration 3972, loss = 0.08300510\n",
      "Iteration 3973, loss = 0.08300169\n",
      "Iteration 3974, loss = 0.08299826\n",
      "Iteration 3975, loss = 0.08299486\n",
      "Iteration 3976, loss = 0.08299145\n",
      "Iteration 3977, loss = 0.08298804\n",
      "Iteration 3978, loss = 0.08298463\n",
      "Iteration 3979, loss = 0.08298122\n",
      "Iteration 3980, loss = 0.08297781\n",
      "Iteration 3981, loss = 0.08297440\n",
      "Iteration 3982, loss = 0.08297099\n",
      "Iteration 3983, loss = 0.08296758\n",
      "Iteration 3984, loss = 0.08296417\n",
      "Iteration 3985, loss = 0.08296078\n",
      "Iteration 3986, loss = 0.08295737\n",
      "Iteration 3987, loss = 0.08295396\n",
      "Iteration 3988, loss = 0.08295055\n",
      "Iteration 3989, loss = 0.08294715\n",
      "Iteration 3990, loss = 0.08294376\n",
      "Iteration 3991, loss = 0.08294036\n",
      "Iteration 3992, loss = 0.08293696\n",
      "Iteration 3993, loss = 0.08293356\n",
      "Iteration 3994, loss = 0.08293015\n",
      "Iteration 3995, loss = 0.08292676\n",
      "Iteration 3996, loss = 0.08292336\n",
      "Iteration 3997, loss = 0.08291996\n",
      "Iteration 3998, loss = 0.08291656\n",
      "Iteration 3999, loss = 0.08291316\n",
      "Iteration 4000, loss = 0.08290978\n",
      "Iteration 4001, loss = 0.08290638\n",
      "Iteration 4002, loss = 0.08290298\n",
      "Iteration 4003, loss = 0.08289960\n",
      "Iteration 4004, loss = 0.08289619\n",
      "Iteration 4005, loss = 0.08289282\n",
      "Iteration 4006, loss = 0.08288942\n",
      "Iteration 4007, loss = 0.08288603\n",
      "Iteration 4008, loss = 0.08288264\n",
      "Iteration 4009, loss = 0.08287924\n",
      "Iteration 4010, loss = 0.08287587\n",
      "Iteration 4011, loss = 0.08287247\n",
      "Iteration 4012, loss = 0.08286909\n",
      "Iteration 4013, loss = 0.08286570\n",
      "Iteration 4014, loss = 0.08286230\n",
      "Iteration 4015, loss = 0.08285896\n",
      "Iteration 4016, loss = 0.08285554\n",
      "Iteration 4017, loss = 0.08285216\n",
      "Iteration 4018, loss = 0.08284878\n",
      "Iteration 4019, loss = 0.08284539\n",
      "Iteration 4020, loss = 0.08284203\n",
      "Iteration 4021, loss = 0.08283864\n",
      "Iteration 4022, loss = 0.08283526\n",
      "Iteration 4023, loss = 0.08283188\n",
      "Iteration 4024, loss = 0.08282849\n",
      "Iteration 4025, loss = 0.08282513\n",
      "Iteration 4026, loss = 0.08282174\n",
      "Iteration 4027, loss = 0.08281837\n",
      "Iteration 4028, loss = 0.08281499\n",
      "Iteration 4029, loss = 0.08281162\n",
      "Iteration 4030, loss = 0.08280825\n",
      "Iteration 4031, loss = 0.08280487\n",
      "Iteration 4032, loss = 0.08280150\n",
      "Iteration 4033, loss = 0.08279812\n",
      "Iteration 4034, loss = 0.08279475\n",
      "Iteration 4035, loss = 0.08279139\n",
      "Iteration 4036, loss = 0.08278801\n",
      "Iteration 4037, loss = 0.08278464\n",
      "Iteration 4038, loss = 0.08278127\n",
      "Iteration 4039, loss = 0.08277790\n",
      "Iteration 4040, loss = 0.08277454\n",
      "Iteration 4041, loss = 0.08277117\n",
      "Iteration 4042, loss = 0.08276780\n",
      "Iteration 4043, loss = 0.08276444\n",
      "Iteration 4044, loss = 0.08276107\n",
      "Iteration 4045, loss = 0.08275771\n",
      "Iteration 4046, loss = 0.08275435\n",
      "Iteration 4047, loss = 0.08275098\n",
      "Iteration 4048, loss = 0.08274762\n",
      "Iteration 4049, loss = 0.08274425\n",
      "Iteration 4050, loss = 0.08274090\n",
      "Iteration 4051, loss = 0.08273753\n",
      "Iteration 4052, loss = 0.08273418\n",
      "Iteration 4053, loss = 0.08273082\n",
      "Iteration 4054, loss = 0.08272745\n",
      "Iteration 4055, loss = 0.08272410\n",
      "Iteration 4056, loss = 0.08272075\n",
      "Iteration 4057, loss = 0.08271739\n",
      "Iteration 4058, loss = 0.08271403\n",
      "Iteration 4059, loss = 0.08271067\n",
      "Iteration 4060, loss = 0.08270732\n",
      "Iteration 4061, loss = 0.08270397\n",
      "Iteration 4062, loss = 0.08270061\n",
      "Iteration 4063, loss = 0.08269726\n",
      "Iteration 4064, loss = 0.08269390\n",
      "Iteration 4065, loss = 0.08269056\n",
      "Iteration 4066, loss = 0.08268721\n",
      "Iteration 4067, loss = 0.08268386\n",
      "Iteration 4068, loss = 0.08268051\n",
      "Iteration 4069, loss = 0.08267716\n",
      "Iteration 4070, loss = 0.08267381\n",
      "Iteration 4071, loss = 0.08267047\n",
      "Iteration 4072, loss = 0.08266712\n",
      "Iteration 4073, loss = 0.08266378\n",
      "Iteration 4074, loss = 0.08266043\n",
      "Iteration 4075, loss = 0.08265708\n",
      "Iteration 4076, loss = 0.08265374\n",
      "Iteration 4077, loss = 0.08265040\n",
      "Iteration 4078, loss = 0.08264706\n",
      "Iteration 4079, loss = 0.08264372\n",
      "Iteration 4080, loss = 0.08264038\n",
      "Iteration 4081, loss = 0.08263705\n",
      "Iteration 4082, loss = 0.08263371\n",
      "Iteration 4083, loss = 0.08263038\n",
      "Iteration 4084, loss = 0.08262704\n",
      "Iteration 4085, loss = 0.08262370\n",
      "Iteration 4086, loss = 0.08262037\n",
      "Iteration 4087, loss = 0.08261703\n",
      "Iteration 4088, loss = 0.08261370\n",
      "Iteration 4089, loss = 0.08261036\n",
      "Iteration 4090, loss = 0.08260704\n",
      "Iteration 4091, loss = 0.08260371\n",
      "Iteration 4092, loss = 0.08260038\n",
      "Iteration 4093, loss = 0.08259705\n",
      "Iteration 4094, loss = 0.08259372\n",
      "Iteration 4095, loss = 0.08259039\n",
      "Iteration 4096, loss = 0.08258707\n",
      "Iteration 4097, loss = 0.08258374\n",
      "Iteration 4098, loss = 0.08258042\n",
      "Iteration 4099, loss = 0.08257708\n",
      "Iteration 4100, loss = 0.08257376\n",
      "Iteration 4101, loss = 0.08257044\n",
      "Iteration 4102, loss = 0.08256711\n",
      "Iteration 4103, loss = 0.08256379\n",
      "Iteration 4104, loss = 0.08256047\n",
      "Iteration 4105, loss = 0.08255714\n",
      "Iteration 4106, loss = 0.08255383\n",
      "Iteration 4107, loss = 0.08255051\n",
      "Iteration 4108, loss = 0.08254719\n",
      "Iteration 4109, loss = 0.08254387\n",
      "Iteration 4110, loss = 0.08254055\n",
      "Iteration 4111, loss = 0.08253724\n",
      "Iteration 4112, loss = 0.08253392\n",
      "Iteration 4113, loss = 0.08253060\n",
      "Iteration 4114, loss = 0.08252728\n",
      "Iteration 4115, loss = 0.08252397\n",
      "Iteration 4116, loss = 0.08252067\n",
      "Iteration 4117, loss = 0.08251735\n",
      "Iteration 4118, loss = 0.08251404\n",
      "Iteration 4119, loss = 0.08251072\n",
      "Iteration 4120, loss = 0.08250741\n",
      "Iteration 4121, loss = 0.08250411\n",
      "Iteration 4122, loss = 0.08250079\n",
      "Iteration 4123, loss = 0.08249748\n",
      "Iteration 4124, loss = 0.08249417\n",
      "Iteration 4125, loss = 0.08249086\n",
      "Iteration 4126, loss = 0.08248756\n",
      "Iteration 4127, loss = 0.08248425\n",
      "Iteration 4128, loss = 0.08248095\n",
      "Iteration 4129, loss = 0.08247764\n",
      "Iteration 4130, loss = 0.08247433\n",
      "Iteration 4131, loss = 0.08247104\n",
      "Iteration 4132, loss = 0.08246773\n",
      "Iteration 4133, loss = 0.08246443\n",
      "Iteration 4134, loss = 0.08246112\n",
      "Iteration 4135, loss = 0.08245782\n",
      "Iteration 4136, loss = 0.08245452\n",
      "Iteration 4137, loss = 0.08245122\n",
      "Iteration 4138, loss = 0.08244792\n",
      "Iteration 4139, loss = 0.08244461\n",
      "Iteration 4140, loss = 0.08244131\n",
      "Iteration 4141, loss = 0.08243802\n",
      "Iteration 4142, loss = 0.08243473\n",
      "Iteration 4143, loss = 0.08243143\n",
      "Iteration 4144, loss = 0.08242813\n",
      "Iteration 4145, loss = 0.08242483\n",
      "Iteration 4146, loss = 0.08242154\n",
      "Iteration 4147, loss = 0.08241825\n",
      "Iteration 4148, loss = 0.08241495\n",
      "Iteration 4149, loss = 0.08241166\n",
      "Iteration 4150, loss = 0.08240837\n",
      "Iteration 4151, loss = 0.08240508\n",
      "Iteration 4152, loss = 0.08240179\n",
      "Iteration 4153, loss = 0.08239849\n",
      "Iteration 4154, loss = 0.08239520\n",
      "Iteration 4155, loss = 0.08239191\n",
      "Iteration 4156, loss = 0.08238863\n",
      "Iteration 4157, loss = 0.08238534\n",
      "Iteration 4158, loss = 0.08238205\n",
      "Iteration 4159, loss = 0.08237876\n",
      "Iteration 4160, loss = 0.08237548\n",
      "Iteration 4161, loss = 0.08237220\n",
      "Iteration 4162, loss = 0.08236891\n",
      "Iteration 4163, loss = 0.08236563\n",
      "Iteration 4164, loss = 0.08236234\n",
      "Iteration 4165, loss = 0.08235905\n",
      "Iteration 4166, loss = 0.08235577\n",
      "Iteration 4167, loss = 0.08235249\n",
      "Iteration 4168, loss = 0.08234921\n",
      "Iteration 4169, loss = 0.08234593\n",
      "Iteration 4170, loss = 0.08234265\n",
      "Iteration 4171, loss = 0.08233938\n",
      "Iteration 4172, loss = 0.08233610\n",
      "Iteration 4173, loss = 0.08233282\n",
      "Iteration 4174, loss = 0.08232954\n",
      "Iteration 4175, loss = 0.08232627\n",
      "Iteration 4176, loss = 0.08232300\n",
      "Iteration 4177, loss = 0.08231972\n",
      "Iteration 4178, loss = 0.08231645\n",
      "Iteration 4179, loss = 0.08231317\n",
      "Iteration 4180, loss = 0.08230990\n",
      "Iteration 4181, loss = 0.08230663\n",
      "Iteration 4182, loss = 0.08230336\n",
      "Iteration 4183, loss = 0.08230009\n",
      "Iteration 4184, loss = 0.08229682\n",
      "Iteration 4185, loss = 0.08229356\n",
      "Iteration 4186, loss = 0.08229029\n",
      "Iteration 4187, loss = 0.08228702\n",
      "Iteration 4188, loss = 0.08228376\n",
      "Iteration 4189, loss = 0.08228049\n",
      "Iteration 4190, loss = 0.08227723\n",
      "Iteration 4191, loss = 0.08227396\n",
      "Iteration 4192, loss = 0.08227070\n",
      "Iteration 4193, loss = 0.08226743\n",
      "Iteration 4194, loss = 0.08226417\n",
      "Iteration 4195, loss = 0.08226092\n",
      "Iteration 4196, loss = 0.08225765\n",
      "Iteration 4197, loss = 0.08225440\n",
      "Iteration 4198, loss = 0.08225113\n",
      "Iteration 4199, loss = 0.08224787\n",
      "Iteration 4200, loss = 0.08224462\n",
      "Iteration 4201, loss = 0.08224136\n",
      "Iteration 4202, loss = 0.08223810\n",
      "Iteration 4203, loss = 0.08223484\n",
      "Iteration 4204, loss = 0.08223158\n",
      "Iteration 4205, loss = 0.08222834\n",
      "Iteration 4206, loss = 0.08222508\n",
      "Iteration 4207, loss = 0.08222182\n",
      "Iteration 4208, loss = 0.08221856\n",
      "Iteration 4209, loss = 0.08221531\n",
      "Iteration 4210, loss = 0.08221207\n",
      "Iteration 4211, loss = 0.08220882\n",
      "Iteration 4212, loss = 0.08220557\n",
      "Iteration 4213, loss = 0.08220231\n",
      "Iteration 4214, loss = 0.08219906\n",
      "Iteration 4215, loss = 0.08219583\n",
      "Iteration 4216, loss = 0.08219257\n",
      "Iteration 4217, loss = 0.08218932\n",
      "Iteration 4218, loss = 0.08218607\n",
      "Iteration 4219, loss = 0.08218282\n",
      "Iteration 4220, loss = 0.08217959\n",
      "Iteration 4221, loss = 0.08217634\n",
      "Iteration 4222, loss = 0.08217309\n",
      "Iteration 4223, loss = 0.08216985\n",
      "Iteration 4224, loss = 0.08216660\n",
      "Iteration 4225, loss = 0.08216336\n",
      "Iteration 4226, loss = 0.08216012\n",
      "Iteration 4227, loss = 0.08215688\n",
      "Iteration 4228, loss = 0.08215364\n",
      "Iteration 4229, loss = 0.08215039\n",
      "Iteration 4230, loss = 0.08214717\n",
      "Iteration 4231, loss = 0.08214392\n",
      "Iteration 4232, loss = 0.08214068\n",
      "Iteration 4233, loss = 0.08213744\n",
      "Iteration 4234, loss = 0.08213420\n",
      "Iteration 4235, loss = 0.08213097\n",
      "Iteration 4236, loss = 0.08212773\n",
      "Iteration 4237, loss = 0.08212450\n",
      "Iteration 4238, loss = 0.08212126\n",
      "Iteration 4239, loss = 0.08211802\n",
      "Iteration 4240, loss = 0.08211480\n",
      "Iteration 4241, loss = 0.08211156\n",
      "Iteration 4242, loss = 0.08210833\n",
      "Iteration 4243, loss = 0.08210509\n",
      "Iteration 4244, loss = 0.08210185\n",
      "Iteration 4245, loss = 0.08209864\n",
      "Iteration 4246, loss = 0.08209540\n",
      "Iteration 4247, loss = 0.08209217\n",
      "Iteration 4248, loss = 0.08208894\n",
      "Iteration 4249, loss = 0.08208572\n",
      "Iteration 4250, loss = 0.08208249\n",
      "Iteration 4251, loss = 0.08207926\n",
      "Iteration 4252, loss = 0.08207603\n",
      "Iteration 4253, loss = 0.08207280\n",
      "Iteration 4254, loss = 0.08206957\n",
      "Iteration 4255, loss = 0.08206637\n",
      "Iteration 4256, loss = 0.08206313\n",
      "Iteration 4257, loss = 0.08205991\n",
      "Iteration 4258, loss = 0.08205668\n",
      "Iteration 4259, loss = 0.08205347\n",
      "Iteration 4260, loss = 0.08205025\n",
      "Iteration 4261, loss = 0.08204703\n",
      "Iteration 4262, loss = 0.08204381\n",
      "Iteration 4263, loss = 0.08204059\n",
      "Iteration 4264, loss = 0.08203737\n",
      "Iteration 4265, loss = 0.08203417\n",
      "Iteration 4266, loss = 0.08203094\n",
      "Iteration 4267, loss = 0.08202772\n",
      "Iteration 4268, loss = 0.08202450\n",
      "Iteration 4269, loss = 0.08202129\n",
      "Iteration 4270, loss = 0.08201809\n",
      "Iteration 4271, loss = 0.08201487\n",
      "Iteration 4272, loss = 0.08201165\n",
      "Iteration 4273, loss = 0.08200844\n",
      "Iteration 4274, loss = 0.08200522\n",
      "Iteration 4275, loss = 0.08200204\n",
      "Iteration 4276, loss = 0.08199881\n",
      "Iteration 4277, loss = 0.08199560\n",
      "Iteration 4278, loss = 0.08199239\n",
      "Iteration 4279, loss = 0.08198919\n",
      "Iteration 4280, loss = 0.08198598\n",
      "Iteration 4281, loss = 0.08198277\n",
      "Iteration 4282, loss = 0.08197956\n",
      "Iteration 4283, loss = 0.08197635\n",
      "Iteration 4284, loss = 0.08197314\n",
      "Iteration 4285, loss = 0.08196996\n",
      "Iteration 4286, loss = 0.08196674\n",
      "Iteration 4287, loss = 0.08196354\n",
      "Iteration 4288, loss = 0.08196034\n",
      "Iteration 4289, loss = 0.08195713\n",
      "Iteration 4290, loss = 0.08195395\n",
      "Iteration 4291, loss = 0.08195075\n",
      "Iteration 4292, loss = 0.08194754\n",
      "Iteration 4293, loss = 0.08194434\n",
      "Iteration 4294, loss = 0.08194114\n",
      "Iteration 4295, loss = 0.08193796\n",
      "Iteration 4296, loss = 0.08193476\n",
      "Iteration 4297, loss = 0.08193156\n",
      "Iteration 4298, loss = 0.08192836\n",
      "Iteration 4299, loss = 0.08192516\n",
      "Iteration 4300, loss = 0.08192198\n",
      "Iteration 4301, loss = 0.08191878\n",
      "Iteration 4302, loss = 0.08191559\n",
      "Iteration 4303, loss = 0.08191239\n",
      "Iteration 4304, loss = 0.08190920\n",
      "Iteration 4305, loss = 0.08190602\n",
      "Iteration 4306, loss = 0.08190283\n",
      "Iteration 4307, loss = 0.08189964\n",
      "Iteration 4308, loss = 0.08189644\n",
      "Iteration 4309, loss = 0.08189326\n",
      "Iteration 4310, loss = 0.08189008\n",
      "Iteration 4311, loss = 0.08188689\n",
      "Iteration 4312, loss = 0.08188370\n",
      "Iteration 4313, loss = 0.08188051\n",
      "Iteration 4314, loss = 0.08187734\n",
      "Iteration 4315, loss = 0.08187415\n",
      "Iteration 4316, loss = 0.08187097\n",
      "Iteration 4317, loss = 0.08186780\n",
      "Iteration 4318, loss = 0.08186460\n",
      "Iteration 4319, loss = 0.08186144\n",
      "Iteration 4320, loss = 0.08185825\n",
      "Iteration 4321, loss = 0.08185507\n",
      "Iteration 4322, loss = 0.08185189\n",
      "Iteration 4323, loss = 0.08184871\n",
      "Iteration 4324, loss = 0.08184555\n",
      "Iteration 4325, loss = 0.08184236\n",
      "Iteration 4326, loss = 0.08183919\n",
      "Iteration 4327, loss = 0.08183601\n",
      "Iteration 4328, loss = 0.08183283\n",
      "Iteration 4329, loss = 0.08182967\n",
      "Iteration 4330, loss = 0.08182649\n",
      "Iteration 4331, loss = 0.08182332\n",
      "Iteration 4332, loss = 0.08182014\n",
      "Iteration 4333, loss = 0.08181696\n",
      "Iteration 4334, loss = 0.08181380\n",
      "Iteration 4335, loss = 0.08181063\n",
      "Iteration 4336, loss = 0.08180746\n",
      "Iteration 4337, loss = 0.08180429\n",
      "Iteration 4338, loss = 0.08180112\n",
      "Iteration 4339, loss = 0.08179797\n",
      "Iteration 4340, loss = 0.08179479\n",
      "Iteration 4341, loss = 0.08179162\n",
      "Iteration 4342, loss = 0.08178845\n",
      "Iteration 4343, loss = 0.08178529\n",
      "Iteration 4344, loss = 0.08178213\n",
      "Iteration 4345, loss = 0.08177896\n",
      "Iteration 4346, loss = 0.08177580\n",
      "Iteration 4347, loss = 0.08177263\n",
      "Iteration 4348, loss = 0.08176946\n",
      "Iteration 4349, loss = 0.08176632\n",
      "Iteration 4350, loss = 0.08176314\n",
      "Iteration 4351, loss = 0.08175998\n",
      "Iteration 4352, loss = 0.08175682\n",
      "Iteration 4353, loss = 0.08175368\n",
      "Iteration 4354, loss = 0.08175051\n",
      "Iteration 4355, loss = 0.08174736\n",
      "Iteration 4356, loss = 0.08174419\n",
      "Iteration 4357, loss = 0.08174103\n",
      "Iteration 4358, loss = 0.08173787\n",
      "Iteration 4359, loss = 0.08173473\n",
      "Iteration 4360, loss = 0.08173157\n",
      "Iteration 4361, loss = 0.08172841\n",
      "Iteration 4362, loss = 0.08172526\n",
      "Iteration 4363, loss = 0.08172209\n",
      "Iteration 4364, loss = 0.08171896\n",
      "Iteration 4365, loss = 0.08171579\n",
      "Iteration 4366, loss = 0.08171264\n",
      "Iteration 4367, loss = 0.08170948\n",
      "Iteration 4368, loss = 0.08170635\n",
      "Iteration 4369, loss = 0.08170320\n",
      "Iteration 4370, loss = 0.08170005\n",
      "Iteration 4371, loss = 0.08169690\n",
      "Iteration 4372, loss = 0.08169374\n",
      "Iteration 4373, loss = 0.08169059\n",
      "Iteration 4374, loss = 0.08168746\n",
      "Iteration 4375, loss = 0.08168430\n",
      "Iteration 4376, loss = 0.08168115\n",
      "Iteration 4377, loss = 0.08167800\n",
      "Iteration 4378, loss = 0.08167487\n",
      "Iteration 4379, loss = 0.08167174\n",
      "Iteration 4380, loss = 0.08166859\n",
      "Iteration 4381, loss = 0.08166545\n",
      "Iteration 4382, loss = 0.08166230\n",
      "Iteration 4383, loss = 0.08165915\n",
      "Iteration 4384, loss = 0.08165605\n",
      "Iteration 4385, loss = 0.08165288\n",
      "Iteration 4386, loss = 0.08164974\n",
      "Iteration 4387, loss = 0.08164662\n",
      "Iteration 4388, loss = 0.08164348\n",
      "Iteration 4389, loss = 0.08164035\n",
      "Iteration 4390, loss = 0.08163721\n",
      "Iteration 4391, loss = 0.08163407\n",
      "Iteration 4392, loss = 0.08163093\n",
      "Iteration 4393, loss = 0.08162780\n",
      "Iteration 4394, loss = 0.08162467\n",
      "Iteration 4395, loss = 0.08162154\n",
      "Iteration 4396, loss = 0.08161840\n",
      "Iteration 4397, loss = 0.08161527\n",
      "Iteration 4398, loss = 0.08161215\n",
      "Iteration 4399, loss = 0.08160901\n",
      "Iteration 4400, loss = 0.08160588\n",
      "Iteration 4401, loss = 0.08160275\n",
      "Iteration 4402, loss = 0.08159963\n",
      "Iteration 4403, loss = 0.08159650\n",
      "Iteration 4404, loss = 0.08159338\n",
      "Iteration 4405, loss = 0.08159025\n",
      "Iteration 4406, loss = 0.08158712\n",
      "Iteration 4407, loss = 0.08158399\n",
      "Iteration 4408, loss = 0.08158088\n",
      "Iteration 4409, loss = 0.08157775\n",
      "Iteration 4410, loss = 0.08157462\n",
      "Iteration 4411, loss = 0.08157150\n",
      "Iteration 4412, loss = 0.08156838\n",
      "Iteration 4413, loss = 0.08156526\n",
      "Iteration 4414, loss = 0.08156214\n",
      "Iteration 4415, loss = 0.08155901\n",
      "Iteration 4416, loss = 0.08155589\n",
      "Iteration 4417, loss = 0.08155277\n",
      "Iteration 4418, loss = 0.08154966\n",
      "Iteration 4419, loss = 0.08154655\n",
      "Iteration 4420, loss = 0.08154342\n",
      "Iteration 4421, loss = 0.08154031\n",
      "Iteration 4422, loss = 0.08153718\n",
      "Iteration 4423, loss = 0.08153409\n",
      "Iteration 4424, loss = 0.08153096\n",
      "Iteration 4425, loss = 0.08152784\n",
      "Iteration 4426, loss = 0.08152473\n",
      "Iteration 4427, loss = 0.08152162\n",
      "Iteration 4428, loss = 0.08151851\n",
      "Iteration 4429, loss = 0.08151540\n",
      "Iteration 4430, loss = 0.08151228\n",
      "Iteration 4431, loss = 0.08150917\n",
      "Iteration 4432, loss = 0.08150605\n",
      "Iteration 4433, loss = 0.08150296\n",
      "Iteration 4434, loss = 0.08149983\n",
      "Iteration 4435, loss = 0.08149672\n",
      "Iteration 4436, loss = 0.08149361\n",
      "Iteration 4437, loss = 0.08149051\n",
      "Iteration 4438, loss = 0.08148741\n",
      "Iteration 4439, loss = 0.08148430\n",
      "Iteration 4440, loss = 0.08148119\n",
      "Iteration 4441, loss = 0.08147808\n",
      "Iteration 4442, loss = 0.08147497\n",
      "Iteration 4443, loss = 0.08147188\n",
      "Iteration 4444, loss = 0.08146877\n",
      "Iteration 4445, loss = 0.08146566\n",
      "Iteration 4446, loss = 0.08146256\n",
      "Iteration 4447, loss = 0.08145947\n",
      "Iteration 4448, loss = 0.08145635\n",
      "Iteration 4449, loss = 0.08145325\n",
      "Iteration 4450, loss = 0.08145015\n",
      "Iteration 4451, loss = 0.08144704\n",
      "Iteration 4452, loss = 0.08144396\n",
      "Iteration 4453, loss = 0.08144085\n",
      "Iteration 4454, loss = 0.08143775\n",
      "Iteration 4455, loss = 0.08143465\n",
      "Iteration 4456, loss = 0.08143155\n",
      "Iteration 4457, loss = 0.08142846\n",
      "Iteration 4458, loss = 0.08142537\n",
      "Iteration 4459, loss = 0.08142226\n",
      "Iteration 4460, loss = 0.08141917\n",
      "Iteration 4461, loss = 0.08141608\n",
      "Iteration 4462, loss = 0.08141298\n",
      "Iteration 4463, loss = 0.08140988\n",
      "Iteration 4464, loss = 0.08140678\n",
      "Iteration 4465, loss = 0.08140369\n",
      "Iteration 4466, loss = 0.08140058\n",
      "Iteration 4467, loss = 0.08139750\n",
      "Iteration 4468, loss = 0.08139440\n",
      "Iteration 4469, loss = 0.08139130\n",
      "Iteration 4470, loss = 0.08138821\n",
      "Iteration 4471, loss = 0.08138511\n",
      "Iteration 4472, loss = 0.08138202\n",
      "Iteration 4473, loss = 0.08137893\n",
      "Iteration 4474, loss = 0.08137583\n",
      "Iteration 4475, loss = 0.08137274\n",
      "Iteration 4476, loss = 0.08136964\n",
      "Iteration 4477, loss = 0.08136656\n",
      "Iteration 4478, loss = 0.08136347\n",
      "Iteration 4479, loss = 0.08136038\n",
      "Iteration 4480, loss = 0.08135728\n",
      "Iteration 4481, loss = 0.08135420\n",
      "Iteration 4482, loss = 0.08135111\n",
      "Iteration 4483, loss = 0.08134803\n",
      "Iteration 4484, loss = 0.08134493\n",
      "Iteration 4485, loss = 0.08134184\n",
      "Iteration 4486, loss = 0.08133877\n",
      "Iteration 4487, loss = 0.08133568\n",
      "Iteration 4488, loss = 0.08133259\n",
      "Iteration 4489, loss = 0.08132950\n",
      "Iteration 4490, loss = 0.08132641\n",
      "Iteration 4491, loss = 0.08132334\n",
      "Iteration 4492, loss = 0.08132026\n",
      "Iteration 4493, loss = 0.08131717\n",
      "Iteration 4494, loss = 0.08131409\n",
      "Iteration 4495, loss = 0.08131100\n",
      "Iteration 4496, loss = 0.08130792\n",
      "Iteration 4497, loss = 0.08130484\n",
      "Iteration 4498, loss = 0.08130176\n",
      "Iteration 4499, loss = 0.08129868\n",
      "Iteration 4500, loss = 0.08129560\n",
      "Iteration 4501, loss = 0.08129252\n",
      "Iteration 4502, loss = 0.08128945\n",
      "Iteration 4503, loss = 0.08128637\n",
      "Iteration 4504, loss = 0.08128329\n",
      "Iteration 4505, loss = 0.08128020\n",
      "Iteration 4506, loss = 0.08127715\n",
      "Iteration 4507, loss = 0.08127406\n",
      "Iteration 4508, loss = 0.08127098\n",
      "Iteration 4509, loss = 0.08126791\n",
      "Iteration 4510, loss = 0.08126486\n",
      "Iteration 4511, loss = 0.08126177\n",
      "Iteration 4512, loss = 0.08125870\n",
      "Iteration 4513, loss = 0.08125563\n",
      "Iteration 4514, loss = 0.08125256\n",
      "Iteration 4515, loss = 0.08124950\n",
      "Iteration 4516, loss = 0.08124643\n",
      "Iteration 4517, loss = 0.08124336\n",
      "Iteration 4518, loss = 0.08124028\n",
      "Iteration 4519, loss = 0.08123721\n",
      "Iteration 4520, loss = 0.08123416\n",
      "Iteration 4521, loss = 0.08123109\n",
      "Iteration 4522, loss = 0.08122802\n",
      "Iteration 4523, loss = 0.08122495\n",
      "Iteration 4524, loss = 0.08122189\n",
      "Iteration 4525, loss = 0.08121884\n",
      "Iteration 4526, loss = 0.08121577\n",
      "Iteration 4527, loss = 0.08121271\n",
      "Iteration 4528, loss = 0.08120964\n",
      "Iteration 4529, loss = 0.08120658\n",
      "Iteration 4530, loss = 0.08120353\n",
      "Iteration 4531, loss = 0.08120047\n",
      "Iteration 4532, loss = 0.08119740\n",
      "Iteration 4533, loss = 0.08119434\n",
      "Iteration 4534, loss = 0.08119128\n",
      "Iteration 4535, loss = 0.08118823\n",
      "Iteration 4536, loss = 0.08118517\n",
      "Iteration 4537, loss = 0.08118211\n",
      "Iteration 4538, loss = 0.08117905\n",
      "Iteration 4539, loss = 0.08117599\n",
      "Iteration 4540, loss = 0.08117294\n",
      "Iteration 4541, loss = 0.08116989\n",
      "Iteration 4542, loss = 0.08116683\n",
      "Iteration 4543, loss = 0.08116378\n",
      "Iteration 4544, loss = 0.08116073\n",
      "Iteration 4545, loss = 0.08115769\n",
      "Iteration 4546, loss = 0.08115465\n",
      "Iteration 4547, loss = 0.08115160\n",
      "Iteration 4548, loss = 0.08114855\n",
      "Iteration 4549, loss = 0.08114550\n",
      "Iteration 4550, loss = 0.08114247\n",
      "Iteration 4551, loss = 0.08113942\n",
      "Iteration 4552, loss = 0.08113638\n",
      "Iteration 4553, loss = 0.08113334\n",
      "Iteration 4554, loss = 0.08113029\n",
      "Iteration 4555, loss = 0.08112726\n",
      "Iteration 4556, loss = 0.08112422\n",
      "Iteration 4557, loss = 0.08112117\n",
      "Iteration 4558, loss = 0.08111813\n",
      "Iteration 4559, loss = 0.08111510\n",
      "Iteration 4560, loss = 0.08111206\n",
      "Iteration 4561, loss = 0.08110902\n",
      "Iteration 4562, loss = 0.08110598\n",
      "Iteration 4563, loss = 0.08110294\n",
      "Iteration 4564, loss = 0.08109993\n",
      "Iteration 4565, loss = 0.08109688\n",
      "Iteration 4566, loss = 0.08109384\n",
      "Iteration 4567, loss = 0.08109080\n",
      "Iteration 4568, loss = 0.08108778\n",
      "Iteration 4569, loss = 0.08108475\n",
      "Iteration 4570, loss = 0.08108172\n",
      "Iteration 4571, loss = 0.08107869\n",
      "Iteration 4572, loss = 0.08107565\n",
      "Iteration 4573, loss = 0.08107262\n",
      "Iteration 4574, loss = 0.08106961\n",
      "Iteration 4575, loss = 0.08106656\n",
      "Iteration 4576, loss = 0.08106353\n",
      "Iteration 4577, loss = 0.08106050\n",
      "Iteration 4578, loss = 0.08105749\n",
      "Iteration 4579, loss = 0.08105445\n",
      "Iteration 4580, loss = 0.08105142\n",
      "Iteration 4581, loss = 0.08104840\n",
      "Iteration 4582, loss = 0.08104537\n",
      "Iteration 4583, loss = 0.08104235\n",
      "Iteration 4584, loss = 0.08103932\n",
      "Iteration 4585, loss = 0.08103630\n",
      "Iteration 4586, loss = 0.08103327\n",
      "Iteration 4587, loss = 0.08103024\n",
      "Iteration 4588, loss = 0.08102723\n",
      "Iteration 4589, loss = 0.08102420\n",
      "Iteration 4590, loss = 0.08102118\n",
      "Iteration 4591, loss = 0.08101816\n",
      "Iteration 4592, loss = 0.08101513\n",
      "Iteration 4593, loss = 0.08101213\n",
      "Iteration 4594, loss = 0.08100911\n",
      "Iteration 4595, loss = 0.08100608\n",
      "Iteration 4596, loss = 0.08100306\n",
      "Iteration 4597, loss = 0.08100004\n",
      "Iteration 4598, loss = 0.08099703\n",
      "Iteration 4599, loss = 0.08099402\n",
      "Iteration 4600, loss = 0.08099100\n",
      "Iteration 4601, loss = 0.08098798\n",
      "Iteration 4602, loss = 0.08098496\n",
      "Iteration 4603, loss = 0.08098196\n",
      "Iteration 4604, loss = 0.08097893\n",
      "Iteration 4605, loss = 0.08097592\n",
      "Iteration 4606, loss = 0.08097290\n",
      "Iteration 4607, loss = 0.08096990\n",
      "Iteration 4608, loss = 0.08096688\n",
      "Iteration 4609, loss = 0.08096387\n",
      "Iteration 4610, loss = 0.08096085\n",
      "Iteration 4611, loss = 0.08095784\n",
      "Iteration 4612, loss = 0.08095483\n",
      "Iteration 4613, loss = 0.08095183\n",
      "Iteration 4614, loss = 0.08094881\n",
      "Iteration 4615, loss = 0.08094580\n",
      "Iteration 4616, loss = 0.08094279\n",
      "Iteration 4617, loss = 0.08093979\n",
      "Iteration 4618, loss = 0.08093678\n",
      "Iteration 4619, loss = 0.08093377\n",
      "Iteration 4620, loss = 0.08093077\n",
      "Iteration 4621, loss = 0.08092776\n",
      "Iteration 4622, loss = 0.08092477\n",
      "Iteration 4623, loss = 0.08092175\n",
      "Iteration 4624, loss = 0.08091875\n",
      "Iteration 4625, loss = 0.08091574\n",
      "Iteration 4626, loss = 0.08091274\n",
      "Iteration 4627, loss = 0.08090974\n",
      "Iteration 4628, loss = 0.08090675\n",
      "Iteration 4629, loss = 0.08090374\n",
      "Iteration 4630, loss = 0.08090074\n",
      "Iteration 4631, loss = 0.08089773\n",
      "Iteration 4632, loss = 0.08089474\n",
      "Iteration 4633, loss = 0.08089173\n",
      "Iteration 4634, loss = 0.08088873\n",
      "Iteration 4635, loss = 0.08088573\n",
      "Iteration 4636, loss = 0.08088275\n",
      "Iteration 4637, loss = 0.08087974\n",
      "Iteration 4638, loss = 0.08087674\n",
      "Iteration 4639, loss = 0.08087374\n",
      "Iteration 4640, loss = 0.08087075\n",
      "Iteration 4641, loss = 0.08086775\n",
      "Iteration 4642, loss = 0.08086476\n",
      "Iteration 4643, loss = 0.08086176\n",
      "Iteration 4644, loss = 0.08085876\n",
      "Iteration 4645, loss = 0.08085577\n",
      "Iteration 4646, loss = 0.08085278\n",
      "Iteration 4647, loss = 0.08084979\n",
      "Iteration 4648, loss = 0.08084680\n",
      "Iteration 4649, loss = 0.08084380\n",
      "Iteration 4650, loss = 0.08084081\n",
      "Iteration 4651, loss = 0.08083783\n",
      "Iteration 4652, loss = 0.08083483\n",
      "Iteration 4653, loss = 0.08083184\n",
      "Iteration 4654, loss = 0.08082885\n",
      "Iteration 4655, loss = 0.08082587\n",
      "Iteration 4656, loss = 0.08082288\n",
      "Iteration 4657, loss = 0.08081989\n",
      "Iteration 4658, loss = 0.08081691\n",
      "Iteration 4659, loss = 0.08081391\n",
      "Iteration 4660, loss = 0.08081091\n",
      "Iteration 4661, loss = 0.08080792\n",
      "Iteration 4662, loss = 0.08080490\n",
      "Iteration 4663, loss = 0.08080190\n",
      "Iteration 4664, loss = 0.08079888\n",
      "Iteration 4665, loss = 0.08079589\n",
      "Iteration 4666, loss = 0.08079287\n",
      "Iteration 4667, loss = 0.08078986\n",
      "Iteration 4668, loss = 0.08078685\n",
      "Iteration 4669, loss = 0.08078383\n",
      "Iteration 4670, loss = 0.08078083\n",
      "Iteration 4671, loss = 0.08077781\n",
      "Iteration 4672, loss = 0.08077480\n",
      "Iteration 4673, loss = 0.08077179\n",
      "Iteration 4674, loss = 0.08076877\n",
      "Iteration 4675, loss = 0.08076578\n",
      "Iteration 4676, loss = 0.08076275\n",
      "Iteration 4677, loss = 0.08075974\n",
      "Iteration 4678, loss = 0.08075672\n",
      "Iteration 4679, loss = 0.08075371\n",
      "Iteration 4680, loss = 0.08075071\n",
      "Iteration 4681, loss = 0.08074770\n",
      "Iteration 4682, loss = 0.08074469\n",
      "Iteration 4683, loss = 0.08074168\n",
      "Iteration 4684, loss = 0.08073866\n",
      "Iteration 4685, loss = 0.08073566\n",
      "Iteration 4686, loss = 0.08073264\n",
      "Iteration 4687, loss = 0.08072963\n",
      "Iteration 4688, loss = 0.08072662\n",
      "Iteration 4689, loss = 0.08072362\n",
      "Iteration 4690, loss = 0.08072061\n",
      "Iteration 4691, loss = 0.08071760\n",
      "Iteration 4692, loss = 0.08071459\n",
      "Iteration 4693, loss = 0.08071158\n",
      "Iteration 4694, loss = 0.08070858\n",
      "Iteration 4695, loss = 0.08070557\n",
      "Iteration 4696, loss = 0.08070256\n",
      "Iteration 4697, loss = 0.08069956\n",
      "Iteration 4698, loss = 0.08069655\n",
      "Iteration 4699, loss = 0.08069355\n",
      "Iteration 4700, loss = 0.08069055\n",
      "Iteration 4701, loss = 0.08068754\n",
      "Iteration 4702, loss = 0.08068454\n",
      "Iteration 4703, loss = 0.08068153\n",
      "Iteration 4704, loss = 0.08067854\n",
      "Iteration 4705, loss = 0.08067554\n",
      "Iteration 4706, loss = 0.08067253\n",
      "Iteration 4707, loss = 0.08066953\n",
      "Iteration 4708, loss = 0.08066652\n",
      "Iteration 4709, loss = 0.08066353\n",
      "Iteration 4710, loss = 0.08066053\n",
      "Iteration 4711, loss = 0.08065753\n",
      "Iteration 4712, loss = 0.08065452\n",
      "Iteration 4713, loss = 0.08065154\n",
      "Iteration 4714, loss = 0.08064854\n",
      "Iteration 4715, loss = 0.08064554\n",
      "Iteration 4716, loss = 0.08064254\n",
      "Iteration 4717, loss = 0.08063954\n",
      "Iteration 4718, loss = 0.08063655\n",
      "Iteration 4719, loss = 0.08063356\n",
      "Iteration 4720, loss = 0.08063057\n",
      "Iteration 4721, loss = 0.08062757\n",
      "Iteration 4722, loss = 0.08062457\n",
      "Iteration 4723, loss = 0.08062159\n",
      "Iteration 4724, loss = 0.08061859\n",
      "Iteration 4725, loss = 0.08061560\n",
      "Iteration 4726, loss = 0.08061260\n",
      "Iteration 4727, loss = 0.08060961\n",
      "Iteration 4728, loss = 0.08060663\n",
      "Iteration 4729, loss = 0.08060364\n",
      "Iteration 4730, loss = 0.08060065\n",
      "Iteration 4731, loss = 0.08059765\n",
      "Iteration 4732, loss = 0.08059466\n",
      "Iteration 4733, loss = 0.08059169\n",
      "Iteration 4734, loss = 0.08058869\n",
      "Iteration 4735, loss = 0.08058570\n",
      "Iteration 4736, loss = 0.08058271\n",
      "Iteration 4737, loss = 0.08057973\n",
      "Iteration 4738, loss = 0.08057674\n",
      "Iteration 4739, loss = 0.08057376\n",
      "Iteration 4740, loss = 0.08057077\n",
      "Iteration 4741, loss = 0.08056778\n",
      "Iteration 4742, loss = 0.08056481\n",
      "Iteration 4743, loss = 0.08056182\n",
      "Iteration 4744, loss = 0.08055883\n",
      "Iteration 4745, loss = 0.08055585\n",
      "Iteration 4746, loss = 0.08055286\n",
      "Iteration 4747, loss = 0.08054990\n",
      "Iteration 4748, loss = 0.08054691\n",
      "Iteration 4749, loss = 0.08054393\n",
      "Iteration 4750, loss = 0.08054094\n",
      "Iteration 4751, loss = 0.08053797\n",
      "Iteration 4752, loss = 0.08053500\n",
      "Iteration 4753, loss = 0.08053202\n",
      "Iteration 4754, loss = 0.08052904\n",
      "Iteration 4755, loss = 0.08052606\n",
      "Iteration 4756, loss = 0.08052307\n",
      "Iteration 4757, loss = 0.08052012\n",
      "Iteration 4758, loss = 0.08051713\n",
      "Iteration 4759, loss = 0.08051415\n",
      "Iteration 4760, loss = 0.08051117\n",
      "Iteration 4761, loss = 0.08050821\n",
      "Iteration 4762, loss = 0.08050524\n",
      "Iteration 4763, loss = 0.08050226\n",
      "Iteration 4764, loss = 0.08049928\n",
      "Iteration 4765, loss = 0.08049631\n",
      "Iteration 4766, loss = 0.08049333\n",
      "Iteration 4767, loss = 0.08049038\n",
      "Iteration 4768, loss = 0.08048739\n",
      "Iteration 4769, loss = 0.08048442\n",
      "Iteration 4770, loss = 0.08048148\n",
      "Iteration 4771, loss = 0.08047852\n",
      "Iteration 4772, loss = 0.08047556\n",
      "Iteration 4773, loss = 0.08047261\n",
      "Iteration 4774, loss = 0.08046965\n",
      "Iteration 4775, loss = 0.08046670\n",
      "Iteration 4776, loss = 0.08046377\n",
      "Iteration 4777, loss = 0.08046080\n",
      "Iteration 4778, loss = 0.08045785\n",
      "Iteration 4779, loss = 0.08045490\n",
      "Iteration 4780, loss = 0.08045197\n",
      "Iteration 4781, loss = 0.08044901\n",
      "Iteration 4782, loss = 0.08044606\n",
      "Iteration 4783, loss = 0.08044312\n",
      "Iteration 4784, loss = 0.08044017\n",
      "Iteration 4785, loss = 0.08043724\n",
      "Iteration 4786, loss = 0.08043429\n",
      "Iteration 4787, loss = 0.08043135\n",
      "Iteration 4788, loss = 0.08042840\n",
      "Iteration 4789, loss = 0.08042546\n",
      "Iteration 4790, loss = 0.08042254\n",
      "Iteration 4791, loss = 0.08041958\n",
      "Iteration 4792, loss = 0.08041664\n",
      "Iteration 4793, loss = 0.08041370\n",
      "Iteration 4794, loss = 0.08041077\n",
      "Iteration 4795, loss = 0.08040783\n",
      "Iteration 4796, loss = 0.08040491\n",
      "Iteration 4797, loss = 0.08040196\n",
      "Iteration 4798, loss = 0.08039902\n",
      "Iteration 4799, loss = 0.08039608\n",
      "Iteration 4800, loss = 0.08039316\n",
      "Iteration 4801, loss = 0.08039023\n",
      "Iteration 4802, loss = 0.08038729\n",
      "Iteration 4803, loss = 0.08038435\n",
      "Iteration 4804, loss = 0.08038142\n",
      "Iteration 4805, loss = 0.08037849\n",
      "Iteration 4806, loss = 0.08037556\n",
      "Iteration 4807, loss = 0.08037263\n",
      "Iteration 4808, loss = 0.08036969\n",
      "Iteration 4809, loss = 0.08036677\n",
      "Iteration 4810, loss = 0.08036383\n",
      "Iteration 4811, loss = 0.08036091\n",
      "Iteration 4812, loss = 0.08035797\n",
      "Iteration 4813, loss = 0.08035504\n",
      "Iteration 4814, loss = 0.08035213\n",
      "Iteration 4815, loss = 0.08034920\n",
      "Iteration 4816, loss = 0.08034627\n",
      "Iteration 4817, loss = 0.08034334\n",
      "Iteration 4818, loss = 0.08034041\n",
      "Iteration 4819, loss = 0.08033750\n",
      "Iteration 4820, loss = 0.08033456\n",
      "Iteration 4821, loss = 0.08033164\n",
      "Iteration 4822, loss = 0.08032871\n",
      "Iteration 4823, loss = 0.08032580\n",
      "Iteration 4824, loss = 0.08032288\n",
      "Iteration 4825, loss = 0.08031996\n",
      "Iteration 4826, loss = 0.08031703\n",
      "Iteration 4827, loss = 0.08031410\n",
      "Iteration 4828, loss = 0.08031118\n",
      "Iteration 4829, loss = 0.08030827\n",
      "Iteration 4830, loss = 0.08030535\n",
      "Iteration 4831, loss = 0.08030244\n",
      "Iteration 4832, loss = 0.08029951\n",
      "Iteration 4833, loss = 0.08029661\n",
      "Iteration 4834, loss = 0.08029368\n",
      "Iteration 4835, loss = 0.08029076\n",
      "Iteration 4836, loss = 0.08028785\n",
      "Iteration 4837, loss = 0.08028494\n",
      "Iteration 4838, loss = 0.08028202\n",
      "Iteration 4839, loss = 0.08027911\n",
      "Iteration 4840, loss = 0.08027619\n",
      "Iteration 4841, loss = 0.08027328\n",
      "Iteration 4842, loss = 0.08027037\n",
      "Iteration 4843, loss = 0.08026746\n",
      "Iteration 4844, loss = 0.08026456\n",
      "Iteration 4845, loss = 0.08026164\n",
      "Iteration 4846, loss = 0.08025873\n",
      "Iteration 4847, loss = 0.08025584\n",
      "Iteration 4848, loss = 0.08025292\n",
      "Iteration 4849, loss = 0.08025002\n",
      "Iteration 4850, loss = 0.08024711\n",
      "Iteration 4851, loss = 0.08024421\n",
      "Iteration 4852, loss = 0.08024131\n",
      "Iteration 4853, loss = 0.08023841\n",
      "Iteration 4854, loss = 0.08023550\n",
      "Iteration 4855, loss = 0.08023260\n",
      "Iteration 4856, loss = 0.08022969\n",
      "Iteration 4857, loss = 0.08022682\n",
      "Iteration 4858, loss = 0.08022389\n",
      "Iteration 4859, loss = 0.08022099\n",
      "Iteration 4860, loss = 0.08021809\n",
      "Iteration 4861, loss = 0.08021520\n",
      "Iteration 4862, loss = 0.08021230\n",
      "Iteration 4863, loss = 0.08020941\n",
      "Iteration 4864, loss = 0.08020650\n",
      "Iteration 4865, loss = 0.08020360\n",
      "Iteration 4866, loss = 0.08020071\n",
      "Iteration 4867, loss = 0.08019781\n",
      "Iteration 4868, loss = 0.08019491\n",
      "Iteration 4869, loss = 0.08019201\n",
      "Iteration 4870, loss = 0.08018912\n",
      "Iteration 4871, loss = 0.08018623\n",
      "Iteration 4872, loss = 0.08018334\n",
      "Iteration 4873, loss = 0.08018044\n",
      "Iteration 4874, loss = 0.08017755\n",
      "Iteration 4875, loss = 0.08017465\n",
      "Iteration 4876, loss = 0.08017178\n",
      "Iteration 4877, loss = 0.08016887\n",
      "Iteration 4878, loss = 0.08016597\n",
      "Iteration 4879, loss = 0.08016309\n",
      "Iteration 4880, loss = 0.08016020\n",
      "Iteration 4881, loss = 0.08015731\n",
      "Iteration 4882, loss = 0.08015442\n",
      "Iteration 4883, loss = 0.08015154\n",
      "Iteration 4884, loss = 0.08014864\n",
      "Iteration 4885, loss = 0.08014576\n",
      "Iteration 4886, loss = 0.08014287\n",
      "Iteration 4887, loss = 0.08013998\n",
      "Iteration 4888, loss = 0.08013709\n",
      "Iteration 4889, loss = 0.08013421\n",
      "Iteration 4890, loss = 0.08013133\n",
      "Iteration 4891, loss = 0.08012845\n",
      "Iteration 4892, loss = 0.08012556\n",
      "Iteration 4893, loss = 0.08012267\n",
      "Iteration 4894, loss = 0.08011979\n",
      "Iteration 4895, loss = 0.08011692\n",
      "Iteration 4896, loss = 0.08011403\n",
      "Iteration 4897, loss = 0.08011115\n",
      "Iteration 4898, loss = 0.08010826\n",
      "Iteration 4899, loss = 0.08010539\n",
      "Iteration 4900, loss = 0.08010251\n",
      "Iteration 4901, loss = 0.08009962\n",
      "Iteration 4902, loss = 0.08009674\n",
      "Iteration 4903, loss = 0.08009386\n",
      "Iteration 4904, loss = 0.08009099\n",
      "Iteration 4905, loss = 0.08008811\n",
      "Iteration 4906, loss = 0.08008524\n",
      "Iteration 4907, loss = 0.08008235\n",
      "Iteration 4908, loss = 0.08007947\n",
      "Iteration 4909, loss = 0.08007662\n",
      "Iteration 4910, loss = 0.08007372\n",
      "Iteration 4911, loss = 0.08007084\n",
      "Iteration 4912, loss = 0.08006797\n",
      "Iteration 4913, loss = 0.08006511\n",
      "Iteration 4914, loss = 0.08006223\n",
      "Iteration 4915, loss = 0.08005936\n",
      "Iteration 4916, loss = 0.08005648\n",
      "Iteration 4917, loss = 0.08005360\n",
      "Iteration 4918, loss = 0.08005074\n",
      "Iteration 4919, loss = 0.08004786\n",
      "Iteration 4920, loss = 0.08004499\n",
      "Iteration 4921, loss = 0.08004212\n",
      "Iteration 4922, loss = 0.08003925\n",
      "Iteration 4923, loss = 0.08003638\n",
      "Iteration 4924, loss = 0.08003351\n",
      "Iteration 4925, loss = 0.08003064\n",
      "Iteration 4926, loss = 0.08002777\n",
      "Iteration 4927, loss = 0.08002490\n",
      "Iteration 4928, loss = 0.08002204\n",
      "Iteration 4929, loss = 0.08001917\n",
      "Iteration 4930, loss = 0.08001630\n",
      "Iteration 4931, loss = 0.08001343\n",
      "Iteration 4932, loss = 0.08001057\n",
      "Iteration 4933, loss = 0.08000770\n",
      "Iteration 4934, loss = 0.08000484\n",
      "Iteration 4935, loss = 0.08000197\n",
      "Iteration 4936, loss = 0.07999911\n",
      "Iteration 4937, loss = 0.07999625\n",
      "Iteration 4938, loss = 0.07999338\n",
      "Iteration 4939, loss = 0.07999052\n",
      "Iteration 4940, loss = 0.07998765\n",
      "Iteration 4941, loss = 0.07998479\n",
      "Iteration 4942, loss = 0.07998193\n",
      "Iteration 4943, loss = 0.07997907\n",
      "Iteration 4944, loss = 0.07997621\n",
      "Iteration 4945, loss = 0.07997334\n",
      "Iteration 4946, loss = 0.07997049\n",
      "Iteration 4947, loss = 0.07996763\n",
      "Iteration 4948, loss = 0.07996477\n",
      "Iteration 4949, loss = 0.07996191\n",
      "Iteration 4950, loss = 0.07995904\n",
      "Iteration 4951, loss = 0.07995621\n",
      "Iteration 4952, loss = 0.07995333\n",
      "Iteration 4953, loss = 0.07995048\n",
      "Iteration 4954, loss = 0.07994762\n",
      "Iteration 4955, loss = 0.07994477\n",
      "Iteration 4956, loss = 0.07994191\n",
      "Iteration 4957, loss = 0.07993906\n",
      "Iteration 4958, loss = 0.07993620\n",
      "Iteration 4959, loss = 0.07993334\n",
      "Iteration 4960, loss = 0.07993050\n",
      "Iteration 4961, loss = 0.07992764\n",
      "Iteration 4962, loss = 0.07992479\n",
      "Iteration 4963, loss = 0.07992193\n",
      "Iteration 4964, loss = 0.07991908\n",
      "Iteration 4965, loss = 0.07991625\n",
      "Iteration 4966, loss = 0.07991338\n",
      "Iteration 4967, loss = 0.07991053\n",
      "Iteration 4968, loss = 0.07990776\n",
      "Iteration 4969, loss = 0.07990495\n",
      "Iteration 4970, loss = 0.07990208\n",
      "Iteration 4971, loss = 0.07989925\n",
      "Iteration 4972, loss = 0.07989643\n",
      "Iteration 4973, loss = 0.07989361\n",
      "Iteration 4974, loss = 0.07989079\n",
      "Iteration 4975, loss = 0.07988797\n",
      "Iteration 4976, loss = 0.07988514\n",
      "Iteration 4977, loss = 0.07988232\n",
      "Iteration 4978, loss = 0.07987951\n",
      "Iteration 4979, loss = 0.07987670\n",
      "Iteration 4980, loss = 0.07987387\n",
      "Iteration 4981, loss = 0.07987104\n",
      "Iteration 4982, loss = 0.07986824\n",
      "Iteration 4983, loss = 0.07986543\n",
      "Iteration 4984, loss = 0.07986261\n",
      "Iteration 4985, loss = 0.07985979\n",
      "Iteration 4986, loss = 0.07985697\n",
      "Iteration 4987, loss = 0.07985417\n",
      "Iteration 4988, loss = 0.07985136\n",
      "Iteration 4989, loss = 0.07984854\n",
      "Iteration 4990, loss = 0.07984572\n",
      "Iteration 4991, loss = 0.07984291\n",
      "Iteration 4992, loss = 0.07984009\n",
      "Iteration 4993, loss = 0.07983730\n",
      "Iteration 4994, loss = 0.07983448\n",
      "Iteration 4995, loss = 0.07983167\n",
      "Iteration 4996, loss = 0.07982885\n",
      "Iteration 4997, loss = 0.07982604\n",
      "Iteration 4998, loss = 0.07982327\n",
      "Iteration 4999, loss = 0.07982043\n",
      "Iteration 5000, loss = 0.07981762\n",
      "Iteration 5001, loss = 0.07981481\n",
      "Iteration 5002, loss = 0.07981201\n",
      "Iteration 5003, loss = 0.07980921\n",
      "Iteration 5004, loss = 0.07980641\n",
      "Iteration 5005, loss = 0.07980359\n",
      "Iteration 5006, loss = 0.07980078\n",
      "Iteration 5007, loss = 0.07979798\n",
      "Iteration 5008, loss = 0.07979519\n",
      "Iteration 5009, loss = 0.07979237\n",
      "Iteration 5010, loss = 0.07978957\n",
      "Iteration 5011, loss = 0.07978676\n",
      "Iteration 5012, loss = 0.07978396\n",
      "Iteration 5013, loss = 0.07978118\n",
      "Iteration 5014, loss = 0.07977838\n",
      "Iteration 5015, loss = 0.07977557\n",
      "Iteration 5016, loss = 0.07977276\n",
      "Iteration 5017, loss = 0.07976996\n",
      "Iteration 5018, loss = 0.07976715\n",
      "Iteration 5019, loss = 0.07976437\n",
      "Iteration 5020, loss = 0.07976156\n",
      "Iteration 5021, loss = 0.07975877\n",
      "Iteration 5022, loss = 0.07975596\n",
      "Iteration 5023, loss = 0.07975317\n",
      "Iteration 5024, loss = 0.07975037\n",
      "Iteration 5025, loss = 0.07974758\n",
      "Iteration 5026, loss = 0.07974478\n",
      "Iteration 5027, loss = 0.07974198\n",
      "Iteration 5028, loss = 0.07973919\n",
      "Iteration 5029, loss = 0.07973639\n",
      "Iteration 5030, loss = 0.07973360\n",
      "Iteration 5031, loss = 0.07973081\n",
      "Iteration 5032, loss = 0.07972801\n",
      "Iteration 5033, loss = 0.07972522\n",
      "Iteration 5034, loss = 0.07972243\n",
      "Iteration 5035, loss = 0.07971965\n",
      "Iteration 5036, loss = 0.07971685\n",
      "Iteration 5037, loss = 0.07971405\n",
      "Iteration 5038, loss = 0.07971125\n",
      "Iteration 5039, loss = 0.07970850\n",
      "Iteration 5040, loss = 0.07970569\n",
      "Iteration 5041, loss = 0.07970290\n",
      "Iteration 5042, loss = 0.07970011\n",
      "Iteration 5043, loss = 0.07969734\n",
      "Iteration 5044, loss = 0.07969454\n",
      "Iteration 5045, loss = 0.07969176\n",
      "Iteration 5046, loss = 0.07968897\n",
      "Iteration 5047, loss = 0.07968618\n",
      "Iteration 5048, loss = 0.07968339\n",
      "Iteration 5049, loss = 0.07968062\n",
      "Iteration 5050, loss = 0.07967783\n",
      "Iteration 5051, loss = 0.07967505\n",
      "Iteration 5052, loss = 0.07967226\n",
      "Iteration 5053, loss = 0.07966948\n",
      "Iteration 5054, loss = 0.07966674\n",
      "Iteration 5055, loss = 0.07966396\n",
      "Iteration 5056, loss = 0.07966118\n",
      "Iteration 5057, loss = 0.07965841\n",
      "Iteration 5058, loss = 0.07965565\n",
      "Iteration 5059, loss = 0.07965289\n",
      "Iteration 5060, loss = 0.07965012\n",
      "Iteration 5061, loss = 0.07964735\n",
      "Iteration 5062, loss = 0.07964457\n",
      "Iteration 5063, loss = 0.07964181\n",
      "Iteration 5064, loss = 0.07963905\n",
      "Iteration 5065, loss = 0.07963627\n",
      "Iteration 5066, loss = 0.07963350\n",
      "Iteration 5067, loss = 0.07963073\n",
      "Iteration 5068, loss = 0.07962796\n",
      "Iteration 5069, loss = 0.07962521\n",
      "Iteration 5070, loss = 0.07962244\n",
      "Iteration 5071, loss = 0.07961967\n",
      "Iteration 5072, loss = 0.07961691\n",
      "Iteration 5073, loss = 0.07961414\n",
      "Iteration 5074, loss = 0.07961139\n",
      "Iteration 5075, loss = 0.07960862\n",
      "Iteration 5076, loss = 0.07960585\n",
      "Iteration 5077, loss = 0.07960310\n",
      "Iteration 5078, loss = 0.07960033\n",
      "Iteration 5079, loss = 0.07959761\n",
      "Iteration 5080, loss = 0.07959481\n",
      "Iteration 5081, loss = 0.07959205\n",
      "Iteration 5082, loss = 0.07958931\n",
      "Iteration 5083, loss = 0.07958653\n",
      "Iteration 5084, loss = 0.07958378\n",
      "Iteration 5085, loss = 0.07958103\n",
      "Iteration 5086, loss = 0.07957828\n",
      "Iteration 5087, loss = 0.07957552\n",
      "Iteration 5088, loss = 0.07957275\n",
      "Iteration 5089, loss = 0.07957001\n",
      "Iteration 5090, loss = 0.07956724\n",
      "Iteration 5091, loss = 0.07956450\n",
      "Iteration 5092, loss = 0.07956173\n",
      "Iteration 5093, loss = 0.07955897\n",
      "Iteration 5094, loss = 0.07955625\n",
      "Iteration 5095, loss = 0.07955349\n",
      "Iteration 5096, loss = 0.07955072\n",
      "Iteration 5097, loss = 0.07954797\n",
      "Iteration 5098, loss = 0.07954521\n",
      "Iteration 5099, loss = 0.07954247\n",
      "Iteration 5100, loss = 0.07953973\n",
      "Iteration 5101, loss = 0.07953698\n",
      "Iteration 5102, loss = 0.07953422\n",
      "Iteration 5103, loss = 0.07953147\n",
      "Iteration 5104, loss = 0.07952872\n",
      "Iteration 5105, loss = 0.07952599\n",
      "Iteration 5106, loss = 0.07952322\n",
      "Iteration 5107, loss = 0.07952047\n",
      "Iteration 5108, loss = 0.07951772\n",
      "Iteration 5109, loss = 0.07951500\n",
      "Iteration 5110, loss = 0.07951226\n",
      "Iteration 5111, loss = 0.07950950\n",
      "Iteration 5112, loss = 0.07950676\n",
      "Iteration 5113, loss = 0.07950400\n",
      "Iteration 5114, loss = 0.07950127\n",
      "Iteration 5115, loss = 0.07949852\n",
      "Iteration 5116, loss = 0.07949577\n",
      "Iteration 5117, loss = 0.07949302\n",
      "Iteration 5118, loss = 0.07949028\n",
      "Iteration 5119, loss = 0.07948755\n",
      "Iteration 5120, loss = 0.07948481\n",
      "Iteration 5121, loss = 0.07948207\n",
      "Iteration 5122, loss = 0.07947932\n",
      "Iteration 5123, loss = 0.07947659\n",
      "Iteration 5124, loss = 0.07947385\n",
      "Iteration 5125, loss = 0.07947110\n",
      "Iteration 5126, loss = 0.07946836\n",
      "Iteration 5127, loss = 0.07946562\n",
      "Iteration 5128, loss = 0.07946289\n",
      "Iteration 5129, loss = 0.07946016\n",
      "Iteration 5130, loss = 0.07945742\n",
      "Iteration 5131, loss = 0.07945468\n",
      "Iteration 5132, loss = 0.07945194\n",
      "Iteration 5133, loss = 0.07944920\n",
      "Iteration 5134, loss = 0.07944649\n",
      "Iteration 5135, loss = 0.07944373\n",
      "Iteration 5136, loss = 0.07944099\n",
      "Iteration 5137, loss = 0.07943827\n",
      "Iteration 5138, loss = 0.07943554\n",
      "Iteration 5139, loss = 0.07943280\n",
      "Iteration 5140, loss = 0.07943006\n",
      "Iteration 5141, loss = 0.07942734\n",
      "Iteration 5142, loss = 0.07942460\n",
      "Iteration 5143, loss = 0.07942187\n",
      "Iteration 5144, loss = 0.07941915\n",
      "Iteration 5145, loss = 0.07941641\n",
      "Iteration 5146, loss = 0.07941370\n",
      "Iteration 5147, loss = 0.07941095\n",
      "Iteration 5148, loss = 0.07940822\n",
      "Iteration 5149, loss = 0.07940550\n",
      "Iteration 5150, loss = 0.07940278\n",
      "Iteration 5151, loss = 0.07940005\n",
      "Iteration 5152, loss = 0.07939732\n",
      "Iteration 5153, loss = 0.07939458\n",
      "Iteration 5154, loss = 0.07939187\n",
      "Iteration 5155, loss = 0.07938915\n",
      "Iteration 5156, loss = 0.07938642\n",
      "Iteration 5157, loss = 0.07938369\n",
      "Iteration 5158, loss = 0.07938097\n",
      "Iteration 5159, loss = 0.07937825\n",
      "Iteration 5160, loss = 0.07937555\n",
      "Iteration 5161, loss = 0.07937280\n",
      "Iteration 5162, loss = 0.07937007\n",
      "Iteration 5163, loss = 0.07936736\n",
      "Iteration 5164, loss = 0.07936466\n",
      "Iteration 5165, loss = 0.07936192\n",
      "Iteration 5166, loss = 0.07935920\n",
      "Iteration 5167, loss = 0.07935647\n",
      "Iteration 5168, loss = 0.07935378\n",
      "Iteration 5169, loss = 0.07935106\n",
      "Iteration 5170, loss = 0.07934833\n",
      "Iteration 5171, loss = 0.07934561\n",
      "Iteration 5172, loss = 0.07934288\n",
      "Iteration 5173, loss = 0.07934019\n",
      "Iteration 5174, loss = 0.07933747\n",
      "Iteration 5175, loss = 0.07933476\n",
      "Iteration 5176, loss = 0.07933204\n",
      "Iteration 5177, loss = 0.07932931\n",
      "Iteration 5178, loss = 0.07932662\n",
      "Iteration 5179, loss = 0.07932390\n",
      "Iteration 5180, loss = 0.07932118\n",
      "Iteration 5181, loss = 0.07931846\n",
      "Iteration 5182, loss = 0.07931574\n",
      "Iteration 5183, loss = 0.07931306\n",
      "Iteration 5184, loss = 0.07931034\n",
      "Iteration 5185, loss = 0.07930762\n",
      "Iteration 5186, loss = 0.07930490\n",
      "Iteration 5187, loss = 0.07930219\n",
      "Iteration 5188, loss = 0.07929952\n",
      "Iteration 5189, loss = 0.07929678\n",
      "Iteration 5190, loss = 0.07929406\n",
      "Iteration 5191, loss = 0.07929134\n",
      "Iteration 5192, loss = 0.07928865\n",
      "Iteration 5193, loss = 0.07928595\n",
      "Iteration 5194, loss = 0.07928324\n",
      "Iteration 5195, loss = 0.07928053\n",
      "Iteration 5196, loss = 0.07927781\n",
      "Iteration 5197, loss = 0.07927511\n",
      "Iteration 5198, loss = 0.07927243\n",
      "Iteration 5199, loss = 0.07926970\n",
      "Iteration 5200, loss = 0.07926698\n",
      "Iteration 5201, loss = 0.07926429\n",
      "Iteration 5202, loss = 0.07926159\n",
      "Iteration 5203, loss = 0.07925889\n",
      "Iteration 5204, loss = 0.07925618\n",
      "Iteration 5205, loss = 0.07925347\n",
      "Iteration 5206, loss = 0.07925077\n",
      "Iteration 5207, loss = 0.07924808\n",
      "Iteration 5208, loss = 0.07924537\n",
      "Iteration 5209, loss = 0.07924266\n",
      "Iteration 5210, loss = 0.07923996\n",
      "Iteration 5211, loss = 0.07923725\n",
      "Iteration 5212, loss = 0.07923459\n",
      "Iteration 5213, loss = 0.07923186\n",
      "Iteration 5214, loss = 0.07922915\n",
      "Iteration 5215, loss = 0.07922646\n",
      "Iteration 5216, loss = 0.07922377\n",
      "Iteration 5217, loss = 0.07922106\n",
      "Iteration 5218, loss = 0.07921836\n",
      "Iteration 5219, loss = 0.07921568\n",
      "Iteration 5220, loss = 0.07921297\n",
      "Iteration 5221, loss = 0.07921028\n",
      "Iteration 5222, loss = 0.07920759\n",
      "Iteration 5223, loss = 0.07920489\n",
      "Iteration 5224, loss = 0.07920222\n",
      "Iteration 5225, loss = 0.07919950\n",
      "Iteration 5226, loss = 0.07919682\n",
      "Iteration 5227, loss = 0.07919413\n",
      "Iteration 5228, loss = 0.07919145\n",
      "Iteration 5229, loss = 0.07918875\n",
      "Iteration 5230, loss = 0.07918606\n",
      "Iteration 5231, loss = 0.07918338\n",
      "Iteration 5232, loss = 0.07918068\n",
      "Iteration 5233, loss = 0.07917801\n",
      "Iteration 5234, loss = 0.07917531\n",
      "Iteration 5235, loss = 0.07917261\n",
      "Iteration 5236, loss = 0.07916996\n",
      "Iteration 5237, loss = 0.07916725\n",
      "Iteration 5238, loss = 0.07916457\n",
      "Iteration 5239, loss = 0.07916188\n",
      "Iteration 5240, loss = 0.07915921\n",
      "Iteration 5241, loss = 0.07915652\n",
      "Iteration 5242, loss = 0.07915384\n",
      "Iteration 5243, loss = 0.07915116\n",
      "Iteration 5244, loss = 0.07914847\n",
      "Iteration 5245, loss = 0.07914578\n",
      "Iteration 5246, loss = 0.07914311\n",
      "Iteration 5247, loss = 0.07914046\n",
      "Iteration 5248, loss = 0.07913775\n",
      "Iteration 5249, loss = 0.07913507\n",
      "Iteration 5250, loss = 0.07913238\n",
      "Iteration 5251, loss = 0.07912973\n",
      "Iteration 5252, loss = 0.07912705\n",
      "Iteration 5253, loss = 0.07912437\n",
      "Iteration 5254, loss = 0.07912168\n",
      "Iteration 5255, loss = 0.07911901\n",
      "Iteration 5256, loss = 0.07911633\n",
      "Iteration 5257, loss = 0.07911365\n",
      "Iteration 5258, loss = 0.07911098\n",
      "Iteration 5259, loss = 0.07910829\n",
      "Iteration 5260, loss = 0.07910565\n",
      "Iteration 5261, loss = 0.07910296\n",
      "Iteration 5262, loss = 0.07910028\n",
      "Iteration 5263, loss = 0.07909760\n",
      "Iteration 5264, loss = 0.07909494\n",
      "Iteration 5265, loss = 0.07909227\n",
      "Iteration 5266, loss = 0.07908959\n",
      "Iteration 5267, loss = 0.07908692\n",
      "Iteration 5268, loss = 0.07908424\n",
      "Iteration 5269, loss = 0.07908158\n",
      "Iteration 5270, loss = 0.07907892\n",
      "Iteration 5271, loss = 0.07907625\n",
      "Iteration 5272, loss = 0.07907357\n",
      "Iteration 5273, loss = 0.07907089\n",
      "Iteration 5274, loss = 0.07906824\n",
      "Iteration 5275, loss = 0.07906557\n",
      "Iteration 5276, loss = 0.07906290\n",
      "Iteration 5277, loss = 0.07906022\n",
      "Iteration 5278, loss = 0.07905755\n",
      "Iteration 5279, loss = 0.07905492\n",
      "Iteration 5280, loss = 0.07905223\n",
      "Iteration 5281, loss = 0.07904956\n",
      "Iteration 5282, loss = 0.07904689\n",
      "Iteration 5283, loss = 0.07904424\n",
      "Iteration 5284, loss = 0.07904158\n",
      "Iteration 5285, loss = 0.07903891\n",
      "Iteration 5286, loss = 0.07903624\n",
      "Iteration 5287, loss = 0.07903357\n",
      "Iteration 5288, loss = 0.07903091\n",
      "Iteration 5289, loss = 0.07902826\n",
      "Iteration 5290, loss = 0.07902560\n",
      "Iteration 5291, loss = 0.07902293\n",
      "Iteration 5292, loss = 0.07902026\n",
      "Iteration 5293, loss = 0.07901761\n",
      "Iteration 5294, loss = 0.07901495\n",
      "Iteration 5295, loss = 0.07901229\n",
      "Iteration 5296, loss = 0.07900962\n",
      "Iteration 5297, loss = 0.07900697\n",
      "Iteration 5298, loss = 0.07900432\n",
      "Iteration 5299, loss = 0.07900165\n",
      "Iteration 5300, loss = 0.07899899\n",
      "Iteration 5301, loss = 0.07899634\n",
      "Iteration 5302, loss = 0.07899368\n",
      "Iteration 5303, loss = 0.07899103\n",
      "Iteration 5304, loss = 0.07898837\n",
      "Iteration 5305, loss = 0.07898571\n",
      "Iteration 5306, loss = 0.07898306\n",
      "Iteration 5307, loss = 0.07898040\n",
      "Iteration 5308, loss = 0.07897776\n",
      "Iteration 5309, loss = 0.07897509\n",
      "Iteration 5310, loss = 0.07897243\n",
      "Iteration 5311, loss = 0.07896979\n",
      "Iteration 5312, loss = 0.07896714\n",
      "Iteration 5313, loss = 0.07896449\n",
      "Iteration 5314, loss = 0.07896183\n",
      "Iteration 5315, loss = 0.07895918\n",
      "Iteration 5316, loss = 0.07895652\n",
      "Iteration 5317, loss = 0.07895390\n",
      "Iteration 5318, loss = 0.07895121\n",
      "Iteration 5319, loss = 0.07894858\n",
      "Iteration 5320, loss = 0.07894592\n",
      "Iteration 5321, loss = 0.07894329\n",
      "Iteration 5322, loss = 0.07894063\n",
      "Iteration 5323, loss = 0.07893797\n",
      "Iteration 5324, loss = 0.07893535\n",
      "Iteration 5325, loss = 0.07893268\n",
      "Iteration 5326, loss = 0.07893004\n",
      "Iteration 5327, loss = 0.07892739\n",
      "Iteration 5328, loss = 0.07892475\n",
      "Iteration 5329, loss = 0.07892210\n",
      "Iteration 5330, loss = 0.07891944\n",
      "Iteration 5331, loss = 0.07891682\n",
      "Iteration 5332, loss = 0.07891416\n",
      "Iteration 5333, loss = 0.07891153\n",
      "Iteration 5334, loss = 0.07890887\n",
      "Iteration 5335, loss = 0.07890624\n",
      "Iteration 5336, loss = 0.07890359\n",
      "Iteration 5337, loss = 0.07890096\n",
      "Iteration 5338, loss = 0.07889831\n",
      "Iteration 5339, loss = 0.07889566\n",
      "Iteration 5340, loss = 0.07889302\n",
      "Iteration 5341, loss = 0.07889038\n",
      "Iteration 5342, loss = 0.07888775\n",
      "Iteration 5343, loss = 0.07888510\n",
      "Iteration 5344, loss = 0.07888245\n",
      "Iteration 5345, loss = 0.07887983\n",
      "Iteration 5346, loss = 0.07887719\n",
      "Iteration 5347, loss = 0.07887455\n",
      "Iteration 5348, loss = 0.07887190\n",
      "Iteration 5349, loss = 0.07886926\n",
      "Iteration 5350, loss = 0.07886663\n",
      "Iteration 5351, loss = 0.07886401\n",
      "Iteration 5352, loss = 0.07886136\n",
      "Iteration 5353, loss = 0.07885872\n",
      "Iteration 5354, loss = 0.07885607\n",
      "Iteration 5355, loss = 0.07885348\n",
      "Iteration 5356, loss = 0.07885081\n",
      "Iteration 5357, loss = 0.07884818\n",
      "Iteration 5358, loss = 0.07884555\n",
      "Iteration 5359, loss = 0.07884292\n",
      "Iteration 5360, loss = 0.07884030\n",
      "Iteration 5361, loss = 0.07883765\n",
      "Iteration 5362, loss = 0.07883501\n",
      "Iteration 5363, loss = 0.07883237\n",
      "Iteration 5364, loss = 0.07882977\n",
      "Iteration 5365, loss = 0.07882712\n",
      "Iteration 5366, loss = 0.07882449\n",
      "Iteration 5367, loss = 0.07882185\n",
      "Iteration 5368, loss = 0.07881924\n",
      "Iteration 5369, loss = 0.07881661\n",
      "Iteration 5370, loss = 0.07881397\n",
      "Iteration 5371, loss = 0.07881134\n",
      "Iteration 5372, loss = 0.07880870\n",
      "Iteration 5373, loss = 0.07880610\n",
      "Iteration 5374, loss = 0.07880346\n",
      "Iteration 5375, loss = 0.07880083\n",
      "Iteration 5376, loss = 0.07879820\n",
      "Iteration 5377, loss = 0.07879557\n",
      "Iteration 5378, loss = 0.07879297\n",
      "Iteration 5379, loss = 0.07879033\n",
      "Iteration 5380, loss = 0.07878771\n",
      "Iteration 5381, loss = 0.07878507\n",
      "Iteration 5382, loss = 0.07878244\n",
      "Iteration 5383, loss = 0.07877984\n",
      "Iteration 5384, loss = 0.07877721\n",
      "Iteration 5385, loss = 0.07877458\n",
      "Iteration 5386, loss = 0.07877195\n",
      "Iteration 5387, loss = 0.07876935\n",
      "Iteration 5388, loss = 0.07876672\n",
      "Iteration 5389, loss = 0.07876409\n",
      "Iteration 5390, loss = 0.07876146\n",
      "Iteration 5391, loss = 0.07875884\n",
      "Iteration 5392, loss = 0.07875624\n",
      "Iteration 5393, loss = 0.07875362\n",
      "Iteration 5394, loss = 0.07875099\n",
      "Iteration 5395, loss = 0.07874837\n",
      "Iteration 5396, loss = 0.07874575\n",
      "Iteration 5397, loss = 0.07874315\n",
      "Iteration 5398, loss = 0.07874052\n",
      "Iteration 5399, loss = 0.07873790\n",
      "Iteration 5400, loss = 0.07873528\n",
      "Iteration 5401, loss = 0.07873268\n",
      "Iteration 5402, loss = 0.07873005\n",
      "Iteration 5403, loss = 0.07872743\n",
      "Iteration 5404, loss = 0.07872481\n",
      "Iteration 5405, loss = 0.07872221\n",
      "Iteration 5406, loss = 0.07871959\n",
      "Iteration 5407, loss = 0.07871698\n",
      "Iteration 5408, loss = 0.07871436\n",
      "Iteration 5409, loss = 0.07871174\n",
      "Iteration 5410, loss = 0.07870914\n",
      "Iteration 5411, loss = 0.07870653\n",
      "Iteration 5412, loss = 0.07870391\n",
      "Iteration 5413, loss = 0.07870129\n",
      "Iteration 5414, loss = 0.07869868\n",
      "Iteration 5415, loss = 0.07869610\n",
      "Iteration 5416, loss = 0.07869346\n",
      "Iteration 5417, loss = 0.07869084\n",
      "Iteration 5418, loss = 0.07868824\n",
      "Iteration 5419, loss = 0.07868565\n",
      "Iteration 5420, loss = 0.07868303\n",
      "Iteration 5421, loss = 0.07868042\n",
      "Iteration 5422, loss = 0.07867780\n",
      "Iteration 5423, loss = 0.07867520\n",
      "Iteration 5424, loss = 0.07867261\n",
      "Iteration 5425, loss = 0.07866998\n",
      "Iteration 5426, loss = 0.07866737\n",
      "Iteration 5427, loss = 0.07866477\n",
      "Iteration 5428, loss = 0.07866217\n",
      "Iteration 5429, loss = 0.07865956\n",
      "Iteration 5430, loss = 0.07865696\n",
      "Iteration 5431, loss = 0.07865435\n",
      "Iteration 5432, loss = 0.07865175\n",
      "Iteration 5433, loss = 0.07864915\n",
      "Iteration 5434, loss = 0.07864655\n",
      "Iteration 5435, loss = 0.07864394\n",
      "Iteration 5436, loss = 0.07864132\n",
      "Iteration 5437, loss = 0.07863874\n",
      "Iteration 5438, loss = 0.07863614\n",
      "Iteration 5439, loss = 0.07863352\n",
      "Iteration 5440, loss = 0.07863092\n",
      "Iteration 5441, loss = 0.07862832\n",
      "Iteration 5442, loss = 0.07862574\n",
      "Iteration 5443, loss = 0.07862313\n",
      "Iteration 5444, loss = 0.07862052\n",
      "Iteration 5445, loss = 0.07861793\n",
      "Iteration 5446, loss = 0.07861532\n",
      "Iteration 5447, loss = 0.07861275\n",
      "Iteration 5448, loss = 0.07861013\n",
      "Iteration 5449, loss = 0.07860752\n",
      "Iteration 5450, loss = 0.07860494\n",
      "Iteration 5451, loss = 0.07860233\n",
      "Iteration 5452, loss = 0.07859974\n",
      "Iteration 5453, loss = 0.07859715\n",
      "Iteration 5454, loss = 0.07859456\n",
      "Iteration 5455, loss = 0.07859196\n",
      "Iteration 5456, loss = 0.07858937\n",
      "Iteration 5457, loss = 0.07858677\n",
      "Iteration 5458, loss = 0.07858417\n",
      "Iteration 5459, loss = 0.07858158\n",
      "Iteration 5460, loss = 0.07857897\n",
      "Iteration 5461, loss = 0.07857641\n",
      "Iteration 5462, loss = 0.07857378\n",
      "Iteration 5463, loss = 0.07857122\n",
      "Iteration 5464, loss = 0.07856860\n",
      "Iteration 5465, loss = 0.07856603\n",
      "Iteration 5466, loss = 0.07856344\n",
      "Iteration 5467, loss = 0.07856086\n",
      "Iteration 5468, loss = 0.07855826\n",
      "Iteration 5469, loss = 0.07855566\n",
      "Iteration 5470, loss = 0.07855307\n",
      "Iteration 5471, loss = 0.07855048\n",
      "Iteration 5472, loss = 0.07854790\n",
      "Iteration 5473, loss = 0.07854530\n",
      "Iteration 5474, loss = 0.07854272\n",
      "Iteration 5475, loss = 0.07854013\n",
      "Iteration 5476, loss = 0.07853755\n",
      "Iteration 5477, loss = 0.07853496\n",
      "Iteration 5478, loss = 0.07853237\n",
      "Iteration 5479, loss = 0.07852979\n",
      "Iteration 5480, loss = 0.07852720\n",
      "Iteration 5481, loss = 0.07852462\n",
      "Iteration 5482, loss = 0.07852202\n",
      "Iteration 5483, loss = 0.07851944\n",
      "Iteration 5484, loss = 0.07851686\n",
      "Iteration 5485, loss = 0.07851429\n",
      "Iteration 5486, loss = 0.07851170\n",
      "Iteration 5487, loss = 0.07850910\n",
      "Iteration 5488, loss = 0.07850653\n",
      "Iteration 5489, loss = 0.07850394\n",
      "Iteration 5490, loss = 0.07850138\n",
      "Iteration 5491, loss = 0.07849878\n",
      "Iteration 5492, loss = 0.07849620\n",
      "Iteration 5493, loss = 0.07849363\n",
      "Iteration 5494, loss = 0.07849108\n",
      "Iteration 5495, loss = 0.07848849\n",
      "Iteration 5496, loss = 0.07848591\n",
      "Iteration 5497, loss = 0.07848333\n",
      "Iteration 5498, loss = 0.07848077\n",
      "Iteration 5499, loss = 0.07847821\n",
      "Iteration 5500, loss = 0.07847563\n",
      "Iteration 5501, loss = 0.07847304\n",
      "Iteration 5502, loss = 0.07847051\n",
      "Iteration 5503, loss = 0.07846791\n",
      "Iteration 5504, loss = 0.07846534\n",
      "Iteration 5505, loss = 0.07846277\n",
      "Iteration 5506, loss = 0.07846021\n",
      "Iteration 5507, loss = 0.07845765\n",
      "Iteration 5508, loss = 0.07845510\n",
      "Iteration 5509, loss = 0.07845250\n",
      "Iteration 5510, loss = 0.07844993\n",
      "Iteration 5511, loss = 0.07844738\n",
      "Iteration 5512, loss = 0.07844481\n",
      "Iteration 5513, loss = 0.07844224\n",
      "Iteration 5514, loss = 0.07843967\n",
      "Iteration 5515, loss = 0.07843712\n",
      "Iteration 5516, loss = 0.07843454\n",
      "Iteration 5517, loss = 0.07843200\n",
      "Iteration 5518, loss = 0.07842942\n",
      "Iteration 5519, loss = 0.07842684\n",
      "Iteration 5520, loss = 0.07842432\n",
      "Iteration 5521, loss = 0.07842173\n",
      "Iteration 5522, loss = 0.07841917\n",
      "Iteration 5523, loss = 0.07841660\n",
      "Iteration 5524, loss = 0.07841406\n",
      "Iteration 5525, loss = 0.07841149\n",
      "Iteration 5526, loss = 0.07840894\n",
      "Iteration 5527, loss = 0.07840637\n",
      "Iteration 5528, loss = 0.07840380\n",
      "Iteration 5529, loss = 0.07840124\n",
      "Iteration 5530, loss = 0.07839870\n",
      "Iteration 5531, loss = 0.07839613\n",
      "Iteration 5532, loss = 0.07839357\n",
      "Iteration 5533, loss = 0.07839101\n",
      "Iteration 5534, loss = 0.07838848\n",
      "Iteration 5535, loss = 0.07838591\n",
      "Iteration 5536, loss = 0.07838335\n",
      "Iteration 5537, loss = 0.07838078\n",
      "Iteration 5538, loss = 0.07837826\n",
      "Iteration 5539, loss = 0.07837568\n",
      "Iteration 5540, loss = 0.07837313\n",
      "Iteration 5541, loss = 0.07837057\n",
      "Iteration 5542, loss = 0.07836804\n",
      "Iteration 5543, loss = 0.07836547\n",
      "Iteration 5544, loss = 0.07836292\n",
      "Iteration 5545, loss = 0.07836036\n",
      "Iteration 5546, loss = 0.07835780\n",
      "Iteration 5547, loss = 0.07835527\n",
      "Iteration 5548, loss = 0.07835273\n",
      "Iteration 5549, loss = 0.07835019\n",
      "Iteration 5550, loss = 0.07834764\n",
      "Iteration 5551, loss = 0.07834509\n",
      "Iteration 5552, loss = 0.07834260\n",
      "Iteration 5553, loss = 0.07834002\n",
      "Iteration 5554, loss = 0.07833748\n",
      "Iteration 5555, loss = 0.07833496\n",
      "Iteration 5556, loss = 0.07833243\n",
      "Iteration 5557, loss = 0.07832989\n",
      "Iteration 5558, loss = 0.07832736\n",
      "Iteration 5559, loss = 0.07832481\n",
      "Iteration 5560, loss = 0.07832228\n",
      "Iteration 5561, loss = 0.07831977\n",
      "Iteration 5562, loss = 0.07831722\n",
      "Iteration 5563, loss = 0.07831468\n",
      "Iteration 5564, loss = 0.07831214\n",
      "Iteration 5565, loss = 0.07830965\n",
      "Iteration 5566, loss = 0.07830710\n",
      "Iteration 5567, loss = 0.07830457\n",
      "Iteration 5568, loss = 0.07830203\n",
      "Iteration 5569, loss = 0.07829951\n",
      "Iteration 5570, loss = 0.07829700\n",
      "Iteration 5571, loss = 0.07829446\n",
      "Iteration 5572, loss = 0.07829192\n",
      "Iteration 5573, loss = 0.07828938\n",
      "Iteration 5574, loss = 0.07828688\n",
      "Iteration 5575, loss = 0.07828435\n",
      "Iteration 5576, loss = 0.07828181\n",
      "Iteration 5577, loss = 0.07827928\n",
      "Iteration 5578, loss = 0.07827677\n",
      "Iteration 5579, loss = 0.07827425\n",
      "Iteration 5580, loss = 0.07827171\n",
      "Iteration 5581, loss = 0.07826918\n",
      "Iteration 5582, loss = 0.07826666\n",
      "Iteration 5583, loss = 0.07826415\n",
      "Iteration 5584, loss = 0.07826163\n",
      "Iteration 5585, loss = 0.07825910\n",
      "Iteration 5586, loss = 0.07825656\n",
      "Iteration 5587, loss = 0.07825405\n",
      "Iteration 5588, loss = 0.07825153\n",
      "Iteration 5589, loss = 0.07824901\n",
      "Iteration 5590, loss = 0.07824648\n",
      "Iteration 5591, loss = 0.07824395\n",
      "Iteration 5592, loss = 0.07824147\n",
      "Iteration 5593, loss = 0.07823893\n",
      "Iteration 5594, loss = 0.07823640\n",
      "Iteration 5595, loss = 0.07823388\n",
      "Iteration 5596, loss = 0.07823137\n",
      "Iteration 5597, loss = 0.07822887\n",
      "Iteration 5598, loss = 0.07822634\n",
      "Iteration 5599, loss = 0.07822381\n",
      "Iteration 5600, loss = 0.07822129\n",
      "Iteration 5601, loss = 0.07821880\n",
      "Iteration 5602, loss = 0.07821627\n",
      "Iteration 5603, loss = 0.07821375\n",
      "Iteration 5604, loss = 0.07821123\n",
      "Iteration 5605, loss = 0.07820873\n",
      "Iteration 5606, loss = 0.07820621\n",
      "Iteration 5607, loss = 0.07820369\n",
      "Iteration 5608, loss = 0.07820117\n",
      "Iteration 5609, loss = 0.07819866\n",
      "Iteration 5610, loss = 0.07819616\n",
      "Iteration 5611, loss = 0.07819365\n",
      "Iteration 5612, loss = 0.07819112\n",
      "Iteration 5613, loss = 0.07818860\n",
      "Iteration 5614, loss = 0.07818614\n",
      "Iteration 5615, loss = 0.07818359\n",
      "Iteration 5616, loss = 0.07818107\n",
      "Iteration 5617, loss = 0.07817856\n",
      "Iteration 5618, loss = 0.07817608\n",
      "Iteration 5619, loss = 0.07817356\n",
      "Iteration 5620, loss = 0.07817105\n",
      "Iteration 5621, loss = 0.07816853\n",
      "Iteration 5622, loss = 0.07816601\n",
      "Iteration 5623, loss = 0.07816354\n",
      "Iteration 5624, loss = 0.07816102\n",
      "Iteration 5625, loss = 0.07815850\n",
      "Iteration 5626, loss = 0.07815600\n",
      "Iteration 5627, loss = 0.07815349\n",
      "Iteration 5628, loss = 0.07815099\n",
      "Iteration 5629, loss = 0.07814848\n",
      "Iteration 5630, loss = 0.07814597\n",
      "Iteration 5631, loss = 0.07814346\n",
      "Iteration 5632, loss = 0.07814099\n",
      "Iteration 5633, loss = 0.07813846\n",
      "Iteration 5634, loss = 0.07813595\n",
      "Iteration 5635, loss = 0.07813345\n",
      "Iteration 5636, loss = 0.07813097\n",
      "Iteration 5637, loss = 0.07812846\n",
      "Iteration 5638, loss = 0.07812596\n",
      "Iteration 5639, loss = 0.07812344\n",
      "Iteration 5640, loss = 0.07812093\n",
      "Iteration 5641, loss = 0.07811847\n",
      "Iteration 5642, loss = 0.07811594\n",
      "Iteration 5643, loss = 0.07811343\n",
      "Iteration 5644, loss = 0.07811094\n",
      "Iteration 5645, loss = 0.07810845\n",
      "Iteration 5646, loss = 0.07810596\n",
      "Iteration 5647, loss = 0.07810345\n",
      "Iteration 5648, loss = 0.07810094\n",
      "Iteration 5649, loss = 0.07809845\n",
      "Iteration 5650, loss = 0.07809598\n",
      "Iteration 5651, loss = 0.07809344\n",
      "Iteration 5652, loss = 0.07809094\n",
      "Iteration 5653, loss = 0.07808848\n",
      "Iteration 5654, loss = 0.07808598\n",
      "Iteration 5655, loss = 0.07808348\n",
      "Iteration 5656, loss = 0.07808098\n",
      "Iteration 5657, loss = 0.07807847\n",
      "Iteration 5658, loss = 0.07807600\n",
      "Iteration 5659, loss = 0.07807350\n",
      "Iteration 5660, loss = 0.07807100\n",
      "Iteration 5661, loss = 0.07806850\n",
      "Iteration 5662, loss = 0.07806601\n",
      "Iteration 5663, loss = 0.07806354\n",
      "Iteration 5664, loss = 0.07806105\n",
      "Iteration 5665, loss = 0.07805855\n",
      "Iteration 5666, loss = 0.07805604\n",
      "Iteration 5667, loss = 0.07805357\n",
      "Iteration 5668, loss = 0.07805108\n",
      "Iteration 5669, loss = 0.07804859\n",
      "Iteration 5670, loss = 0.07804609\n",
      "Iteration 5671, loss = 0.07804362\n",
      "Iteration 5672, loss = 0.07804112\n",
      "Iteration 5673, loss = 0.07803863\n",
      "Iteration 5674, loss = 0.07803614\n",
      "Iteration 5675, loss = 0.07803365\n",
      "Iteration 5676, loss = 0.07803120\n",
      "Iteration 5677, loss = 0.07802869\n",
      "Iteration 5678, loss = 0.07802620\n",
      "Iteration 5679, loss = 0.07802370\n",
      "Iteration 5680, loss = 0.07802126\n",
      "Iteration 5681, loss = 0.07801875\n",
      "Iteration 5682, loss = 0.07801626\n",
      "Iteration 5683, loss = 0.07801378\n",
      "Iteration 5684, loss = 0.07801129\n",
      "Iteration 5685, loss = 0.07800883\n",
      "Iteration 5686, loss = 0.07800634\n",
      "Iteration 5687, loss = 0.07800386\n",
      "Iteration 5688, loss = 0.07800137\n",
      "Iteration 5689, loss = 0.07799889\n",
      "Iteration 5690, loss = 0.07799641\n",
      "Iteration 5691, loss = 0.07799393\n",
      "Iteration 5692, loss = 0.07799144\n",
      "Iteration 5693, loss = 0.07798897\n",
      "Iteration 5694, loss = 0.07798650\n",
      "Iteration 5695, loss = 0.07798402\n",
      "Iteration 5696, loss = 0.07798153\n",
      "Iteration 5697, loss = 0.07797905\n",
      "Iteration 5698, loss = 0.07797659\n",
      "Iteration 5699, loss = 0.07797410\n",
      "Iteration 5700, loss = 0.07797161\n",
      "Iteration 5701, loss = 0.07796915\n",
      "Iteration 5702, loss = 0.07796668\n",
      "Iteration 5703, loss = 0.07796419\n",
      "Iteration 5704, loss = 0.07796171\n",
      "Iteration 5705, loss = 0.07795924\n",
      "Iteration 5706, loss = 0.07795676\n",
      "Iteration 5707, loss = 0.07795430\n",
      "Iteration 5708, loss = 0.07795182\n",
      "Iteration 5709, loss = 0.07794934\n",
      "Iteration 5710, loss = 0.07794688\n",
      "Iteration 5711, loss = 0.07794440\n",
      "Iteration 5712, loss = 0.07794193\n",
      "Iteration 5713, loss = 0.07793945\n",
      "Iteration 5714, loss = 0.07793698\n",
      "Iteration 5715, loss = 0.07793451\n",
      "Iteration 5716, loss = 0.07793204\n",
      "Iteration 5717, loss = 0.07792957\n",
      "Iteration 5718, loss = 0.07792709\n",
      "Iteration 5719, loss = 0.07792462\n",
      "Iteration 5720, loss = 0.07792217\n",
      "Iteration 5721, loss = 0.07791969\n",
      "Iteration 5722, loss = 0.07791721\n",
      "Iteration 5723, loss = 0.07791474\n",
      "Iteration 5724, loss = 0.07791230\n",
      "Iteration 5725, loss = 0.07790981\n",
      "Iteration 5726, loss = 0.07790734\n",
      "Iteration 5727, loss = 0.07790488\n",
      "Iteration 5728, loss = 0.07790242\n",
      "Iteration 5729, loss = 0.07789997\n",
      "Iteration 5730, loss = 0.07789750\n",
      "Iteration 5731, loss = 0.07789503\n",
      "Iteration 5732, loss = 0.07789257\n",
      "Iteration 5733, loss = 0.07789013\n",
      "Iteration 5734, loss = 0.07788765\n",
      "Iteration 5735, loss = 0.07788518\n",
      "Iteration 5736, loss = 0.07788272\n",
      "Iteration 5737, loss = 0.07788028\n",
      "Iteration 5738, loss = 0.07787781\n",
      "Iteration 5739, loss = 0.07787535\n",
      "Iteration 5740, loss = 0.07787289\n",
      "Iteration 5741, loss = 0.07787043\n",
      "Iteration 5742, loss = 0.07786798\n",
      "Iteration 5743, loss = 0.07786552\n",
      "Iteration 5744, loss = 0.07786306\n",
      "Iteration 5745, loss = 0.07786061\n",
      "Iteration 5746, loss = 0.07785815\n",
      "Iteration 5747, loss = 0.07785569\n",
      "Iteration 5748, loss = 0.07785324\n",
      "Iteration 5749, loss = 0.07785078\n",
      "Iteration 5750, loss = 0.07784833\n",
      "Iteration 5751, loss = 0.07784589\n",
      "Iteration 5752, loss = 0.07784342\n",
      "Iteration 5753, loss = 0.07784096\n",
      "Iteration 5754, loss = 0.07783852\n",
      "Iteration 5755, loss = 0.07783607\n",
      "Iteration 5756, loss = 0.07783360\n",
      "Iteration 5757, loss = 0.07783116\n",
      "Iteration 5758, loss = 0.07782870\n",
      "Iteration 5759, loss = 0.07782627\n",
      "Iteration 5760, loss = 0.07782381\n",
      "Iteration 5761, loss = 0.07782135\n",
      "Iteration 5762, loss = 0.07781891\n",
      "Iteration 5763, loss = 0.07781644\n",
      "Iteration 5764, loss = 0.07781402\n",
      "Iteration 5765, loss = 0.07781154\n",
      "Iteration 5766, loss = 0.07780910\n",
      "Iteration 5767, loss = 0.07780666\n",
      "Iteration 5768, loss = 0.07780422\n",
      "Iteration 5769, loss = 0.07780176\n",
      "Iteration 5770, loss = 0.07779931\n",
      "Iteration 5771, loss = 0.07779688\n",
      "Iteration 5772, loss = 0.07779442\n",
      "Iteration 5773, loss = 0.07779197\n",
      "Iteration 5774, loss = 0.07778953\n",
      "Iteration 5775, loss = 0.07778708\n",
      "Iteration 5776, loss = 0.07778464\n",
      "Iteration 5777, loss = 0.07778220\n",
      "Iteration 5778, loss = 0.07777975\n",
      "Iteration 5779, loss = 0.07777730\n",
      "Iteration 5780, loss = 0.07777487\n",
      "Iteration 5781, loss = 0.07777242\n",
      "Iteration 5782, loss = 0.07776997\n",
      "Iteration 5783, loss = 0.07776753\n",
      "Iteration 5784, loss = 0.07776508\n",
      "Iteration 5785, loss = 0.07776266\n",
      "Iteration 5786, loss = 0.07776020\n",
      "Iteration 5787, loss = 0.07775775\n",
      "Iteration 5788, loss = 0.07775532\n",
      "Iteration 5789, loss = 0.07775288\n",
      "Iteration 5790, loss = 0.07775045\n",
      "Iteration 5791, loss = 0.07774800\n",
      "Iteration 5792, loss = 0.07774556\n",
      "Iteration 5793, loss = 0.07774311\n",
      "Iteration 5794, loss = 0.07774070\n",
      "Iteration 5795, loss = 0.07773824\n",
      "Iteration 5796, loss = 0.07773580\n",
      "Iteration 5797, loss = 0.07773336\n",
      "Iteration 5798, loss = 0.07773095\n",
      "Iteration 5799, loss = 0.07772851\n",
      "Iteration 5800, loss = 0.07772606\n",
      "Iteration 5801, loss = 0.07772362\n",
      "Iteration 5802, loss = 0.07772118\n",
      "Iteration 5803, loss = 0.07771877\n",
      "Iteration 5804, loss = 0.07771631\n",
      "Iteration 5805, loss = 0.07771388\n",
      "Iteration 5806, loss = 0.07771145\n",
      "Iteration 5807, loss = 0.07770902\n",
      "Iteration 5808, loss = 0.07770658\n",
      "Iteration 5809, loss = 0.07770414\n",
      "Iteration 5810, loss = 0.07770171\n",
      "Iteration 5811, loss = 0.07769929\n",
      "Iteration 5812, loss = 0.07769685\n",
      "Iteration 5813, loss = 0.07769441\n",
      "Iteration 5814, loss = 0.07769199\n",
      "Iteration 5815, loss = 0.07768956\n",
      "Iteration 5816, loss = 0.07768713\n",
      "Iteration 5817, loss = 0.07768470\n",
      "Iteration 5818, loss = 0.07768227\n",
      "Iteration 5819, loss = 0.07767983\n",
      "Iteration 5820, loss = 0.07767743\n",
      "Iteration 5821, loss = 0.07767498\n",
      "Iteration 5822, loss = 0.07767255\n",
      "Iteration 5823, loss = 0.07767012\n",
      "Iteration 5824, loss = 0.07766771\n",
      "Iteration 5825, loss = 0.07766529\n",
      "Iteration 5826, loss = 0.07766285\n",
      "Iteration 5827, loss = 0.07766043\n",
      "Iteration 5828, loss = 0.07765798\n",
      "Iteration 5829, loss = 0.07765559\n",
      "Iteration 5830, loss = 0.07765313\n",
      "Iteration 5831, loss = 0.07765072\n",
      "Iteration 5832, loss = 0.07764831\n",
      "Iteration 5833, loss = 0.07764587\n",
      "Iteration 5834, loss = 0.07764344\n",
      "Iteration 5835, loss = 0.07764102\n",
      "Iteration 5836, loss = 0.07763859\n",
      "Iteration 5837, loss = 0.07763619\n",
      "Iteration 5838, loss = 0.07763375\n",
      "Iteration 5839, loss = 0.07763132\n",
      "Iteration 5840, loss = 0.07762891\n",
      "Iteration 5841, loss = 0.07762650\n",
      "Iteration 5842, loss = 0.07762406\n",
      "Iteration 5843, loss = 0.07762164\n",
      "Iteration 5844, loss = 0.07761922\n",
      "Iteration 5845, loss = 0.07761681\n",
      "Iteration 5846, loss = 0.07761439\n",
      "Iteration 5847, loss = 0.07761197\n",
      "Iteration 5848, loss = 0.07760954\n",
      "Iteration 5849, loss = 0.07760714\n",
      "Iteration 5850, loss = 0.07760472\n",
      "Iteration 5851, loss = 0.07760228\n",
      "Iteration 5852, loss = 0.07759988\n",
      "Iteration 5853, loss = 0.07759746\n",
      "Iteration 5854, loss = 0.07759507\n",
      "Iteration 5855, loss = 0.07759263\n",
      "Iteration 5856, loss = 0.07759020\n",
      "Iteration 5857, loss = 0.07758777\n",
      "Iteration 5858, loss = 0.07758539\n",
      "Iteration 5859, loss = 0.07758296\n",
      "Iteration 5860, loss = 0.07758055\n",
      "Iteration 5861, loss = 0.07757813\n",
      "Iteration 5862, loss = 0.07757571\n",
      "Iteration 5863, loss = 0.07757332\n",
      "Iteration 5864, loss = 0.07757090\n",
      "Iteration 5865, loss = 0.07756847\n",
      "Iteration 5866, loss = 0.07756607\n",
      "Iteration 5867, loss = 0.07756366\n",
      "Iteration 5868, loss = 0.07756124\n",
      "Iteration 5869, loss = 0.07755882\n",
      "Iteration 5870, loss = 0.07755642\n",
      "Iteration 5871, loss = 0.07755402\n",
      "Iteration 5872, loss = 0.07755159\n",
      "Iteration 5873, loss = 0.07754918\n",
      "Iteration 5874, loss = 0.07754678\n",
      "Iteration 5875, loss = 0.07754436\n",
      "Iteration 5876, loss = 0.07754196\n",
      "Iteration 5877, loss = 0.07753955\n",
      "Iteration 5878, loss = 0.07753715\n",
      "Iteration 5879, loss = 0.07753473\n",
      "Iteration 5880, loss = 0.07753235\n",
      "Iteration 5881, loss = 0.07752991\n",
      "Iteration 5882, loss = 0.07752750\n",
      "Iteration 5883, loss = 0.07752511\n",
      "Iteration 5884, loss = 0.07752271\n",
      "Iteration 5885, loss = 0.07752031\n",
      "Iteration 5886, loss = 0.07751789\n",
      "Iteration 5887, loss = 0.07751549\n",
      "Iteration 5888, loss = 0.07751307\n",
      "Iteration 5889, loss = 0.07751071\n",
      "Iteration 5890, loss = 0.07750826\n",
      "Iteration 5891, loss = 0.07750587\n",
      "Iteration 5892, loss = 0.07750348\n",
      "Iteration 5893, loss = 0.07750108\n",
      "Iteration 5894, loss = 0.07749868\n",
      "Iteration 5895, loss = 0.07749628\n",
      "Iteration 5896, loss = 0.07749387\n",
      "Iteration 5897, loss = 0.07749147\n",
      "Iteration 5898, loss = 0.07748907\n",
      "Iteration 5899, loss = 0.07748667\n",
      "Iteration 5900, loss = 0.07748428\n",
      "Iteration 5901, loss = 0.07748188\n",
      "Iteration 5902, loss = 0.07747947\n",
      "Iteration 5903, loss = 0.07747707\n",
      "Iteration 5904, loss = 0.07747468\n",
      "Iteration 5905, loss = 0.07747230\n",
      "Iteration 5906, loss = 0.07746989\n",
      "Iteration 5907, loss = 0.07746749\n",
      "Iteration 5908, loss = 0.07746509\n",
      "Iteration 5909, loss = 0.07746272\n",
      "Iteration 5910, loss = 0.07746031\n",
      "Iteration 5911, loss = 0.07745791\n",
      "Iteration 5912, loss = 0.07745553\n",
      "Iteration 5913, loss = 0.07745313\n",
      "Iteration 5914, loss = 0.07745076\n",
      "Iteration 5915, loss = 0.07744835\n",
      "Iteration 5916, loss = 0.07744595\n",
      "Iteration 5917, loss = 0.07744354\n",
      "Iteration 5918, loss = 0.07744118\n",
      "Iteration 5919, loss = 0.07743879\n",
      "Iteration 5920, loss = 0.07743639\n",
      "Iteration 5921, loss = 0.07743400\n",
      "Iteration 5922, loss = 0.07743160\n",
      "Iteration 5923, loss = 0.07742925\n",
      "Iteration 5924, loss = 0.07742682\n",
      "Iteration 5925, loss = 0.07742443\n",
      "Iteration 5926, loss = 0.07742207\n",
      "Iteration 5927, loss = 0.07741968\n",
      "Iteration 5928, loss = 0.07741729\n",
      "Iteration 5929, loss = 0.07741489\n",
      "Iteration 5930, loss = 0.07741252\n",
      "Iteration 5931, loss = 0.07741011\n",
      "Iteration 5932, loss = 0.07740773\n",
      "Iteration 5933, loss = 0.07740535\n",
      "Iteration 5934, loss = 0.07740295\n",
      "Iteration 5935, loss = 0.07740060\n",
      "Iteration 5936, loss = 0.07739819\n",
      "Iteration 5937, loss = 0.07739580\n",
      "Iteration 5938, loss = 0.07739341\n",
      "Iteration 5939, loss = 0.07739106\n",
      "Iteration 5940, loss = 0.07738865\n",
      "Iteration 5941, loss = 0.07738626\n",
      "Iteration 5942, loss = 0.07738388\n",
      "Iteration 5943, loss = 0.07738152\n",
      "Iteration 5944, loss = 0.07737913\n",
      "Iteration 5945, loss = 0.07737674\n",
      "Iteration 5946, loss = 0.07737435\n",
      "Iteration 5947, loss = 0.07737198\n",
      "Iteration 5948, loss = 0.07736961\n",
      "Iteration 5949, loss = 0.07736722\n",
      "Iteration 5950, loss = 0.07736484\n",
      "Iteration 5951, loss = 0.07736246\n",
      "Iteration 5952, loss = 0.07736009\n",
      "Iteration 5953, loss = 0.07735770\n",
      "Iteration 5954, loss = 0.07735531\n",
      "Iteration 5955, loss = 0.07735293\n",
      "Iteration 5956, loss = 0.07735059\n",
      "Iteration 5957, loss = 0.07734819\n",
      "Iteration 5958, loss = 0.07734580\n",
      "Iteration 5959, loss = 0.07734342\n",
      "Iteration 5960, loss = 0.07734108\n",
      "Iteration 5961, loss = 0.07733868\n",
      "Iteration 5962, loss = 0.07733630\n",
      "Iteration 5963, loss = 0.07733391\n",
      "Iteration 5964, loss = 0.07733156\n",
      "Iteration 5965, loss = 0.07732918\n",
      "Iteration 5966, loss = 0.07732680\n",
      "Iteration 5967, loss = 0.07732442\n",
      "Iteration 5968, loss = 0.07732205\n",
      "Iteration 5969, loss = 0.07731971\n",
      "Iteration 5970, loss = 0.07731731\n",
      "Iteration 5971, loss = 0.07731492\n",
      "Iteration 5972, loss = 0.07731256\n",
      "Iteration 5973, loss = 0.07731020\n",
      "Iteration 5974, loss = 0.07730782\n",
      "Iteration 5975, loss = 0.07730544\n",
      "Iteration 5976, loss = 0.07730306\n",
      "Iteration 5977, loss = 0.07730071\n",
      "Iteration 5978, loss = 0.07729834\n",
      "Iteration 5979, loss = 0.07729596\n",
      "Iteration 5980, loss = 0.07729358\n",
      "Iteration 5981, loss = 0.07729124\n",
      "Iteration 5982, loss = 0.07728885\n",
      "Iteration 5983, loss = 0.07728648\n",
      "Iteration 5984, loss = 0.07728411\n",
      "Iteration 5985, loss = 0.07728175\n",
      "Iteration 5986, loss = 0.07727939\n",
      "Iteration 5987, loss = 0.07727702\n",
      "Iteration 5988, loss = 0.07727464\n",
      "Iteration 5989, loss = 0.07727226\n",
      "Iteration 5990, loss = 0.07726993\n",
      "Iteration 5991, loss = 0.07726754\n",
      "Iteration 5992, loss = 0.07726517\n",
      "Iteration 5993, loss = 0.07726281\n",
      "Iteration 5994, loss = 0.07726046\n",
      "Iteration 5995, loss = 0.07725809\n",
      "Iteration 5996, loss = 0.07725572\n",
      "Iteration 5997, loss = 0.07725334\n",
      "Iteration 5998, loss = 0.07725101\n",
      "Iteration 5999, loss = 0.07724862\n",
      "Iteration 6000, loss = 0.07724625\n",
      "Iteration 6001, loss = 0.07724389\n",
      "Iteration 6002, loss = 0.07724155\n",
      "Iteration 6003, loss = 0.07723918\n",
      "Iteration 6004, loss = 0.07723681\n",
      "Iteration 6005, loss = 0.07723444\n",
      "Iteration 6006, loss = 0.07723209\n",
      "Iteration 6007, loss = 0.07722974\n",
      "Iteration 6008, loss = 0.07722737\n",
      "Iteration 6009, loss = 0.07722500\n",
      "Iteration 6010, loss = 0.07722264\n",
      "Iteration 6011, loss = 0.07722030\n",
      "Iteration 6012, loss = 0.07721793\n",
      "Iteration 6013, loss = 0.07721557\n",
      "Iteration 6014, loss = 0.07721320\n",
      "Iteration 6015, loss = 0.07721087\n",
      "Iteration 6016, loss = 0.07720850\n",
      "Iteration 6017, loss = 0.07720613\n",
      "Iteration 6018, loss = 0.07720377\n",
      "Iteration 6019, loss = 0.07720144\n",
      "Iteration 6020, loss = 0.07719907\n",
      "Iteration 6021, loss = 0.07719671\n",
      "Iteration 6022, loss = 0.07719434\n",
      "Iteration 6023, loss = 0.07719200\n",
      "Iteration 6024, loss = 0.07718965\n",
      "Iteration 6025, loss = 0.07718729\n",
      "Iteration 6026, loss = 0.07718492\n",
      "Iteration 6027, loss = 0.07718257\n",
      "Iteration 6028, loss = 0.07718023\n",
      "Iteration 6029, loss = 0.07717787\n",
      "Iteration 6030, loss = 0.07717551\n",
      "Iteration 6031, loss = 0.07717315\n",
      "Iteration 6032, loss = 0.07717082\n",
      "Iteration 6033, loss = 0.07716847\n",
      "Iteration 6034, loss = 0.07716610\n",
      "Iteration 6035, loss = 0.07716374\n",
      "Iteration 6036, loss = 0.07716143\n",
      "Iteration 6037, loss = 0.07715906\n",
      "Iteration 6038, loss = 0.07715671\n",
      "Iteration 6039, loss = 0.07715435\n",
      "Iteration 6040, loss = 0.07715203\n",
      "Iteration 6041, loss = 0.07714966\n",
      "Iteration 6042, loss = 0.07714731\n",
      "Iteration 6043, loss = 0.07714495\n",
      "Iteration 6044, loss = 0.07714263\n",
      "Iteration 6045, loss = 0.07714028\n",
      "Iteration 6046, loss = 0.07713792\n",
      "Iteration 6047, loss = 0.07713556\n",
      "Iteration 6048, loss = 0.07713323\n",
      "Iteration 6049, loss = 0.07713088\n",
      "Iteration 6050, loss = 0.07712854\n",
      "Iteration 6051, loss = 0.07712618\n",
      "Iteration 6052, loss = 0.07712385\n",
      "Iteration 6053, loss = 0.07712151\n",
      "Iteration 6054, loss = 0.07711916\n",
      "Iteration 6055, loss = 0.07711681\n",
      "Iteration 6056, loss = 0.07711447\n",
      "Iteration 6057, loss = 0.07711214\n",
      "Iteration 6058, loss = 0.07710978\n",
      "Iteration 6059, loss = 0.07710743\n",
      "Iteration 6060, loss = 0.07710510\n",
      "Iteration 6061, loss = 0.07710277\n",
      "Iteration 6062, loss = 0.07710043\n",
      "Iteration 6063, loss = 0.07709808\n",
      "Iteration 6064, loss = 0.07709573\n",
      "Iteration 6065, loss = 0.07709341\n",
      "Iteration 6066, loss = 0.07709107\n",
      "Iteration 6067, loss = 0.07708873\n",
      "Iteration 6068, loss = 0.07708637\n",
      "Iteration 6069, loss = 0.07708406\n",
      "Iteration 6070, loss = 0.07708171\n",
      "Iteration 6071, loss = 0.07707937\n",
      "Iteration 6072, loss = 0.07707703\n",
      "Iteration 6073, loss = 0.07707470\n",
      "Iteration 6074, loss = 0.07707237\n",
      "Iteration 6075, loss = 0.07707003\n",
      "Iteration 6076, loss = 0.07706769\n",
      "Iteration 6077, loss = 0.07706536\n",
      "Iteration 6078, loss = 0.07706303\n",
      "Iteration 6079, loss = 0.07706068\n",
      "Iteration 6080, loss = 0.07705834\n",
      "Iteration 6081, loss = 0.07705603\n",
      "Iteration 6082, loss = 0.07705369\n",
      "Iteration 6083, loss = 0.07705135\n",
      "Iteration 6084, loss = 0.07704901\n",
      "Iteration 6085, loss = 0.07704670\n",
      "Iteration 6086, loss = 0.07704435\n",
      "Iteration 6087, loss = 0.07704202\n",
      "Iteration 6088, loss = 0.07703968\n",
      "Iteration 6089, loss = 0.07703737\n",
      "Iteration 6090, loss = 0.07703503\n",
      "Iteration 6091, loss = 0.07703270\n",
      "Iteration 6092, loss = 0.07703036\n",
      "Iteration 6093, loss = 0.07702804\n",
      "Iteration 6094, loss = 0.07702570\n",
      "Iteration 6095, loss = 0.07702338\n",
      "Iteration 6096, loss = 0.07702105\n",
      "Iteration 6097, loss = 0.07701871\n",
      "Iteration 6098, loss = 0.07701639\n",
      "Iteration 6099, loss = 0.07701408\n",
      "Iteration 6100, loss = 0.07701173\n",
      "Iteration 6101, loss = 0.07700940\n",
      "Iteration 6102, loss = 0.07700710\n",
      "Iteration 6103, loss = 0.07700476\n",
      "Iteration 6104, loss = 0.07700243\n",
      "Iteration 6105, loss = 0.07700011\n",
      "Iteration 6106, loss = 0.07699778\n",
      "Iteration 6107, loss = 0.07699546\n",
      "Iteration 6108, loss = 0.07699313\n",
      "Iteration 6109, loss = 0.07699079\n",
      "Iteration 6110, loss = 0.07698848\n",
      "Iteration 6111, loss = 0.07698615\n",
      "Iteration 6112, loss = 0.07698383\n",
      "Iteration 6113, loss = 0.07698150\n",
      "Iteration 6114, loss = 0.07697919\n",
      "Iteration 6115, loss = 0.07697687\n",
      "Iteration 6116, loss = 0.07697455\n",
      "Iteration 6117, loss = 0.07697221\n",
      "Iteration 6118, loss = 0.07696989\n",
      "Iteration 6119, loss = 0.07696757\n",
      "Iteration 6120, loss = 0.07696527\n",
      "Iteration 6121, loss = 0.07696292\n",
      "Iteration 6122, loss = 0.07696061\n",
      "Iteration 6123, loss = 0.07695829\n",
      "Iteration 6124, loss = 0.07695597\n",
      "Iteration 6125, loss = 0.07695365\n",
      "Iteration 6126, loss = 0.07695133\n",
      "Iteration 6127, loss = 0.07694901\n",
      "Iteration 6128, loss = 0.07694670\n",
      "Iteration 6129, loss = 0.07694437\n",
      "Iteration 6130, loss = 0.07694206\n",
      "Iteration 6131, loss = 0.07693973\n",
      "Iteration 6132, loss = 0.07693743\n",
      "Iteration 6133, loss = 0.07693510\n",
      "Iteration 6134, loss = 0.07693278\n",
      "Iteration 6135, loss = 0.07693047\n",
      "Iteration 6136, loss = 0.07692816\n",
      "Iteration 6137, loss = 0.07692584\n",
      "Iteration 6138, loss = 0.07692351\n",
      "Iteration 6139, loss = 0.07692124\n",
      "Iteration 6140, loss = 0.07691888\n",
      "Iteration 6141, loss = 0.07691657\n",
      "Iteration 6142, loss = 0.07691428\n",
      "Iteration 6143, loss = 0.07691197\n",
      "Iteration 6144, loss = 0.07690964\n",
      "Iteration 6145, loss = 0.07690734\n",
      "Iteration 6146, loss = 0.07690500\n",
      "Iteration 6147, loss = 0.07690269\n",
      "Iteration 6148, loss = 0.07690039\n",
      "Iteration 6149, loss = 0.07689807\n",
      "Iteration 6150, loss = 0.07689576\n",
      "Iteration 6151, loss = 0.07689346\n",
      "Iteration 6152, loss = 0.07689115\n",
      "Iteration 6153, loss = 0.07688883\n",
      "Iteration 6154, loss = 0.07688651\n",
      "Iteration 6155, loss = 0.07688421\n",
      "Iteration 6156, loss = 0.07688190\n",
      "Iteration 6157, loss = 0.07687959\n",
      "Iteration 6158, loss = 0.07687727\n",
      "Iteration 6159, loss = 0.07687499\n",
      "Iteration 6160, loss = 0.07687267\n",
      "Iteration 6161, loss = 0.07687036\n",
      "Iteration 6162, loss = 0.07686805\n",
      "Iteration 6163, loss = 0.07686574\n",
      "Iteration 6164, loss = 0.07686343\n",
      "Iteration 6165, loss = 0.07686112\n",
      "Iteration 6166, loss = 0.07685881\n",
      "Iteration 6167, loss = 0.07685653\n",
      "Iteration 6168, loss = 0.07685421\n",
      "Iteration 6169, loss = 0.07685189\n",
      "Iteration 6170, loss = 0.07684960\n",
      "Iteration 6171, loss = 0.07684731\n",
      "Iteration 6172, loss = 0.07684498\n",
      "Iteration 6173, loss = 0.07684267\n",
      "Iteration 6174, loss = 0.07684037\n",
      "Iteration 6175, loss = 0.07683810\n",
      "Iteration 6176, loss = 0.07683577\n",
      "Iteration 6177, loss = 0.07683346\n",
      "Iteration 6178, loss = 0.07683115\n",
      "Iteration 6179, loss = 0.07682888\n",
      "Iteration 6180, loss = 0.07682656\n",
      "Iteration 6181, loss = 0.07682425\n",
      "Iteration 6182, loss = 0.07682195\n",
      "Iteration 6183, loss = 0.07681967\n",
      "Iteration 6184, loss = 0.07681735\n",
      "Iteration 6185, loss = 0.07681505\n",
      "Iteration 6186, loss = 0.07681276\n",
      "Iteration 6187, loss = 0.07681047\n",
      "Iteration 6188, loss = 0.07680816\n",
      "Iteration 6189, loss = 0.07680586\n",
      "Iteration 6190, loss = 0.07680356\n",
      "Iteration 6191, loss = 0.07680127\n",
      "Iteration 6192, loss = 0.07679898\n",
      "Iteration 6193, loss = 0.07679667\n",
      "Iteration 6194, loss = 0.07679436\n",
      "Iteration 6195, loss = 0.07679209\n",
      "Iteration 6196, loss = 0.07678979\n",
      "Iteration 6197, loss = 0.07678748\n",
      "Iteration 6198, loss = 0.07678519\n",
      "Iteration 6199, loss = 0.07678290\n",
      "Iteration 6200, loss = 0.07678062\n",
      "Iteration 6201, loss = 0.07677831\n",
      "Iteration 6202, loss = 0.07677601\n",
      "Iteration 6203, loss = 0.07677372\n",
      "Iteration 6204, loss = 0.07677146\n",
      "Iteration 6205, loss = 0.07676913\n",
      "Iteration 6206, loss = 0.07676683\n",
      "Iteration 6207, loss = 0.07676457\n",
      "Iteration 6208, loss = 0.07676227\n",
      "Iteration 6209, loss = 0.07675998\n",
      "Iteration 6210, loss = 0.07675767\n",
      "Iteration 6211, loss = 0.07675539\n",
      "Iteration 6212, loss = 0.07675310\n",
      "Iteration 6213, loss = 0.07675080\n",
      "Iteration 6214, loss = 0.07674850\n",
      "Iteration 6215, loss = 0.07674623\n",
      "Iteration 6216, loss = 0.07674394\n",
      "Iteration 6217, loss = 0.07674165\n",
      "Iteration 6218, loss = 0.07673935\n",
      "Iteration 6219, loss = 0.07673705\n",
      "Iteration 6220, loss = 0.07673481\n",
      "Iteration 6221, loss = 0.07673249\n",
      "Iteration 6222, loss = 0.07673019\n",
      "Iteration 6223, loss = 0.07672792\n",
      "Iteration 6224, loss = 0.07672565\n",
      "Iteration 6225, loss = 0.07672334\n",
      "Iteration 6226, loss = 0.07672105\n",
      "Iteration 6227, loss = 0.07671876\n",
      "Iteration 6228, loss = 0.07671650\n",
      "Iteration 6229, loss = 0.07671421\n",
      "Iteration 6230, loss = 0.07671191\n",
      "Iteration 6231, loss = 0.07670962\n",
      "Iteration 6232, loss = 0.07670737\n",
      "Iteration 6233, loss = 0.07670505\n",
      "Iteration 6234, loss = 0.07670276\n",
      "Iteration 6235, loss = 0.07670048\n",
      "Iteration 6236, loss = 0.07669823\n",
      "Iteration 6237, loss = 0.07669593\n",
      "Iteration 6238, loss = 0.07669364\n",
      "Iteration 6239, loss = 0.07669134\n",
      "Iteration 6240, loss = 0.07668911\n",
      "Iteration 6241, loss = 0.07668679\n",
      "Iteration 6242, loss = 0.07668451\n",
      "Iteration 6243, loss = 0.07668223\n",
      "Iteration 6244, loss = 0.07667997\n",
      "Iteration 6245, loss = 0.07667768\n",
      "Iteration 6246, loss = 0.07667540\n",
      "Iteration 6247, loss = 0.07667310\n",
      "Iteration 6248, loss = 0.07667086\n",
      "Iteration 6249, loss = 0.07666856\n",
      "Iteration 6250, loss = 0.07666628\n",
      "Iteration 6251, loss = 0.07666401\n",
      "Iteration 6252, loss = 0.07666174\n",
      "Iteration 6253, loss = 0.07665947\n",
      "Iteration 6254, loss = 0.07665719\n",
      "Iteration 6255, loss = 0.07665490\n",
      "Iteration 6256, loss = 0.07665265\n",
      "Iteration 6257, loss = 0.07665037\n",
      "Iteration 6258, loss = 0.07664808\n",
      "Iteration 6259, loss = 0.07664581\n",
      "Iteration 6260, loss = 0.07664355\n",
      "Iteration 6261, loss = 0.07664128\n",
      "Iteration 6262, loss = 0.07663901\n",
      "Iteration 6263, loss = 0.07663672\n",
      "Iteration 6264, loss = 0.07663447\n",
      "Iteration 6265, loss = 0.07663219\n",
      "Iteration 6266, loss = 0.07662991\n",
      "Iteration 6267, loss = 0.07662764\n",
      "Iteration 6268, loss = 0.07662538\n",
      "Iteration 6269, loss = 0.07662312\n",
      "Iteration 6270, loss = 0.07662084\n",
      "Iteration 6271, loss = 0.07661856\n",
      "Iteration 6272, loss = 0.07661631\n",
      "Iteration 6273, loss = 0.07661404\n",
      "Iteration 6274, loss = 0.07661176\n",
      "Iteration 6275, loss = 0.07660950\n",
      "Iteration 6276, loss = 0.07660723\n",
      "Iteration 6277, loss = 0.07660498\n",
      "Iteration 6278, loss = 0.07660270\n",
      "Iteration 6279, loss = 0.07660042\n",
      "Iteration 6280, loss = 0.07659818\n",
      "Iteration 6281, loss = 0.07659590\n",
      "Iteration 6282, loss = 0.07659363\n",
      "Iteration 6283, loss = 0.07659137\n",
      "Iteration 6284, loss = 0.07658911\n",
      "Iteration 6285, loss = 0.07658684\n",
      "Iteration 6286, loss = 0.07658458\n",
      "Iteration 6287, loss = 0.07658231\n",
      "Iteration 6288, loss = 0.07658007\n",
      "Iteration 6289, loss = 0.07657778\n",
      "Iteration 6290, loss = 0.07657553\n",
      "Iteration 6291, loss = 0.07657326\n",
      "Iteration 6292, loss = 0.07657100\n",
      "Iteration 6293, loss = 0.07656874\n",
      "Iteration 6294, loss = 0.07656648\n",
      "Iteration 6295, loss = 0.07656421\n",
      "Iteration 6296, loss = 0.07656198\n",
      "Iteration 6297, loss = 0.07655968\n",
      "Iteration 6298, loss = 0.07655743\n",
      "Iteration 6299, loss = 0.07655518\n",
      "Iteration 6300, loss = 0.07655291\n",
      "Iteration 6301, loss = 0.07655065\n",
      "Iteration 6302, loss = 0.07654840\n",
      "Iteration 6303, loss = 0.07654613\n",
      "Iteration 6304, loss = 0.07654389\n",
      "Iteration 6305, loss = 0.07654163\n",
      "Iteration 6306, loss = 0.07653936\n",
      "Iteration 6307, loss = 0.07653710\n",
      "Iteration 6308, loss = 0.07653487\n",
      "Iteration 6309, loss = 0.07653258\n",
      "Iteration 6310, loss = 0.07653032\n",
      "Iteration 6311, loss = 0.07652809\n",
      "Iteration 6312, loss = 0.07652582\n",
      "Iteration 6313, loss = 0.07652356\n",
      "Iteration 6314, loss = 0.07652131\n",
      "Iteration 6315, loss = 0.07651905\n",
      "Iteration 6316, loss = 0.07651681\n",
      "Iteration 6317, loss = 0.07651455\n",
      "Iteration 6318, loss = 0.07651229\n",
      "Iteration 6319, loss = 0.07651003\n",
      "Iteration 6320, loss = 0.07650782\n",
      "Iteration 6321, loss = 0.07650552\n",
      "Iteration 6322, loss = 0.07650326\n",
      "Iteration 6323, loss = 0.07650104\n",
      "Iteration 6324, loss = 0.07649879\n",
      "Iteration 6325, loss = 0.07649654\n",
      "Iteration 6326, loss = 0.07649427\n",
      "Iteration 6327, loss = 0.07649203\n",
      "Iteration 6328, loss = 0.07648979\n",
      "Iteration 6329, loss = 0.07648751\n",
      "Iteration 6330, loss = 0.07648527\n",
      "Iteration 6331, loss = 0.07648304\n",
      "Iteration 6332, loss = 0.07648077\n",
      "Iteration 6333, loss = 0.07647852\n",
      "Iteration 6334, loss = 0.07647626\n",
      "Iteration 6335, loss = 0.07647404\n",
      "Iteration 6336, loss = 0.07647177\n",
      "Iteration 6337, loss = 0.07646952\n",
      "Iteration 6338, loss = 0.07646727\n",
      "Iteration 6339, loss = 0.07646504\n",
      "Iteration 6340, loss = 0.07646279\n",
      "Iteration 6341, loss = 0.07646054\n",
      "Iteration 6342, loss = 0.07645828\n",
      "Iteration 6343, loss = 0.07645606\n",
      "Iteration 6344, loss = 0.07645380\n",
      "Iteration 6345, loss = 0.07645155\n",
      "Iteration 6346, loss = 0.07644929\n",
      "Iteration 6347, loss = 0.07644708\n",
      "Iteration 6348, loss = 0.07644482\n",
      "Iteration 6349, loss = 0.07644257\n",
      "Iteration 6350, loss = 0.07644031\n",
      "Iteration 6351, loss = 0.07643811\n",
      "Iteration 6352, loss = 0.07643584\n",
      "Iteration 6353, loss = 0.07643359\n",
      "Iteration 6354, loss = 0.07643134\n",
      "Iteration 6355, loss = 0.07642914\n",
      "Iteration 6356, loss = 0.07642687\n",
      "Iteration 6357, loss = 0.07642462\n",
      "Iteration 6358, loss = 0.07642237\n",
      "Iteration 6359, loss = 0.07642016\n",
      "Iteration 6360, loss = 0.07641791\n",
      "Iteration 6361, loss = 0.07641566\n",
      "Iteration 6362, loss = 0.07641341\n",
      "Iteration 6363, loss = 0.07641118\n",
      "Iteration 6364, loss = 0.07640894\n",
      "Iteration 6365, loss = 0.07640670\n",
      "Iteration 6366, loss = 0.07640445\n",
      "Iteration 6367, loss = 0.07640224\n",
      "Iteration 6368, loss = 0.07639999\n",
      "Iteration 6369, loss = 0.07639774\n",
      "Iteration 6370, loss = 0.07639551\n",
      "Iteration 6371, loss = 0.07639327\n",
      "Iteration 6372, loss = 0.07639105\n",
      "Iteration 6373, loss = 0.07638879\n",
      "Iteration 6374, loss = 0.07638654\n",
      "Iteration 6375, loss = 0.07638435\n",
      "Iteration 6376, loss = 0.07638208\n",
      "Iteration 6377, loss = 0.07637984\n",
      "Iteration 6378, loss = 0.07637761\n",
      "Iteration 6379, loss = 0.07637537\n",
      "Iteration 6380, loss = 0.07637315\n",
      "Iteration 6381, loss = 0.07637091\n",
      "Iteration 6382, loss = 0.07636866\n",
      "Iteration 6383, loss = 0.07636644\n",
      "Iteration 6384, loss = 0.07636421\n",
      "Iteration 6385, loss = 0.07636197\n",
      "Iteration 6386, loss = 0.07635972\n",
      "Iteration 6387, loss = 0.07635752\n",
      "Iteration 6388, loss = 0.07635527\n",
      "Iteration 6389, loss = 0.07635303\n",
      "Iteration 6390, loss = 0.07635082\n",
      "Iteration 6391, loss = 0.07634858\n",
      "Iteration 6392, loss = 0.07634636\n",
      "Iteration 6393, loss = 0.07634411\n",
      "Iteration 6394, loss = 0.07634187\n",
      "Iteration 6395, loss = 0.07633967\n",
      "Iteration 6396, loss = 0.07633742\n",
      "Iteration 6397, loss = 0.07633518\n",
      "Iteration 6398, loss = 0.07633297\n",
      "Iteration 6399, loss = 0.07633073\n",
      "Iteration 6400, loss = 0.07632851\n",
      "Iteration 6401, loss = 0.07632628\n",
      "Iteration 6402, loss = 0.07632404\n",
      "Iteration 6403, loss = 0.07632185\n",
      "Iteration 6404, loss = 0.07631957\n",
      "Iteration 6405, loss = 0.07631737\n",
      "Iteration 6406, loss = 0.07631514\n",
      "Iteration 6407, loss = 0.07631291\n",
      "Iteration 6408, loss = 0.07631068\n",
      "Iteration 6409, loss = 0.07630844\n",
      "Iteration 6410, loss = 0.07630623\n",
      "Iteration 6411, loss = 0.07630399\n",
      "Iteration 6412, loss = 0.07630177\n",
      "Iteration 6413, loss = 0.07629953\n",
      "Iteration 6414, loss = 0.07629734\n",
      "Iteration 6415, loss = 0.07629509\n",
      "Iteration 6416, loss = 0.07629286\n",
      "Iteration 6417, loss = 0.07629065\n",
      "Iteration 6418, loss = 0.07628843\n",
      "Iteration 6419, loss = 0.07628620\n",
      "Iteration 6420, loss = 0.07628397\n",
      "Iteration 6421, loss = 0.07628174\n",
      "Iteration 6422, loss = 0.07627955\n",
      "Iteration 6423, loss = 0.07627730\n",
      "Iteration 6424, loss = 0.07627507\n",
      "Iteration 6425, loss = 0.07627287\n",
      "Iteration 6426, loss = 0.07627065\n",
      "Iteration 6427, loss = 0.07626842\n",
      "Iteration 6428, loss = 0.07626619\n",
      "Iteration 6429, loss = 0.07626399\n",
      "Iteration 6430, loss = 0.07626177\n",
      "Iteration 6431, loss = 0.07625954\n",
      "Iteration 6432, loss = 0.07625733\n",
      "Iteration 6433, loss = 0.07625511\n",
      "Iteration 6434, loss = 0.07625290\n",
      "Iteration 6435, loss = 0.07625068\n",
      "Iteration 6436, loss = 0.07624845\n",
      "Iteration 6437, loss = 0.07624622\n",
      "Iteration 6438, loss = 0.07624400\n",
      "Iteration 6439, loss = 0.07624176\n",
      "Iteration 6440, loss = 0.07623952\n",
      "Iteration 6441, loss = 0.07623731\n",
      "Iteration 6442, loss = 0.07623508\n",
      "Iteration 6443, loss = 0.07623284\n",
      "Iteration 6444, loss = 0.07623059\n",
      "Iteration 6445, loss = 0.07622841\n",
      "Iteration 6446, loss = 0.07622614\n",
      "Iteration 6447, loss = 0.07622390\n",
      "Iteration 6448, loss = 0.07622167\n",
      "Iteration 6449, loss = 0.07621947\n",
      "Iteration 6450, loss = 0.07621723\n",
      "Iteration 6451, loss = 0.07621499\n",
      "Iteration 6452, loss = 0.07621274\n",
      "Iteration 6453, loss = 0.07621054\n",
      "Iteration 6454, loss = 0.07620829\n",
      "Iteration 6455, loss = 0.07620605\n",
      "Iteration 6456, loss = 0.07620382\n",
      "Iteration 6457, loss = 0.07620162\n",
      "Iteration 6458, loss = 0.07619938\n",
      "Iteration 6459, loss = 0.07619714\n",
      "Iteration 6460, loss = 0.07619490\n",
      "Iteration 6461, loss = 0.07619271\n",
      "Iteration 6462, loss = 0.07619044\n",
      "Iteration 6463, loss = 0.07618821\n",
      "Iteration 6464, loss = 0.07618600\n",
      "Iteration 6465, loss = 0.07618378\n",
      "Iteration 6466, loss = 0.07618153\n",
      "Iteration 6467, loss = 0.07617930\n",
      "Iteration 6468, loss = 0.07617708\n",
      "Iteration 6469, loss = 0.07617486\n",
      "Iteration 6470, loss = 0.07617263\n",
      "Iteration 6471, loss = 0.07617039\n",
      "Iteration 6472, loss = 0.07616818\n",
      "Iteration 6473, loss = 0.07616594\n",
      "Iteration 6474, loss = 0.07616371\n",
      "Iteration 6475, loss = 0.07616147\n",
      "Iteration 6476, loss = 0.07615927\n",
      "Iteration 6477, loss = 0.07615704\n",
      "Iteration 6478, loss = 0.07615480\n",
      "Iteration 6479, loss = 0.07615257\n",
      "Iteration 6480, loss = 0.07615037\n",
      "Iteration 6481, loss = 0.07614816\n",
      "Iteration 6482, loss = 0.07614592\n",
      "Iteration 6483, loss = 0.07614369\n",
      "Iteration 6484, loss = 0.07614149\n",
      "Iteration 6485, loss = 0.07613927\n",
      "Iteration 6486, loss = 0.07613705\n",
      "Iteration 6487, loss = 0.07613483\n",
      "Iteration 6488, loss = 0.07613261\n",
      "Iteration 6489, loss = 0.07613039\n",
      "Iteration 6490, loss = 0.07612817\n",
      "Iteration 6491, loss = 0.07612594\n",
      "Iteration 6492, loss = 0.07612375\n",
      "Iteration 6493, loss = 0.07612154\n",
      "Iteration 6494, loss = 0.07611931\n",
      "Iteration 6495, loss = 0.07611708\n",
      "Iteration 6496, loss = 0.07611488\n",
      "Iteration 6497, loss = 0.07611267\n",
      "Iteration 6498, loss = 0.07611045\n",
      "Iteration 6499, loss = 0.07610823\n",
      "Iteration 6500, loss = 0.07610602\n",
      "Iteration 6501, loss = 0.07610381\n",
      "Iteration 6502, loss = 0.07610159\n",
      "Iteration 6503, loss = 0.07609938\n",
      "Iteration 6504, loss = 0.07609716\n",
      "Iteration 6505, loss = 0.07609495\n",
      "Iteration 6506, loss = 0.07609273\n",
      "Iteration 6507, loss = 0.07609055\n",
      "Iteration 6508, loss = 0.07608831\n",
      "Iteration 6509, loss = 0.07608610\n",
      "Iteration 6510, loss = 0.07608390\n",
      "Iteration 6511, loss = 0.07608169\n",
      "Iteration 6512, loss = 0.07607948\n",
      "Iteration 6513, loss = 0.07607727\n",
      "Iteration 6514, loss = 0.07607505\n",
      "Iteration 6515, loss = 0.07607287\n",
      "Iteration 6516, loss = 0.07607063\n",
      "Iteration 6517, loss = 0.07606842\n",
      "Iteration 6518, loss = 0.07606623\n",
      "Iteration 6519, loss = 0.07606403\n",
      "Iteration 6520, loss = 0.07606181\n",
      "Iteration 6521, loss = 0.07605960\n",
      "Iteration 6522, loss = 0.07605738\n",
      "Iteration 6523, loss = 0.07605521\n",
      "Iteration 6524, loss = 0.07605296\n",
      "Iteration 6525, loss = 0.07605076\n",
      "Iteration 6526, loss = 0.07604857\n",
      "Iteration 6527, loss = 0.07604637\n",
      "Iteration 6528, loss = 0.07604415\n",
      "Iteration 6529, loss = 0.07604194\n",
      "Iteration 6530, loss = 0.07603974\n",
      "Iteration 6531, loss = 0.07603754\n",
      "Iteration 6532, loss = 0.07603533\n",
      "Iteration 6533, loss = 0.07603311\n",
      "Iteration 6534, loss = 0.07603095\n",
      "Iteration 6535, loss = 0.07602872\n",
      "Iteration 6536, loss = 0.07602650\n",
      "Iteration 6537, loss = 0.07602432\n",
      "Iteration 6538, loss = 0.07602212\n",
      "Iteration 6539, loss = 0.07601991\n",
      "Iteration 6540, loss = 0.07601770\n",
      "Iteration 6541, loss = 0.07601549\n",
      "Iteration 6542, loss = 0.07601331\n",
      "Iteration 6543, loss = 0.07601111\n",
      "Iteration 6544, loss = 0.07600889\n",
      "Iteration 6545, loss = 0.07600669\n",
      "Iteration 6546, loss = 0.07600453\n",
      "Iteration 6547, loss = 0.07600230\n",
      "Iteration 6548, loss = 0.07600009\n",
      "Iteration 6549, loss = 0.07599791\n",
      "Iteration 6550, loss = 0.07599571\n",
      "Iteration 6551, loss = 0.07599351\n",
      "Iteration 6552, loss = 0.07599130\n",
      "Iteration 6553, loss = 0.07598912\n",
      "Iteration 6554, loss = 0.07598692\n",
      "Iteration 6555, loss = 0.07598470\n",
      "Iteration 6556, loss = 0.07598250\n",
      "Iteration 6557, loss = 0.07598033\n",
      "Iteration 6558, loss = 0.07597813\n",
      "Iteration 6559, loss = 0.07597592\n",
      "Iteration 6560, loss = 0.07597371\n",
      "Iteration 6561, loss = 0.07597156\n",
      "Iteration 6562, loss = 0.07596933\n",
      "Iteration 6563, loss = 0.07596713\n",
      "Iteration 6564, loss = 0.07596496\n",
      "Iteration 6565, loss = 0.07596275\n",
      "Iteration 6566, loss = 0.07596056\n",
      "Iteration 6567, loss = 0.07595836\n",
      "Iteration 6568, loss = 0.07595618\n",
      "Iteration 6569, loss = 0.07595397\n",
      "Iteration 6570, loss = 0.07595178\n",
      "Iteration 6571, loss = 0.07594958\n",
      "Iteration 6572, loss = 0.07594741\n",
      "Iteration 6573, loss = 0.07594520\n",
      "Iteration 6574, loss = 0.07594303\n",
      "Iteration 6575, loss = 0.07594081\n",
      "Iteration 6576, loss = 0.07593864\n",
      "Iteration 6577, loss = 0.07593647\n",
      "Iteration 6578, loss = 0.07593427\n",
      "Iteration 6579, loss = 0.07593207\n",
      "Iteration 6580, loss = 0.07592990\n",
      "Iteration 6581, loss = 0.07592768\n",
      "Iteration 6582, loss = 0.07592551\n",
      "Iteration 6583, loss = 0.07592332\n",
      "Iteration 6584, loss = 0.07592112\n",
      "Iteration 6585, loss = 0.07591896\n",
      "Iteration 6586, loss = 0.07591675\n",
      "Iteration 6587, loss = 0.07591457\n",
      "Iteration 6588, loss = 0.07591239\n",
      "Iteration 6589, loss = 0.07591020\n",
      "Iteration 6590, loss = 0.07590801\n",
      "Iteration 6591, loss = 0.07590584\n",
      "Iteration 6592, loss = 0.07590364\n",
      "Iteration 6593, loss = 0.07590145\n",
      "Iteration 6594, loss = 0.07589927\n",
      "Iteration 6595, loss = 0.07589711\n",
      "Iteration 6596, loss = 0.07589490\n",
      "Iteration 6597, loss = 0.07589271\n",
      "Iteration 6598, loss = 0.07589055\n",
      "Iteration 6599, loss = 0.07588837\n",
      "Iteration 6600, loss = 0.07588618\n",
      "Iteration 6601, loss = 0.07588399\n",
      "Iteration 6602, loss = 0.07588180\n",
      "Iteration 6603, loss = 0.07587966\n",
      "Iteration 6604, loss = 0.07587744\n",
      "Iteration 6605, loss = 0.07587525\n",
      "Iteration 6606, loss = 0.07587311\n",
      "Iteration 6607, loss = 0.07587092\n",
      "Iteration 6608, loss = 0.07586873\n",
      "Iteration 6609, loss = 0.07586654\n",
      "Iteration 6610, loss = 0.07586436\n",
      "Iteration 6611, loss = 0.07586220\n",
      "Iteration 6612, loss = 0.07586001\n",
      "Iteration 6613, loss = 0.07585782\n",
      "Iteration 6614, loss = 0.07585567\n",
      "Iteration 6615, loss = 0.07585349\n",
      "Iteration 6616, loss = 0.07585130\n",
      "Iteration 6617, loss = 0.07584913\n",
      "Iteration 6618, loss = 0.07584698\n",
      "Iteration 6619, loss = 0.07584480\n",
      "Iteration 6620, loss = 0.07584262\n",
      "Iteration 6621, loss = 0.07584043\n",
      "Iteration 6622, loss = 0.07583830\n",
      "Iteration 6623, loss = 0.07583609\n",
      "Iteration 6624, loss = 0.07583392\n",
      "Iteration 6625, loss = 0.07583177\n",
      "Iteration 6626, loss = 0.07582961\n",
      "Iteration 6627, loss = 0.07582743\n",
      "Iteration 6628, loss = 0.07582525\n",
      "Iteration 6629, loss = 0.07582307\n",
      "Iteration 6630, loss = 0.07582092\n",
      "Iteration 6631, loss = 0.07581875\n",
      "Iteration 6632, loss = 0.07581656\n",
      "Iteration 6633, loss = 0.07581442\n",
      "Iteration 6634, loss = 0.07581224\n",
      "Iteration 6635, loss = 0.07581006\n",
      "Iteration 6636, loss = 0.07580789\n",
      "Iteration 6637, loss = 0.07580574\n",
      "Iteration 6638, loss = 0.07580358\n",
      "Iteration 6639, loss = 0.07580140\n",
      "Iteration 6640, loss = 0.07579922\n",
      "Iteration 6641, loss = 0.07579708\n",
      "Iteration 6642, loss = 0.07579491\n",
      "Iteration 6643, loss = 0.07579273\n",
      "Iteration 6644, loss = 0.07579056\n",
      "Iteration 6645, loss = 0.07578842\n",
      "Iteration 6646, loss = 0.07578625\n",
      "Iteration 6647, loss = 0.07578407\n",
      "Iteration 6648, loss = 0.07578192\n",
      "Iteration 6649, loss = 0.07577976\n",
      "Iteration 6650, loss = 0.07577758\n",
      "Iteration 6651, loss = 0.07577542\n",
      "Iteration 6652, loss = 0.07577326\n",
      "Iteration 6653, loss = 0.07577111\n",
      "Iteration 6654, loss = 0.07576895\n",
      "Iteration 6655, loss = 0.07576677\n",
      "Iteration 6656, loss = 0.07576463\n",
      "Iteration 6657, loss = 0.07576244\n",
      "Iteration 6658, loss = 0.07576028\n",
      "Iteration 6659, loss = 0.07575814\n",
      "Iteration 6660, loss = 0.07575598\n",
      "Iteration 6661, loss = 0.07575382\n",
      "Iteration 6662, loss = 0.07575165\n",
      "Iteration 6663, loss = 0.07574947\n",
      "Iteration 6664, loss = 0.07574736\n",
      "Iteration 6665, loss = 0.07574517\n",
      "Iteration 6666, loss = 0.07574303\n",
      "Iteration 6667, loss = 0.07574087\n",
      "Iteration 6668, loss = 0.07573871\n",
      "Iteration 6669, loss = 0.07573656\n",
      "Iteration 6670, loss = 0.07573438\n",
      "Iteration 6671, loss = 0.07573223\n",
      "Iteration 6672, loss = 0.07573006\n",
      "Iteration 6673, loss = 0.07572791\n",
      "Iteration 6674, loss = 0.07572575\n",
      "Iteration 6675, loss = 0.07572360\n",
      "Iteration 6676, loss = 0.07572143\n",
      "Iteration 6677, loss = 0.07571928\n",
      "Iteration 6678, loss = 0.07571714\n",
      "Iteration 6679, loss = 0.07571498\n",
      "Iteration 6680, loss = 0.07571281\n",
      "Iteration 6681, loss = 0.07571066\n",
      "Iteration 6682, loss = 0.07570853\n",
      "Iteration 6683, loss = 0.07570635\n",
      "Iteration 6684, loss = 0.07570419\n",
      "Iteration 6685, loss = 0.07570205\n",
      "Iteration 6686, loss = 0.07569991\n",
      "Iteration 6687, loss = 0.07569775\n",
      "Iteration 6688, loss = 0.07569558\n",
      "Iteration 6689, loss = 0.07569343\n",
      "Iteration 6690, loss = 0.07569129\n",
      "Iteration 6691, loss = 0.07568914\n",
      "Iteration 6692, loss = 0.07568698\n",
      "Iteration 6693, loss = 0.07568483\n",
      "Iteration 6694, loss = 0.07568269\n",
      "Iteration 6695, loss = 0.07568053\n",
      "Iteration 6696, loss = 0.07567836\n",
      "Iteration 6697, loss = 0.07567626\n",
      "Iteration 6698, loss = 0.07567407\n",
      "Iteration 6699, loss = 0.07567191\n",
      "Iteration 6700, loss = 0.07566978\n",
      "Iteration 6701, loss = 0.07566764\n",
      "Iteration 6702, loss = 0.07566549\n",
      "Iteration 6703, loss = 0.07566332\n",
      "Iteration 6704, loss = 0.07566117\n",
      "Iteration 6705, loss = 0.07565907\n",
      "Iteration 6706, loss = 0.07565688\n",
      "Iteration 6707, loss = 0.07565473\n",
      "Iteration 6708, loss = 0.07565260\n",
      "Iteration 6709, loss = 0.07565046\n",
      "Iteration 6710, loss = 0.07564831\n",
      "Iteration 6711, loss = 0.07564614\n",
      "Iteration 6712, loss = 0.07564400\n",
      "Iteration 6713, loss = 0.07564186\n",
      "Iteration 6714, loss = 0.07563971\n",
      "Iteration 6715, loss = 0.07563755\n",
      "Iteration 6716, loss = 0.07563542\n",
      "Iteration 6717, loss = 0.07563328\n",
      "Iteration 6718, loss = 0.07563113\n",
      "Iteration 6719, loss = 0.07562896\n",
      "Iteration 6720, loss = 0.07562686\n",
      "Iteration 6721, loss = 0.07562468\n",
      "Iteration 6722, loss = 0.07562254\n",
      "Iteration 6723, loss = 0.07562041\n",
      "Iteration 6724, loss = 0.07561826\n",
      "Iteration 6725, loss = 0.07561612\n",
      "Iteration 6726, loss = 0.07561396\n",
      "Iteration 6727, loss = 0.07561182\n",
      "Iteration 6728, loss = 0.07560968\n",
      "Iteration 6729, loss = 0.07560753\n",
      "Iteration 6730, loss = 0.07560538\n",
      "Iteration 6731, loss = 0.07560326\n",
      "Iteration 6732, loss = 0.07560112\n",
      "Iteration 6733, loss = 0.07559897\n",
      "Iteration 6734, loss = 0.07559681\n",
      "Iteration 6735, loss = 0.07559471\n",
      "Iteration 6736, loss = 0.07559255\n",
      "Iteration 6737, loss = 0.07559041\n",
      "Iteration 6738, loss = 0.07558827\n",
      "Iteration 6739, loss = 0.07558613\n",
      "Iteration 6740, loss = 0.07558400\n",
      "Iteration 6741, loss = 0.07558185\n",
      "Iteration 6742, loss = 0.07557972\n",
      "Iteration 6743, loss = 0.07557757\n",
      "Iteration 6744, loss = 0.07557543\n",
      "Iteration 6745, loss = 0.07557330\n",
      "Iteration 6746, loss = 0.07557116\n",
      "Iteration 6747, loss = 0.07556902\n",
      "Iteration 6748, loss = 0.07556689\n",
      "Iteration 6749, loss = 0.07556475\n",
      "Iteration 6750, loss = 0.07556262\n",
      "Iteration 6751, loss = 0.07556048\n",
      "Iteration 6752, loss = 0.07555834\n",
      "Iteration 6753, loss = 0.07555621\n",
      "Iteration 6754, loss = 0.07555407\n",
      "Iteration 6755, loss = 0.07555193\n",
      "Iteration 6756, loss = 0.07554980\n",
      "Iteration 6757, loss = 0.07554768\n",
      "Iteration 6758, loss = 0.07554554\n",
      "Iteration 6759, loss = 0.07554339\n",
      "Iteration 6760, loss = 0.07554127\n",
      "Iteration 6761, loss = 0.07553915\n",
      "Iteration 6762, loss = 0.07553698\n",
      "Iteration 6763, loss = 0.07553487\n",
      "Iteration 6764, loss = 0.07553275\n",
      "Iteration 6765, loss = 0.07553061\n",
      "Iteration 6766, loss = 0.07552847\n",
      "Iteration 6767, loss = 0.07552633\n",
      "Iteration 6768, loss = 0.07552421\n",
      "Iteration 6769, loss = 0.07552208\n",
      "Iteration 6770, loss = 0.07551994\n",
      "Iteration 6771, loss = 0.07551780\n",
      "Iteration 6772, loss = 0.07551570\n",
      "Iteration 6773, loss = 0.07551356\n",
      "Iteration 6774, loss = 0.07551142\n",
      "Iteration 6775, loss = 0.07550928\n",
      "Iteration 6776, loss = 0.07550718\n",
      "Iteration 6777, loss = 0.07550504\n",
      "Iteration 6778, loss = 0.07550290\n",
      "Iteration 6779, loss = 0.07550078\n",
      "Iteration 6780, loss = 0.07549866\n",
      "Iteration 6781, loss = 0.07549652\n",
      "Iteration 6782, loss = 0.07549438\n",
      "Iteration 6783, loss = 0.07549230\n",
      "Iteration 6784, loss = 0.07549013\n",
      "Iteration 6785, loss = 0.07548800\n",
      "Iteration 6786, loss = 0.07548590\n",
      "Iteration 6787, loss = 0.07548379\n",
      "Iteration 6788, loss = 0.07548165\n",
      "Iteration 6789, loss = 0.07547951\n",
      "Iteration 6790, loss = 0.07547737\n",
      "Iteration 6791, loss = 0.07547530\n",
      "Iteration 6792, loss = 0.07547313\n",
      "Iteration 6793, loss = 0.07547102\n",
      "Iteration 6794, loss = 0.07546889\n",
      "Iteration 6795, loss = 0.07546677\n",
      "Iteration 6796, loss = 0.07546464\n",
      "Iteration 6797, loss = 0.07546250\n",
      "Iteration 6798, loss = 0.07546041\n",
      "Iteration 6799, loss = 0.07545827\n",
      "Iteration 6800, loss = 0.07545615\n",
      "Iteration 6801, loss = 0.07545404\n",
      "Iteration 6802, loss = 0.07545191\n",
      "Iteration 6803, loss = 0.07544980\n",
      "Iteration 6804, loss = 0.07544766\n",
      "Iteration 6805, loss = 0.07544553\n",
      "Iteration 6806, loss = 0.07544343\n",
      "Iteration 6807, loss = 0.07544130\n",
      "Iteration 6808, loss = 0.07543917\n",
      "Iteration 6809, loss = 0.07543709\n",
      "Iteration 6810, loss = 0.07543492\n",
      "Iteration 6811, loss = 0.07543281\n",
      "Iteration 6812, loss = 0.07543072\n",
      "Iteration 6813, loss = 0.07542859\n",
      "Iteration 6814, loss = 0.07542647\n",
      "Iteration 6815, loss = 0.07542435\n",
      "Iteration 6816, loss = 0.07542222\n",
      "Iteration 6817, loss = 0.07542012\n",
      "Iteration 6818, loss = 0.07541799\n",
      "Iteration 6819, loss = 0.07541587\n",
      "Iteration 6820, loss = 0.07541378\n",
      "Iteration 6821, loss = 0.07541163\n",
      "Iteration 6822, loss = 0.07540951\n",
      "Iteration 6823, loss = 0.07540742\n",
      "Iteration 6824, loss = 0.07540529\n",
      "Iteration 6825, loss = 0.07540317\n",
      "Iteration 6826, loss = 0.07540106\n",
      "Iteration 6827, loss = 0.07539896\n",
      "Iteration 6828, loss = 0.07539683\n",
      "Iteration 6829, loss = 0.07539470\n",
      "Iteration 6830, loss = 0.07539261\n",
      "Iteration 6831, loss = 0.07539050\n",
      "Iteration 6832, loss = 0.07538838\n",
      "Iteration 6833, loss = 0.07538626\n",
      "Iteration 6834, loss = 0.07538414\n",
      "Iteration 6835, loss = 0.07538207\n",
      "Iteration 6836, loss = 0.07537991\n",
      "Iteration 6837, loss = 0.07537780\n",
      "Iteration 6838, loss = 0.07537571\n",
      "Iteration 6839, loss = 0.07537360\n",
      "Iteration 6840, loss = 0.07537148\n",
      "Iteration 6841, loss = 0.07536936\n",
      "Iteration 6842, loss = 0.07536727\n",
      "Iteration 6843, loss = 0.07536516\n",
      "Iteration 6844, loss = 0.07536304\n",
      "Iteration 6845, loss = 0.07536093\n",
      "Iteration 6846, loss = 0.07535885\n",
      "Iteration 6847, loss = 0.07535672\n",
      "Iteration 6848, loss = 0.07535460\n",
      "Iteration 6849, loss = 0.07535252\n",
      "Iteration 6850, loss = 0.07535041\n",
      "Iteration 6851, loss = 0.07534830\n",
      "Iteration 6852, loss = 0.07534618\n",
      "Iteration 6853, loss = 0.07534412\n",
      "Iteration 6854, loss = 0.07534198\n",
      "Iteration 6855, loss = 0.07533987\n",
      "Iteration 6856, loss = 0.07533778\n",
      "Iteration 6857, loss = 0.07533568\n",
      "Iteration 6858, loss = 0.07533357\n",
      "Iteration 6859, loss = 0.07533146\n",
      "Iteration 6860, loss = 0.07532936\n",
      "Iteration 6861, loss = 0.07532727\n",
      "Iteration 6862, loss = 0.07532515\n",
      "Iteration 6863, loss = 0.07532304\n",
      "Iteration 6864, loss = 0.07532097\n",
      "Iteration 6865, loss = 0.07531885\n",
      "Iteration 6866, loss = 0.07531674\n",
      "Iteration 6867, loss = 0.07531465\n",
      "Iteration 6868, loss = 0.07531254\n",
      "Iteration 6869, loss = 0.07531045\n",
      "Iteration 6870, loss = 0.07530834\n",
      "Iteration 6871, loss = 0.07530625\n",
      "Iteration 6872, loss = 0.07530414\n",
      "Iteration 6873, loss = 0.07530205\n",
      "Iteration 6874, loss = 0.07529993\n",
      "Iteration 6875, loss = 0.07529787\n",
      "Iteration 6876, loss = 0.07529574\n",
      "Iteration 6877, loss = 0.07529364\n",
      "Iteration 6878, loss = 0.07529157\n",
      "Iteration 6879, loss = 0.07528945\n",
      "Iteration 6880, loss = 0.07528735\n",
      "Iteration 6881, loss = 0.07528527\n",
      "Iteration 6882, loss = 0.07528317\n",
      "Iteration 6883, loss = 0.07528106\n",
      "Iteration 6884, loss = 0.07527897\n",
      "Iteration 6885, loss = 0.07527687\n",
      "Iteration 6886, loss = 0.07527480\n",
      "Iteration 6887, loss = 0.07527269\n",
      "Iteration 6888, loss = 0.07527058\n",
      "Iteration 6889, loss = 0.07526849\n",
      "Iteration 6890, loss = 0.07526641\n",
      "Iteration 6891, loss = 0.07526429\n",
      "Iteration 6892, loss = 0.07526220\n",
      "Iteration 6893, loss = 0.07526013\n",
      "Iteration 6894, loss = 0.07525804\n",
      "Iteration 6895, loss = 0.07525593\n",
      "Iteration 6896, loss = 0.07525382\n",
      "Iteration 6897, loss = 0.07525175\n",
      "Iteration 6898, loss = 0.07524965\n",
      "Iteration 6899, loss = 0.07524754\n",
      "Iteration 6900, loss = 0.07524548\n",
      "Iteration 6901, loss = 0.07524337\n",
      "Iteration 6902, loss = 0.07524128\n",
      "Iteration 6903, loss = 0.07523918\n",
      "Iteration 6904, loss = 0.07523711\n",
      "Iteration 6905, loss = 0.07523501\n",
      "Iteration 6906, loss = 0.07523291\n",
      "Iteration 6907, loss = 0.07523081\n",
      "Iteration 6908, loss = 0.07522876\n",
      "Iteration 6909, loss = 0.07522666\n",
      "Iteration 6910, loss = 0.07522456\n",
      "Iteration 6911, loss = 0.07522247\n",
      "Iteration 6912, loss = 0.07522040\n",
      "Iteration 6913, loss = 0.07521831\n",
      "Iteration 6914, loss = 0.07521620\n",
      "Iteration 6915, loss = 0.07521413\n",
      "Iteration 6916, loss = 0.07521204\n",
      "Iteration 6917, loss = 0.07520995\n",
      "Iteration 6918, loss = 0.07520786\n",
      "Iteration 6919, loss = 0.07520579\n",
      "Iteration 6920, loss = 0.07520371\n",
      "Iteration 6921, loss = 0.07520162\n",
      "Iteration 6922, loss = 0.07519952\n",
      "Iteration 6923, loss = 0.07519746\n",
      "Iteration 6924, loss = 0.07519537\n",
      "Iteration 6925, loss = 0.07519327\n",
      "Iteration 6926, loss = 0.07519120\n",
      "Iteration 6927, loss = 0.07518913\n",
      "Iteration 6928, loss = 0.07518703\n",
      "Iteration 6929, loss = 0.07518495\n",
      "Iteration 6930, loss = 0.07518288\n",
      "Iteration 6931, loss = 0.07518080\n",
      "Iteration 6932, loss = 0.07517871\n",
      "Iteration 6933, loss = 0.07517663\n",
      "Iteration 6934, loss = 0.07517455\n",
      "Iteration 6935, loss = 0.07517248\n",
      "Iteration 6936, loss = 0.07517038\n",
      "Iteration 6937, loss = 0.07516833\n",
      "Iteration 6938, loss = 0.07516623\n",
      "Iteration 6939, loss = 0.07516414\n",
      "Iteration 6940, loss = 0.07516209\n",
      "Iteration 6941, loss = 0.07515999\n",
      "Iteration 6942, loss = 0.07515791\n",
      "Iteration 6943, loss = 0.07515583\n",
      "Iteration 6944, loss = 0.07515378\n",
      "Iteration 6945, loss = 0.07515168\n",
      "Iteration 6946, loss = 0.07514960\n",
      "Iteration 6947, loss = 0.07514753\n",
      "Iteration 6948, loss = 0.07514546\n",
      "Iteration 6949, loss = 0.07514338\n",
      "Iteration 6950, loss = 0.07514131\n",
      "Iteration 6951, loss = 0.07513922\n",
      "Iteration 6952, loss = 0.07513716\n",
      "Iteration 6953, loss = 0.07513507\n",
      "Iteration 6954, loss = 0.07513301\n",
      "Iteration 6955, loss = 0.07513094\n",
      "Iteration 6956, loss = 0.07512885\n",
      "Iteration 6957, loss = 0.07512677\n",
      "Iteration 6958, loss = 0.07512471\n",
      "Iteration 6959, loss = 0.07512264\n",
      "Iteration 6960, loss = 0.07512055\n",
      "Iteration 6961, loss = 0.07511846\n",
      "Iteration 6962, loss = 0.07511642\n",
      "Iteration 6963, loss = 0.07511432\n",
      "Iteration 6964, loss = 0.07511224\n",
      "Iteration 6965, loss = 0.07511017\n",
      "Iteration 6966, loss = 0.07510812\n",
      "Iteration 6967, loss = 0.07510602\n",
      "Iteration 6968, loss = 0.07510393\n",
      "Iteration 6969, loss = 0.07510190\n",
      "Iteration 6970, loss = 0.07509981\n",
      "Iteration 6971, loss = 0.07509773\n",
      "Iteration 6972, loss = 0.07509564\n",
      "Iteration 6973, loss = 0.07509361\n",
      "Iteration 6974, loss = 0.07509151\n",
      "Iteration 6975, loss = 0.07508943\n",
      "Iteration 6976, loss = 0.07508736\n",
      "Iteration 6977, loss = 0.07508532\n",
      "Iteration 6978, loss = 0.07508323\n",
      "Iteration 6979, loss = 0.07508115\n",
      "Iteration 6980, loss = 0.07507908\n",
      "Iteration 6981, loss = 0.07507702\n",
      "Iteration 6982, loss = 0.07507495\n",
      "Iteration 6983, loss = 0.07507286\n",
      "Iteration 6984, loss = 0.07507082\n",
      "Iteration 6985, loss = 0.07506873\n",
      "Iteration 6986, loss = 0.07506666\n",
      "Iteration 6987, loss = 0.07506460\n",
      "Iteration 6988, loss = 0.07506253\n",
      "Iteration 6989, loss = 0.07506046\n",
      "Iteration 6990, loss = 0.07505838\n",
      "Iteration 6991, loss = 0.07505633\n",
      "Iteration 6992, loss = 0.07505425\n",
      "Iteration 6993, loss = 0.07505218\n",
      "Iteration 6994, loss = 0.07505012\n",
      "Iteration 6995, loss = 0.07504805\n",
      "Iteration 6996, loss = 0.07504598\n",
      "Iteration 6997, loss = 0.07504392\n",
      "Iteration 6998, loss = 0.07504185\n",
      "Iteration 6999, loss = 0.07503979\n",
      "Iteration 7000, loss = 0.07503772\n",
      "Iteration 7001, loss = 0.07503565\n",
      "Iteration 7002, loss = 0.07503361\n",
      "Iteration 7003, loss = 0.07503151\n",
      "Iteration 7004, loss = 0.07502946\n",
      "Iteration 7005, loss = 0.07502742\n",
      "Iteration 7006, loss = 0.07502534\n",
      "Iteration 7007, loss = 0.07502326\n",
      "Iteration 7008, loss = 0.07502120\n",
      "Iteration 7009, loss = 0.07501916\n",
      "Iteration 7010, loss = 0.07501707\n",
      "Iteration 7011, loss = 0.07501501\n",
      "Iteration 7012, loss = 0.07501297\n",
      "Iteration 7013, loss = 0.07501090\n",
      "Iteration 7014, loss = 0.07500884\n",
      "Iteration 7015, loss = 0.07500676\n",
      "Iteration 7016, loss = 0.07500472\n",
      "Iteration 7017, loss = 0.07500265\n",
      "Iteration 7018, loss = 0.07500059\n",
      "Iteration 7019, loss = 0.07499852\n",
      "Iteration 7020, loss = 0.07499649\n",
      "Iteration 7021, loss = 0.07499441\n",
      "Iteration 7022, loss = 0.07499233\n",
      "Iteration 7023, loss = 0.07499031\n",
      "Iteration 7024, loss = 0.07498823\n",
      "Iteration 7025, loss = 0.07498617\n",
      "Iteration 7026, loss = 0.07498409\n",
      "Iteration 7027, loss = 0.07498209\n",
      "Iteration 7028, loss = 0.07498000\n",
      "Iteration 7029, loss = 0.07497793\n",
      "Iteration 7030, loss = 0.07497588\n",
      "Iteration 7031, loss = 0.07497384\n",
      "Iteration 7032, loss = 0.07497176\n",
      "Iteration 7033, loss = 0.07496971\n",
      "Iteration 7034, loss = 0.07496767\n",
      "Iteration 7035, loss = 0.07496560\n",
      "Iteration 7036, loss = 0.07496354\n",
      "Iteration 7037, loss = 0.07496147\n",
      "Iteration 7038, loss = 0.07495945\n",
      "Iteration 7039, loss = 0.07495739\n",
      "Iteration 7040, loss = 0.07495533\n",
      "Iteration 7041, loss = 0.07495325\n",
      "Iteration 7042, loss = 0.07495123\n",
      "Iteration 7043, loss = 0.07494916\n",
      "Iteration 7044, loss = 0.07494710\n",
      "Iteration 7045, loss = 0.07494506\n",
      "Iteration 7046, loss = 0.07494301\n",
      "Iteration 7047, loss = 0.07494093\n",
      "Iteration 7048, loss = 0.07493890\n",
      "Iteration 7049, loss = 0.07493684\n",
      "Iteration 7050, loss = 0.07493480\n",
      "Iteration 7051, loss = 0.07493273\n",
      "Iteration 7052, loss = 0.07493070\n",
      "Iteration 7053, loss = 0.07492862\n",
      "Iteration 7054, loss = 0.07492659\n",
      "Iteration 7055, loss = 0.07492453\n",
      "Iteration 7056, loss = 0.07492248\n",
      "Iteration 7057, loss = 0.07492045\n",
      "Iteration 7058, loss = 0.07491838\n",
      "Iteration 7059, loss = 0.07491634\n",
      "Iteration 7060, loss = 0.07491428\n",
      "Iteration 7061, loss = 0.07491223\n",
      "Iteration 7062, loss = 0.07491018\n",
      "Iteration 7063, loss = 0.07490816\n",
      "Iteration 7064, loss = 0.07490608\n",
      "Iteration 7065, loss = 0.07490402\n",
      "Iteration 7066, loss = 0.07490201\n",
      "Iteration 7067, loss = 0.07489995\n",
      "Iteration 7068, loss = 0.07489790\n",
      "Iteration 7069, loss = 0.07489585\n",
      "Iteration 7070, loss = 0.07489380\n",
      "Iteration 7071, loss = 0.07489175\n",
      "Iteration 7072, loss = 0.07488970\n",
      "Iteration 7073, loss = 0.07488765\n",
      "Iteration 7074, loss = 0.07488562\n",
      "Iteration 7075, loss = 0.07488356\n",
      "Iteration 7076, loss = 0.07488151\n",
      "Iteration 7077, loss = 0.07487949\n",
      "Iteration 7078, loss = 0.07487743\n",
      "Iteration 7079, loss = 0.07487537\n",
      "Iteration 7080, loss = 0.07487334\n",
      "Iteration 7081, loss = 0.07487130\n",
      "Iteration 7082, loss = 0.07486925\n",
      "Iteration 7083, loss = 0.07486720\n",
      "Iteration 7084, loss = 0.07486517\n",
      "Iteration 7085, loss = 0.07486312\n",
      "Iteration 7086, loss = 0.07486107\n",
      "Iteration 7087, loss = 0.07485902\n",
      "Iteration 7088, loss = 0.07485701\n",
      "Iteration 7089, loss = 0.07485496\n",
      "Iteration 7090, loss = 0.07485290\n",
      "Iteration 7091, loss = 0.07485085\n",
      "Iteration 7092, loss = 0.07484883\n",
      "Iteration 7093, loss = 0.07484678\n",
      "Iteration 7094, loss = 0.07484472\n",
      "Iteration 7095, loss = 0.07484273\n",
      "Iteration 7096, loss = 0.07484065\n",
      "Iteration 7097, loss = 0.07483860\n",
      "Iteration 7098, loss = 0.07483659\n",
      "Iteration 7099, loss = 0.07483454\n",
      "Iteration 7100, loss = 0.07483249\n",
      "Iteration 7101, loss = 0.07483044\n",
      "Iteration 7102, loss = 0.07482843\n",
      "Iteration 7103, loss = 0.07482640\n",
      "Iteration 7104, loss = 0.07482434\n",
      "Iteration 7105, loss = 0.07482228\n",
      "Iteration 7106, loss = 0.07482029\n",
      "Iteration 7107, loss = 0.07481822\n",
      "Iteration 7108, loss = 0.07481620\n",
      "Iteration 7109, loss = 0.07481415\n",
      "Iteration 7110, loss = 0.07481212\n",
      "Iteration 7111, loss = 0.07481009\n",
      "Iteration 7112, loss = 0.07480802\n",
      "Iteration 7113, loss = 0.07480602\n",
      "Iteration 7114, loss = 0.07480397\n",
      "Iteration 7115, loss = 0.07480193\n",
      "Iteration 7116, loss = 0.07479990\n",
      "Iteration 7117, loss = 0.07479787\n",
      "Iteration 7118, loss = 0.07479583\n",
      "Iteration 7119, loss = 0.07479378\n",
      "Iteration 7120, loss = 0.07479178\n",
      "Iteration 7121, loss = 0.07478971\n",
      "Iteration 7122, loss = 0.07478769\n",
      "Iteration 7123, loss = 0.07478567\n",
      "Iteration 7124, loss = 0.07478362\n",
      "Iteration 7125, loss = 0.07478160\n",
      "Iteration 7126, loss = 0.07477955\n",
      "Iteration 7127, loss = 0.07477754\n",
      "Iteration 7128, loss = 0.07477548\n",
      "Iteration 7129, loss = 0.07477343\n",
      "Iteration 7130, loss = 0.07477146\n",
      "Iteration 7131, loss = 0.07476938\n",
      "Iteration 7132, loss = 0.07476736\n",
      "Iteration 7133, loss = 0.07476533\n",
      "Iteration 7134, loss = 0.07476332\n",
      "Iteration 7135, loss = 0.07476128\n",
      "Iteration 7136, loss = 0.07475924\n",
      "Iteration 7137, loss = 0.07475719\n",
      "Iteration 7138, loss = 0.07475519\n",
      "Iteration 7139, loss = 0.07475315\n",
      "Iteration 7140, loss = 0.07475110\n",
      "Iteration 7141, loss = 0.07474911\n",
      "Iteration 7142, loss = 0.07474705\n",
      "Iteration 7143, loss = 0.07474502\n",
      "Iteration 7144, loss = 0.07474301\n",
      "Iteration 7145, loss = 0.07474098\n",
      "Iteration 7146, loss = 0.07473894\n",
      "Iteration 7147, loss = 0.07473691\n",
      "Iteration 7148, loss = 0.07473489\n",
      "Iteration 7149, loss = 0.07473288\n",
      "Iteration 7150, loss = 0.07473084\n",
      "Iteration 7151, loss = 0.07472880\n",
      "Iteration 7152, loss = 0.07472682\n",
      "Iteration 7153, loss = 0.07472477\n",
      "Iteration 7154, loss = 0.07472276\n",
      "Iteration 7155, loss = 0.07472073\n",
      "Iteration 7156, loss = 0.07471872\n",
      "Iteration 7157, loss = 0.07471669\n",
      "Iteration 7158, loss = 0.07471466\n",
      "Iteration 7159, loss = 0.07471265\n",
      "Iteration 7160, loss = 0.07471065\n",
      "Iteration 7161, loss = 0.07470862\n",
      "Iteration 7162, loss = 0.07470661\n",
      "Iteration 7163, loss = 0.07470457\n",
      "Iteration 7164, loss = 0.07470257\n",
      "Iteration 7165, loss = 0.07470055\n",
      "Iteration 7166, loss = 0.07469852\n",
      "Iteration 7167, loss = 0.07469652\n",
      "Iteration 7168, loss = 0.07469449\n",
      "Iteration 7169, loss = 0.07469249\n",
      "Iteration 7170, loss = 0.07469046\n",
      "Iteration 7171, loss = 0.07468845\n",
      "Iteration 7172, loss = 0.07468644\n",
      "Iteration 7173, loss = 0.07468442\n",
      "Iteration 7174, loss = 0.07468240\n",
      "Iteration 7175, loss = 0.07468039\n",
      "Iteration 7176, loss = 0.07467840\n",
      "Iteration 7177, loss = 0.07467637\n",
      "Iteration 7178, loss = 0.07467434\n",
      "Iteration 7179, loss = 0.07467236\n",
      "Iteration 7180, loss = 0.07467033\n",
      "Iteration 7181, loss = 0.07466831\n",
      "Iteration 7182, loss = 0.07466630\n",
      "Iteration 7183, loss = 0.07466431\n",
      "Iteration 7184, loss = 0.07466228\n",
      "Iteration 7185, loss = 0.07466026\n",
      "Iteration 7186, loss = 0.07465827\n",
      "Iteration 7187, loss = 0.07465626\n",
      "Iteration 7188, loss = 0.07465424\n",
      "Iteration 7189, loss = 0.07465221\n",
      "Iteration 7190, loss = 0.07465024\n",
      "Iteration 7191, loss = 0.07464822\n",
      "Iteration 7192, loss = 0.07464620\n",
      "Iteration 7193, loss = 0.07464419\n",
      "Iteration 7194, loss = 0.07464221\n",
      "Iteration 7195, loss = 0.07464018\n",
      "Iteration 7196, loss = 0.07463816\n",
      "Iteration 7197, loss = 0.07463617\n",
      "Iteration 7198, loss = 0.07463416\n",
      "Iteration 7199, loss = 0.07463214\n",
      "Iteration 7200, loss = 0.07463013\n",
      "Iteration 7201, loss = 0.07462814\n",
      "Iteration 7202, loss = 0.07462613\n",
      "Iteration 7203, loss = 0.07462411\n",
      "Iteration 7204, loss = 0.07462211\n",
      "Iteration 7205, loss = 0.07462013\n",
      "Iteration 7206, loss = 0.07461809\n",
      "Iteration 7207, loss = 0.07461609\n",
      "Iteration 7208, loss = 0.07461410\n",
      "Iteration 7209, loss = 0.07461210\n",
      "Iteration 7210, loss = 0.07461007\n",
      "Iteration 7211, loss = 0.07460809\n",
      "Iteration 7212, loss = 0.07460607\n",
      "Iteration 7213, loss = 0.07460407\n",
      "Iteration 7214, loss = 0.07460207\n",
      "Iteration 7215, loss = 0.07460007\n",
      "Iteration 7216, loss = 0.07459806\n",
      "Iteration 7217, loss = 0.07459605\n",
      "Iteration 7218, loss = 0.07459406\n",
      "Iteration 7219, loss = 0.07459205\n",
      "Iteration 7220, loss = 0.07459005\n",
      "Iteration 7221, loss = 0.07458804\n",
      "Iteration 7222, loss = 0.07458605\n",
      "Iteration 7223, loss = 0.07458404\n",
      "Iteration 7224, loss = 0.07458205\n",
      "Iteration 7225, loss = 0.07458005\n",
      "Iteration 7226, loss = 0.07457805\n",
      "Iteration 7227, loss = 0.07457603\n",
      "Iteration 7228, loss = 0.07457406\n",
      "Iteration 7229, loss = 0.07457205\n",
      "Iteration 7230, loss = 0.07457004\n",
      "Iteration 7231, loss = 0.07456805\n",
      "Iteration 7232, loss = 0.07456606\n",
      "Iteration 7233, loss = 0.07456405\n",
      "Iteration 7234, loss = 0.07456203\n",
      "Iteration 7235, loss = 0.07456004\n",
      "Iteration 7236, loss = 0.07455805\n",
      "Iteration 7237, loss = 0.07455605\n",
      "Iteration 7238, loss = 0.07455403\n",
      "Iteration 7239, loss = 0.07455209\n",
      "Iteration 7240, loss = 0.07455005\n",
      "Iteration 7241, loss = 0.07454804\n",
      "Iteration 7242, loss = 0.07454607\n",
      "Iteration 7243, loss = 0.07454407\n",
      "Iteration 7244, loss = 0.07454207\n",
      "Iteration 7245, loss = 0.07454006\n",
      "Iteration 7246, loss = 0.07453810\n",
      "Iteration 7247, loss = 0.07453608\n",
      "Iteration 7248, loss = 0.07453408\n",
      "Iteration 7249, loss = 0.07453209\n",
      "Iteration 7250, loss = 0.07453012\n",
      "Iteration 7251, loss = 0.07452810\n",
      "Iteration 7252, loss = 0.07452609\n",
      "Iteration 7253, loss = 0.07452413\n",
      "Iteration 7254, loss = 0.07452212\n",
      "Iteration 7255, loss = 0.07452011\n",
      "Iteration 7256, loss = 0.07451815\n",
      "Iteration 7257, loss = 0.07451613\n",
      "Iteration 7258, loss = 0.07451416\n",
      "Iteration 7259, loss = 0.07451215\n",
      "Iteration 7260, loss = 0.07451017\n",
      "Iteration 7261, loss = 0.07450818\n",
      "Iteration 7262, loss = 0.07450618\n",
      "Iteration 7263, loss = 0.07450417\n",
      "Iteration 7264, loss = 0.07450221\n",
      "Iteration 7265, loss = 0.07450020\n",
      "Iteration 7266, loss = 0.07449821\n",
      "Iteration 7267, loss = 0.07449622\n",
      "Iteration 7268, loss = 0.07449423\n",
      "Iteration 7269, loss = 0.07449224\n",
      "Iteration 7270, loss = 0.07449025\n",
      "Iteration 7271, loss = 0.07448826\n",
      "Iteration 7272, loss = 0.07448627\n",
      "Iteration 7273, loss = 0.07448427\n",
      "Iteration 7274, loss = 0.07448231\n",
      "Iteration 7275, loss = 0.07448029\n",
      "Iteration 7276, loss = 0.07447830\n",
      "Iteration 7277, loss = 0.07447634\n",
      "Iteration 7278, loss = 0.07447436\n",
      "Iteration 7279, loss = 0.07447235\n",
      "Iteration 7280, loss = 0.07447037\n",
      "Iteration 7281, loss = 0.07446838\n",
      "Iteration 7282, loss = 0.07446638\n",
      "Iteration 7283, loss = 0.07446439\n",
      "Iteration 7284, loss = 0.07446243\n",
      "Iteration 7285, loss = 0.07446042\n",
      "Iteration 7286, loss = 0.07445843\n",
      "Iteration 7287, loss = 0.07445645\n",
      "Iteration 7288, loss = 0.07445448\n",
      "Iteration 7289, loss = 0.07445249\n",
      "Iteration 7290, loss = 0.07445049\n",
      "Iteration 7291, loss = 0.07444853\n",
      "Iteration 7292, loss = 0.07444653\n",
      "Iteration 7293, loss = 0.07444454\n",
      "Iteration 7294, loss = 0.07444255\n",
      "Iteration 7295, loss = 0.07444060\n",
      "Iteration 7296, loss = 0.07443861\n",
      "Iteration 7297, loss = 0.07443662\n",
      "Iteration 7298, loss = 0.07443463\n",
      "Iteration 7299, loss = 0.07443267\n",
      "Iteration 7300, loss = 0.07443066\n",
      "Iteration 7301, loss = 0.07442869\n",
      "Iteration 7302, loss = 0.07442672\n",
      "Iteration 7303, loss = 0.07442474\n",
      "Iteration 7304, loss = 0.07442274\n",
      "Iteration 7305, loss = 0.07442078\n",
      "Iteration 7306, loss = 0.07441878\n",
      "Iteration 7307, loss = 0.07441680\n",
      "Iteration 7308, loss = 0.07441483\n",
      "Iteration 7309, loss = 0.07441284\n",
      "Iteration 7310, loss = 0.07441088\n",
      "Iteration 7311, loss = 0.07440888\n",
      "Iteration 7312, loss = 0.07440694\n",
      "Iteration 7313, loss = 0.07440492\n",
      "Iteration 7314, loss = 0.07440295\n",
      "Iteration 7315, loss = 0.07440099\n",
      "Iteration 7316, loss = 0.07439902\n",
      "Iteration 7317, loss = 0.07439703\n",
      "Iteration 7318, loss = 0.07439505\n",
      "Iteration 7319, loss = 0.07439307\n",
      "Iteration 7320, loss = 0.07439108\n",
      "Iteration 7321, loss = 0.07438912\n",
      "Iteration 7322, loss = 0.07438714\n",
      "Iteration 7323, loss = 0.07438516\n",
      "Iteration 7324, loss = 0.07438318\n",
      "Iteration 7325, loss = 0.07438121\n",
      "Iteration 7326, loss = 0.07437924\n",
      "Iteration 7327, loss = 0.07437726\n",
      "Iteration 7328, loss = 0.07437529\n",
      "Iteration 7329, loss = 0.07437331\n",
      "Iteration 7330, loss = 0.07437133\n",
      "Iteration 7331, loss = 0.07436935\n",
      "Iteration 7332, loss = 0.07436739\n",
      "Iteration 7333, loss = 0.07436542\n",
      "Iteration 7334, loss = 0.07436344\n",
      "Iteration 7335, loss = 0.07436146\n",
      "Iteration 7336, loss = 0.07435950\n",
      "Iteration 7337, loss = 0.07435752\n",
      "Iteration 7338, loss = 0.07435555\n",
      "Iteration 7339, loss = 0.07435357\n",
      "Iteration 7340, loss = 0.07435162\n",
      "Iteration 7341, loss = 0.07434964\n",
      "Iteration 7342, loss = 0.07434765\n",
      "Iteration 7343, loss = 0.07434571\n",
      "Iteration 7344, loss = 0.07434372\n",
      "Iteration 7345, loss = 0.07434173\n",
      "Iteration 7346, loss = 0.07433979\n",
      "Iteration 7347, loss = 0.07433781\n",
      "Iteration 7348, loss = 0.07433584\n",
      "Iteration 7349, loss = 0.07433386\n",
      "Iteration 7350, loss = 0.07433190\n",
      "Iteration 7351, loss = 0.07432995\n",
      "Iteration 7352, loss = 0.07432795\n",
      "Iteration 7353, loss = 0.07432599\n",
      "Iteration 7354, loss = 0.07432404\n",
      "Iteration 7355, loss = 0.07432206\n",
      "Iteration 7356, loss = 0.07432007\n",
      "Iteration 7357, loss = 0.07431815\n",
      "Iteration 7358, loss = 0.07431614\n",
      "Iteration 7359, loss = 0.07431419\n",
      "Iteration 7360, loss = 0.07431222\n",
      "Iteration 7361, loss = 0.07431025\n",
      "Iteration 7362, loss = 0.07430829\n",
      "Iteration 7363, loss = 0.07430630\n",
      "Iteration 7364, loss = 0.07430437\n",
      "Iteration 7365, loss = 0.07430236\n",
      "Iteration 7366, loss = 0.07430043\n",
      "Iteration 7367, loss = 0.07429846\n",
      "Iteration 7368, loss = 0.07429649\n",
      "Iteration 7369, loss = 0.07429452\n",
      "Iteration 7370, loss = 0.07429255\n",
      "Iteration 7371, loss = 0.07429059\n",
      "Iteration 7372, loss = 0.07428863\n",
      "Iteration 7373, loss = 0.07428666\n",
      "Iteration 7374, loss = 0.07428473\n",
      "Iteration 7375, loss = 0.07428273\n",
      "Iteration 7376, loss = 0.07428076\n",
      "Iteration 7377, loss = 0.07427884\n",
      "Iteration 7378, loss = 0.07427685\n",
      "Iteration 7379, loss = 0.07427489\n",
      "Iteration 7380, loss = 0.07427293\n",
      "Iteration 7381, loss = 0.07427097\n",
      "Iteration 7382, loss = 0.07426901\n",
      "Iteration 7383, loss = 0.07426704\n",
      "Iteration 7384, loss = 0.07426510\n",
      "Iteration 7385, loss = 0.07426313\n",
      "Iteration 7386, loss = 0.07426115\n",
      "Iteration 7387, loss = 0.07425922\n",
      "Iteration 7388, loss = 0.07425725\n",
      "Iteration 7389, loss = 0.07425529\n",
      "Iteration 7390, loss = 0.07425331\n",
      "Iteration 7391, loss = 0.07425140\n",
      "Iteration 7392, loss = 0.07424941\n",
      "Iteration 7393, loss = 0.07424744\n",
      "Iteration 7394, loss = 0.07424550\n",
      "Iteration 7395, loss = 0.07424354\n",
      "Iteration 7396, loss = 0.07424159\n",
      "Iteration 7397, loss = 0.07423962\n",
      "Iteration 7398, loss = 0.07423768\n",
      "Iteration 7399, loss = 0.07423573\n",
      "Iteration 7400, loss = 0.07423375\n",
      "Iteration 7401, loss = 0.07423179\n",
      "Iteration 7402, loss = 0.07422986\n",
      "Iteration 7403, loss = 0.07422789\n",
      "Iteration 7404, loss = 0.07422593\n",
      "Iteration 7405, loss = 0.07422398\n",
      "Iteration 7406, loss = 0.07422203\n",
      "Iteration 7407, loss = 0.07422006\n",
      "Iteration 7408, loss = 0.07421812\n",
      "Iteration 7409, loss = 0.07421615\n",
      "Iteration 7410, loss = 0.07421421\n",
      "Iteration 7411, loss = 0.07421226\n",
      "Iteration 7412, loss = 0.07421030\n",
      "Iteration 7413, loss = 0.07420835\n",
      "Iteration 7414, loss = 0.07420639\n",
      "Iteration 7415, loss = 0.07420447\n",
      "Iteration 7416, loss = 0.07420248\n",
      "Iteration 7417, loss = 0.07420053\n",
      "Iteration 7418, loss = 0.07419860\n",
      "Iteration 7419, loss = 0.07419665\n",
      "Iteration 7420, loss = 0.07419468\n",
      "Iteration 7421, loss = 0.07419274\n",
      "Iteration 7422, loss = 0.07419080\n",
      "Iteration 7423, loss = 0.07418882\n",
      "Iteration 7424, loss = 0.07418687\n",
      "Iteration 7425, loss = 0.07418495\n",
      "Iteration 7426, loss = 0.07418299\n",
      "Iteration 7427, loss = 0.07418103\n",
      "Iteration 7428, loss = 0.07417908\n",
      "Iteration 7429, loss = 0.07417715\n",
      "Iteration 7430, loss = 0.07417517\n",
      "Iteration 7431, loss = 0.07417322\n",
      "Iteration 7432, loss = 0.07417130\n",
      "Iteration 7433, loss = 0.07416934\n",
      "Iteration 7434, loss = 0.07416738\n",
      "Iteration 7435, loss = 0.07416543\n",
      "Iteration 7436, loss = 0.07416350\n",
      "Iteration 7437, loss = 0.07416155\n",
      "Iteration 7438, loss = 0.07415957\n",
      "Iteration 7439, loss = 0.07415766\n",
      "Iteration 7440, loss = 0.07415571\n",
      "Iteration 7441, loss = 0.07415376\n",
      "Iteration 7442, loss = 0.07415180\n",
      "Iteration 7443, loss = 0.07414987\n",
      "Iteration 7444, loss = 0.07414791\n",
      "Iteration 7445, loss = 0.07414597\n",
      "Iteration 7446, loss = 0.07414402\n",
      "Iteration 7447, loss = 0.07414210\n",
      "Iteration 7448, loss = 0.07414012\n",
      "Iteration 7449, loss = 0.07413821\n",
      "Iteration 7450, loss = 0.07413624\n",
      "Iteration 7451, loss = 0.07413429\n",
      "Iteration 7452, loss = 0.07413237\n",
      "Iteration 7453, loss = 0.07413040\n",
      "Iteration 7454, loss = 0.07412847\n",
      "Iteration 7455, loss = 0.07412652\n",
      "Iteration 7456, loss = 0.07412459\n",
      "Iteration 7457, loss = 0.07412265\n",
      "Iteration 7458, loss = 0.07412069\n",
      "Iteration 7459, loss = 0.07411876\n",
      "Iteration 7460, loss = 0.07411681\n",
      "Iteration 7461, loss = 0.07411487\n",
      "Iteration 7462, loss = 0.07411294\n",
      "Iteration 7463, loss = 0.07411099\n",
      "Iteration 7464, loss = 0.07410904\n",
      "Iteration 7465, loss = 0.07410709\n",
      "Iteration 7466, loss = 0.07410519\n",
      "Iteration 7467, loss = 0.07410321\n",
      "Iteration 7468, loss = 0.07410127\n",
      "Iteration 7469, loss = 0.07409936\n",
      "Iteration 7470, loss = 0.07409741\n",
      "Iteration 7471, loss = 0.07409547\n",
      "Iteration 7472, loss = 0.07409352\n",
      "Iteration 7473, loss = 0.07409161\n",
      "Iteration 7474, loss = 0.07408963\n",
      "Iteration 7475, loss = 0.07408770\n",
      "Iteration 7476, loss = 0.07408578\n",
      "Iteration 7477, loss = 0.07408385\n",
      "Iteration 7478, loss = 0.07408189\n",
      "Iteration 7479, loss = 0.07407993\n",
      "Iteration 7480, loss = 0.07407804\n",
      "Iteration 7481, loss = 0.07407607\n",
      "Iteration 7482, loss = 0.07407413\n",
      "Iteration 7483, loss = 0.07407223\n",
      "Iteration 7484, loss = 0.07407027\n",
      "Iteration 7485, loss = 0.07406832\n",
      "Iteration 7486, loss = 0.07406641\n",
      "Iteration 7487, loss = 0.07406448\n",
      "Iteration 7488, loss = 0.07406253\n",
      "Iteration 7489, loss = 0.07406056\n",
      "Iteration 7490, loss = 0.07405869\n",
      "Iteration 7491, loss = 0.07405670\n",
      "Iteration 7492, loss = 0.07405479\n",
      "Iteration 7493, loss = 0.07405285\n",
      "Iteration 7494, loss = 0.07405093\n",
      "Iteration 7495, loss = 0.07404897\n",
      "Iteration 7496, loss = 0.07404703\n",
      "Iteration 7497, loss = 0.07404512\n",
      "Iteration 7498, loss = 0.07404317\n",
      "Iteration 7499, loss = 0.07404124\n",
      "Iteration 7500, loss = 0.07403932\n",
      "Iteration 7501, loss = 0.07403738\n",
      "Iteration 7502, loss = 0.07403544\n",
      "Iteration 7503, loss = 0.07403352\n",
      "Iteration 7504, loss = 0.07403157\n",
      "Iteration 7505, loss = 0.07402965\n",
      "Iteration 7506, loss = 0.07402771\n",
      "Iteration 7507, loss = 0.07402579\n",
      "Iteration 7508, loss = 0.07402385\n",
      "Iteration 7509, loss = 0.07402192\n",
      "Iteration 7510, loss = 0.07402000\n",
      "Iteration 7511, loss = 0.07401807\n",
      "Iteration 7512, loss = 0.07401613\n",
      "Iteration 7513, loss = 0.07401420\n",
      "Iteration 7514, loss = 0.07401228\n",
      "Iteration 7515, loss = 0.07401033\n",
      "Iteration 7516, loss = 0.07400842\n",
      "Iteration 7517, loss = 0.07400649\n",
      "Iteration 7518, loss = 0.07400456\n",
      "Iteration 7519, loss = 0.07400262\n",
      "Iteration 7520, loss = 0.07400073\n",
      "Iteration 7521, loss = 0.07399877\n",
      "Iteration 7522, loss = 0.07399683\n",
      "Iteration 7523, loss = 0.07399494\n",
      "Iteration 7524, loss = 0.07399301\n",
      "Iteration 7525, loss = 0.07399108\n",
      "Iteration 7526, loss = 0.07398913\n",
      "Iteration 7527, loss = 0.07398726\n",
      "Iteration 7528, loss = 0.07398528\n",
      "Iteration 7529, loss = 0.07398336\n",
      "Iteration 7530, loss = 0.07398146\n",
      "Iteration 7531, loss = 0.07397954\n",
      "Iteration 7532, loss = 0.07397759\n",
      "Iteration 7533, loss = 0.07397566\n",
      "Iteration 7534, loss = 0.07397376\n",
      "Iteration 7535, loss = 0.07397183\n",
      "Iteration 7536, loss = 0.07396988\n",
      "Iteration 7537, loss = 0.07396801\n",
      "Iteration 7538, loss = 0.07396605\n",
      "Iteration 7539, loss = 0.07396415\n",
      "Iteration 7540, loss = 0.07396222\n",
      "Iteration 7541, loss = 0.07396030\n",
      "Iteration 7542, loss = 0.07395839\n",
      "Iteration 7543, loss = 0.07395644\n",
      "Iteration 7544, loss = 0.07395454\n",
      "Iteration 7545, loss = 0.07395261\n",
      "Iteration 7546, loss = 0.07395069\n",
      "Iteration 7547, loss = 0.07394879\n",
      "Iteration 7548, loss = 0.07394683\n",
      "Iteration 7549, loss = 0.07394494\n",
      "Iteration 7550, loss = 0.07394302\n",
      "Iteration 7551, loss = 0.07394109\n",
      "Iteration 7552, loss = 0.07393919\n",
      "Iteration 7553, loss = 0.07393723\n",
      "Iteration 7554, loss = 0.07393537\n",
      "Iteration 7555, loss = 0.07393340\n",
      "Iteration 7556, loss = 0.07393152\n",
      "Iteration 7557, loss = 0.07392961\n",
      "Iteration 7558, loss = 0.07392768\n",
      "Iteration 7559, loss = 0.07392574\n",
      "Iteration 7560, loss = 0.07392382\n",
      "Iteration 7561, loss = 0.07392194\n",
      "Iteration 7562, loss = 0.07391998\n",
      "Iteration 7563, loss = 0.07391810\n",
      "Iteration 7564, loss = 0.07391617\n",
      "Iteration 7565, loss = 0.07391425\n",
      "Iteration 7566, loss = 0.07391232\n",
      "Iteration 7567, loss = 0.07391042\n",
      "Iteration 7568, loss = 0.07390850\n",
      "Iteration 7569, loss = 0.07390657\n",
      "Iteration 7570, loss = 0.07390467\n",
      "Iteration 7571, loss = 0.07390276\n",
      "Iteration 7572, loss = 0.07390082\n",
      "Iteration 7573, loss = 0.07389892\n",
      "Iteration 7574, loss = 0.07389702\n",
      "Iteration 7575, loss = 0.07389510\n",
      "Iteration 7576, loss = 0.07389317\n",
      "Iteration 7577, loss = 0.07389128\n",
      "Iteration 7578, loss = 0.07388936\n",
      "Iteration 7579, loss = 0.07388743\n",
      "Iteration 7580, loss = 0.07388554\n",
      "Iteration 7581, loss = 0.07388363\n",
      "Iteration 7582, loss = 0.07388171\n",
      "Iteration 7583, loss = 0.07387978\n",
      "Iteration 7584, loss = 0.07387789\n",
      "Iteration 7585, loss = 0.07387597\n",
      "Iteration 7586, loss = 0.07387405\n",
      "Iteration 7587, loss = 0.07387215\n",
      "Iteration 7588, loss = 0.07387023\n",
      "Iteration 7589, loss = 0.07386833\n",
      "Iteration 7590, loss = 0.07386642\n",
      "Iteration 7591, loss = 0.07386450\n",
      "Iteration 7592, loss = 0.07386258\n",
      "Iteration 7593, loss = 0.07386068\n",
      "Iteration 7594, loss = 0.07385877\n",
      "Iteration 7595, loss = 0.07385686\n",
      "Iteration 7596, loss = 0.07385496\n",
      "Iteration 7597, loss = 0.07385306\n",
      "Iteration 7598, loss = 0.07385112\n",
      "Iteration 7599, loss = 0.07384922\n",
      "Iteration 7600, loss = 0.07384734\n",
      "Iteration 7601, loss = 0.07384542\n",
      "Iteration 7602, loss = 0.07384351\n",
      "Iteration 7603, loss = 0.07384159\n",
      "Iteration 7604, loss = 0.07383971\n",
      "Iteration 7605, loss = 0.07383777\n",
      "Iteration 7606, loss = 0.07383587\n",
      "Iteration 7607, loss = 0.07383397\n",
      "Iteration 7608, loss = 0.07383208\n",
      "Iteration 7609, loss = 0.07383014\n",
      "Iteration 7610, loss = 0.07382825\n",
      "Iteration 7611, loss = 0.07382635\n",
      "Iteration 7612, loss = 0.07382443\n",
      "Iteration 7613, loss = 0.07382252\n",
      "Iteration 7614, loss = 0.07382064\n",
      "Iteration 7615, loss = 0.07381873\n",
      "Iteration 7616, loss = 0.07381680\n",
      "Iteration 7617, loss = 0.07381494\n",
      "Iteration 7618, loss = 0.07381302\n",
      "Iteration 7619, loss = 0.07381112\n",
      "Iteration 7620, loss = 0.07380923\n",
      "Iteration 7621, loss = 0.07380735\n",
      "Iteration 7622, loss = 0.07380543\n",
      "Iteration 7623, loss = 0.07380352\n",
      "Iteration 7624, loss = 0.07380164\n",
      "Iteration 7625, loss = 0.07379975\n",
      "Iteration 7626, loss = 0.07379784\n",
      "Iteration 7627, loss = 0.07379594\n",
      "Iteration 7628, loss = 0.07379406\n",
      "Iteration 7629, loss = 0.07379215\n",
      "Iteration 7630, loss = 0.07379027\n",
      "Iteration 7631, loss = 0.07378835\n",
      "Iteration 7632, loss = 0.07378646\n",
      "Iteration 7633, loss = 0.07378457\n",
      "Iteration 7634, loss = 0.07378268\n",
      "Iteration 7635, loss = 0.07378076\n",
      "Iteration 7636, loss = 0.07377889\n",
      "Iteration 7637, loss = 0.07377701\n",
      "Iteration 7638, loss = 0.07377508\n",
      "Iteration 7639, loss = 0.07377320\n",
      "Iteration 7640, loss = 0.07377133\n",
      "Iteration 7641, loss = 0.07376943\n",
      "Iteration 7642, loss = 0.07376752\n",
      "Iteration 7643, loss = 0.07376562\n",
      "Iteration 7644, loss = 0.07376375\n",
      "Iteration 7645, loss = 0.07376184\n",
      "Iteration 7646, loss = 0.07375995\n",
      "Iteration 7647, loss = 0.07375809\n",
      "Iteration 7648, loss = 0.07375616\n",
      "Iteration 7649, loss = 0.07375428\n",
      "Iteration 7650, loss = 0.07375241\n",
      "Iteration 7651, loss = 0.07375051\n",
      "Iteration 7652, loss = 0.07374860\n",
      "Iteration 7653, loss = 0.07374671\n",
      "Iteration 7654, loss = 0.07374484\n",
      "Iteration 7655, loss = 0.07374295\n",
      "Iteration 7656, loss = 0.07374103\n",
      "Iteration 7657, loss = 0.07373919\n",
      "Iteration 7658, loss = 0.07373727\n",
      "Iteration 7659, loss = 0.07373538\n",
      "Iteration 7660, loss = 0.07373351\n",
      "Iteration 7661, loss = 0.07373163\n",
      "Iteration 7662, loss = 0.07372971\n",
      "Iteration 7663, loss = 0.07372782\n",
      "Iteration 7664, loss = 0.07372596\n",
      "Iteration 7665, loss = 0.07372406\n",
      "Iteration 7666, loss = 0.07372216\n",
      "Iteration 7667, loss = 0.07372030\n",
      "Iteration 7668, loss = 0.07371840\n",
      "Iteration 7669, loss = 0.07371651\n",
      "Iteration 7670, loss = 0.07371463\n",
      "Iteration 7671, loss = 0.07371275\n",
      "Iteration 7672, loss = 0.07371085\n",
      "Iteration 7673, loss = 0.07370897\n",
      "Iteration 7674, loss = 0.07370708\n",
      "Iteration 7675, loss = 0.07370520\n",
      "Iteration 7676, loss = 0.07370331\n",
      "Iteration 7677, loss = 0.07370143\n",
      "Iteration 7678, loss = 0.07369953\n",
      "Iteration 7679, loss = 0.07369767\n",
      "Iteration 7680, loss = 0.07369579\n",
      "Iteration 7681, loss = 0.07369388\n",
      "Iteration 7682, loss = 0.07369200\n",
      "Iteration 7683, loss = 0.07369016\n",
      "Iteration 7684, loss = 0.07368826\n",
      "Iteration 7685, loss = 0.07368637\n",
      "Iteration 7686, loss = 0.07368447\n",
      "Iteration 7687, loss = 0.07368262\n",
      "Iteration 7688, loss = 0.07368071\n",
      "Iteration 7689, loss = 0.07367883\n",
      "Iteration 7690, loss = 0.07367697\n",
      "Iteration 7691, loss = 0.07367506\n",
      "Iteration 7692, loss = 0.07367318\n",
      "Iteration 7693, loss = 0.07367132\n",
      "Iteration 7694, loss = 0.07366943\n",
      "Iteration 7695, loss = 0.07366754\n",
      "Iteration 7696, loss = 0.07366567\n",
      "Iteration 7697, loss = 0.07366380\n",
      "Iteration 7698, loss = 0.07366190\n",
      "Iteration 7699, loss = 0.07366001\n",
      "Iteration 7700, loss = 0.07365816\n",
      "Iteration 7701, loss = 0.07365627\n",
      "Iteration 7702, loss = 0.07365438\n",
      "Iteration 7703, loss = 0.07365251\n",
      "Iteration 7704, loss = 0.07365064\n",
      "Iteration 7705, loss = 0.07364874\n",
      "Iteration 7706, loss = 0.07364687\n",
      "Iteration 7707, loss = 0.07364501\n",
      "Iteration 7708, loss = 0.07364311\n",
      "Iteration 7709, loss = 0.07364125\n",
      "Iteration 7710, loss = 0.07363936\n",
      "Iteration 7711, loss = 0.07363750\n",
      "Iteration 7712, loss = 0.07363560\n",
      "Iteration 7713, loss = 0.07363375\n",
      "Iteration 7714, loss = 0.07363185\n",
      "Iteration 7715, loss = 0.07362998\n",
      "Iteration 7716, loss = 0.07362812\n",
      "Iteration 7717, loss = 0.07362624\n",
      "Iteration 7718, loss = 0.07362436\n",
      "Iteration 7719, loss = 0.07362247\n",
      "Iteration 7720, loss = 0.07362063\n",
      "Iteration 7721, loss = 0.07361871\n",
      "Iteration 7722, loss = 0.07361688\n",
      "Iteration 7723, loss = 0.07361498\n",
      "Iteration 7724, loss = 0.07361311\n",
      "Iteration 7725, loss = 0.07361123\n",
      "Iteration 7726, loss = 0.07360937\n",
      "Iteration 7727, loss = 0.07360748\n",
      "Iteration 7728, loss = 0.07360560\n",
      "Iteration 7729, loss = 0.07360376\n",
      "Iteration 7730, loss = 0.07360187\n",
      "Iteration 7731, loss = 0.07359998\n",
      "Iteration 7732, loss = 0.07359813\n",
      "Iteration 7733, loss = 0.07359626\n",
      "Iteration 7734, loss = 0.07359438\n",
      "Iteration 7735, loss = 0.07359251\n",
      "Iteration 7736, loss = 0.07359066\n",
      "Iteration 7737, loss = 0.07358877\n",
      "Iteration 7738, loss = 0.07358688\n",
      "Iteration 7739, loss = 0.07358504\n",
      "Iteration 7740, loss = 0.07358315\n",
      "Iteration 7741, loss = 0.07358129\n",
      "Iteration 7742, loss = 0.07357943\n",
      "Iteration 7743, loss = 0.07357754\n",
      "Iteration 7744, loss = 0.07357568\n",
      "Iteration 7745, loss = 0.07357379\n",
      "Iteration 7746, loss = 0.07357196\n",
      "Iteration 7747, loss = 0.07357006\n",
      "Iteration 7748, loss = 0.07356820\n",
      "Iteration 7749, loss = 0.07356634\n",
      "Iteration 7750, loss = 0.07356448\n",
      "Iteration 7751, loss = 0.07356259\n",
      "Iteration 7752, loss = 0.07356072\n",
      "Iteration 7753, loss = 0.07355886\n",
      "Iteration 7754, loss = 0.07355701\n",
      "Iteration 7755, loss = 0.07355512\n",
      "Iteration 7756, loss = 0.07355327\n",
      "Iteration 7757, loss = 0.07355139\n",
      "Iteration 7758, loss = 0.07354953\n",
      "Iteration 7759, loss = 0.07354767\n",
      "Iteration 7760, loss = 0.07354579\n",
      "Iteration 7761, loss = 0.07354393\n",
      "Iteration 7762, loss = 0.07354206\n",
      "Iteration 7763, loss = 0.07354021\n",
      "Iteration 7764, loss = 0.07353833\n",
      "Iteration 7765, loss = 0.07353647\n",
      "Iteration 7766, loss = 0.07353461\n",
      "Iteration 7767, loss = 0.07353274\n",
      "Iteration 7768, loss = 0.07353086\n",
      "Iteration 7769, loss = 0.07352904\n",
      "Iteration 7770, loss = 0.07352714\n",
      "Iteration 7771, loss = 0.07352527\n",
      "Iteration 7772, loss = 0.07352344\n",
      "Iteration 7773, loss = 0.07352156\n",
      "Iteration 7774, loss = 0.07351969\n",
      "Iteration 7775, loss = 0.07351782\n",
      "Iteration 7776, loss = 0.07351601\n",
      "Iteration 7777, loss = 0.07351410\n",
      "Iteration 7778, loss = 0.07351226\n",
      "Iteration 7779, loss = 0.07351041\n",
      "Iteration 7780, loss = 0.07350854\n",
      "Iteration 7781, loss = 0.07350666\n",
      "Iteration 7782, loss = 0.07350480\n",
      "Iteration 7783, loss = 0.07350294\n",
      "Iteration 7784, loss = 0.07350107\n",
      "Iteration 7785, loss = 0.07349921\n",
      "Iteration 7786, loss = 0.07349737\n",
      "Iteration 7787, loss = 0.07349549\n",
      "Iteration 7788, loss = 0.07349363\n",
      "Iteration 7789, loss = 0.07349178\n",
      "Iteration 7790, loss = 0.07348992\n",
      "Iteration 7791, loss = 0.07348805\n",
      "Iteration 7792, loss = 0.07348622\n",
      "Iteration 7793, loss = 0.07348434\n",
      "Iteration 7794, loss = 0.07348248\n",
      "Iteration 7795, loss = 0.07348064\n",
      "Iteration 7796, loss = 0.07347876\n",
      "Iteration 7797, loss = 0.07347692\n",
      "Iteration 7798, loss = 0.07347505\n",
      "Iteration 7799, loss = 0.07347320\n",
      "Iteration 7800, loss = 0.07347133\n",
      "Iteration 7801, loss = 0.07346948\n",
      "Iteration 7802, loss = 0.07346764\n",
      "Iteration 7803, loss = 0.07346577\n",
      "Iteration 7804, loss = 0.07346392\n",
      "Iteration 7805, loss = 0.07346207\n",
      "Iteration 7806, loss = 0.07346020\n",
      "Iteration 7807, loss = 0.07345834\n",
      "Iteration 7808, loss = 0.07345652\n",
      "Iteration 7809, loss = 0.07345465\n",
      "Iteration 7810, loss = 0.07345279\n",
      "Iteration 7811, loss = 0.07345093\n",
      "Iteration 7812, loss = 0.07344912\n",
      "Iteration 7813, loss = 0.07344722\n",
      "Iteration 7814, loss = 0.07344540\n",
      "Iteration 7815, loss = 0.07344355\n",
      "Iteration 7816, loss = 0.07344168\n",
      "Iteration 7817, loss = 0.07343982\n",
      "Iteration 7818, loss = 0.07343798\n",
      "Iteration 7819, loss = 0.07343612\n",
      "Iteration 7820, loss = 0.07343425\n",
      "Iteration 7821, loss = 0.07343243\n",
      "Iteration 7822, loss = 0.07343057\n",
      "Iteration 7823, loss = 0.07342871\n",
      "Iteration 7824, loss = 0.07342686\n",
      "Iteration 7825, loss = 0.07342503\n",
      "Iteration 7826, loss = 0.07342316\n",
      "Iteration 7827, loss = 0.07342130\n",
      "Iteration 7828, loss = 0.07341947\n",
      "Iteration 7829, loss = 0.07341763\n",
      "Iteration 7830, loss = 0.07341577\n",
      "Iteration 7831, loss = 0.07341392\n",
      "Iteration 7832, loss = 0.07341207\n",
      "Iteration 7833, loss = 0.07341021\n",
      "Iteration 7834, loss = 0.07340839\n",
      "Iteration 7835, loss = 0.07340652\n",
      "Iteration 7836, loss = 0.07340468\n",
      "Iteration 7837, loss = 0.07340283\n",
      "Iteration 7838, loss = 0.07340098\n",
      "Iteration 7839, loss = 0.07339914\n",
      "Iteration 7840, loss = 0.07339728\n",
      "Iteration 7841, loss = 0.07339545\n",
      "Iteration 7842, loss = 0.07339360\n",
      "Iteration 7843, loss = 0.07339175\n",
      "Iteration 7844, loss = 0.07338992\n",
      "Iteration 7845, loss = 0.07338806\n",
      "Iteration 7846, loss = 0.07338619\n",
      "Iteration 7847, loss = 0.07338441\n",
      "Iteration 7848, loss = 0.07338251\n",
      "Iteration 7849, loss = 0.07338068\n",
      "Iteration 7850, loss = 0.07337884\n",
      "Iteration 7851, loss = 0.07337700\n",
      "Iteration 7852, loss = 0.07337514\n",
      "Iteration 7853, loss = 0.07337328\n",
      "Iteration 7854, loss = 0.07337147\n",
      "Iteration 7855, loss = 0.07336963\n",
      "Iteration 7856, loss = 0.07336776\n",
      "Iteration 7857, loss = 0.07336594\n",
      "Iteration 7858, loss = 0.07336408\n",
      "Iteration 7859, loss = 0.07336223\n",
      "Iteration 7860, loss = 0.07336041\n",
      "Iteration 7861, loss = 0.07335856\n",
      "Iteration 7862, loss = 0.07335671\n",
      "Iteration 7863, loss = 0.07335487\n",
      "Iteration 7864, loss = 0.07335304\n",
      "Iteration 7865, loss = 0.07335119\n",
      "Iteration 7866, loss = 0.07334934\n",
      "Iteration 7867, loss = 0.07334752\n",
      "Iteration 7868, loss = 0.07334568\n",
      "Iteration 7869, loss = 0.07334383\n",
      "Iteration 7870, loss = 0.07334199\n",
      "Iteration 7871, loss = 0.07334015\n",
      "Iteration 7872, loss = 0.07333830\n",
      "Iteration 7873, loss = 0.07333648\n",
      "Iteration 7874, loss = 0.07333462\n",
      "Iteration 7875, loss = 0.07333280\n",
      "Iteration 7876, loss = 0.07333097\n",
      "Iteration 7877, loss = 0.07332913\n",
      "Iteration 7878, loss = 0.07332728\n",
      "Iteration 7879, loss = 0.07332545\n",
      "Iteration 7880, loss = 0.07332363\n",
      "Iteration 7881, loss = 0.07332178\n",
      "Iteration 7882, loss = 0.07331995\n",
      "Iteration 7883, loss = 0.07331811\n",
      "Iteration 7884, loss = 0.07331627\n",
      "Iteration 7885, loss = 0.07331444\n",
      "Iteration 7886, loss = 0.07331263\n",
      "Iteration 7887, loss = 0.07331079\n",
      "Iteration 7888, loss = 0.07330895\n",
      "Iteration 7889, loss = 0.07330713\n",
      "Iteration 7890, loss = 0.07330531\n",
      "Iteration 7891, loss = 0.07330346\n",
      "Iteration 7892, loss = 0.07330164\n",
      "Iteration 7893, loss = 0.07329983\n",
      "Iteration 7894, loss = 0.07329800\n",
      "Iteration 7895, loss = 0.07329615\n",
      "Iteration 7896, loss = 0.07329436\n",
      "Iteration 7897, loss = 0.07329252\n",
      "Iteration 7898, loss = 0.07329067\n",
      "Iteration 7899, loss = 0.07328888\n",
      "Iteration 7900, loss = 0.07328705\n",
      "Iteration 7901, loss = 0.07328521\n",
      "Iteration 7902, loss = 0.07328341\n",
      "Iteration 7903, loss = 0.07328157\n",
      "Iteration 7904, loss = 0.07327974\n",
      "Iteration 7905, loss = 0.07327792\n",
      "Iteration 7906, loss = 0.07327610\n",
      "Iteration 7907, loss = 0.07327428\n",
      "Iteration 7908, loss = 0.07327243\n",
      "Iteration 7909, loss = 0.07327064\n",
      "Iteration 7910, loss = 0.07326881\n",
      "Iteration 7911, loss = 0.07326696\n",
      "Iteration 7912, loss = 0.07326516\n",
      "Iteration 7913, loss = 0.07326334\n",
      "Iteration 7914, loss = 0.07326151\n",
      "Iteration 7915, loss = 0.07325966\n",
      "Iteration 7916, loss = 0.07325787\n",
      "Iteration 7917, loss = 0.07325606\n",
      "Iteration 7918, loss = 0.07325420\n",
      "Iteration 7919, loss = 0.07325241\n",
      "Iteration 7920, loss = 0.07325057\n",
      "Iteration 7921, loss = 0.07324876\n",
      "Iteration 7922, loss = 0.07324693\n",
      "Iteration 7923, loss = 0.07324510\n",
      "Iteration 7924, loss = 0.07324330\n",
      "Iteration 7925, loss = 0.07324148\n",
      "Iteration 7926, loss = 0.07323964\n",
      "Iteration 7927, loss = 0.07323782\n",
      "Iteration 7928, loss = 0.07323603\n",
      "Iteration 7929, loss = 0.07323418\n",
      "Iteration 7930, loss = 0.07323235\n",
      "Iteration 7931, loss = 0.07323058\n",
      "Iteration 7932, loss = 0.07322873\n",
      "Iteration 7933, loss = 0.07322690\n",
      "Iteration 7934, loss = 0.07322509\n",
      "Iteration 7935, loss = 0.07322329\n",
      "Iteration 7936, loss = 0.07322146\n",
      "Iteration 7937, loss = 0.07321962\n",
      "Iteration 7938, loss = 0.07321784\n",
      "Iteration 7939, loss = 0.07321600\n",
      "Iteration 7940, loss = 0.07321416\n",
      "Iteration 7941, loss = 0.07321239\n",
      "Iteration 7942, loss = 0.07321055\n",
      "Iteration 7943, loss = 0.07320872\n",
      "Iteration 7944, loss = 0.07320693\n",
      "Iteration 7945, loss = 0.07320510\n",
      "Iteration 7946, loss = 0.07320328\n",
      "Iteration 7947, loss = 0.07320146\n",
      "Iteration 7948, loss = 0.07319966\n",
      "Iteration 7949, loss = 0.07319784\n",
      "Iteration 7950, loss = 0.07319601\n",
      "Iteration 7951, loss = 0.07319422\n",
      "Iteration 7952, loss = 0.07319239\n",
      "Iteration 7953, loss = 0.07319057\n",
      "Iteration 7954, loss = 0.07318877\n",
      "Iteration 7955, loss = 0.07318695\n",
      "Iteration 7956, loss = 0.07318513\n",
      "Iteration 7957, loss = 0.07318332\n",
      "Iteration 7958, loss = 0.07318150\n",
      "Iteration 7959, loss = 0.07317970\n",
      "Iteration 7960, loss = 0.07317787\n",
      "Iteration 7961, loss = 0.07317607\n",
      "Iteration 7962, loss = 0.07317425\n",
      "Iteration 7963, loss = 0.07317244\n",
      "Iteration 7964, loss = 0.07317066\n",
      "Iteration 7965, loss = 0.07316880\n",
      "Iteration 7966, loss = 0.07316701\n",
      "Iteration 7967, loss = 0.07316520\n",
      "Iteration 7968, loss = 0.07316338\n",
      "Iteration 7969, loss = 0.07316156\n",
      "Iteration 7970, loss = 0.07315977\n",
      "Iteration 7971, loss = 0.07315795\n",
      "Iteration 7972, loss = 0.07315613\n",
      "Iteration 7973, loss = 0.07315434\n",
      "Iteration 7974, loss = 0.07315252\n",
      "Iteration 7975, loss = 0.07315069\n",
      "Iteration 7976, loss = 0.07314890\n",
      "Iteration 7977, loss = 0.07314710\n",
      "Iteration 7978, loss = 0.07314528\n",
      "Iteration 7979, loss = 0.07314345\n",
      "Iteration 7980, loss = 0.07314170\n",
      "Iteration 7981, loss = 0.07313984\n",
      "Iteration 7982, loss = 0.07313804\n",
      "Iteration 7983, loss = 0.07313624\n",
      "Iteration 7984, loss = 0.07313444\n",
      "Iteration 7985, loss = 0.07313262\n",
      "Iteration 7986, loss = 0.07313081\n",
      "Iteration 7987, loss = 0.07312902\n",
      "Iteration 7988, loss = 0.07312719\n",
      "Iteration 7989, loss = 0.07312539\n",
      "Iteration 7990, loss = 0.07312359\n",
      "Iteration 7991, loss = 0.07312178\n",
      "Iteration 7992, loss = 0.07311997\n",
      "Iteration 7993, loss = 0.07311818\n",
      "Iteration 7994, loss = 0.07311638\n",
      "Iteration 7995, loss = 0.07311454\n",
      "Iteration 7996, loss = 0.07311277\n",
      "Iteration 7997, loss = 0.07311094\n",
      "Iteration 7998, loss = 0.07310915\n",
      "Iteration 7999, loss = 0.07310735\n",
      "Iteration 8000, loss = 0.07310554\n",
      "Iteration 8001, loss = 0.07310373\n",
      "Iteration 8002, loss = 0.07310193\n",
      "Iteration 8003, loss = 0.07310012\n",
      "Iteration 8004, loss = 0.07309832\n",
      "Iteration 8005, loss = 0.07309651\n",
      "Iteration 8006, loss = 0.07309472\n",
      "Iteration 8007, loss = 0.07309290\n",
      "Iteration 8008, loss = 0.07309112\n",
      "Iteration 8009, loss = 0.07308932\n",
      "Iteration 8010, loss = 0.07308750\n",
      "Iteration 8011, loss = 0.07308571\n",
      "Iteration 8012, loss = 0.07308392\n",
      "Iteration 8013, loss = 0.07308209\n",
      "Iteration 8014, loss = 0.07308028\n",
      "Iteration 8015, loss = 0.07307853\n",
      "Iteration 8016, loss = 0.07307670\n",
      "Iteration 8017, loss = 0.07307489\n",
      "Iteration 8018, loss = 0.07307310\n",
      "Iteration 8019, loss = 0.07307131\n",
      "Iteration 8020, loss = 0.07306949\n",
      "Iteration 8021, loss = 0.07306769\n",
      "Iteration 8022, loss = 0.07306591\n",
      "Iteration 8023, loss = 0.07306410\n",
      "Iteration 8024, loss = 0.07306229\n",
      "Iteration 8025, loss = 0.07306050\n",
      "Iteration 8026, loss = 0.07305873\n",
      "Iteration 8027, loss = 0.07305688\n",
      "Iteration 8028, loss = 0.07305513\n",
      "Iteration 8029, loss = 0.07305330\n",
      "Iteration 8030, loss = 0.07305151\n",
      "Iteration 8031, loss = 0.07304972\n",
      "Iteration 8032, loss = 0.07304792\n",
      "Iteration 8033, loss = 0.07304611\n",
      "Iteration 8034, loss = 0.07304433\n",
      "Iteration 8035, loss = 0.07304251\n",
      "Iteration 8036, loss = 0.07304074\n",
      "Iteration 8037, loss = 0.07303892\n",
      "Iteration 8038, loss = 0.07303713\n",
      "Iteration 8039, loss = 0.07303534\n",
      "Iteration 8040, loss = 0.07303353\n",
      "Iteration 8041, loss = 0.07303175\n",
      "Iteration 8042, loss = 0.07302995\n",
      "Iteration 8043, loss = 0.07302815\n",
      "Iteration 8044, loss = 0.07302639\n",
      "Iteration 8045, loss = 0.07302456\n",
      "Iteration 8046, loss = 0.07302277\n",
      "Iteration 8047, loss = 0.07302099\n",
      "Iteration 8048, loss = 0.07301919\n",
      "Iteration 8049, loss = 0.07301738\n",
      "Iteration 8050, loss = 0.07301562\n",
      "Iteration 8051, loss = 0.07301380\n",
      "Iteration 8052, loss = 0.07301199\n",
      "Iteration 8053, loss = 0.07301023\n",
      "Iteration 8054, loss = 0.07300844\n",
      "Iteration 8055, loss = 0.07300662\n",
      "Iteration 8056, loss = 0.07300485\n",
      "Iteration 8057, loss = 0.07300306\n",
      "Iteration 8058, loss = 0.07300126\n",
      "Iteration 8059, loss = 0.07299945\n",
      "Iteration 8060, loss = 0.07299770\n",
      "Iteration 8061, loss = 0.07299589\n",
      "Iteration 8062, loss = 0.07299408\n",
      "Iteration 8063, loss = 0.07299232\n",
      "Iteration 8064, loss = 0.07299052\n",
      "Iteration 8065, loss = 0.07298872\n",
      "Iteration 8066, loss = 0.07298694\n",
      "Iteration 8067, loss = 0.07298516\n",
      "Iteration 8068, loss = 0.07298335\n",
      "Iteration 8069, loss = 0.07298156\n",
      "Iteration 8070, loss = 0.07297978\n",
      "Iteration 8071, loss = 0.07297800\n",
      "Iteration 8072, loss = 0.07297620\n",
      "Iteration 8073, loss = 0.07297442\n",
      "Iteration 8074, loss = 0.07297264\n",
      "Iteration 8075, loss = 0.07297083\n",
      "Iteration 8076, loss = 0.07296906\n",
      "Iteration 8077, loss = 0.07296727\n",
      "Iteration 8078, loss = 0.07296547\n",
      "Iteration 8079, loss = 0.07296371\n",
      "Iteration 8080, loss = 0.07296189\n",
      "Iteration 8081, loss = 0.07296013\n",
      "Iteration 8082, loss = 0.07295835\n",
      "Iteration 8083, loss = 0.07295655\n",
      "Iteration 8084, loss = 0.07295476\n",
      "Iteration 8085, loss = 0.07295298\n",
      "Iteration 8086, loss = 0.07295119\n",
      "Iteration 8087, loss = 0.07294938\n",
      "Iteration 8088, loss = 0.07294765\n",
      "Iteration 8089, loss = 0.07294584\n",
      "Iteration 8090, loss = 0.07294404\n",
      "Iteration 8091, loss = 0.07294227\n",
      "Iteration 8092, loss = 0.07294050\n",
      "Iteration 8093, loss = 0.07293869\n",
      "Iteration 8094, loss = 0.07293692\n",
      "Iteration 8095, loss = 0.07293516\n",
      "Iteration 8096, loss = 0.07293336\n",
      "Iteration 8097, loss = 0.07293156\n",
      "Iteration 8098, loss = 0.07292980\n",
      "Iteration 8099, loss = 0.07292801\n",
      "Iteration 8100, loss = 0.07292621\n",
      "Iteration 8101, loss = 0.07292446\n",
      "Iteration 8102, loss = 0.07292266\n",
      "Iteration 8103, loss = 0.07292087\n",
      "Iteration 8104, loss = 0.07291910\n",
      "Iteration 8105, loss = 0.07291731\n",
      "Iteration 8106, loss = 0.07291552\n",
      "Iteration 8107, loss = 0.07291375\n",
      "Iteration 8108, loss = 0.07291197\n",
      "Iteration 8109, loss = 0.07291018\n",
      "Iteration 8110, loss = 0.07290843\n",
      "Iteration 8111, loss = 0.07290663\n",
      "Iteration 8112, loss = 0.07290485\n",
      "Iteration 8113, loss = 0.07290307\n",
      "Iteration 8114, loss = 0.07290130\n",
      "Iteration 8115, loss = 0.07289951\n",
      "Iteration 8116, loss = 0.07289773\n",
      "Iteration 8117, loss = 0.07289597\n",
      "Iteration 8118, loss = 0.07289416\n",
      "Iteration 8119, loss = 0.07289240\n",
      "Iteration 8120, loss = 0.07289063\n",
      "Iteration 8121, loss = 0.07288884\n",
      "Iteration 8122, loss = 0.07288706\n",
      "Iteration 8123, loss = 0.07288530\n",
      "Iteration 8124, loss = 0.07288350\n",
      "Iteration 8125, loss = 0.07288172\n",
      "Iteration 8126, loss = 0.07287998\n",
      "Iteration 8127, loss = 0.07287818\n",
      "Iteration 8128, loss = 0.07287638\n",
      "Iteration 8129, loss = 0.07287465\n",
      "Iteration 8130, loss = 0.07287285\n",
      "Iteration 8131, loss = 0.07287106\n",
      "Iteration 8132, loss = 0.07286930\n",
      "Iteration 8133, loss = 0.07286753\n",
      "Iteration 8134, loss = 0.07286575\n",
      "Iteration 8135, loss = 0.07286395\n",
      "Iteration 8136, loss = 0.07286223\n",
      "Iteration 8137, loss = 0.07286041\n",
      "Iteration 8138, loss = 0.07285866\n",
      "Iteration 8139, loss = 0.07285687\n",
      "Iteration 8140, loss = 0.07285513\n",
      "Iteration 8141, loss = 0.07285331\n",
      "Iteration 8142, loss = 0.07285157\n",
      "Iteration 8143, loss = 0.07284978\n",
      "Iteration 8144, loss = 0.07284800\n",
      "Iteration 8145, loss = 0.07284626\n",
      "Iteration 8146, loss = 0.07284445\n",
      "Iteration 8147, loss = 0.07284269\n",
      "Iteration 8148, loss = 0.07284093\n",
      "Iteration 8149, loss = 0.07283914\n",
      "Iteration 8150, loss = 0.07283736\n",
      "Iteration 8151, loss = 0.07283562\n",
      "Iteration 8152, loss = 0.07283383\n",
      "Iteration 8153, loss = 0.07283205\n",
      "Iteration 8154, loss = 0.07283030\n",
      "Iteration 8155, loss = 0.07282852\n",
      "Iteration 8156, loss = 0.07282675\n",
      "Iteration 8157, loss = 0.07282497\n",
      "Iteration 8158, loss = 0.07282322\n",
      "Iteration 8159, loss = 0.07282143\n",
      "Iteration 8160, loss = 0.07281966\n",
      "Iteration 8161, loss = 0.07281791\n",
      "Iteration 8162, loss = 0.07281613\n",
      "Iteration 8163, loss = 0.07281434\n",
      "Iteration 8164, loss = 0.07281262\n",
      "Iteration 8165, loss = 0.07281081\n",
      "Iteration 8166, loss = 0.07280904\n",
      "Iteration 8167, loss = 0.07280730\n",
      "Iteration 8168, loss = 0.07280553\n",
      "Iteration 8169, loss = 0.07280374\n",
      "Iteration 8170, loss = 0.07280199\n",
      "Iteration 8171, loss = 0.07280022\n",
      "Iteration 8172, loss = 0.07279843\n",
      "Iteration 8173, loss = 0.07279667\n",
      "Iteration 8174, loss = 0.07279492\n",
      "Iteration 8175, loss = 0.07279315\n",
      "Iteration 8176, loss = 0.07279137\n",
      "Iteration 8177, loss = 0.07278963\n",
      "Iteration 8178, loss = 0.07278787\n",
      "Iteration 8179, loss = 0.07278607\n",
      "Iteration 8180, loss = 0.07278433\n",
      "Iteration 8181, loss = 0.07278255\n",
      "Iteration 8182, loss = 0.07278078\n",
      "Iteration 8183, loss = 0.07277904\n",
      "Iteration 8184, loss = 0.07277726\n",
      "Iteration 8185, loss = 0.07277549\n",
      "Iteration 8186, loss = 0.07277373\n",
      "Iteration 8187, loss = 0.07277195\n",
      "Iteration 8188, loss = 0.07277020\n",
      "Iteration 8189, loss = 0.07276844\n",
      "Iteration 8190, loss = 0.07276667\n",
      "Iteration 8191, loss = 0.07276491\n",
      "Iteration 8192, loss = 0.07276315\n",
      "Iteration 8193, loss = 0.07276137\n",
      "Iteration 8194, loss = 0.07275962\n",
      "Iteration 8195, loss = 0.07275786\n",
      "Iteration 8196, loss = 0.07275611\n",
      "Iteration 8197, loss = 0.07275432\n",
      "Iteration 8198, loss = 0.07275256\n",
      "Iteration 8199, loss = 0.07275082\n",
      "Iteration 8200, loss = 0.07274904\n",
      "Iteration 8201, loss = 0.07274728\n",
      "Iteration 8202, loss = 0.07274554\n",
      "Iteration 8203, loss = 0.07274376\n",
      "Iteration 8204, loss = 0.07274199\n",
      "Iteration 8205, loss = 0.07274027\n",
      "Iteration 8206, loss = 0.07273849\n",
      "Iteration 8207, loss = 0.07273672\n",
      "Iteration 8208, loss = 0.07273498\n",
      "Iteration 8209, loss = 0.07273321\n",
      "Iteration 8210, loss = 0.07273143\n",
      "Iteration 8211, loss = 0.07272970\n",
      "Iteration 8212, loss = 0.07272793\n",
      "Iteration 8213, loss = 0.07272617\n",
      "Iteration 8214, loss = 0.07272443\n",
      "Iteration 8215, loss = 0.07272265\n",
      "Iteration 8216, loss = 0.07272089\n",
      "Iteration 8217, loss = 0.07271915\n",
      "Iteration 8218, loss = 0.07271738\n",
      "Iteration 8219, loss = 0.07271563\n",
      "Iteration 8220, loss = 0.07271387\n",
      "Iteration 8221, loss = 0.07271212\n",
      "Iteration 8222, loss = 0.07271037\n",
      "Iteration 8223, loss = 0.07270860\n",
      "Iteration 8224, loss = 0.07270686\n",
      "Iteration 8225, loss = 0.07270509\n",
      "Iteration 8226, loss = 0.07270335\n",
      "Iteration 8227, loss = 0.07270159\n",
      "Iteration 8228, loss = 0.07269982\n",
      "Iteration 8229, loss = 0.07269807\n",
      "Iteration 8230, loss = 0.07269633\n",
      "Iteration 8231, loss = 0.07269457\n",
      "Iteration 8232, loss = 0.07269280\n",
      "Iteration 8233, loss = 0.07269107\n",
      "Iteration 8234, loss = 0.07268930\n",
      "Iteration 8235, loss = 0.07268754\n",
      "Iteration 8236, loss = 0.07268583\n",
      "Iteration 8237, loss = 0.07268403\n",
      "Iteration 8238, loss = 0.07268227\n",
      "Iteration 8239, loss = 0.07268056\n",
      "Iteration 8240, loss = 0.07267879\n",
      "Iteration 8241, loss = 0.07267702\n",
      "Iteration 8242, loss = 0.07267529\n",
      "Iteration 8243, loss = 0.07267354\n",
      "Iteration 8244, loss = 0.07267177\n",
      "Iteration 8245, loss = 0.07267002\n",
      "Iteration 8246, loss = 0.07266829\n",
      "Iteration 8247, loss = 0.07266653\n",
      "Iteration 8248, loss = 0.07266477\n",
      "Iteration 8249, loss = 0.07266303\n",
      "Iteration 8250, loss = 0.07266129\n",
      "Iteration 8251, loss = 0.07265951\n",
      "Iteration 8252, loss = 0.07265779\n",
      "Iteration 8253, loss = 0.07265602\n",
      "Iteration 8254, loss = 0.07265427\n",
      "Iteration 8255, loss = 0.07265252\n",
      "Iteration 8256, loss = 0.07265079\n",
      "Iteration 8257, loss = 0.07264902\n",
      "Iteration 8258, loss = 0.07264728\n",
      "Iteration 8259, loss = 0.07264554\n",
      "Iteration 8260, loss = 0.07264377\n",
      "Iteration 8261, loss = 0.07264207\n",
      "Iteration 8262, loss = 0.07264032\n",
      "Iteration 8263, loss = 0.07263861\n",
      "Iteration 8264, loss = 0.07263690\n",
      "Iteration 8265, loss = 0.07263518\n",
      "Iteration 8266, loss = 0.07263345\n",
      "Iteration 8267, loss = 0.07263172\n",
      "Iteration 8268, loss = 0.07263004\n",
      "Iteration 8269, loss = 0.07262827\n",
      "Iteration 8270, loss = 0.07262659\n",
      "Iteration 8271, loss = 0.07262485\n",
      "Iteration 8272, loss = 0.07262312\n",
      "Iteration 8273, loss = 0.07262139\n",
      "Iteration 8274, loss = 0.07261971\n",
      "Iteration 8275, loss = 0.07261793\n",
      "Iteration 8276, loss = 0.07261623\n",
      "Iteration 8277, loss = 0.07261451\n",
      "Iteration 8278, loss = 0.07261277\n",
      "Iteration 8279, loss = 0.07261103\n",
      "Iteration 8280, loss = 0.07260934\n",
      "Iteration 8281, loss = 0.07260762\n",
      "Iteration 8282, loss = 0.07260587\n",
      "Iteration 8283, loss = 0.07260416\n",
      "Iteration 8284, loss = 0.07260244\n",
      "Iteration 8285, loss = 0.07260070\n",
      "Iteration 8286, loss = 0.07259900\n",
      "Iteration 8287, loss = 0.07259728\n",
      "Iteration 8288, loss = 0.07259554\n",
      "Iteration 8289, loss = 0.07259382\n",
      "Iteration 8290, loss = 0.07259210\n",
      "Iteration 8291, loss = 0.07259038\n",
      "Iteration 8292, loss = 0.07258867\n",
      "Iteration 8293, loss = 0.07258693\n",
      "Iteration 8294, loss = 0.07258522\n",
      "Iteration 8295, loss = 0.07258352\n",
      "Iteration 8296, loss = 0.07258179\n",
      "Iteration 8297, loss = 0.07258006\n",
      "Iteration 8298, loss = 0.07257837\n",
      "Iteration 8299, loss = 0.07257663\n",
      "Iteration 8300, loss = 0.07257491\n",
      "Iteration 8301, loss = 0.07257320\n",
      "Iteration 8302, loss = 0.07257149\n",
      "Iteration 8303, loss = 0.07256975\n",
      "Iteration 8304, loss = 0.07256805\n",
      "Iteration 8305, loss = 0.07256634\n",
      "Iteration 8306, loss = 0.07256461\n",
      "Iteration 8307, loss = 0.07256290\n",
      "Iteration 8308, loss = 0.07256120\n",
      "Iteration 8309, loss = 0.07255946\n",
      "Iteration 8310, loss = 0.07255775\n",
      "Iteration 8311, loss = 0.07255605\n",
      "Iteration 8312, loss = 0.07255432\n",
      "Iteration 8313, loss = 0.07255259\n",
      "Iteration 8314, loss = 0.07255091\n",
      "Iteration 8315, loss = 0.07254917\n",
      "Iteration 8316, loss = 0.07254745\n",
      "Iteration 8317, loss = 0.07254577\n",
      "Iteration 8318, loss = 0.07254404\n",
      "Iteration 8319, loss = 0.07254231\n",
      "Iteration 8320, loss = 0.07254060\n",
      "Iteration 8321, loss = 0.07253890\n",
      "Iteration 8322, loss = 0.07253717\n",
      "Iteration 8323, loss = 0.07253547\n",
      "Iteration 8324, loss = 0.07253377\n",
      "Iteration 8325, loss = 0.07253204\n",
      "Iteration 8326, loss = 0.07253034\n",
      "Iteration 8327, loss = 0.07252860\n",
      "Iteration 8328, loss = 0.07252691\n",
      "Iteration 8329, loss = 0.07252519\n",
      "Iteration 8330, loss = 0.07252348\n",
      "Iteration 8331, loss = 0.07252177\n",
      "Iteration 8332, loss = 0.07252006\n",
      "Iteration 8333, loss = 0.07251835\n",
      "Iteration 8334, loss = 0.07251662\n",
      "Iteration 8335, loss = 0.07251493\n",
      "Iteration 8336, loss = 0.07251322\n",
      "Iteration 8337, loss = 0.07251150\n",
      "Iteration 8338, loss = 0.07250979\n",
      "Iteration 8339, loss = 0.07250811\n",
      "Iteration 8340, loss = 0.07250635\n",
      "Iteration 8341, loss = 0.07250469\n",
      "Iteration 8342, loss = 0.07250296\n",
      "Iteration 8343, loss = 0.07250125\n",
      "Iteration 8344, loss = 0.07249951\n",
      "Iteration 8345, loss = 0.07249785\n",
      "Iteration 8346, loss = 0.07249612\n",
      "Iteration 8347, loss = 0.07249440\n",
      "Iteration 8348, loss = 0.07249271\n",
      "Iteration 8349, loss = 0.07249099\n",
      "Iteration 8350, loss = 0.07248927\n",
      "Iteration 8351, loss = 0.07248759\n",
      "Iteration 8352, loss = 0.07248587\n",
      "Iteration 8353, loss = 0.07248416\n",
      "Iteration 8354, loss = 0.07248246\n",
      "Iteration 8355, loss = 0.07248077\n",
      "Iteration 8356, loss = 0.07247904\n",
      "Iteration 8357, loss = 0.07247733\n",
      "Iteration 8358, loss = 0.07247564\n",
      "Iteration 8359, loss = 0.07247392\n",
      "Iteration 8360, loss = 0.07247222\n",
      "Iteration 8361, loss = 0.07247050\n",
      "Iteration 8362, loss = 0.07246881\n",
      "Iteration 8363, loss = 0.07246710\n",
      "Iteration 8364, loss = 0.07246540\n",
      "Iteration 8365, loss = 0.07246369\n",
      "Iteration 8366, loss = 0.07246199\n",
      "Iteration 8367, loss = 0.07246028\n",
      "Iteration 8368, loss = 0.07245857\n",
      "Iteration 8369, loss = 0.07245687\n",
      "Iteration 8370, loss = 0.07245517\n",
      "Iteration 8371, loss = 0.07245346\n",
      "Iteration 8372, loss = 0.07245175\n",
      "Iteration 8373, loss = 0.07245007\n",
      "Iteration 8374, loss = 0.07244835\n",
      "Iteration 8375, loss = 0.07244664\n",
      "Iteration 8376, loss = 0.07244497\n",
      "Iteration 8377, loss = 0.07244322\n",
      "Iteration 8378, loss = 0.07244156\n",
      "Iteration 8379, loss = 0.07243984\n",
      "Iteration 8380, loss = 0.07243814\n",
      "Iteration 8381, loss = 0.07243642\n",
      "Iteration 8382, loss = 0.07243477\n",
      "Iteration 8383, loss = 0.07243302\n",
      "Iteration 8384, loss = 0.07243133\n",
      "Iteration 8385, loss = 0.07242963\n",
      "Iteration 8386, loss = 0.07242793\n",
      "Iteration 8387, loss = 0.07242621\n",
      "Iteration 8388, loss = 0.07242454\n",
      "Iteration 8389, loss = 0.07242283\n",
      "Iteration 8390, loss = 0.07242111\n",
      "Iteration 8391, loss = 0.07241944\n",
      "Iteration 8392, loss = 0.07241775\n",
      "Iteration 8393, loss = 0.07241603\n",
      "Iteration 8394, loss = 0.07241432\n",
      "Iteration 8395, loss = 0.07241264\n",
      "Iteration 8396, loss = 0.07241093\n",
      "Iteration 8397, loss = 0.07240924\n",
      "Iteration 8398, loss = 0.07240752\n",
      "Iteration 8399, loss = 0.07240583\n",
      "Iteration 8400, loss = 0.07240414\n",
      "Iteration 8401, loss = 0.07240243\n",
      "Iteration 8402, loss = 0.07240074\n",
      "Iteration 8403, loss = 0.07239905\n",
      "Iteration 8404, loss = 0.07239735\n",
      "Iteration 8405, loss = 0.07239563\n",
      "Iteration 8406, loss = 0.07239397\n",
      "Iteration 8407, loss = 0.07239225\n",
      "Iteration 8408, loss = 0.07239054\n",
      "Iteration 8409, loss = 0.07238887\n",
      "Iteration 8410, loss = 0.07238716\n",
      "Iteration 8411, loss = 0.07238546\n",
      "Iteration 8412, loss = 0.07238377\n",
      "Iteration 8413, loss = 0.07238209\n",
      "Iteration 8414, loss = 0.07238038\n",
      "Iteration 8415, loss = 0.07237866\n",
      "Iteration 8416, loss = 0.07237704\n",
      "Iteration 8417, loss = 0.07237528\n",
      "Iteration 8418, loss = 0.07237362\n",
      "Iteration 8419, loss = 0.07237193\n",
      "Iteration 8420, loss = 0.07237022\n",
      "Iteration 8421, loss = 0.07236851\n",
      "Iteration 8422, loss = 0.07236683\n",
      "Iteration 8423, loss = 0.07236513\n",
      "Iteration 8424, loss = 0.07236342\n",
      "Iteration 8425, loss = 0.07236175\n",
      "Iteration 8426, loss = 0.07236006\n",
      "Iteration 8427, loss = 0.07235834\n",
      "Iteration 8428, loss = 0.07235667\n",
      "Iteration 8429, loss = 0.07235498\n",
      "Iteration 8430, loss = 0.07235327\n",
      "Iteration 8431, loss = 0.07235158\n",
      "Iteration 8432, loss = 0.07234992\n",
      "Iteration 8433, loss = 0.07234821\n",
      "Iteration 8434, loss = 0.07234652\n",
      "Iteration 8435, loss = 0.07234481\n",
      "Iteration 8436, loss = 0.07234312\n",
      "Iteration 8437, loss = 0.07234145\n",
      "Iteration 8438, loss = 0.07233973\n",
      "Iteration 8439, loss = 0.07233806\n",
      "Iteration 8440, loss = 0.07233638\n",
      "Iteration 8441, loss = 0.07233469\n",
      "Iteration 8442, loss = 0.07233298\n",
      "Iteration 8443, loss = 0.07233130\n",
      "Iteration 8444, loss = 0.07232961\n",
      "Iteration 8445, loss = 0.07232790\n",
      "Iteration 8446, loss = 0.07232624\n",
      "Iteration 8447, loss = 0.07232454\n",
      "Iteration 8448, loss = 0.07232284\n",
      "Iteration 8449, loss = 0.07232117\n",
      "Iteration 8450, loss = 0.07231948\n",
      "Iteration 8451, loss = 0.07231777\n",
      "Iteration 8452, loss = 0.07231610\n",
      "Iteration 8453, loss = 0.07231441\n",
      "Iteration 8454, loss = 0.07231271\n",
      "Iteration 8455, loss = 0.07231102\n",
      "Iteration 8456, loss = 0.07230936\n",
      "Iteration 8457, loss = 0.07230765\n",
      "Iteration 8458, loss = 0.07230596\n",
      "Iteration 8459, loss = 0.07230429\n",
      "Iteration 8460, loss = 0.07230260\n",
      "Iteration 8461, loss = 0.07230090\n",
      "Iteration 8462, loss = 0.07229922\n",
      "Iteration 8463, loss = 0.07229755\n",
      "Iteration 8464, loss = 0.07229584\n",
      "Iteration 8465, loss = 0.07229420\n",
      "Iteration 8466, loss = 0.07229251\n",
      "Iteration 8467, loss = 0.07229083\n",
      "Iteration 8468, loss = 0.07228917\n",
      "Iteration 8469, loss = 0.07228746\n",
      "Iteration 8470, loss = 0.07228580\n",
      "Iteration 8471, loss = 0.07228413\n",
      "Iteration 8472, loss = 0.07228244\n",
      "Iteration 8473, loss = 0.07228077\n",
      "Iteration 8474, loss = 0.07227910\n",
      "Iteration 8475, loss = 0.07227740\n",
      "Iteration 8476, loss = 0.07227576\n",
      "Iteration 8477, loss = 0.07227409\n",
      "Iteration 8478, loss = 0.07227240\n",
      "Iteration 8479, loss = 0.07227073\n",
      "Iteration 8480, loss = 0.07226910\n",
      "Iteration 8481, loss = 0.07226739\n",
      "Iteration 8482, loss = 0.07226573\n",
      "Iteration 8483, loss = 0.07226408\n",
      "Iteration 8484, loss = 0.07226240\n",
      "Iteration 8485, loss = 0.07226072\n",
      "Iteration 8486, loss = 0.07225908\n",
      "Iteration 8487, loss = 0.07225739\n",
      "Iteration 8488, loss = 0.07225572\n",
      "Iteration 8489, loss = 0.07225407\n",
      "Iteration 8490, loss = 0.07225240\n",
      "Iteration 8491, loss = 0.07225073\n",
      "Iteration 8492, loss = 0.07224907\n",
      "Iteration 8493, loss = 0.07224740\n",
      "Iteration 8494, loss = 0.07224572\n",
      "Iteration 8495, loss = 0.07224409\n",
      "Iteration 8496, loss = 0.07224242\n",
      "Iteration 8497, loss = 0.07224074\n",
      "Iteration 8498, loss = 0.07223907\n",
      "Iteration 8499, loss = 0.07223742\n",
      "Iteration 8500, loss = 0.07223574\n",
      "Iteration 8501, loss = 0.07223409\n",
      "Iteration 8502, loss = 0.07223242\n",
      "Iteration 8503, loss = 0.07223075\n",
      "Iteration 8504, loss = 0.07222912\n",
      "Iteration 8505, loss = 0.07222744\n",
      "Iteration 8506, loss = 0.07222578\n",
      "Iteration 8507, loss = 0.07222409\n",
      "Iteration 8508, loss = 0.07222245\n",
      "Iteration 8509, loss = 0.07222078\n",
      "Iteration 8510, loss = 0.07221913\n",
      "Iteration 8511, loss = 0.07221746\n",
      "Iteration 8512, loss = 0.07221579\n",
      "Iteration 8513, loss = 0.07221415\n",
      "Iteration 8514, loss = 0.07221247\n",
      "Iteration 8515, loss = 0.07221081\n",
      "Iteration 8516, loss = 0.07220918\n",
      "Iteration 8517, loss = 0.07220749\n",
      "Iteration 8518, loss = 0.07220581\n",
      "Iteration 8519, loss = 0.07220420\n",
      "Iteration 8520, loss = 0.07220251\n",
      "Iteration 8521, loss = 0.07220084\n",
      "Iteration 8522, loss = 0.07219920\n",
      "Iteration 8523, loss = 0.07219754\n",
      "Iteration 8524, loss = 0.07219586\n",
      "Iteration 8525, loss = 0.07219422\n",
      "Iteration 8526, loss = 0.07219256\n",
      "Iteration 8527, loss = 0.07219089\n",
      "Iteration 8528, loss = 0.07218924\n",
      "Iteration 8529, loss = 0.07218759\n",
      "Iteration 8530, loss = 0.07218593\n",
      "Iteration 8531, loss = 0.07218426\n",
      "Iteration 8532, loss = 0.07218261\n",
      "Iteration 8533, loss = 0.07218095\n",
      "Iteration 8534, loss = 0.07217930\n",
      "Iteration 8535, loss = 0.07217763\n",
      "Iteration 8536, loss = 0.07217600\n",
      "Iteration 8537, loss = 0.07217432\n",
      "Iteration 8538, loss = 0.07217266\n",
      "Iteration 8539, loss = 0.07217103\n",
      "Iteration 8540, loss = 0.07216934\n",
      "Iteration 8541, loss = 0.07216772\n",
      "Iteration 8542, loss = 0.07216603\n",
      "Iteration 8543, loss = 0.07216442\n",
      "Iteration 8544, loss = 0.07216275\n",
      "Iteration 8545, loss = 0.07216108\n",
      "Iteration 8546, loss = 0.07215942\n",
      "Iteration 8547, loss = 0.07215778\n",
      "Iteration 8548, loss = 0.07215609\n",
      "Iteration 8549, loss = 0.07215447\n",
      "Iteration 8550, loss = 0.07215281\n",
      "Iteration 8551, loss = 0.07215114\n",
      "Iteration 8552, loss = 0.07214949\n",
      "Iteration 8553, loss = 0.07214786\n",
      "Iteration 8554, loss = 0.07214619\n",
      "Iteration 8555, loss = 0.07214451\n",
      "Iteration 8556, loss = 0.07214291\n",
      "Iteration 8557, loss = 0.07214121\n",
      "Iteration 8558, loss = 0.07213957\n",
      "Iteration 8559, loss = 0.07213793\n",
      "Iteration 8560, loss = 0.07213627\n",
      "Iteration 8561, loss = 0.07213459\n",
      "Iteration 8562, loss = 0.07213298\n",
      "Iteration 8563, loss = 0.07213132\n",
      "Iteration 8564, loss = 0.07212965\n",
      "Iteration 8565, loss = 0.07212802\n",
      "Iteration 8566, loss = 0.07212635\n",
      "Iteration 8567, loss = 0.07212471\n",
      "Iteration 8568, loss = 0.07212305\n",
      "Iteration 8569, loss = 0.07212142\n",
      "Iteration 8570, loss = 0.07211973\n",
      "Iteration 8571, loss = 0.07211812\n",
      "Iteration 8572, loss = 0.07211644\n",
      "Iteration 8573, loss = 0.07211480\n",
      "Iteration 8574, loss = 0.07211316\n",
      "Iteration 8575, loss = 0.07211149\n",
      "Iteration 8576, loss = 0.07210985\n",
      "Iteration 8577, loss = 0.07210821\n",
      "Iteration 8578, loss = 0.07210654\n",
      "Iteration 8579, loss = 0.07210491\n",
      "Iteration 8580, loss = 0.07210327\n",
      "Iteration 8581, loss = 0.07210161\n",
      "Iteration 8582, loss = 0.07209995\n",
      "Iteration 8583, loss = 0.07209833\n",
      "Iteration 8584, loss = 0.07209665\n",
      "Iteration 8585, loss = 0.07209501\n",
      "Iteration 8586, loss = 0.07209338\n",
      "Iteration 8587, loss = 0.07209173\n",
      "Iteration 8588, loss = 0.07209005\n",
      "Iteration 8589, loss = 0.07208843\n",
      "Iteration 8590, loss = 0.07208677\n",
      "Iteration 8591, loss = 0.07208512\n",
      "Iteration 8592, loss = 0.07208349\n",
      "Iteration 8593, loss = 0.07208185\n",
      "Iteration 8594, loss = 0.07208018\n",
      "Iteration 8595, loss = 0.07207855\n",
      "Iteration 8596, loss = 0.07207690\n",
      "Iteration 8597, loss = 0.07207524\n",
      "Iteration 8598, loss = 0.07207363\n",
      "Iteration 8599, loss = 0.07207195\n",
      "Iteration 8600, loss = 0.07207033\n",
      "Iteration 8601, loss = 0.07206868\n",
      "Iteration 8602, loss = 0.07206702\n",
      "Iteration 8603, loss = 0.07206539\n",
      "Iteration 8604, loss = 0.07206373\n",
      "Iteration 8605, loss = 0.07206209\n",
      "Iteration 8606, loss = 0.07206044\n",
      "Iteration 8607, loss = 0.07205882\n",
      "Iteration 8608, loss = 0.07205714\n",
      "Iteration 8609, loss = 0.07205552\n",
      "Iteration 8610, loss = 0.07205388\n",
      "Iteration 8611, loss = 0.07205222\n",
      "Iteration 8612, loss = 0.07205058\n",
      "Iteration 8613, loss = 0.07204895\n",
      "Iteration 8614, loss = 0.07204729\n",
      "Iteration 8615, loss = 0.07204564\n",
      "Iteration 8616, loss = 0.07204404\n",
      "Iteration 8617, loss = 0.07204237\n",
      "Iteration 8618, loss = 0.07204071\n",
      "Iteration 8619, loss = 0.07203911\n",
      "Iteration 8620, loss = 0.07203745\n",
      "Iteration 8621, loss = 0.07203579\n",
      "Iteration 8622, loss = 0.07203417\n",
      "Iteration 8623, loss = 0.07203252\n",
      "Iteration 8624, loss = 0.07203086\n",
      "Iteration 8625, loss = 0.07202925\n",
      "Iteration 8626, loss = 0.07202760\n",
      "Iteration 8627, loss = 0.07202594\n",
      "Iteration 8628, loss = 0.07202433\n",
      "Iteration 8629, loss = 0.07202269\n",
      "Iteration 8630, loss = 0.07202104\n",
      "Iteration 8631, loss = 0.07201938\n",
      "Iteration 8632, loss = 0.07201777\n",
      "Iteration 8633, loss = 0.07201611\n",
      "Iteration 8634, loss = 0.07201448\n",
      "Iteration 8635, loss = 0.07201284\n",
      "Iteration 8636, loss = 0.07201118\n",
      "Iteration 8637, loss = 0.07200958\n",
      "Iteration 8638, loss = 0.07200790\n",
      "Iteration 8639, loss = 0.07200629\n",
      "Iteration 8640, loss = 0.07200465\n",
      "Iteration 8641, loss = 0.07200302\n",
      "Iteration 8642, loss = 0.07200137\n",
      "Iteration 8643, loss = 0.07199974\n",
      "Iteration 8644, loss = 0.07199810\n",
      "Iteration 8645, loss = 0.07199644\n",
      "Iteration 8646, loss = 0.07199485\n",
      "Iteration 8647, loss = 0.07199317\n",
      "Iteration 8648, loss = 0.07199154\n",
      "Iteration 8649, loss = 0.07198993\n",
      "Iteration 8650, loss = 0.07198826\n",
      "Iteration 8651, loss = 0.07198662\n",
      "Iteration 8652, loss = 0.07198503\n",
      "Iteration 8653, loss = 0.07198337\n",
      "Iteration 8654, loss = 0.07198172\n",
      "Iteration 8655, loss = 0.07198010\n",
      "Iteration 8656, loss = 0.07197847\n",
      "Iteration 8657, loss = 0.07197681\n",
      "Iteration 8658, loss = 0.07197520\n",
      "Iteration 8659, loss = 0.07197355\n",
      "Iteration 8660, loss = 0.07197191\n",
      "Iteration 8661, loss = 0.07197030\n",
      "Iteration 8662, loss = 0.07196867\n",
      "Iteration 8663, loss = 0.07196702\n",
      "Iteration 8664, loss = 0.07196536\n",
      "Iteration 8665, loss = 0.07196377\n",
      "Iteration 8666, loss = 0.07196210\n",
      "Iteration 8667, loss = 0.07196049\n",
      "Iteration 8668, loss = 0.07195885\n",
      "Iteration 8669, loss = 0.07195721\n",
      "Iteration 8670, loss = 0.07195560\n",
      "Iteration 8671, loss = 0.07195394\n",
      "Iteration 8672, loss = 0.07195232\n",
      "Iteration 8673, loss = 0.07195070\n",
      "Iteration 8674, loss = 0.07194906\n",
      "Iteration 8675, loss = 0.07194743\n",
      "Iteration 8676, loss = 0.07194580\n",
      "Iteration 8677, loss = 0.07194416\n",
      "Iteration 8678, loss = 0.07194252\n",
      "Iteration 8679, loss = 0.07194093\n",
      "Iteration 8680, loss = 0.07193926\n",
      "Iteration 8681, loss = 0.07193763\n",
      "Iteration 8682, loss = 0.07193602\n",
      "Iteration 8683, loss = 0.07193438\n",
      "Iteration 8684, loss = 0.07193273\n",
      "Iteration 8685, loss = 0.07193113\n",
      "Iteration 8686, loss = 0.07192948\n",
      "Iteration 8687, loss = 0.07192784\n",
      "Iteration 8688, loss = 0.07192624\n",
      "Iteration 8689, loss = 0.07192460\n",
      "Iteration 8690, loss = 0.07192295\n",
      "Iteration 8691, loss = 0.07192136\n",
      "Iteration 8692, loss = 0.07191972\n",
      "Iteration 8693, loss = 0.07191808\n",
      "Iteration 8694, loss = 0.07191644\n",
      "Iteration 8695, loss = 0.07191484\n",
      "Iteration 8696, loss = 0.07191317\n",
      "Iteration 8697, loss = 0.07191158\n",
      "Iteration 8698, loss = 0.07190993\n",
      "Iteration 8699, loss = 0.07190832\n",
      "Iteration 8700, loss = 0.07190670\n",
      "Iteration 8701, loss = 0.07190505\n",
      "Iteration 8702, loss = 0.07190345\n",
      "Iteration 8703, loss = 0.07190181\n",
      "Iteration 8704, loss = 0.07190018\n",
      "Iteration 8705, loss = 0.07189854\n",
      "Iteration 8706, loss = 0.07189694\n",
      "Iteration 8707, loss = 0.07189530\n",
      "Iteration 8708, loss = 0.07189368\n",
      "Iteration 8709, loss = 0.07189204\n",
      "Iteration 8710, loss = 0.07189041\n",
      "Iteration 8711, loss = 0.07188879\n",
      "Iteration 8712, loss = 0.07188719\n",
      "Iteration 8713, loss = 0.07188553\n",
      "Iteration 8714, loss = 0.07188392\n",
      "Iteration 8715, loss = 0.07188231\n",
      "Iteration 8716, loss = 0.07188067\n",
      "Iteration 8717, loss = 0.07187904\n",
      "Iteration 8718, loss = 0.07187745\n",
      "Iteration 8719, loss = 0.07187578\n",
      "Iteration 8720, loss = 0.07187416\n",
      "Iteration 8721, loss = 0.07187256\n",
      "Iteration 8722, loss = 0.07187094\n",
      "Iteration 8723, loss = 0.07186929\n",
      "Iteration 8724, loss = 0.07186770\n",
      "Iteration 8725, loss = 0.07186606\n",
      "Iteration 8726, loss = 0.07186442\n",
      "Iteration 8727, loss = 0.07186282\n",
      "Iteration 8728, loss = 0.07186120\n",
      "Iteration 8729, loss = 0.07185955\n",
      "Iteration 8730, loss = 0.07185796\n",
      "Iteration 8731, loss = 0.07185632\n",
      "Iteration 8732, loss = 0.07185470\n",
      "Iteration 8733, loss = 0.07185308\n",
      "Iteration 8734, loss = 0.07185146\n",
      "Iteration 8735, loss = 0.07184982\n",
      "Iteration 8736, loss = 0.07184823\n",
      "Iteration 8737, loss = 0.07184659\n",
      "Iteration 8738, loss = 0.07184499\n",
      "Iteration 8739, loss = 0.07184336\n",
      "Iteration 8740, loss = 0.07184172\n",
      "Iteration 8741, loss = 0.07184012\n",
      "Iteration 8742, loss = 0.07183850\n",
      "Iteration 8743, loss = 0.07183688\n",
      "Iteration 8744, loss = 0.07183524\n",
      "Iteration 8745, loss = 0.07183367\n",
      "Iteration 8746, loss = 0.07183200\n",
      "Iteration 8747, loss = 0.07183039\n",
      "Iteration 8748, loss = 0.07182880\n",
      "Iteration 8749, loss = 0.07182714\n",
      "Iteration 8750, loss = 0.07182555\n",
      "Iteration 8751, loss = 0.07182394\n",
      "Iteration 8752, loss = 0.07182232\n",
      "Iteration 8753, loss = 0.07182067\n",
      "Iteration 8754, loss = 0.07181909\n",
      "Iteration 8755, loss = 0.07181744\n",
      "Iteration 8756, loss = 0.07181585\n",
      "Iteration 8757, loss = 0.07181423\n",
      "Iteration 8758, loss = 0.07181261\n",
      "Iteration 8759, loss = 0.07181097\n",
      "Iteration 8760, loss = 0.07180937\n",
      "Iteration 8761, loss = 0.07180774\n",
      "Iteration 8762, loss = 0.07180614\n",
      "Iteration 8763, loss = 0.07180451\n",
      "Iteration 8764, loss = 0.07180290\n",
      "Iteration 8765, loss = 0.07180129\n",
      "Iteration 8766, loss = 0.07179967\n",
      "Iteration 8767, loss = 0.07179805\n",
      "Iteration 8768, loss = 0.07179644\n",
      "Iteration 8769, loss = 0.07179482\n",
      "Iteration 8770, loss = 0.07179320\n",
      "Iteration 8771, loss = 0.07179159\n",
      "Iteration 8772, loss = 0.07178999\n",
      "Iteration 8773, loss = 0.07178835\n",
      "Iteration 8774, loss = 0.07178676\n",
      "Iteration 8775, loss = 0.07178512\n",
      "Iteration 8776, loss = 0.07178352\n",
      "Iteration 8777, loss = 0.07178190\n",
      "Iteration 8778, loss = 0.07178030\n",
      "Iteration 8779, loss = 0.07177867\n",
      "Iteration 8780, loss = 0.07177707\n",
      "Iteration 8781, loss = 0.07177546\n",
      "Iteration 8782, loss = 0.07177382\n",
      "Iteration 8783, loss = 0.07177224\n",
      "Iteration 8784, loss = 0.07177061\n",
      "Iteration 8785, loss = 0.07176900\n",
      "Iteration 8786, loss = 0.07176739\n",
      "Iteration 8787, loss = 0.07176578\n",
      "Iteration 8788, loss = 0.07176415\n",
      "Iteration 8789, loss = 0.07176257\n",
      "Iteration 8790, loss = 0.07176094\n",
      "Iteration 8791, loss = 0.07175932\n",
      "Iteration 8792, loss = 0.07175772\n",
      "Iteration 8793, loss = 0.07175614\n",
      "Iteration 8794, loss = 0.07175449\n",
      "Iteration 8795, loss = 0.07175287\n",
      "Iteration 8796, loss = 0.07175129\n",
      "Iteration 8797, loss = 0.07174965\n",
      "Iteration 8798, loss = 0.07174807\n",
      "Iteration 8799, loss = 0.07174644\n",
      "Iteration 8800, loss = 0.07174483\n",
      "Iteration 8801, loss = 0.07174324\n",
      "Iteration 8802, loss = 0.07174162\n",
      "Iteration 8803, loss = 0.07174001\n",
      "Iteration 8804, loss = 0.07173840\n",
      "Iteration 8805, loss = 0.07173678\n",
      "Iteration 8806, loss = 0.07173518\n",
      "Iteration 8807, loss = 0.07173359\n",
      "Iteration 8808, loss = 0.07173194\n",
      "Iteration 8809, loss = 0.07173037\n",
      "Iteration 8810, loss = 0.07172876\n",
      "Iteration 8811, loss = 0.07172714\n",
      "Iteration 8812, loss = 0.07172551\n",
      "Iteration 8813, loss = 0.07172396\n",
      "Iteration 8814, loss = 0.07172231\n",
      "Iteration 8815, loss = 0.07172071\n",
      "Iteration 8816, loss = 0.07171912\n",
      "Iteration 8817, loss = 0.07171749\n",
      "Iteration 8818, loss = 0.07171588\n",
      "Iteration 8819, loss = 0.07171432\n",
      "Iteration 8820, loss = 0.07171266\n",
      "Iteration 8821, loss = 0.07171107\n",
      "Iteration 8822, loss = 0.07170948\n",
      "Iteration 8823, loss = 0.07170786\n",
      "Iteration 8824, loss = 0.07170625\n",
      "Iteration 8825, loss = 0.07170465\n",
      "Iteration 8826, loss = 0.07170306\n",
      "Iteration 8827, loss = 0.07170142\n",
      "Iteration 8828, loss = 0.07169986\n",
      "Iteration 8829, loss = 0.07169821\n",
      "Iteration 8830, loss = 0.07169665\n",
      "Iteration 8831, loss = 0.07169504\n",
      "Iteration 8832, loss = 0.07169343\n",
      "Iteration 8833, loss = 0.07169181\n",
      "Iteration 8834, loss = 0.07169022\n",
      "Iteration 8835, loss = 0.07168859\n",
      "Iteration 8836, loss = 0.07168701\n",
      "Iteration 8837, loss = 0.07168540\n",
      "Iteration 8838, loss = 0.07168377\n",
      "Iteration 8839, loss = 0.07168222\n",
      "Iteration 8840, loss = 0.07168059\n",
      "Iteration 8841, loss = 0.07167898\n",
      "Iteration 8842, loss = 0.07167741\n",
      "Iteration 8843, loss = 0.07167580\n",
      "Iteration 8844, loss = 0.07167419\n",
      "Iteration 8845, loss = 0.07167260\n",
      "Iteration 8846, loss = 0.07167101\n",
      "Iteration 8847, loss = 0.07166939\n",
      "Iteration 8848, loss = 0.07166782\n",
      "Iteration 8849, loss = 0.07166621\n",
      "Iteration 8850, loss = 0.07166461\n",
      "Iteration 8851, loss = 0.07166303\n",
      "Iteration 8852, loss = 0.07166143\n",
      "Iteration 8853, loss = 0.07165981\n",
      "Iteration 8854, loss = 0.07165825\n",
      "Iteration 8855, loss = 0.07165666\n",
      "Iteration 8856, loss = 0.07165504\n",
      "Iteration 8857, loss = 0.07165346\n",
      "Iteration 8858, loss = 0.07165186\n",
      "Iteration 8859, loss = 0.07165026\n",
      "Iteration 8860, loss = 0.07164868\n",
      "Iteration 8861, loss = 0.07164707\n",
      "Iteration 8862, loss = 0.07164548\n",
      "Iteration 8863, loss = 0.07164391\n",
      "Iteration 8864, loss = 0.07164229\n",
      "Iteration 8865, loss = 0.07164071\n",
      "Iteration 8866, loss = 0.07163912\n",
      "Iteration 8867, loss = 0.07163751\n",
      "Iteration 8868, loss = 0.07163593\n",
      "Iteration 8869, loss = 0.07163435\n",
      "Iteration 8870, loss = 0.07163275\n",
      "Iteration 8871, loss = 0.07163114\n",
      "Iteration 8872, loss = 0.07162960\n",
      "Iteration 8873, loss = 0.07162796\n",
      "Iteration 8874, loss = 0.07162638\n",
      "Iteration 8875, loss = 0.07162480\n",
      "Iteration 8876, loss = 0.07162320\n",
      "Iteration 8877, loss = 0.07162160\n",
      "Iteration 8878, loss = 0.07162004\n",
      "Iteration 8879, loss = 0.07161844\n",
      "Iteration 8880, loss = 0.07161683\n",
      "Iteration 8881, loss = 0.07161529\n",
      "Iteration 8882, loss = 0.07161366\n",
      "Iteration 8883, loss = 0.07161210\n",
      "Iteration 8884, loss = 0.07161051\n",
      "Iteration 8885, loss = 0.07160892\n",
      "Iteration 8886, loss = 0.07160731\n",
      "Iteration 8887, loss = 0.07160575\n",
      "Iteration 8888, loss = 0.07160414\n",
      "Iteration 8889, loss = 0.07160257\n",
      "Iteration 8890, loss = 0.07160099\n",
      "Iteration 8891, loss = 0.07159940\n",
      "Iteration 8892, loss = 0.07159779\n",
      "Iteration 8893, loss = 0.07159623\n",
      "Iteration 8894, loss = 0.07159463\n",
      "Iteration 8895, loss = 0.07159305\n",
      "Iteration 8896, loss = 0.07159145\n",
      "Iteration 8897, loss = 0.07158988\n",
      "Iteration 8898, loss = 0.07158828\n",
      "Iteration 8899, loss = 0.07158669\n",
      "Iteration 8900, loss = 0.07158511\n",
      "Iteration 8901, loss = 0.07158354\n",
      "Iteration 8902, loss = 0.07158193\n",
      "Iteration 8903, loss = 0.07158035\n",
      "Iteration 8904, loss = 0.07157879\n",
      "Iteration 8905, loss = 0.07157720\n",
      "Iteration 8906, loss = 0.07157559\n",
      "Iteration 8907, loss = 0.07157405\n",
      "Iteration 8908, loss = 0.07157243\n",
      "Iteration 8909, loss = 0.07157084\n",
      "Iteration 8910, loss = 0.07156930\n",
      "Iteration 8911, loss = 0.07156768\n",
      "Iteration 8912, loss = 0.07156611\n",
      "Iteration 8913, loss = 0.07156452\n",
      "Iteration 8914, loss = 0.07156295\n",
      "Iteration 8915, loss = 0.07156134\n",
      "Iteration 8916, loss = 0.07155980\n",
      "Iteration 8917, loss = 0.07155818\n",
      "Iteration 8918, loss = 0.07155663\n",
      "Iteration 8919, loss = 0.07155503\n",
      "Iteration 8920, loss = 0.07155346\n",
      "Iteration 8921, loss = 0.07155186\n",
      "Iteration 8922, loss = 0.07155029\n",
      "Iteration 8923, loss = 0.07154871\n",
      "Iteration 8924, loss = 0.07154712\n",
      "Iteration 8925, loss = 0.07154556\n",
      "Iteration 8926, loss = 0.07154396\n",
      "Iteration 8927, loss = 0.07154238\n",
      "Iteration 8928, loss = 0.07154081\n",
      "Iteration 8929, loss = 0.07153923\n",
      "Iteration 8930, loss = 0.07153764\n",
      "Iteration 8931, loss = 0.07153608\n",
      "Iteration 8932, loss = 0.07153448\n",
      "Iteration 8933, loss = 0.07153291\n",
      "Iteration 8934, loss = 0.07153133\n",
      "Iteration 8935, loss = 0.07152974\n",
      "Iteration 8936, loss = 0.07152819\n",
      "Iteration 8937, loss = 0.07152659\n",
      "Iteration 8938, loss = 0.07152500\n",
      "Iteration 8939, loss = 0.07152346\n",
      "Iteration 8940, loss = 0.07152187\n",
      "Iteration 8941, loss = 0.07152026\n",
      "Iteration 8942, loss = 0.07151872\n",
      "Iteration 8943, loss = 0.07151714\n",
      "Iteration 8944, loss = 0.07151555\n",
      "Iteration 8945, loss = 0.07151397\n",
      "Iteration 8946, loss = 0.07151240\n",
      "Iteration 8947, loss = 0.07151081\n",
      "Iteration 8948, loss = 0.07150925\n",
      "Iteration 8949, loss = 0.07150767\n",
      "Iteration 8950, loss = 0.07150608\n",
      "Iteration 8951, loss = 0.07150452\n",
      "Iteration 8952, loss = 0.07150294\n",
      "Iteration 8953, loss = 0.07150136\n",
      "Iteration 8954, loss = 0.07149981\n",
      "Iteration 8955, loss = 0.07149820\n",
      "Iteration 8956, loss = 0.07149666\n",
      "Iteration 8957, loss = 0.07149508\n",
      "Iteration 8958, loss = 0.07149350\n",
      "Iteration 8959, loss = 0.07149190\n",
      "Iteration 8960, loss = 0.07149035\n",
      "Iteration 8961, loss = 0.07148875\n",
      "Iteration 8962, loss = 0.07148721\n",
      "Iteration 8963, loss = 0.07148562\n",
      "Iteration 8964, loss = 0.07148404\n",
      "Iteration 8965, loss = 0.07148248\n",
      "Iteration 8966, loss = 0.07148090\n",
      "Iteration 8967, loss = 0.07147932\n",
      "Iteration 8968, loss = 0.07147777\n",
      "Iteration 8969, loss = 0.07147618\n",
      "Iteration 8970, loss = 0.07147460\n",
      "Iteration 8971, loss = 0.07147305\n",
      "Iteration 8972, loss = 0.07147146\n",
      "Iteration 8973, loss = 0.07146988\n",
      "Iteration 8974, loss = 0.07146834\n",
      "Iteration 8975, loss = 0.07146676\n",
      "Iteration 8976, loss = 0.07146517\n",
      "Iteration 8977, loss = 0.07146363\n",
      "Iteration 8978, loss = 0.07146206\n",
      "Iteration 8979, loss = 0.07146047\n",
      "Iteration 8980, loss = 0.07145891\n",
      "Iteration 8981, loss = 0.07145734\n",
      "Iteration 8982, loss = 0.07145575\n",
      "Iteration 8983, loss = 0.07145420\n",
      "Iteration 8984, loss = 0.07145265\n",
      "Iteration 8985, loss = 0.07145106\n",
      "Iteration 8986, loss = 0.07144951\n",
      "Iteration 8987, loss = 0.07144790\n",
      "Iteration 8988, loss = 0.07144636\n",
      "Iteration 8989, loss = 0.07144479\n",
      "Iteration 8990, loss = 0.07144321\n",
      "Iteration 8991, loss = 0.07144164\n",
      "Iteration 8992, loss = 0.07144009\n",
      "Iteration 8993, loss = 0.07143850\n",
      "Iteration 8994, loss = 0.07143694\n",
      "Iteration 8995, loss = 0.07143538\n",
      "Iteration 8996, loss = 0.07143381\n",
      "Iteration 8997, loss = 0.07143224\n",
      "Iteration 8998, loss = 0.07143069\n",
      "Iteration 8999, loss = 0.07142911\n",
      "Iteration 9000, loss = 0.07142755\n",
      "Iteration 9001, loss = 0.07142599\n",
      "Iteration 9002, loss = 0.07142441\n",
      "Iteration 9003, loss = 0.07142287\n",
      "Iteration 9004, loss = 0.07142129\n",
      "Iteration 9005, loss = 0.07141971\n",
      "Iteration 9006, loss = 0.07141819\n",
      "Iteration 9007, loss = 0.07141660\n",
      "Iteration 9008, loss = 0.07141503\n",
      "Iteration 9009, loss = 0.07141349\n",
      "Iteration 9010, loss = 0.07141194\n",
      "Iteration 9011, loss = 0.07141035\n",
      "Iteration 9012, loss = 0.07140880\n",
      "Iteration 9013, loss = 0.07140723\n",
      "Iteration 9014, loss = 0.07140566\n",
      "Iteration 9015, loss = 0.07140412\n",
      "Iteration 9016, loss = 0.07140256\n",
      "Iteration 9017, loss = 0.07140100\n",
      "Iteration 9018, loss = 0.07139944\n",
      "Iteration 9019, loss = 0.07139784\n",
      "Iteration 9020, loss = 0.07139631\n",
      "Iteration 9021, loss = 0.07139475\n",
      "Iteration 9022, loss = 0.07139317\n",
      "Iteration 9023, loss = 0.07139163\n",
      "Iteration 9024, loss = 0.07139007\n",
      "Iteration 9025, loss = 0.07138849\n",
      "Iteration 9026, loss = 0.07138695\n",
      "Iteration 9027, loss = 0.07138538\n",
      "Iteration 9028, loss = 0.07138382\n",
      "Iteration 9029, loss = 0.07138226\n",
      "Iteration 9030, loss = 0.07138071\n",
      "Iteration 9031, loss = 0.07137913\n",
      "Iteration 9032, loss = 0.07137761\n",
      "Iteration 9033, loss = 0.07137603\n",
      "Iteration 9034, loss = 0.07137446\n",
      "Iteration 9035, loss = 0.07137294\n",
      "Iteration 9036, loss = 0.07137135\n",
      "Iteration 9037, loss = 0.07136978\n",
      "Iteration 9038, loss = 0.07136826\n",
      "Iteration 9039, loss = 0.07136669\n",
      "Iteration 9040, loss = 0.07136512\n",
      "Iteration 9041, loss = 0.07136358\n",
      "Iteration 9042, loss = 0.07136201\n",
      "Iteration 9043, loss = 0.07136046\n",
      "Iteration 9044, loss = 0.07135890\n",
      "Iteration 9045, loss = 0.07135737\n",
      "Iteration 9046, loss = 0.07135577\n",
      "Iteration 9047, loss = 0.07135425\n",
      "Iteration 9048, loss = 0.07135267\n",
      "Iteration 9049, loss = 0.07135114\n",
      "Iteration 9050, loss = 0.07134956\n",
      "Iteration 9051, loss = 0.07134800\n",
      "Iteration 9052, loss = 0.07134646\n",
      "Iteration 9053, loss = 0.07134490\n",
      "Iteration 9054, loss = 0.07134335\n",
      "Iteration 9055, loss = 0.07134180\n",
      "Iteration 9056, loss = 0.07134024\n",
      "Iteration 9057, loss = 0.07133867\n",
      "Iteration 9058, loss = 0.07133714\n",
      "Iteration 9059, loss = 0.07133557\n",
      "Iteration 9060, loss = 0.07133401\n",
      "Iteration 9061, loss = 0.07133249\n",
      "Iteration 9062, loss = 0.07133091\n",
      "Iteration 9063, loss = 0.07132934\n",
      "Iteration 9064, loss = 0.07132783\n",
      "Iteration 9065, loss = 0.07132628\n",
      "Iteration 9066, loss = 0.07132470\n",
      "Iteration 9067, loss = 0.07132316\n",
      "Iteration 9068, loss = 0.07132159\n",
      "Iteration 9069, loss = 0.07132004\n",
      "Iteration 9070, loss = 0.07131851\n",
      "Iteration 9071, loss = 0.07131695\n",
      "Iteration 9072, loss = 0.07131538\n",
      "Iteration 9073, loss = 0.07131385\n",
      "Iteration 9074, loss = 0.07131228\n",
      "Iteration 9075, loss = 0.07131074\n",
      "Iteration 9076, loss = 0.07130919\n",
      "Iteration 9077, loss = 0.07130764\n",
      "Iteration 9078, loss = 0.07130608\n",
      "Iteration 9079, loss = 0.07130454\n",
      "Iteration 9080, loss = 0.07130299\n",
      "Iteration 9081, loss = 0.07130143\n",
      "Iteration 9082, loss = 0.07129989\n",
      "Iteration 9083, loss = 0.07129833\n",
      "Iteration 9084, loss = 0.07129679\n",
      "Iteration 9085, loss = 0.07129523\n",
      "Iteration 9086, loss = 0.07129368\n",
      "Iteration 9087, loss = 0.07129215\n",
      "Iteration 9088, loss = 0.07129057\n",
      "Iteration 9089, loss = 0.07128904\n",
      "Iteration 9090, loss = 0.07128750\n",
      "Iteration 9091, loss = 0.07128595\n",
      "Iteration 9092, loss = 0.07128438\n",
      "Iteration 9093, loss = 0.07128286\n",
      "Iteration 9094, loss = 0.07128129\n",
      "Iteration 9095, loss = 0.07127973\n",
      "Iteration 9096, loss = 0.07127823\n",
      "Iteration 9097, loss = 0.07127665\n",
      "Iteration 9098, loss = 0.07127510\n",
      "Iteration 9099, loss = 0.07127359\n",
      "Iteration 9100, loss = 0.07127201\n",
      "Iteration 9101, loss = 0.07127047\n",
      "Iteration 9102, loss = 0.07126893\n",
      "Iteration 9103, loss = 0.07126737\n",
      "Iteration 9104, loss = 0.07126583\n",
      "Iteration 9105, loss = 0.07126429\n",
      "Iteration 9106, loss = 0.07126273\n",
      "Iteration 9107, loss = 0.07126120\n",
      "Iteration 9108, loss = 0.07125965\n",
      "Iteration 9109, loss = 0.07125810\n",
      "Iteration 9110, loss = 0.07125657\n",
      "Iteration 9111, loss = 0.07125501\n",
      "Iteration 9112, loss = 0.07125347\n",
      "Iteration 9113, loss = 0.07125195\n",
      "Iteration 9114, loss = 0.07125040\n",
      "Iteration 9115, loss = 0.07124885\n",
      "Iteration 9116, loss = 0.07124730\n",
      "Iteration 9117, loss = 0.07124575\n",
      "Iteration 9118, loss = 0.07124421\n",
      "Iteration 9119, loss = 0.07124268\n",
      "Iteration 9120, loss = 0.07124113\n",
      "Iteration 9121, loss = 0.07123959\n",
      "Iteration 9122, loss = 0.07123807\n",
      "Iteration 9123, loss = 0.07123648\n",
      "Iteration 9124, loss = 0.07123498\n",
      "Iteration 9125, loss = 0.07123342\n",
      "Iteration 9126, loss = 0.07123187\n",
      "Iteration 9127, loss = 0.07123033\n",
      "Iteration 9128, loss = 0.07122880\n",
      "Iteration 9129, loss = 0.07122724\n",
      "Iteration 9130, loss = 0.07122570\n",
      "Iteration 9131, loss = 0.07122418\n",
      "Iteration 9132, loss = 0.07122262\n",
      "Iteration 9133, loss = 0.07122110\n",
      "Iteration 9134, loss = 0.07121956\n",
      "Iteration 9135, loss = 0.07121799\n",
      "Iteration 9136, loss = 0.07121648\n",
      "Iteration 9137, loss = 0.07121494\n",
      "Iteration 9138, loss = 0.07121338\n",
      "Iteration 9139, loss = 0.07121186\n",
      "Iteration 9140, loss = 0.07121033\n",
      "Iteration 9141, loss = 0.07120877\n",
      "Iteration 9142, loss = 0.07120726\n",
      "Iteration 9143, loss = 0.07120568\n",
      "Iteration 9144, loss = 0.07120419\n",
      "Iteration 9145, loss = 0.07120261\n",
      "Iteration 9146, loss = 0.07120110\n",
      "Iteration 9147, loss = 0.07119954\n",
      "Iteration 9148, loss = 0.07119803\n",
      "Iteration 9149, loss = 0.07119646\n",
      "Iteration 9150, loss = 0.07119493\n",
      "Iteration 9151, loss = 0.07119340\n",
      "Iteration 9152, loss = 0.07119184\n",
      "Iteration 9153, loss = 0.07119033\n",
      "Iteration 9154, loss = 0.07118879\n",
      "Iteration 9155, loss = 0.07118724\n",
      "Iteration 9156, loss = 0.07118571\n",
      "Iteration 9157, loss = 0.07118418\n",
      "Iteration 9158, loss = 0.07118261\n",
      "Iteration 9159, loss = 0.07118113\n",
      "Iteration 9160, loss = 0.07117956\n",
      "Iteration 9161, loss = 0.07117802\n",
      "Iteration 9162, loss = 0.07117651\n",
      "Iteration 9163, loss = 0.07117496\n",
      "Iteration 9164, loss = 0.07117341\n",
      "Iteration 9165, loss = 0.07117190\n",
      "Iteration 9166, loss = 0.07117035\n",
      "Iteration 9167, loss = 0.07116883\n",
      "Iteration 9168, loss = 0.07116728\n",
      "Iteration 9169, loss = 0.07116576\n",
      "Iteration 9170, loss = 0.07116421\n",
      "Iteration 9171, loss = 0.07116269\n",
      "Iteration 9172, loss = 0.07116115\n",
      "Iteration 9173, loss = 0.07115961\n",
      "Iteration 9174, loss = 0.07115808\n",
      "Iteration 9175, loss = 0.07115655\n",
      "Iteration 9176, loss = 0.07115501\n",
      "Iteration 9177, loss = 0.07115348\n",
      "Iteration 9178, loss = 0.07115195\n",
      "Iteration 9179, loss = 0.07115043\n",
      "Iteration 9180, loss = 0.07114886\n",
      "Iteration 9181, loss = 0.07114736\n",
      "Iteration 9182, loss = 0.07114582\n",
      "Iteration 9183, loss = 0.07114429\n",
      "Iteration 9184, loss = 0.07114275\n",
      "Iteration 9185, loss = 0.07114123\n",
      "Iteration 9186, loss = 0.07113968\n",
      "Iteration 9187, loss = 0.07113815\n",
      "Iteration 9188, loss = 0.07113664\n",
      "Iteration 9189, loss = 0.07113509\n",
      "Iteration 9190, loss = 0.07113355\n",
      "Iteration 9191, loss = 0.07113205\n",
      "Iteration 9192, loss = 0.07113050\n",
      "Iteration 9193, loss = 0.07112896\n",
      "Iteration 9194, loss = 0.07112745\n",
      "Iteration 9195, loss = 0.07112591\n",
      "Iteration 9196, loss = 0.07112436\n",
      "Iteration 9197, loss = 0.07112285\n",
      "Iteration 9198, loss = 0.07112130\n",
      "Iteration 9199, loss = 0.07111979\n",
      "Iteration 9200, loss = 0.07111825\n",
      "Iteration 9201, loss = 0.07111671\n",
      "Iteration 9202, loss = 0.07111520\n",
      "Iteration 9203, loss = 0.07111367\n",
      "Iteration 9204, loss = 0.07111213\n",
      "Iteration 9205, loss = 0.07111061\n",
      "Iteration 9206, loss = 0.07110906\n",
      "Iteration 9207, loss = 0.07110755\n",
      "Iteration 9208, loss = 0.07110603\n",
      "Iteration 9209, loss = 0.07110448\n",
      "Iteration 9210, loss = 0.07110297\n",
      "Iteration 9211, loss = 0.07110144\n",
      "Iteration 9212, loss = 0.07109991\n",
      "Iteration 9213, loss = 0.07109837\n",
      "Iteration 9214, loss = 0.07109687\n",
      "Iteration 9215, loss = 0.07109531\n",
      "Iteration 9216, loss = 0.07109380\n",
      "Iteration 9217, loss = 0.07109228\n",
      "Iteration 9218, loss = 0.07109072\n",
      "Iteration 9219, loss = 0.07108924\n",
      "Iteration 9220, loss = 0.07108768\n",
      "Iteration 9221, loss = 0.07108614\n",
      "Iteration 9222, loss = 0.07108466\n",
      "Iteration 9223, loss = 0.07108310\n",
      "Iteration 9224, loss = 0.07108157\n",
      "Iteration 9225, loss = 0.07108007\n",
      "Iteration 9226, loss = 0.07107854\n",
      "Iteration 9227, loss = 0.07107698\n",
      "Iteration 9228, loss = 0.07107550\n",
      "Iteration 9229, loss = 0.07107394\n",
      "Iteration 9230, loss = 0.07107246\n",
      "Iteration 9231, loss = 0.07107091\n",
      "Iteration 9232, loss = 0.07106941\n",
      "Iteration 9233, loss = 0.07106783\n",
      "Iteration 9234, loss = 0.07106636\n",
      "Iteration 9235, loss = 0.07106480\n",
      "Iteration 9236, loss = 0.07106330\n",
      "Iteration 9237, loss = 0.07106177\n",
      "Iteration 9238, loss = 0.07106024\n",
      "Iteration 9239, loss = 0.07105871\n",
      "Iteration 9240, loss = 0.07105718\n",
      "Iteration 9241, loss = 0.07105566\n",
      "Iteration 9242, loss = 0.07105415\n",
      "Iteration 9243, loss = 0.07105261\n",
      "Iteration 9244, loss = 0.07105110\n",
      "Iteration 9245, loss = 0.07104959\n",
      "Iteration 9246, loss = 0.07104805\n",
      "Iteration 9247, loss = 0.07104651\n",
      "Iteration 9248, loss = 0.07104504\n",
      "Iteration 9249, loss = 0.07104348\n",
      "Iteration 9250, loss = 0.07104198\n",
      "Iteration 9251, loss = 0.07104047\n",
      "Iteration 9252, loss = 0.07103894\n",
      "Iteration 9253, loss = 0.07103738\n",
      "Iteration 9254, loss = 0.07103594\n",
      "Iteration 9255, loss = 0.07103436\n",
      "Iteration 9256, loss = 0.07103286\n",
      "Iteration 9257, loss = 0.07103135\n",
      "Iteration 9258, loss = 0.07102982\n",
      "Iteration 9259, loss = 0.07102829\n",
      "Iteration 9260, loss = 0.07102678\n",
      "Iteration 9261, loss = 0.07102525\n",
      "Iteration 9262, loss = 0.07102375\n",
      "Iteration 9263, loss = 0.07102224\n",
      "Iteration 9264, loss = 0.07102069\n",
      "Iteration 9265, loss = 0.07101921\n",
      "Iteration 9266, loss = 0.07101766\n",
      "Iteration 9267, loss = 0.07101617\n",
      "Iteration 9268, loss = 0.07101464\n",
      "Iteration 9269, loss = 0.07101312\n",
      "Iteration 9270, loss = 0.07101161\n",
      "Iteration 9271, loss = 0.07101009\n",
      "Iteration 9272, loss = 0.07100856\n",
      "Iteration 9273, loss = 0.07100707\n",
      "Iteration 9274, loss = 0.07100555\n",
      "Iteration 9275, loss = 0.07100401\n",
      "Iteration 9276, loss = 0.07100255\n",
      "Iteration 9277, loss = 0.07100098\n",
      "Iteration 9278, loss = 0.07099948\n",
      "Iteration 9279, loss = 0.07099799\n",
      "Iteration 9280, loss = 0.07099646\n",
      "Iteration 9281, loss = 0.07099493\n",
      "Iteration 9282, loss = 0.07099345\n",
      "Iteration 9283, loss = 0.07099190\n",
      "Iteration 9284, loss = 0.07099039\n",
      "Iteration 9285, loss = 0.07098890\n",
      "Iteration 9286, loss = 0.07098736\n",
      "Iteration 9287, loss = 0.07098585\n",
      "Iteration 9288, loss = 0.07098435\n",
      "Iteration 9289, loss = 0.07098283\n",
      "Iteration 9290, loss = 0.07098132\n",
      "Iteration 9291, loss = 0.07097981\n",
      "Iteration 9292, loss = 0.07097828\n",
      "Iteration 9293, loss = 0.07097679\n",
      "Iteration 9294, loss = 0.07097527\n",
      "Iteration 9295, loss = 0.07097374\n",
      "Iteration 9296, loss = 0.07097227\n",
      "Iteration 9297, loss = 0.07097074\n",
      "Iteration 9298, loss = 0.07096922\n",
      "Iteration 9299, loss = 0.07096772\n",
      "Iteration 9300, loss = 0.07096619\n",
      "Iteration 9301, loss = 0.07096469\n",
      "Iteration 9302, loss = 0.07096318\n",
      "Iteration 9303, loss = 0.07096165\n",
      "Iteration 9304, loss = 0.07096016\n",
      "Iteration 9305, loss = 0.07095866\n",
      "Iteration 9306, loss = 0.07095713\n",
      "Iteration 9307, loss = 0.07095562\n",
      "Iteration 9308, loss = 0.07095412\n",
      "Iteration 9309, loss = 0.07095260\n",
      "Iteration 9310, loss = 0.07095110\n",
      "Iteration 9311, loss = 0.07094959\n",
      "Iteration 9312, loss = 0.07094806\n",
      "Iteration 9313, loss = 0.07094659\n",
      "Iteration 9314, loss = 0.07094505\n",
      "Iteration 9315, loss = 0.07094354\n",
      "Iteration 9316, loss = 0.07094205\n",
      "Iteration 9317, loss = 0.07094053\n",
      "Iteration 9318, loss = 0.07093901\n",
      "Iteration 9319, loss = 0.07093753\n",
      "Iteration 9320, loss = 0.07093601\n",
      "Iteration 9321, loss = 0.07093448\n",
      "Iteration 9322, loss = 0.07093303\n",
      "Iteration 9323, loss = 0.07093148\n",
      "Iteration 9324, loss = 0.07092999\n",
      "Iteration 9325, loss = 0.07092848\n",
      "Iteration 9326, loss = 0.07092697\n",
      "Iteration 9327, loss = 0.07092545\n",
      "Iteration 9328, loss = 0.07092394\n",
      "Iteration 9329, loss = 0.07092245\n",
      "Iteration 9330, loss = 0.07092095\n",
      "Iteration 9331, loss = 0.07091942\n",
      "Iteration 9332, loss = 0.07091792\n",
      "Iteration 9333, loss = 0.07091643\n",
      "Iteration 9334, loss = 0.07091490\n",
      "Iteration 9335, loss = 0.07091342\n",
      "Iteration 9336, loss = 0.07091192\n",
      "Iteration 9337, loss = 0.07091040\n",
      "Iteration 9338, loss = 0.07090892\n",
      "Iteration 9339, loss = 0.07090740\n",
      "Iteration 9340, loss = 0.07090589\n",
      "Iteration 9341, loss = 0.07090438\n",
      "Iteration 9342, loss = 0.07090291\n",
      "Iteration 9343, loss = 0.07090138\n",
      "Iteration 9344, loss = 0.07089988\n",
      "Iteration 9345, loss = 0.07089836\n",
      "Iteration 9346, loss = 0.07089685\n",
      "Iteration 9347, loss = 0.07089537\n",
      "Iteration 9348, loss = 0.07089383\n",
      "Iteration 9349, loss = 0.07089235\n",
      "Iteration 9350, loss = 0.07089084\n",
      "Iteration 9351, loss = 0.07088934\n",
      "Iteration 9352, loss = 0.07088783\n",
      "Iteration 9353, loss = 0.07088636\n",
      "Iteration 9354, loss = 0.07088483\n",
      "Iteration 9355, loss = 0.07088333\n",
      "Iteration 9356, loss = 0.07088183\n",
      "Iteration 9357, loss = 0.07088032\n",
      "Iteration 9358, loss = 0.07087882\n",
      "Iteration 9359, loss = 0.07087732\n",
      "Iteration 9360, loss = 0.07087581\n",
      "Iteration 9361, loss = 0.07087433\n",
      "Iteration 9362, loss = 0.07087282\n",
      "Iteration 9363, loss = 0.07087130\n",
      "Iteration 9364, loss = 0.07086984\n",
      "Iteration 9365, loss = 0.07086832\n",
      "Iteration 9366, loss = 0.07086680\n",
      "Iteration 9367, loss = 0.07086534\n",
      "Iteration 9368, loss = 0.07086379\n",
      "Iteration 9369, loss = 0.07086233\n",
      "Iteration 9370, loss = 0.07086081\n",
      "Iteration 9371, loss = 0.07085930\n",
      "Iteration 9372, loss = 0.07085783\n",
      "Iteration 9373, loss = 0.07085633\n",
      "Iteration 9374, loss = 0.07085480\n",
      "Iteration 9375, loss = 0.07085334\n",
      "Iteration 9376, loss = 0.07085181\n",
      "Iteration 9377, loss = 0.07085031\n",
      "Iteration 9378, loss = 0.07084884\n",
      "Iteration 9379, loss = 0.07084733\n",
      "Iteration 9380, loss = 0.07084581\n",
      "Iteration 9381, loss = 0.07084435\n",
      "Iteration 9382, loss = 0.07084282\n",
      "Iteration 9383, loss = 0.07084134\n",
      "Iteration 9384, loss = 0.07083985\n",
      "Iteration 9385, loss = 0.07083833\n",
      "Iteration 9386, loss = 0.07083683\n",
      "Iteration 9387, loss = 0.07083536\n",
      "Iteration 9388, loss = 0.07083384\n",
      "Iteration 9389, loss = 0.07083235\n",
      "Iteration 9390, loss = 0.07083086\n",
      "Iteration 9391, loss = 0.07082934\n",
      "Iteration 9392, loss = 0.07082787\n",
      "Iteration 9393, loss = 0.07082636\n",
      "Iteration 9394, loss = 0.07082485\n",
      "Iteration 9395, loss = 0.07082340\n",
      "Iteration 9396, loss = 0.07082186\n",
      "Iteration 9397, loss = 0.07082037\n",
      "Iteration 9398, loss = 0.07081891\n",
      "Iteration 9399, loss = 0.07081738\n",
      "Iteration 9400, loss = 0.07081590\n",
      "Iteration 9401, loss = 0.07081444\n",
      "Iteration 9402, loss = 0.07081291\n",
      "Iteration 9403, loss = 0.07081141\n",
      "Iteration 9404, loss = 0.07080996\n",
      "Iteration 9405, loss = 0.07080842\n",
      "Iteration 9406, loss = 0.07080694\n",
      "Iteration 9407, loss = 0.07080546\n",
      "Iteration 9408, loss = 0.07080392\n",
      "Iteration 9409, loss = 0.07080247\n",
      "Iteration 9410, loss = 0.07080095\n",
      "Iteration 9411, loss = 0.07079945\n",
      "Iteration 9412, loss = 0.07079796\n",
      "Iteration 9413, loss = 0.07079649\n",
      "Iteration 9414, loss = 0.07079496\n",
      "Iteration 9415, loss = 0.07079351\n",
      "Iteration 9416, loss = 0.07079199\n",
      "Iteration 9417, loss = 0.07079049\n",
      "Iteration 9418, loss = 0.07078902\n",
      "Iteration 9419, loss = 0.07078752\n",
      "Iteration 9420, loss = 0.07078601\n",
      "Iteration 9421, loss = 0.07078455\n",
      "Iteration 9422, loss = 0.07078302\n",
      "Iteration 9423, loss = 0.07078157\n",
      "Iteration 9424, loss = 0.07078006\n",
      "Iteration 9425, loss = 0.07077856\n",
      "Iteration 9426, loss = 0.07077709\n",
      "Iteration 9427, loss = 0.07077559\n",
      "Iteration 9428, loss = 0.07077409\n",
      "Iteration 9429, loss = 0.07077262\n",
      "Iteration 9430, loss = 0.07077113\n",
      "Iteration 9431, loss = 0.07076961\n",
      "Iteration 9432, loss = 0.07076815\n",
      "Iteration 9433, loss = 0.07076665\n",
      "Iteration 9434, loss = 0.07076514\n",
      "Iteration 9435, loss = 0.07076368\n",
      "Iteration 9436, loss = 0.07076217\n",
      "Iteration 9437, loss = 0.07076070\n",
      "Iteration 9438, loss = 0.07075919\n",
      "Iteration 9439, loss = 0.07075771\n",
      "Iteration 9440, loss = 0.07075622\n",
      "Iteration 9441, loss = 0.07075475\n",
      "Iteration 9442, loss = 0.07075323\n",
      "Iteration 9443, loss = 0.07075178\n",
      "Iteration 9444, loss = 0.07075027\n",
      "Iteration 9445, loss = 0.07074880\n",
      "Iteration 9446, loss = 0.07074731\n",
      "Iteration 9447, loss = 0.07074582\n",
      "Iteration 9448, loss = 0.07074431\n",
      "Iteration 9449, loss = 0.07074284\n",
      "Iteration 9450, loss = 0.07074134\n",
      "Iteration 9451, loss = 0.07073986\n",
      "Iteration 9452, loss = 0.07073837\n",
      "Iteration 9453, loss = 0.07073688\n",
      "Iteration 9454, loss = 0.07073540\n",
      "Iteration 9455, loss = 0.07073394\n",
      "Iteration 9456, loss = 0.07073241\n",
      "Iteration 9457, loss = 0.07073097\n",
      "Iteration 9458, loss = 0.07072944\n",
      "Iteration 9459, loss = 0.07072800\n",
      "Iteration 9460, loss = 0.07072649\n",
      "Iteration 9461, loss = 0.07072503\n",
      "Iteration 9462, loss = 0.07072349\n",
      "Iteration 9463, loss = 0.07072208\n",
      "Iteration 9464, loss = 0.07072056\n",
      "Iteration 9465, loss = 0.07071907\n",
      "Iteration 9466, loss = 0.07071758\n",
      "Iteration 9467, loss = 0.07071609\n",
      "Iteration 9468, loss = 0.07071463\n",
      "Iteration 9469, loss = 0.07071312\n",
      "Iteration 9470, loss = 0.07071166\n",
      "Iteration 9471, loss = 0.07071016\n",
      "Iteration 9472, loss = 0.07070867\n",
      "Iteration 9473, loss = 0.07070719\n",
      "Iteration 9474, loss = 0.07070570\n",
      "Iteration 9475, loss = 0.07070423\n",
      "Iteration 9476, loss = 0.07070274\n",
      "Iteration 9477, loss = 0.07070127\n",
      "Iteration 9478, loss = 0.07069976\n",
      "Iteration 9479, loss = 0.07069832\n",
      "Iteration 9480, loss = 0.07069681\n",
      "Iteration 9481, loss = 0.07069533\n",
      "Iteration 9482, loss = 0.07069386\n",
      "Iteration 9483, loss = 0.07069235\n",
      "Iteration 9484, loss = 0.07069091\n",
      "Iteration 9485, loss = 0.07068940\n",
      "Iteration 9486, loss = 0.07068793\n",
      "Iteration 9487, loss = 0.07068644\n",
      "Iteration 9488, loss = 0.07068497\n",
      "Iteration 9489, loss = 0.07068346\n",
      "Iteration 9490, loss = 0.07068202\n",
      "Iteration 9491, loss = 0.07068051\n",
      "Iteration 9492, loss = 0.07067902\n",
      "Iteration 9493, loss = 0.07067757\n",
      "Iteration 9494, loss = 0.07067608\n",
      "Iteration 9495, loss = 0.07067460\n",
      "Iteration 9496, loss = 0.07067313\n",
      "Iteration 9497, loss = 0.07067163\n",
      "Iteration 9498, loss = 0.07067017\n",
      "Iteration 9499, loss = 0.07066868\n",
      "Iteration 9500, loss = 0.07066720\n",
      "Iteration 9501, loss = 0.07066573\n",
      "Iteration 9502, loss = 0.07066424\n",
      "Iteration 9503, loss = 0.07066276\n",
      "Iteration 9504, loss = 0.07066130\n",
      "Iteration 9505, loss = 0.07065979\n",
      "Iteration 9506, loss = 0.07065833\n",
      "Iteration 9507, loss = 0.07065686\n",
      "Iteration 9508, loss = 0.07065538\n",
      "Iteration 9509, loss = 0.07065390\n",
      "Iteration 9510, loss = 0.07065244\n",
      "Iteration 9511, loss = 0.07065092\n",
      "Iteration 9512, loss = 0.07064949\n",
      "Iteration 9513, loss = 0.07064799\n",
      "Iteration 9514, loss = 0.07064649\n",
      "Iteration 9515, loss = 0.07064506\n",
      "Iteration 9516, loss = 0.07064354\n",
      "Iteration 9517, loss = 0.07064208\n",
      "Iteration 9518, loss = 0.07064062\n",
      "Iteration 9519, loss = 0.07063914\n",
      "Iteration 9520, loss = 0.07063764\n",
      "Iteration 9521, loss = 0.07063620\n",
      "Iteration 9522, loss = 0.07063470\n",
      "Iteration 9523, loss = 0.07063322\n",
      "Iteration 9524, loss = 0.07063176\n",
      "Iteration 9525, loss = 0.07063026\n",
      "Iteration 9526, loss = 0.07062881\n",
      "Iteration 9527, loss = 0.07062733\n",
      "Iteration 9528, loss = 0.07062585\n",
      "Iteration 9529, loss = 0.07062439\n",
      "Iteration 9530, loss = 0.07062291\n",
      "Iteration 9531, loss = 0.07062141\n",
      "Iteration 9532, loss = 0.07061999\n",
      "Iteration 9533, loss = 0.07061846\n",
      "Iteration 9534, loss = 0.07061701\n",
      "Iteration 9535, loss = 0.07061554\n",
      "Iteration 9536, loss = 0.07061405\n",
      "Iteration 9537, loss = 0.07061258\n",
      "Iteration 9538, loss = 0.07061112\n",
      "Iteration 9539, loss = 0.07060964\n",
      "Iteration 9540, loss = 0.07060816\n",
      "Iteration 9541, loss = 0.07060671\n",
      "Iteration 9542, loss = 0.07060520\n",
      "Iteration 9543, loss = 0.07060376\n",
      "Iteration 9544, loss = 0.07060227\n",
      "Iteration 9545, loss = 0.07060078\n",
      "Iteration 9546, loss = 0.07059936\n",
      "Iteration 9547, loss = 0.07059786\n",
      "Iteration 9548, loss = 0.07059637\n",
      "Iteration 9549, loss = 0.07059494\n",
      "Iteration 9550, loss = 0.07059343\n",
      "Iteration 9551, loss = 0.07059199\n",
      "Iteration 9552, loss = 0.07059054\n",
      "Iteration 9553, loss = 0.07058904\n",
      "Iteration 9554, loss = 0.07058756\n",
      "Iteration 9555, loss = 0.07058613\n",
      "Iteration 9556, loss = 0.07058464\n",
      "Iteration 9557, loss = 0.07058319\n",
      "Iteration 9558, loss = 0.07058172\n",
      "Iteration 9559, loss = 0.07058024\n",
      "Iteration 9560, loss = 0.07057880\n",
      "Iteration 9561, loss = 0.07057733\n",
      "Iteration 9562, loss = 0.07057583\n",
      "Iteration 9563, loss = 0.07057443\n",
      "Iteration 9564, loss = 0.07057293\n",
      "Iteration 9565, loss = 0.07057146\n",
      "Iteration 9566, loss = 0.07057002\n",
      "Iteration 9567, loss = 0.07056854\n",
      "Iteration 9568, loss = 0.07056709\n",
      "Iteration 9569, loss = 0.07056563\n",
      "Iteration 9570, loss = 0.07056416\n",
      "Iteration 9571, loss = 0.07056270\n",
      "Iteration 9572, loss = 0.07056126\n",
      "Iteration 9573, loss = 0.07055976\n",
      "Iteration 9574, loss = 0.07055835\n",
      "Iteration 9575, loss = 0.07055685\n",
      "Iteration 9576, loss = 0.07055541\n",
      "Iteration 9577, loss = 0.07055396\n",
      "Iteration 9578, loss = 0.07055248\n",
      "Iteration 9579, loss = 0.07055099\n",
      "Iteration 9580, loss = 0.07054957\n",
      "Iteration 9581, loss = 0.07054808\n",
      "Iteration 9582, loss = 0.07054665\n",
      "Iteration 9583, loss = 0.07054516\n",
      "Iteration 9584, loss = 0.07054371\n",
      "Iteration 9585, loss = 0.07054226\n",
      "Iteration 9586, loss = 0.07054080\n",
      "Iteration 9587, loss = 0.07053932\n",
      "Iteration 9588, loss = 0.07053788\n",
      "Iteration 9589, loss = 0.07053642\n",
      "Iteration 9590, loss = 0.07053496\n",
      "Iteration 9591, loss = 0.07053350\n",
      "Iteration 9592, loss = 0.07053204\n",
      "Iteration 9593, loss = 0.07053059\n",
      "Iteration 9594, loss = 0.07052913\n",
      "Iteration 9595, loss = 0.07052766\n",
      "Iteration 9596, loss = 0.07052621\n",
      "Iteration 9597, loss = 0.07052477\n",
      "Iteration 9598, loss = 0.07052328\n",
      "Iteration 9599, loss = 0.07052185\n",
      "Iteration 9600, loss = 0.07052040\n",
      "Iteration 9601, loss = 0.07051891\n",
      "Iteration 9602, loss = 0.07051748\n",
      "Iteration 9603, loss = 0.07051602\n",
      "Iteration 9604, loss = 0.07051456\n",
      "Iteration 9605, loss = 0.07051310\n",
      "Iteration 9606, loss = 0.07051164\n",
      "Iteration 9607, loss = 0.07051019\n",
      "Iteration 9608, loss = 0.07050875\n",
      "Iteration 9609, loss = 0.07050726\n",
      "Iteration 9610, loss = 0.07050585\n",
      "Iteration 9611, loss = 0.07050437\n",
      "Iteration 9612, loss = 0.07050295\n",
      "Iteration 9613, loss = 0.07050147\n",
      "Iteration 9614, loss = 0.07050002\n",
      "Iteration 9615, loss = 0.07049853\n",
      "Iteration 9616, loss = 0.07049710\n",
      "Iteration 9617, loss = 0.07049564\n",
      "Iteration 9618, loss = 0.07049420\n",
      "Iteration 9619, loss = 0.07049273\n",
      "Iteration 9620, loss = 0.07049128\n",
      "Iteration 9621, loss = 0.07048984\n",
      "Iteration 9622, loss = 0.07048837\n",
      "Iteration 9623, loss = 0.07048693\n",
      "Iteration 9624, loss = 0.07048547\n",
      "Iteration 9625, loss = 0.07048402\n",
      "Iteration 9626, loss = 0.07048257\n",
      "Iteration 9627, loss = 0.07048111\n",
      "Iteration 9628, loss = 0.07047966\n",
      "Iteration 9629, loss = 0.07047823\n",
      "Iteration 9630, loss = 0.07047675\n",
      "Iteration 9631, loss = 0.07047532\n",
      "Iteration 9632, loss = 0.07047386\n",
      "Iteration 9633, loss = 0.07047241\n",
      "Iteration 9634, loss = 0.07047095\n",
      "Iteration 9635, loss = 0.07046950\n",
      "Iteration 9636, loss = 0.07046806\n",
      "Iteration 9637, loss = 0.07046660\n",
      "Iteration 9638, loss = 0.07046514\n",
      "Iteration 9639, loss = 0.07046370\n",
      "Iteration 9640, loss = 0.07046226\n",
      "Iteration 9641, loss = 0.07046079\n",
      "Iteration 9642, loss = 0.07045936\n",
      "Iteration 9643, loss = 0.07045792\n",
      "Iteration 9644, loss = 0.07045645\n",
      "Iteration 9645, loss = 0.07045501\n",
      "Iteration 9646, loss = 0.07045356\n",
      "Iteration 9647, loss = 0.07045211\n",
      "Iteration 9648, loss = 0.07045066\n",
      "Iteration 9649, loss = 0.07044921\n",
      "Iteration 9650, loss = 0.07044776\n",
      "Iteration 9651, loss = 0.07044633\n",
      "Iteration 9652, loss = 0.07044486\n",
      "Iteration 9653, loss = 0.07044342\n",
      "Iteration 9654, loss = 0.07044198\n",
      "Iteration 9655, loss = 0.07044051\n",
      "Iteration 9656, loss = 0.07043908\n",
      "Iteration 9657, loss = 0.07043763\n",
      "Iteration 9658, loss = 0.07043618\n",
      "Iteration 9659, loss = 0.07043473\n",
      "Iteration 9660, loss = 0.07043329\n",
      "Iteration 9661, loss = 0.07043183\n",
      "Iteration 9662, loss = 0.07043041\n",
      "Iteration 9663, loss = 0.07042895\n",
      "Iteration 9664, loss = 0.07042748\n",
      "Iteration 9665, loss = 0.07042607\n",
      "Iteration 9666, loss = 0.07042462\n",
      "Iteration 9667, loss = 0.07042314\n",
      "Iteration 9668, loss = 0.07042175\n",
      "Iteration 9669, loss = 0.07042026\n",
      "Iteration 9670, loss = 0.07041886\n",
      "Iteration 9671, loss = 0.07041739\n",
      "Iteration 9672, loss = 0.07041593\n",
      "Iteration 9673, loss = 0.07041450\n",
      "Iteration 9674, loss = 0.07041306\n",
      "Iteration 9675, loss = 0.07041158\n",
      "Iteration 9676, loss = 0.07041021\n",
      "Iteration 9677, loss = 0.07040869\n",
      "Iteration 9678, loss = 0.07040731\n",
      "Iteration 9679, loss = 0.07040585\n",
      "Iteration 9680, loss = 0.07040438\n",
      "Iteration 9681, loss = 0.07040295\n",
      "Iteration 9682, loss = 0.07040153\n",
      "Iteration 9683, loss = 0.07040004\n",
      "Iteration 9684, loss = 0.07039863\n",
      "Iteration 9685, loss = 0.07039719\n",
      "Iteration 9686, loss = 0.07039571\n",
      "Iteration 9687, loss = 0.07039431\n",
      "Iteration 9688, loss = 0.07039283\n",
      "Iteration 9689, loss = 0.07039140\n",
      "Iteration 9690, loss = 0.07038997\n",
      "Iteration 9691, loss = 0.07038851\n",
      "Iteration 9692, loss = 0.07038708\n",
      "Iteration 9693, loss = 0.07038563\n",
      "Iteration 9694, loss = 0.07038419\n",
      "Iteration 9695, loss = 0.07038276\n",
      "Iteration 9696, loss = 0.07038131\n",
      "Iteration 9697, loss = 0.07037987\n",
      "Iteration 9698, loss = 0.07037844\n",
      "Iteration 9699, loss = 0.07037699\n",
      "Iteration 9700, loss = 0.07037553\n",
      "Iteration 9701, loss = 0.07037413\n",
      "Iteration 9702, loss = 0.07037267\n",
      "Iteration 9703, loss = 0.07037121\n",
      "Iteration 9704, loss = 0.07036980\n",
      "Iteration 9705, loss = 0.07036833\n",
      "Iteration 9706, loss = 0.07036692\n",
      "Iteration 9707, loss = 0.07036546\n",
      "Iteration 9708, loss = 0.07036401\n",
      "Iteration 9709, loss = 0.07036260\n",
      "Iteration 9710, loss = 0.07036116\n",
      "Iteration 9711, loss = 0.07035968\n",
      "Iteration 9712, loss = 0.07035832\n",
      "Iteration 9713, loss = 0.07035682\n",
      "Iteration 9714, loss = 0.07035542\n",
      "Iteration 9715, loss = 0.07035398\n",
      "Iteration 9716, loss = 0.07035251\n",
      "Iteration 9717, loss = 0.07035108\n",
      "Iteration 9718, loss = 0.07034966\n",
      "Iteration 9719, loss = 0.07034818\n",
      "Iteration 9720, loss = 0.07034679\n",
      "Iteration 9721, loss = 0.07034535\n",
      "Iteration 9722, loss = 0.07034388\n",
      "Iteration 9723, loss = 0.07034248\n",
      "Iteration 9724, loss = 0.07034103\n",
      "Iteration 9725, loss = 0.07033960\n",
      "Iteration 9726, loss = 0.07033816\n",
      "Iteration 9727, loss = 0.07033671\n",
      "Iteration 9728, loss = 0.07033527\n",
      "Iteration 9729, loss = 0.07033384\n",
      "Iteration 9730, loss = 0.07033239\n",
      "Iteration 9731, loss = 0.07033098\n",
      "Iteration 9732, loss = 0.07032954\n",
      "Iteration 9733, loss = 0.07032807\n",
      "Iteration 9734, loss = 0.07032667\n",
      "Iteration 9735, loss = 0.07032522\n",
      "Iteration 9736, loss = 0.07032379\n",
      "Iteration 9737, loss = 0.07032236\n",
      "Iteration 9738, loss = 0.07032091\n",
      "Iteration 9739, loss = 0.07031950\n",
      "Iteration 9740, loss = 0.07031805\n",
      "Iteration 9741, loss = 0.07031663\n",
      "Iteration 9742, loss = 0.07031519\n",
      "Iteration 9743, loss = 0.07031376\n",
      "Iteration 9744, loss = 0.07031232\n",
      "Iteration 9745, loss = 0.07031088\n",
      "Iteration 9746, loss = 0.07030947\n",
      "Iteration 9747, loss = 0.07030802\n",
      "Iteration 9748, loss = 0.07030660\n",
      "Iteration 9749, loss = 0.07030515\n",
      "Iteration 9750, loss = 0.07030375\n",
      "Iteration 9751, loss = 0.07030229\n",
      "Iteration 9752, loss = 0.07030086\n",
      "Iteration 9753, loss = 0.07029944\n",
      "Iteration 9754, loss = 0.07029799\n",
      "Iteration 9755, loss = 0.07029659\n",
      "Iteration 9756, loss = 0.07029515\n",
      "Iteration 9757, loss = 0.07029371\n",
      "Iteration 9758, loss = 0.07029226\n",
      "Iteration 9759, loss = 0.07029084\n",
      "Iteration 9760, loss = 0.07028942\n",
      "Iteration 9761, loss = 0.07028800\n",
      "Iteration 9762, loss = 0.07028654\n",
      "Iteration 9763, loss = 0.07028514\n",
      "Iteration 9764, loss = 0.07028369\n",
      "Iteration 9765, loss = 0.07028226\n",
      "Iteration 9766, loss = 0.07028085\n",
      "Iteration 9767, loss = 0.07027939\n",
      "Iteration 9768, loss = 0.07027797\n",
      "Iteration 9769, loss = 0.07027656\n",
      "Iteration 9770, loss = 0.07027511\n",
      "Iteration 9771, loss = 0.07027369\n",
      "Iteration 9772, loss = 0.07027227\n",
      "Iteration 9773, loss = 0.07027082\n",
      "Iteration 9774, loss = 0.07026940\n",
      "Iteration 9775, loss = 0.07026797\n",
      "Iteration 9776, loss = 0.07026654\n",
      "Iteration 9777, loss = 0.07026512\n",
      "Iteration 9778, loss = 0.07026368\n",
      "Iteration 9779, loss = 0.07026226\n",
      "Iteration 9780, loss = 0.07026084\n",
      "Iteration 9781, loss = 0.07025939\n",
      "Iteration 9782, loss = 0.07025798\n",
      "Iteration 9783, loss = 0.07025657\n",
      "Iteration 9784, loss = 0.07025512\n",
      "Iteration 9785, loss = 0.07025370\n",
      "Iteration 9786, loss = 0.07025228\n",
      "Iteration 9787, loss = 0.07025085\n",
      "Iteration 9788, loss = 0.07024942\n",
      "Iteration 9789, loss = 0.07024798\n",
      "Iteration 9790, loss = 0.07024657\n",
      "Iteration 9791, loss = 0.07024515\n",
      "Iteration 9792, loss = 0.07024372\n",
      "Iteration 9793, loss = 0.07024228\n",
      "Iteration 9794, loss = 0.07024090\n",
      "Iteration 9795, loss = 0.07023942\n",
      "Iteration 9796, loss = 0.07023804\n",
      "Iteration 9797, loss = 0.07023660\n",
      "Iteration 9798, loss = 0.07023516\n",
      "Iteration 9799, loss = 0.07023374\n",
      "Iteration 9800, loss = 0.07023231\n",
      "Iteration 9801, loss = 0.07023088\n",
      "Iteration 9802, loss = 0.07022949\n",
      "Iteration 9803, loss = 0.07022803\n",
      "Iteration 9804, loss = 0.07022664\n",
      "Iteration 9805, loss = 0.07022521\n",
      "Iteration 9806, loss = 0.07022377\n",
      "Iteration 9807, loss = 0.07022234\n",
      "Iteration 9808, loss = 0.07022097\n",
      "Iteration 9809, loss = 0.07021950\n",
      "Iteration 9810, loss = 0.07021810\n",
      "Iteration 9811, loss = 0.07021664\n",
      "Iteration 9812, loss = 0.07021524\n",
      "Iteration 9813, loss = 0.07021382\n",
      "Iteration 9814, loss = 0.07021238\n",
      "Iteration 9815, loss = 0.07021097\n",
      "Iteration 9816, loss = 0.07020954\n",
      "Iteration 9817, loss = 0.07020812\n",
      "Iteration 9818, loss = 0.07020671\n",
      "Iteration 9819, loss = 0.07020528\n",
      "Iteration 9820, loss = 0.07020383\n",
      "Iteration 9821, loss = 0.07020246\n",
      "Iteration 9822, loss = 0.07020101\n",
      "Iteration 9823, loss = 0.07019959\n",
      "Iteration 9824, loss = 0.07019820\n",
      "Iteration 9825, loss = 0.07019674\n",
      "Iteration 9826, loss = 0.07019536\n",
      "Iteration 9827, loss = 0.07019393\n",
      "Iteration 9828, loss = 0.07019248\n",
      "Iteration 9829, loss = 0.07019107\n",
      "Iteration 9830, loss = 0.07018965\n",
      "Iteration 9831, loss = 0.07018821\n",
      "Iteration 9832, loss = 0.07018684\n",
      "Iteration 9833, loss = 0.07018540\n",
      "Iteration 9834, loss = 0.07018395\n",
      "Iteration 9835, loss = 0.07018258\n",
      "Iteration 9836, loss = 0.07018112\n",
      "Iteration 9837, loss = 0.07017973\n",
      "Iteration 9838, loss = 0.07017829\n",
      "Iteration 9839, loss = 0.07017685\n",
      "Iteration 9840, loss = 0.07017548\n",
      "Iteration 9841, loss = 0.07017402\n",
      "Iteration 9842, loss = 0.07017261\n",
      "Iteration 9843, loss = 0.07017120\n",
      "Iteration 9844, loss = 0.07016976\n",
      "Iteration 9845, loss = 0.07016836\n",
      "Iteration 9846, loss = 0.07016694\n",
      "Iteration 9847, loss = 0.07016549\n",
      "Iteration 9848, loss = 0.07016412\n",
      "Iteration 9849, loss = 0.07016269\n",
      "Iteration 9850, loss = 0.07016124\n",
      "Iteration 9851, loss = 0.07015985\n",
      "Iteration 9852, loss = 0.07015841\n",
      "Iteration 9853, loss = 0.07015700\n",
      "Iteration 9854, loss = 0.07015559\n",
      "Iteration 9855, loss = 0.07015415\n",
      "Iteration 9856, loss = 0.07015275\n",
      "Iteration 9857, loss = 0.07015133\n",
      "Iteration 9858, loss = 0.07014991\n",
      "Iteration 9859, loss = 0.07014850\n",
      "Iteration 9860, loss = 0.07014709\n",
      "Iteration 9861, loss = 0.07014564\n",
      "Iteration 9862, loss = 0.07014428\n",
      "Iteration 9863, loss = 0.07014283\n",
      "Iteration 9864, loss = 0.07014142\n",
      "Iteration 9865, loss = 0.07013999\n",
      "Iteration 9866, loss = 0.07013857\n",
      "Iteration 9867, loss = 0.07013715\n",
      "Iteration 9868, loss = 0.07013575\n",
      "Iteration 9869, loss = 0.07013430\n",
      "Iteration 9870, loss = 0.07013293\n",
      "Iteration 9871, loss = 0.07013150\n",
      "Iteration 9872, loss = 0.07013006\n",
      "Iteration 9873, loss = 0.07012867\n",
      "Iteration 9874, loss = 0.07012724\n",
      "Iteration 9875, loss = 0.07012583\n",
      "Iteration 9876, loss = 0.07012441\n",
      "Iteration 9877, loss = 0.07012299\n",
      "Iteration 9878, loss = 0.07012160\n",
      "Iteration 9879, loss = 0.07012017\n",
      "Iteration 9880, loss = 0.07011877\n",
      "Iteration 9881, loss = 0.07011736\n",
      "Iteration 9882, loss = 0.07011594\n",
      "Iteration 9883, loss = 0.07011449\n",
      "Iteration 9884, loss = 0.07011313\n",
      "Iteration 9885, loss = 0.07011169\n",
      "Iteration 9886, loss = 0.07011028\n",
      "Iteration 9887, loss = 0.07010885\n",
      "Iteration 9888, loss = 0.07010743\n",
      "Iteration 9889, loss = 0.07010604\n",
      "Iteration 9890, loss = 0.07010462\n",
      "Iteration 9891, loss = 0.07010318\n",
      "Iteration 9892, loss = 0.07010179\n",
      "Iteration 9893, loss = 0.07010038\n",
      "Iteration 9894, loss = 0.07009896\n",
      "Iteration 9895, loss = 0.07009754\n",
      "Iteration 9896, loss = 0.07009614\n",
      "Iteration 9897, loss = 0.07009474\n",
      "Iteration 9898, loss = 0.07009332\n",
      "Iteration 9899, loss = 0.07009190\n",
      "Iteration 9900, loss = 0.07009053\n",
      "Iteration 9901, loss = 0.07008907\n",
      "Iteration 9902, loss = 0.07008768\n",
      "Iteration 9903, loss = 0.07008625\n",
      "Iteration 9904, loss = 0.07008486\n",
      "Iteration 9905, loss = 0.07008344\n",
      "Iteration 9906, loss = 0.07008202\n",
      "Iteration 9907, loss = 0.07008061\n",
      "Iteration 9908, loss = 0.07007920\n",
      "Iteration 9909, loss = 0.07007778\n",
      "Iteration 9910, loss = 0.07007640\n",
      "Iteration 9911, loss = 0.07007497\n",
      "Iteration 9912, loss = 0.07007355\n",
      "Iteration 9913, loss = 0.07007218\n",
      "Iteration 9914, loss = 0.07007072\n",
      "Iteration 9915, loss = 0.07006935\n",
      "Iteration 9916, loss = 0.07006792\n",
      "Iteration 9917, loss = 0.07006651\n",
      "Iteration 9918, loss = 0.07006511\n",
      "Iteration 9919, loss = 0.07006370\n",
      "Iteration 9920, loss = 0.07006229\n",
      "Iteration 9921, loss = 0.07006088\n",
      "Iteration 9922, loss = 0.07005946\n",
      "Iteration 9923, loss = 0.07005806\n",
      "Iteration 9924, loss = 0.07005666\n",
      "Iteration 9925, loss = 0.07005524\n",
      "Iteration 9926, loss = 0.07005384\n",
      "Iteration 9927, loss = 0.07005244\n",
      "Iteration 9928, loss = 0.07005101\n",
      "Iteration 9929, loss = 0.07004962\n",
      "Iteration 9930, loss = 0.07004820\n",
      "Iteration 9931, loss = 0.07004680\n",
      "Iteration 9932, loss = 0.07004542\n",
      "Iteration 9933, loss = 0.07004397\n",
      "Iteration 9934, loss = 0.07004257\n",
      "Iteration 9935, loss = 0.07004120\n",
      "Iteration 9936, loss = 0.07003975\n",
      "Iteration 9937, loss = 0.07003838\n",
      "Iteration 9938, loss = 0.07003695\n",
      "Iteration 9939, loss = 0.07003552\n",
      "Iteration 9940, loss = 0.07003417\n",
      "Iteration 9941, loss = 0.07003274\n",
      "Iteration 9942, loss = 0.07003131\n",
      "Iteration 9943, loss = 0.07002996\n",
      "Iteration 9944, loss = 0.07002851\n",
      "Iteration 9945, loss = 0.07002714\n",
      "Iteration 9946, loss = 0.07002574\n",
      "Iteration 9947, loss = 0.07002430\n",
      "Iteration 9948, loss = 0.07002290\n",
      "Iteration 9949, loss = 0.07002149\n",
      "Iteration 9950, loss = 0.07002007\n",
      "Iteration 9951, loss = 0.07001872\n",
      "Iteration 9952, loss = 0.07001727\n",
      "Iteration 9953, loss = 0.07001590\n",
      "Iteration 9954, loss = 0.07001450\n",
      "Iteration 9955, loss = 0.07001307\n",
      "Iteration 9956, loss = 0.07001165\n",
      "Iteration 9957, loss = 0.07001028\n",
      "Iteration 9958, loss = 0.07000884\n",
      "Iteration 9959, loss = 0.07000749\n",
      "Iteration 9960, loss = 0.07000603\n",
      "Iteration 9961, loss = 0.07000466\n",
      "Iteration 9962, loss = 0.07000326\n",
      "Iteration 9963, loss = 0.07000183\n",
      "Iteration 9964, loss = 0.07000046\n",
      "Iteration 9965, loss = 0.06999904\n",
      "Iteration 9966, loss = 0.06999764\n",
      "Iteration 9967, loss = 0.06999625\n",
      "Iteration 9968, loss = 0.06999484\n",
      "Iteration 9969, loss = 0.06999341\n",
      "Iteration 9970, loss = 0.06999207\n",
      "Iteration 9971, loss = 0.06999062\n",
      "Iteration 9972, loss = 0.06998925\n",
      "Iteration 9973, loss = 0.06998785\n",
      "Iteration 9974, loss = 0.06998642\n",
      "Iteration 9975, loss = 0.06998504\n",
      "Iteration 9976, loss = 0.06998363\n",
      "Iteration 9977, loss = 0.06998222\n",
      "Iteration 9978, loss = 0.06998084\n",
      "Iteration 9979, loss = 0.06997942\n",
      "Iteration 9980, loss = 0.06997802\n",
      "Iteration 9981, loss = 0.06997664\n",
      "Iteration 9982, loss = 0.06997522\n",
      "Iteration 9983, loss = 0.06997384\n",
      "Iteration 9984, loss = 0.06997243\n",
      "Iteration 9985, loss = 0.06997102\n",
      "Iteration 9986, loss = 0.06996963\n",
      "Iteration 9987, loss = 0.06996823\n",
      "Iteration 9988, loss = 0.06996683\n",
      "Iteration 9989, loss = 0.06996544\n",
      "Iteration 9990, loss = 0.06996402\n",
      "Iteration 9991, loss = 0.06996264\n",
      "Iteration 9992, loss = 0.06996124\n",
      "Iteration 9993, loss = 0.06995983\n",
      "Iteration 9994, loss = 0.06995844\n",
      "Iteration 9995, loss = 0.06995705\n",
      "Iteration 9996, loss = 0.06995563\n",
      "Iteration 9997, loss = 0.06995426\n",
      "Iteration 9998, loss = 0.06995285\n",
      "Iteration 9999, loss = 0.06995146\n",
      "Iteration 10000, loss = 0.06995007\n",
      "Iteration 10001, loss = 0.06994866\n",
      "Iteration 10002, loss = 0.06994728\n",
      "Iteration 10003, loss = 0.06994589\n",
      "Iteration 10004, loss = 0.06994448\n",
      "Iteration 10005, loss = 0.06994310\n",
      "Iteration 10006, loss = 0.06994172\n",
      "Iteration 10007, loss = 0.06994030\n",
      "Iteration 10008, loss = 0.06993893\n",
      "Iteration 10009, loss = 0.06993751\n",
      "Iteration 10010, loss = 0.06993615\n",
      "Iteration 10011, loss = 0.06993475\n",
      "Iteration 10012, loss = 0.06993337\n",
      "Iteration 10013, loss = 0.06993198\n",
      "Iteration 10014, loss = 0.06993058\n",
      "Iteration 10015, loss = 0.06992917\n",
      "Iteration 10016, loss = 0.06992779\n",
      "Iteration 10017, loss = 0.06992641\n",
      "Iteration 10018, loss = 0.06992502\n",
      "Iteration 10019, loss = 0.06992362\n",
      "Iteration 10020, loss = 0.06992225\n",
      "Iteration 10021, loss = 0.06992084\n",
      "Iteration 10022, loss = 0.06991944\n",
      "Iteration 10023, loss = 0.06991807\n",
      "Iteration 10024, loss = 0.06991667\n",
      "Iteration 10025, loss = 0.06991528\n",
      "Iteration 10026, loss = 0.06991391\n",
      "Iteration 10027, loss = 0.06991252\n",
      "Iteration 10028, loss = 0.06991111\n",
      "Iteration 10029, loss = 0.06990976\n",
      "Iteration 10030, loss = 0.06990834\n",
      "Iteration 10031, loss = 0.06990698\n",
      "Iteration 10032, loss = 0.06990556\n",
      "Iteration 10033, loss = 0.06990420\n",
      "Iteration 10034, loss = 0.06990280\n",
      "Iteration 10035, loss = 0.06990139\n",
      "Iteration 10036, loss = 0.06990003\n",
      "Iteration 10037, loss = 0.06989863\n",
      "Iteration 10038, loss = 0.06989724\n",
      "Iteration 10039, loss = 0.06989587\n",
      "Iteration 10040, loss = 0.06989447\n",
      "Iteration 10041, loss = 0.06989306\n",
      "Iteration 10042, loss = 0.06989173\n",
      "Iteration 10043, loss = 0.06989029\n",
      "Iteration 10044, loss = 0.06988895\n",
      "Iteration 10045, loss = 0.06988753\n",
      "Iteration 10046, loss = 0.06988614\n",
      "Iteration 10047, loss = 0.06988478\n",
      "Iteration 10048, loss = 0.06988337\n",
      "Iteration 10049, loss = 0.06988199\n",
      "Iteration 10050, loss = 0.06988063\n",
      "Iteration 10051, loss = 0.06987924\n",
      "Iteration 10052, loss = 0.06987783\n",
      "Iteration 10053, loss = 0.06987648\n",
      "Iteration 10054, loss = 0.06987505\n",
      "Iteration 10055, loss = 0.06987372\n",
      "Iteration 10056, loss = 0.06987231\n",
      "Iteration 10057, loss = 0.06987092\n",
      "Iteration 10058, loss = 0.06986955\n",
      "Iteration 10059, loss = 0.06986814\n",
      "Iteration 10060, loss = 0.06986677\n",
      "Iteration 10061, loss = 0.06986540\n",
      "Iteration 10062, loss = 0.06986399\n",
      "Iteration 10063, loss = 0.06986262\n",
      "Iteration 10064, loss = 0.06986123\n",
      "Iteration 10065, loss = 0.06985984\n",
      "Iteration 10066, loss = 0.06985849\n",
      "Iteration 10067, loss = 0.06985709\n",
      "Iteration 10068, loss = 0.06985569\n",
      "Iteration 10069, loss = 0.06985435\n",
      "Iteration 10070, loss = 0.06985293\n",
      "Iteration 10071, loss = 0.06985157\n",
      "Iteration 10072, loss = 0.06985018\n",
      "Iteration 10073, loss = 0.06984878\n",
      "Iteration 10074, loss = 0.06984743\n",
      "Iteration 10075, loss = 0.06984601\n",
      "Iteration 10076, loss = 0.06984465\n",
      "Iteration 10077, loss = 0.06984328\n",
      "Iteration 10078, loss = 0.06984188\n",
      "Iteration 10079, loss = 0.06984050\n",
      "Iteration 10080, loss = 0.06983915\n",
      "Iteration 10081, loss = 0.06983772\n",
      "Iteration 10082, loss = 0.06983640\n",
      "Iteration 10083, loss = 0.06983497\n",
      "Iteration 10084, loss = 0.06983361\n",
      "Iteration 10085, loss = 0.06983223\n",
      "Iteration 10086, loss = 0.06983082\n",
      "Iteration 10087, loss = 0.06982947\n",
      "Iteration 10088, loss = 0.06982809\n",
      "Iteration 10089, loss = 0.06982669\n",
      "Iteration 10090, loss = 0.06982534\n",
      "Iteration 10091, loss = 0.06982393\n",
      "Iteration 10092, loss = 0.06982258\n",
      "Iteration 10093, loss = 0.06982119\n",
      "Iteration 10094, loss = 0.06981981\n",
      "Iteration 10095, loss = 0.06981843\n",
      "Iteration 10096, loss = 0.06981706\n",
      "Iteration 10097, loss = 0.06981566\n",
      "Iteration 10098, loss = 0.06981431\n",
      "Iteration 10099, loss = 0.06981294\n",
      "Iteration 10100, loss = 0.06981152\n",
      "Iteration 10101, loss = 0.06981019\n",
      "Iteration 10102, loss = 0.06980878\n",
      "Iteration 10103, loss = 0.06980743\n",
      "Iteration 10104, loss = 0.06980604\n",
      "Iteration 10105, loss = 0.06980466\n",
      "Iteration 10106, loss = 0.06980327\n",
      "Iteration 10107, loss = 0.06980191\n",
      "Iteration 10108, loss = 0.06980050\n",
      "Iteration 10109, loss = 0.06979917\n",
      "Iteration 10110, loss = 0.06979778\n",
      "Iteration 10111, loss = 0.06979639\n",
      "Iteration 10112, loss = 0.06979501\n",
      "Iteration 10113, loss = 0.06979363\n",
      "Iteration 10114, loss = 0.06979229\n",
      "Iteration 10115, loss = 0.06979088\n",
      "Iteration 10116, loss = 0.06978952\n",
      "Iteration 10117, loss = 0.06978814\n",
      "Iteration 10118, loss = 0.06978676\n",
      "Iteration 10119, loss = 0.06978539\n",
      "Iteration 10120, loss = 0.06978400\n",
      "Iteration 10121, loss = 0.06978263\n",
      "Iteration 10122, loss = 0.06978127\n",
      "Iteration 10123, loss = 0.06977991\n",
      "Iteration 10124, loss = 0.06977850\n",
      "Iteration 10125, loss = 0.06977716\n",
      "Iteration 10126, loss = 0.06977577\n",
      "Iteration 10127, loss = 0.06977441\n",
      "Iteration 10128, loss = 0.06977302\n",
      "Iteration 10129, loss = 0.06977165\n",
      "Iteration 10130, loss = 0.06977028\n",
      "Iteration 10131, loss = 0.06976888\n",
      "Iteration 10132, loss = 0.06976754\n",
      "Iteration 10133, loss = 0.06976615\n",
      "Iteration 10134, loss = 0.06976478\n",
      "Iteration 10135, loss = 0.06976340\n",
      "Iteration 10136, loss = 0.06976203\n",
      "Iteration 10137, loss = 0.06976065\n",
      "Iteration 10138, loss = 0.06975930\n",
      "Iteration 10139, loss = 0.06975792\n",
      "Iteration 10140, loss = 0.06975653\n",
      "Iteration 10141, loss = 0.06975520\n",
      "Iteration 10142, loss = 0.06975380\n",
      "Iteration 10143, loss = 0.06975244\n",
      "Iteration 10144, loss = 0.06975105\n",
      "Iteration 10145, loss = 0.06974969\n",
      "Iteration 10146, loss = 0.06974833\n",
      "Iteration 10147, loss = 0.06974692\n",
      "Iteration 10148, loss = 0.06974560\n",
      "Iteration 10149, loss = 0.06974419\n",
      "Iteration 10150, loss = 0.06974283\n",
      "Iteration 10151, loss = 0.06974148\n",
      "Iteration 10152, loss = 0.06974008\n",
      "Iteration 10153, loss = 0.06973871\n",
      "Iteration 10154, loss = 0.06973736\n",
      "Iteration 10155, loss = 0.06973598\n",
      "Iteration 10156, loss = 0.06973460\n",
      "Iteration 10157, loss = 0.06973325\n",
      "Iteration 10158, loss = 0.06973185\n",
      "Iteration 10159, loss = 0.06973053\n",
      "Iteration 10160, loss = 0.06972913\n",
      "Iteration 10161, loss = 0.06972776\n",
      "Iteration 10162, loss = 0.06972640\n",
      "Iteration 10163, loss = 0.06972502\n",
      "Iteration 10164, loss = 0.06972366\n",
      "Iteration 10165, loss = 0.06972230\n",
      "Iteration 10166, loss = 0.06972091\n",
      "Iteration 10167, loss = 0.06971958\n",
      "Iteration 10168, loss = 0.06971818\n",
      "Iteration 10169, loss = 0.06971682\n",
      "Iteration 10170, loss = 0.06971548\n",
      "Iteration 10171, loss = 0.06971408\n",
      "Iteration 10172, loss = 0.06971273\n",
      "Iteration 10173, loss = 0.06971135\n",
      "Iteration 10174, loss = 0.06970997\n",
      "Iteration 10175, loss = 0.06970865\n",
      "Iteration 10176, loss = 0.06970723\n",
      "Iteration 10177, loss = 0.06970589\n",
      "Iteration 10178, loss = 0.06970453\n",
      "Iteration 10179, loss = 0.06970315\n",
      "Iteration 10180, loss = 0.06970179\n",
      "Iteration 10181, loss = 0.06970042\n",
      "Iteration 10182, loss = 0.06969903\n",
      "Iteration 10183, loss = 0.06969772\n",
      "Iteration 10184, loss = 0.06969633\n",
      "Iteration 10185, loss = 0.06969494\n",
      "Iteration 10186, loss = 0.06969364\n",
      "Iteration 10187, loss = 0.06969222\n",
      "Iteration 10188, loss = 0.06969090\n",
      "Iteration 10189, loss = 0.06968952\n",
      "Iteration 10190, loss = 0.06968814\n",
      "Iteration 10191, loss = 0.06968679\n",
      "Iteration 10192, loss = 0.06968539\n",
      "Iteration 10193, loss = 0.06968405\n",
      "Iteration 10194, loss = 0.06968268\n",
      "Iteration 10195, loss = 0.06968131\n",
      "Iteration 10196, loss = 0.06967996\n",
      "Iteration 10197, loss = 0.06967859\n",
      "Iteration 10198, loss = 0.06967721\n",
      "Iteration 10199, loss = 0.06967589\n",
      "Iteration 10200, loss = 0.06967450\n",
      "Iteration 10201, loss = 0.06967312\n",
      "Iteration 10202, loss = 0.06967179\n",
      "Iteration 10203, loss = 0.06967041\n",
      "Iteration 10204, loss = 0.06966906\n",
      "Iteration 10205, loss = 0.06966769\n",
      "Iteration 10206, loss = 0.06966632\n",
      "Iteration 10207, loss = 0.06966497\n",
      "Iteration 10208, loss = 0.06966361\n",
      "Iteration 10209, loss = 0.06966223\n",
      "Iteration 10210, loss = 0.06966090\n",
      "Iteration 10211, loss = 0.06965950\n",
      "Iteration 10212, loss = 0.06965817\n",
      "Iteration 10213, loss = 0.06965680\n",
      "Iteration 10214, loss = 0.06965542\n",
      "Iteration 10215, loss = 0.06965409\n",
      "Iteration 10216, loss = 0.06965270\n",
      "Iteration 10217, loss = 0.06965135\n",
      "Iteration 10218, loss = 0.06965001\n",
      "Iteration 10219, loss = 0.06964862\n",
      "Iteration 10220, loss = 0.06964729\n",
      "Iteration 10221, loss = 0.06964591\n",
      "Iteration 10222, loss = 0.06964456\n",
      "Iteration 10223, loss = 0.06964320\n",
      "Iteration 10224, loss = 0.06964183\n",
      "Iteration 10225, loss = 0.06964048\n",
      "Iteration 10226, loss = 0.06963913\n",
      "Iteration 10227, loss = 0.06963776\n",
      "Iteration 10228, loss = 0.06963641\n",
      "Iteration 10229, loss = 0.06963505\n",
      "Iteration 10230, loss = 0.06963366\n",
      "Iteration 10231, loss = 0.06963234\n",
      "Iteration 10232, loss = 0.06963096\n",
      "Iteration 10233, loss = 0.06962961\n",
      "Iteration 10234, loss = 0.06962825\n",
      "Iteration 10235, loss = 0.06962689\n",
      "Iteration 10236, loss = 0.06962554\n",
      "Iteration 10237, loss = 0.06962419\n",
      "Iteration 10238, loss = 0.06962281\n",
      "Iteration 10239, loss = 0.06962146\n",
      "Iteration 10240, loss = 0.06962012\n",
      "Iteration 10241, loss = 0.06961875\n",
      "Iteration 10242, loss = 0.06961740\n",
      "Iteration 10243, loss = 0.06961604\n",
      "Iteration 10244, loss = 0.06961469\n",
      "Iteration 10245, loss = 0.06961332\n",
      "Iteration 10246, loss = 0.06961196\n",
      "Iteration 10247, loss = 0.06961061\n",
      "Iteration 10248, loss = 0.06960925\n",
      "Iteration 10249, loss = 0.06960792\n",
      "Iteration 10250, loss = 0.06960652\n",
      "Iteration 10251, loss = 0.06960521\n",
      "Iteration 10252, loss = 0.06960383\n",
      "Iteration 10253, loss = 0.06960247\n",
      "Iteration 10254, loss = 0.06960114\n",
      "Iteration 10255, loss = 0.06959975\n",
      "Iteration 10256, loss = 0.06959842\n",
      "Iteration 10257, loss = 0.06959707\n",
      "Iteration 10258, loss = 0.06959570\n",
      "Iteration 10259, loss = 0.06959434\n",
      "Iteration 10260, loss = 0.06959302\n",
      "Iteration 10261, loss = 0.06959163\n",
      "Iteration 10262, loss = 0.06959030\n",
      "Iteration 10263, loss = 0.06958892\n",
      "Iteration 10264, loss = 0.06958759\n",
      "Iteration 10265, loss = 0.06958623\n",
      "Iteration 10266, loss = 0.06958488\n",
      "Iteration 10267, loss = 0.06958351\n",
      "Iteration 10268, loss = 0.06958216\n",
      "Iteration 10269, loss = 0.06958083\n",
      "Iteration 10270, loss = 0.06957947\n",
      "Iteration 10271, loss = 0.06957810\n",
      "Iteration 10272, loss = 0.06957677\n",
      "Iteration 10273, loss = 0.06957541\n",
      "Iteration 10274, loss = 0.06957404\n",
      "Iteration 10275, loss = 0.06957272\n",
      "Iteration 10276, loss = 0.06957134\n",
      "Iteration 10277, loss = 0.06957000\n",
      "Iteration 10278, loss = 0.06956868\n",
      "Iteration 10279, loss = 0.06956730\n",
      "Iteration 10280, loss = 0.06956593\n",
      "Iteration 10281, loss = 0.06956463\n",
      "Iteration 10282, loss = 0.06956323\n",
      "Iteration 10283, loss = 0.06956192\n",
      "Iteration 10284, loss = 0.06956054\n",
      "Iteration 10285, loss = 0.06955920\n",
      "Iteration 10286, loss = 0.06955787\n",
      "Iteration 10287, loss = 0.06955648\n",
      "Iteration 10288, loss = 0.06955516\n",
      "Iteration 10289, loss = 0.06955381\n",
      "Iteration 10290, loss = 0.06955243\n",
      "Iteration 10291, loss = 0.06955110\n",
      "Iteration 10292, loss = 0.06954973\n",
      "Iteration 10293, loss = 0.06954839\n",
      "Iteration 10294, loss = 0.06954707\n",
      "Iteration 10295, loss = 0.06954569\n",
      "Iteration 10296, loss = 0.06954434\n",
      "Iteration 10297, loss = 0.06954300\n",
      "Iteration 10298, loss = 0.06954163\n",
      "Iteration 10299, loss = 0.06954032\n",
      "Iteration 10300, loss = 0.06953893\n",
      "Iteration 10301, loss = 0.06953761\n",
      "Iteration 10302, loss = 0.06953625\n",
      "Iteration 10303, loss = 0.06953488\n",
      "Iteration 10304, loss = 0.06953357\n",
      "Iteration 10305, loss = 0.06953220\n",
      "Iteration 10306, loss = 0.06953085\n",
      "Iteration 10307, loss = 0.06952953\n",
      "Iteration 10308, loss = 0.06952816\n",
      "Iteration 10309, loss = 0.06952681\n",
      "Iteration 10310, loss = 0.06952549\n",
      "Iteration 10311, loss = 0.06952410\n",
      "Iteration 10312, loss = 0.06952281\n",
      "Iteration 10313, loss = 0.06952142\n",
      "Iteration 10314, loss = 0.06952009\n",
      "Iteration 10315, loss = 0.06951874\n",
      "Iteration 10316, loss = 0.06951738\n",
      "Iteration 10317, loss = 0.06951605\n",
      "Iteration 10318, loss = 0.06951470\n",
      "Iteration 10319, loss = 0.06951334\n",
      "Iteration 10320, loss = 0.06951201\n",
      "Iteration 10321, loss = 0.06951067\n",
      "Iteration 10322, loss = 0.06950929\n",
      "Iteration 10323, loss = 0.06950799\n",
      "Iteration 10324, loss = 0.06950661\n",
      "Iteration 10325, loss = 0.06950528\n",
      "Iteration 10326, loss = 0.06950393\n",
      "Iteration 10327, loss = 0.06950257\n",
      "Iteration 10328, loss = 0.06950124\n",
      "Iteration 10329, loss = 0.06949989\n",
      "Iteration 10330, loss = 0.06949854\n",
      "Iteration 10331, loss = 0.06949721\n",
      "Iteration 10332, loss = 0.06949585\n",
      "Iteration 10333, loss = 0.06949452\n",
      "Iteration 10334, loss = 0.06949317\n",
      "Iteration 10335, loss = 0.06949183\n",
      "Iteration 10336, loss = 0.06949047\n",
      "Iteration 10337, loss = 0.06948913\n",
      "Iteration 10338, loss = 0.06948780\n",
      "Iteration 10339, loss = 0.06948644\n",
      "Iteration 10340, loss = 0.06948509\n",
      "Iteration 10341, loss = 0.06948377\n",
      "Iteration 10342, loss = 0.06948242\n",
      "Iteration 10343, loss = 0.06948107\n",
      "Iteration 10344, loss = 0.06947973\n",
      "Iteration 10345, loss = 0.06947839\n",
      "Iteration 10346, loss = 0.06947705\n",
      "Iteration 10347, loss = 0.06947570\n",
      "Iteration 10348, loss = 0.06947437\n",
      "Iteration 10349, loss = 0.06947301\n",
      "Iteration 10350, loss = 0.06947168\n",
      "Iteration 10351, loss = 0.06947032\n",
      "Iteration 10352, loss = 0.06946900\n",
      "Iteration 10353, loss = 0.06946764\n",
      "Iteration 10354, loss = 0.06946631\n",
      "Iteration 10355, loss = 0.06946498\n",
      "Iteration 10356, loss = 0.06946359\n",
      "Iteration 10357, loss = 0.06946232\n",
      "Iteration 10358, loss = 0.06946095\n",
      "Iteration 10359, loss = 0.06945961\n",
      "Iteration 10360, loss = 0.06945824\n",
      "Iteration 10361, loss = 0.06945692\n",
      "Iteration 10362, loss = 0.06945559\n",
      "Iteration 10363, loss = 0.06945424\n",
      "Iteration 10364, loss = 0.06945291\n",
      "Iteration 10365, loss = 0.06945156\n",
      "Iteration 10366, loss = 0.06945022\n",
      "Iteration 10367, loss = 0.06944887\n",
      "Iteration 10368, loss = 0.06944753\n",
      "Iteration 10369, loss = 0.06944620\n",
      "Iteration 10370, loss = 0.06944485\n",
      "Iteration 10371, loss = 0.06944350\n",
      "Iteration 10372, loss = 0.06944218\n",
      "Iteration 10373, loss = 0.06944082\n",
      "Iteration 10374, loss = 0.06943950\n",
      "Iteration 10375, loss = 0.06943816\n",
      "Iteration 10376, loss = 0.06943682\n",
      "Iteration 10377, loss = 0.06943548\n",
      "Iteration 10378, loss = 0.06943413\n",
      "Iteration 10379, loss = 0.06943280\n",
      "Iteration 10380, loss = 0.06943148\n",
      "Iteration 10381, loss = 0.06943010\n",
      "Iteration 10382, loss = 0.06942881\n",
      "Iteration 10383, loss = 0.06942745\n",
      "Iteration 10384, loss = 0.06942611\n",
      "Iteration 10385, loss = 0.06942480\n",
      "Iteration 10386, loss = 0.06942342\n",
      "Iteration 10387, loss = 0.06942210\n",
      "Iteration 10388, loss = 0.06942079\n",
      "Iteration 10389, loss = 0.06941942\n",
      "Iteration 10390, loss = 0.06941808\n",
      "Iteration 10391, loss = 0.06941676\n",
      "Iteration 10392, loss = 0.06941540\n",
      "Iteration 10393, loss = 0.06941410\n",
      "Iteration 10394, loss = 0.06941273\n",
      "Iteration 10395, loss = 0.06941141\n",
      "Iteration 10396, loss = 0.06941007\n",
      "Iteration 10397, loss = 0.06940873\n",
      "Iteration 10398, loss = 0.06940742\n",
      "Iteration 10399, loss = 0.06940605\n",
      "Iteration 10400, loss = 0.06940472\n",
      "Iteration 10401, loss = 0.06940340\n",
      "Iteration 10402, loss = 0.06940206\n",
      "Iteration 10403, loss = 0.06940071\n",
      "Iteration 10404, loss = 0.06939940\n",
      "Iteration 10405, loss = 0.06939804\n",
      "Iteration 10406, loss = 0.06939674\n",
      "Iteration 10407, loss = 0.06939537\n",
      "Iteration 10408, loss = 0.06939405\n",
      "Iteration 10409, loss = 0.06939272\n",
      "Iteration 10410, loss = 0.06939137\n",
      "Iteration 10411, loss = 0.06939005\n",
      "Iteration 10412, loss = 0.06938871\n",
      "Iteration 10413, loss = 0.06938736\n",
      "Iteration 10414, loss = 0.06938606\n",
      "Iteration 10415, loss = 0.06938471\n",
      "Iteration 10416, loss = 0.06938336\n",
      "Iteration 10417, loss = 0.06938206\n",
      "Iteration 10418, loss = 0.06938070\n",
      "Iteration 10419, loss = 0.06937938\n",
      "Iteration 10420, loss = 0.06937805\n",
      "Iteration 10421, loss = 0.06937670\n",
      "Iteration 10422, loss = 0.06937540\n",
      "Iteration 10423, loss = 0.06937404\n",
      "Iteration 10424, loss = 0.06937273\n",
      "Iteration 10425, loss = 0.06937140\n",
      "Iteration 10426, loss = 0.06937004\n",
      "Iteration 10427, loss = 0.06936875\n",
      "Iteration 10428, loss = 0.06936740\n",
      "Iteration 10429, loss = 0.06936604\n",
      "Iteration 10430, loss = 0.06936475\n",
      "Iteration 10431, loss = 0.06936340\n",
      "Iteration 10432, loss = 0.06936207\n",
      "Iteration 10433, loss = 0.06936075\n",
      "Iteration 10434, loss = 0.06935941\n",
      "Iteration 10435, loss = 0.06935808\n",
      "Iteration 10436, loss = 0.06935675\n",
      "Iteration 10437, loss = 0.06935542\n",
      "Iteration 10438, loss = 0.06935410\n",
      "Iteration 10439, loss = 0.06935274\n",
      "Iteration 10440, loss = 0.06935145\n",
      "Iteration 10441, loss = 0.06935011\n",
      "Iteration 10442, loss = 0.06934877\n",
      "Iteration 10443, loss = 0.06934746\n",
      "Iteration 10444, loss = 0.06934611\n",
      "Iteration 10445, loss = 0.06934480\n",
      "Iteration 10446, loss = 0.06934347\n",
      "Iteration 10447, loss = 0.06934213\n",
      "Iteration 10448, loss = 0.06934081\n",
      "Iteration 10449, loss = 0.06933948\n",
      "Iteration 10450, loss = 0.06933815\n",
      "Iteration 10451, loss = 0.06933683\n",
      "Iteration 10452, loss = 0.06933549\n",
      "Iteration 10453, loss = 0.06933417\n",
      "Iteration 10454, loss = 0.06933285\n",
      "Iteration 10455, loss = 0.06933149\n",
      "Iteration 10456, loss = 0.06933020\n",
      "Iteration 10457, loss = 0.06932885\n",
      "Iteration 10458, loss = 0.06932754\n",
      "Iteration 10459, loss = 0.06932620\n",
      "Iteration 10460, loss = 0.06932489\n",
      "Iteration 10461, loss = 0.06932355\n",
      "Iteration 10462, loss = 0.06932223\n",
      "Iteration 10463, loss = 0.06932091\n",
      "Iteration 10464, loss = 0.06931957\n",
      "Iteration 10465, loss = 0.06931826\n",
      "Iteration 10466, loss = 0.06931692\n",
      "Iteration 10467, loss = 0.06931561\n",
      "Iteration 10468, loss = 0.06931428\n",
      "Iteration 10469, loss = 0.06931294\n",
      "Iteration 10470, loss = 0.06931163\n",
      "Iteration 10471, loss = 0.06931032\n",
      "Iteration 10472, loss = 0.06930897\n",
      "Iteration 10473, loss = 0.06930769\n",
      "Iteration 10474, loss = 0.06930632\n",
      "Iteration 10475, loss = 0.06930500\n",
      "Iteration 10476, loss = 0.06930372\n",
      "Iteration 10477, loss = 0.06930236\n",
      "Iteration 10478, loss = 0.06930103\n",
      "Iteration 10479, loss = 0.06929973\n",
      "Iteration 10480, loss = 0.06929837\n",
      "Iteration 10481, loss = 0.06929709\n",
      "Iteration 10482, loss = 0.06929575\n",
      "Iteration 10483, loss = 0.06929443\n",
      "Iteration 10484, loss = 0.06929310\n",
      "Iteration 10485, loss = 0.06929176\n",
      "Iteration 10486, loss = 0.06929047\n",
      "Iteration 10487, loss = 0.06928913\n",
      "Iteration 10488, loss = 0.06928780\n",
      "Iteration 10489, loss = 0.06928652\n",
      "Iteration 10490, loss = 0.06928514\n",
      "Iteration 10491, loss = 0.06928385\n",
      "Iteration 10492, loss = 0.06928253\n",
      "Iteration 10493, loss = 0.06928118\n",
      "Iteration 10494, loss = 0.06927989\n",
      "Iteration 10495, loss = 0.06927854\n",
      "Iteration 10496, loss = 0.06927723\n",
      "Iteration 10497, loss = 0.06927592\n",
      "Iteration 10498, loss = 0.06927458\n",
      "Iteration 10499, loss = 0.06927328\n",
      "Iteration 10500, loss = 0.06927195\n",
      "Iteration 10501, loss = 0.06927062\n",
      "Iteration 10502, loss = 0.06926933\n",
      "Iteration 10503, loss = 0.06926797\n",
      "Iteration 10504, loss = 0.06926666\n",
      "Iteration 10505, loss = 0.06926535\n",
      "Iteration 10506, loss = 0.06926400\n",
      "Iteration 10507, loss = 0.06926271\n",
      "Iteration 10508, loss = 0.06926138\n",
      "Iteration 10509, loss = 0.06926005\n",
      "Iteration 10510, loss = 0.06925875\n",
      "Iteration 10511, loss = 0.06925741\n",
      "Iteration 10512, loss = 0.06925609\n",
      "Iteration 10513, loss = 0.06925478\n",
      "Iteration 10514, loss = 0.06925344\n",
      "Iteration 10515, loss = 0.06925216\n",
      "Iteration 10516, loss = 0.06925081\n",
      "Iteration 10517, loss = 0.06924948\n",
      "Iteration 10518, loss = 0.06924819\n",
      "Iteration 10519, loss = 0.06924686\n",
      "Iteration 10520, loss = 0.06924554\n",
      "Iteration 10521, loss = 0.06924422\n",
      "Iteration 10522, loss = 0.06924290\n",
      "Iteration 10523, loss = 0.06924159\n",
      "Iteration 10524, loss = 0.06924026\n",
      "Iteration 10525, loss = 0.06923895\n",
      "Iteration 10526, loss = 0.06923763\n",
      "Iteration 10527, loss = 0.06923629\n",
      "Iteration 10528, loss = 0.06923501\n",
      "Iteration 10529, loss = 0.06923367\n",
      "Iteration 10530, loss = 0.06923234\n",
      "Iteration 10531, loss = 0.06923106\n",
      "Iteration 10532, loss = 0.06922969\n",
      "Iteration 10533, loss = 0.06922843\n",
      "Iteration 10534, loss = 0.06922707\n",
      "Iteration 10535, loss = 0.06922578\n",
      "Iteration 10536, loss = 0.06922445\n",
      "Iteration 10537, loss = 0.06922312\n",
      "Iteration 10538, loss = 0.06922184\n",
      "Iteration 10539, loss = 0.06922051\n",
      "Iteration 10540, loss = 0.06921921\n",
      "Iteration 10541, loss = 0.06921787\n",
      "Iteration 10542, loss = 0.06921658\n",
      "Iteration 10543, loss = 0.06921526\n",
      "Iteration 10544, loss = 0.06921394\n",
      "Iteration 10545, loss = 0.06921262\n",
      "Iteration 10546, loss = 0.06921132\n",
      "Iteration 10547, loss = 0.06921001\n",
      "Iteration 10548, loss = 0.06920866\n",
      "Iteration 10549, loss = 0.06920739\n",
      "Iteration 10550, loss = 0.06920605\n",
      "Iteration 10551, loss = 0.06920475\n",
      "Iteration 10552, loss = 0.06920342\n",
      "Iteration 10553, loss = 0.06920213\n",
      "Iteration 10554, loss = 0.06920081\n",
      "Iteration 10555, loss = 0.06919948\n",
      "Iteration 10556, loss = 0.06919820\n",
      "Iteration 10557, loss = 0.06919685\n",
      "Iteration 10558, loss = 0.06919557\n",
      "Iteration 10559, loss = 0.06919424\n",
      "Iteration 10560, loss = 0.06919294\n",
      "Iteration 10561, loss = 0.06919162\n",
      "Iteration 10562, loss = 0.06919031\n",
      "Iteration 10563, loss = 0.06918899\n",
      "Iteration 10564, loss = 0.06918770\n",
      "Iteration 10565, loss = 0.06918638\n",
      "Iteration 10566, loss = 0.06918506\n",
      "Iteration 10567, loss = 0.06918376\n",
      "Iteration 10568, loss = 0.06918244\n",
      "Iteration 10569, loss = 0.06918115\n",
      "Iteration 10570, loss = 0.06917981\n",
      "Iteration 10571, loss = 0.06917853\n",
      "Iteration 10572, loss = 0.06917720\n",
      "Iteration 10573, loss = 0.06917588\n",
      "Iteration 10574, loss = 0.06917460\n",
      "Iteration 10575, loss = 0.06917325\n",
      "Iteration 10576, loss = 0.06917198\n",
      "Iteration 10577, loss = 0.06917066\n",
      "Iteration 10578, loss = 0.06916933\n",
      "Iteration 10579, loss = 0.06916805\n",
      "Iteration 10580, loss = 0.06916673\n",
      "Iteration 10581, loss = 0.06916541\n",
      "Iteration 10582, loss = 0.06916413\n",
      "Iteration 10583, loss = 0.06916280\n",
      "Iteration 10584, loss = 0.06916149\n",
      "Iteration 10585, loss = 0.06916019\n",
      "Iteration 10586, loss = 0.06915887\n",
      "Iteration 10587, loss = 0.06915758\n",
      "Iteration 10588, loss = 0.06915624\n",
      "Iteration 10589, loss = 0.06915497\n",
      "Iteration 10590, loss = 0.06915363\n",
      "Iteration 10591, loss = 0.06915232\n",
      "Iteration 10592, loss = 0.06915105\n",
      "Iteration 10593, loss = 0.06914970\n",
      "Iteration 10594, loss = 0.06914843\n",
      "Iteration 10595, loss = 0.06914711\n",
      "Iteration 10596, loss = 0.06914578\n",
      "Iteration 10597, loss = 0.06914452\n",
      "Iteration 10598, loss = 0.06914317\n",
      "Iteration 10599, loss = 0.06914188\n",
      "Iteration 10600, loss = 0.06914059\n",
      "Iteration 10601, loss = 0.06913926\n",
      "Iteration 10602, loss = 0.06913796\n",
      "Iteration 10603, loss = 0.06913666\n",
      "Iteration 10604, loss = 0.06913533\n",
      "Iteration 10605, loss = 0.06913408\n",
      "Iteration 10606, loss = 0.06913272\n",
      "Iteration 10607, loss = 0.06913144\n",
      "Iteration 10608, loss = 0.06913014\n",
      "Iteration 10609, loss = 0.06912881\n",
      "Iteration 10610, loss = 0.06912755\n",
      "Iteration 10611, loss = 0.06912622\n",
      "Iteration 10612, loss = 0.06912490\n",
      "Iteration 10613, loss = 0.06912364\n",
      "Iteration 10614, loss = 0.06912229\n",
      "Iteration 10615, loss = 0.06912102\n",
      "Iteration 10616, loss = 0.06911969\n",
      "Iteration 10617, loss = 0.06911837\n",
      "Iteration 10618, loss = 0.06911711\n",
      "Iteration 10619, loss = 0.06911576\n",
      "Iteration 10620, loss = 0.06911450\n",
      "Iteration 10621, loss = 0.06911319\n",
      "Iteration 10622, loss = 0.06911186\n",
      "Iteration 10623, loss = 0.06911058\n",
      "Iteration 10624, loss = 0.06910925\n",
      "Iteration 10625, loss = 0.06910796\n",
      "Iteration 10626, loss = 0.06910667\n",
      "Iteration 10627, loss = 0.06910533\n",
      "Iteration 10628, loss = 0.06910408\n",
      "Iteration 10629, loss = 0.06910273\n",
      "Iteration 10630, loss = 0.06910147\n",
      "Iteration 10631, loss = 0.06910017\n",
      "Iteration 10632, loss = 0.06909886\n",
      "Iteration 10633, loss = 0.06909753\n",
      "Iteration 10634, loss = 0.06909625\n",
      "Iteration 10635, loss = 0.06909494\n",
      "Iteration 10636, loss = 0.06909366\n",
      "Iteration 10637, loss = 0.06909234\n",
      "Iteration 10638, loss = 0.06909105\n",
      "Iteration 10639, loss = 0.06908975\n",
      "Iteration 10640, loss = 0.06908842\n",
      "Iteration 10641, loss = 0.06908715\n",
      "Iteration 10642, loss = 0.06908583\n",
      "Iteration 10643, loss = 0.06908455\n",
      "Iteration 10644, loss = 0.06908322\n",
      "Iteration 10645, loss = 0.06908193\n",
      "Iteration 10646, loss = 0.06908064\n",
      "Iteration 10647, loss = 0.06907933\n",
      "Iteration 10648, loss = 0.06907802\n",
      "Iteration 10649, loss = 0.06907673\n",
      "Iteration 10650, loss = 0.06907543\n",
      "Iteration 10651, loss = 0.06907414\n",
      "Iteration 10652, loss = 0.06907282\n",
      "Iteration 10653, loss = 0.06907153\n",
      "Iteration 10654, loss = 0.06907023\n",
      "Iteration 10655, loss = 0.06906894\n",
      "Iteration 10656, loss = 0.06906763\n",
      "Iteration 10657, loss = 0.06906635\n",
      "Iteration 10658, loss = 0.06906506\n",
      "Iteration 10659, loss = 0.06906374\n",
      "Iteration 10660, loss = 0.06906243\n",
      "Iteration 10661, loss = 0.06906115\n",
      "Iteration 10662, loss = 0.06905984\n",
      "Iteration 10663, loss = 0.06905855\n",
      "Iteration 10664, loss = 0.06905726\n",
      "Iteration 10665, loss = 0.06905597\n",
      "Iteration 10666, loss = 0.06905466\n",
      "Iteration 10667, loss = 0.06905337\n",
      "Iteration 10668, loss = 0.06905204\n",
      "Iteration 10669, loss = 0.06905078\n",
      "Iteration 10670, loss = 0.06904944\n",
      "Iteration 10671, loss = 0.06904820\n",
      "Iteration 10672, loss = 0.06904686\n",
      "Iteration 10673, loss = 0.06904557\n",
      "Iteration 10674, loss = 0.06904430\n",
      "Iteration 10675, loss = 0.06904298\n",
      "Iteration 10676, loss = 0.06904169\n",
      "Iteration 10677, loss = 0.06904041\n",
      "Iteration 10678, loss = 0.06903911\n",
      "Iteration 10679, loss = 0.06903781\n",
      "Iteration 10680, loss = 0.06903650\n",
      "Iteration 10681, loss = 0.06903520\n",
      "Iteration 10682, loss = 0.06903394\n",
      "Iteration 10683, loss = 0.06903262\n",
      "Iteration 10684, loss = 0.06903134\n",
      "Iteration 10685, loss = 0.06903004\n",
      "Iteration 10686, loss = 0.06902872\n",
      "Iteration 10687, loss = 0.06902747\n",
      "Iteration 10688, loss = 0.06902612\n",
      "Iteration 10689, loss = 0.06902490\n",
      "Iteration 10690, loss = 0.06902357\n",
      "Iteration 10691, loss = 0.06902228\n",
      "Iteration 10692, loss = 0.06902099\n",
      "Iteration 10693, loss = 0.06901968\n",
      "Iteration 10694, loss = 0.06901841\n",
      "Iteration 10695, loss = 0.06901710\n",
      "Iteration 10696, loss = 0.06901580\n",
      "Iteration 10697, loss = 0.06901456\n",
      "Iteration 10698, loss = 0.06901322\n",
      "Iteration 10699, loss = 0.06901196\n",
      "Iteration 10700, loss = 0.06901068\n",
      "Iteration 10701, loss = 0.06900936\n",
      "Iteration 10702, loss = 0.06900806\n",
      "Iteration 10703, loss = 0.06900678\n",
      "Iteration 10704, loss = 0.06900546\n",
      "Iteration 10705, loss = 0.06900423\n",
      "Iteration 10706, loss = 0.06900291\n",
      "Iteration 10707, loss = 0.06900162\n",
      "Iteration 10708, loss = 0.06900033\n",
      "Iteration 10709, loss = 0.06899902\n",
      "Iteration 10710, loss = 0.06899778\n",
      "Iteration 10711, loss = 0.06899645\n",
      "Iteration 10712, loss = 0.06899518\n",
      "Iteration 10713, loss = 0.06899389\n",
      "Iteration 10714, loss = 0.06899258\n",
      "Iteration 10715, loss = 0.06899134\n",
      "Iteration 10716, loss = 0.06899001\n",
      "Iteration 10717, loss = 0.06898874\n",
      "Iteration 10718, loss = 0.06898747\n",
      "Iteration 10719, loss = 0.06898615\n",
      "Iteration 10720, loss = 0.06898487\n",
      "Iteration 10721, loss = 0.06898358\n",
      "Iteration 10722, loss = 0.06898229\n",
      "Iteration 10723, loss = 0.06898101\n",
      "Iteration 10724, loss = 0.06897969\n",
      "Iteration 10725, loss = 0.06897846\n",
      "Iteration 10726, loss = 0.06897712\n",
      "Iteration 10727, loss = 0.06897588\n",
      "Iteration 10728, loss = 0.06897459\n",
      "Iteration 10729, loss = 0.06897328\n",
      "Iteration 10730, loss = 0.06897198\n",
      "Iteration 10731, loss = 0.06897072\n",
      "Iteration 10732, loss = 0.06896941\n",
      "Iteration 10733, loss = 0.06896814\n",
      "Iteration 10734, loss = 0.06896687\n",
      "Iteration 10735, loss = 0.06896556\n",
      "Iteration 10736, loss = 0.06896428\n",
      "Iteration 10737, loss = 0.06896299\n",
      "Iteration 10738, loss = 0.06896171\n",
      "Iteration 10739, loss = 0.06896044\n",
      "Iteration 10740, loss = 0.06895912\n",
      "Iteration 10741, loss = 0.06895787\n",
      "Iteration 10742, loss = 0.06895656\n",
      "Iteration 10743, loss = 0.06895529\n",
      "Iteration 10744, loss = 0.06895402\n",
      "Iteration 10745, loss = 0.06895270\n",
      "Iteration 10746, loss = 0.06895143\n",
      "Iteration 10747, loss = 0.06895013\n",
      "Iteration 10748, loss = 0.06894888\n",
      "Iteration 10749, loss = 0.06894757\n",
      "Iteration 10750, loss = 0.06894630\n",
      "Iteration 10751, loss = 0.06894500\n",
      "Iteration 10752, loss = 0.06894372\n",
      "Iteration 10753, loss = 0.06894246\n",
      "Iteration 10754, loss = 0.06894115\n",
      "Iteration 10755, loss = 0.06893987\n",
      "Iteration 10756, loss = 0.06893859\n",
      "Iteration 10757, loss = 0.06893732\n",
      "Iteration 10758, loss = 0.06893603\n",
      "Iteration 10759, loss = 0.06893473\n",
      "Iteration 10760, loss = 0.06893347\n",
      "Iteration 10761, loss = 0.06893217\n",
      "Iteration 10762, loss = 0.06893090\n",
      "Iteration 10763, loss = 0.06892963\n",
      "Iteration 10764, loss = 0.06892832\n",
      "Iteration 10765, loss = 0.06892706\n",
      "Iteration 10766, loss = 0.06892577\n",
      "Iteration 10767, loss = 0.06892447\n",
      "Iteration 10768, loss = 0.06892322\n",
      "Iteration 10769, loss = 0.06892191\n",
      "Iteration 10770, loss = 0.06892065\n",
      "Iteration 10771, loss = 0.06891937\n",
      "Iteration 10772, loss = 0.06891807\n",
      "Iteration 10773, loss = 0.06891681\n",
      "Iteration 10774, loss = 0.06891550\n",
      "Iteration 10775, loss = 0.06891424\n",
      "Iteration 10776, loss = 0.06891296\n",
      "Iteration 10777, loss = 0.06891167\n",
      "Iteration 10778, loss = 0.06891040\n",
      "Iteration 10779, loss = 0.06890912\n",
      "Iteration 10780, loss = 0.06890782\n",
      "Iteration 10781, loss = 0.06890660\n",
      "Iteration 10782, loss = 0.06890526\n",
      "Iteration 10783, loss = 0.06890402\n",
      "Iteration 10784, loss = 0.06890273\n",
      "Iteration 10785, loss = 0.06890142\n",
      "Iteration 10786, loss = 0.06890017\n",
      "Iteration 10787, loss = 0.06889887\n",
      "Iteration 10788, loss = 0.06889760\n",
      "Iteration 10789, loss = 0.06889633\n",
      "Iteration 10790, loss = 0.06889502\n",
      "Iteration 10791, loss = 0.06889381\n",
      "Iteration 10792, loss = 0.06889247\n",
      "Iteration 10793, loss = 0.06889123\n",
      "Iteration 10794, loss = 0.06888996\n",
      "Iteration 10795, loss = 0.06888865\n",
      "Iteration 10796, loss = 0.06888738\n",
      "Iteration 10797, loss = 0.06888610\n",
      "Iteration 10798, loss = 0.06888481\n",
      "Iteration 10799, loss = 0.06888356\n",
      "Iteration 10800, loss = 0.06888226\n",
      "Iteration 10801, loss = 0.06888101\n",
      "Iteration 10802, loss = 0.06887969\n",
      "Iteration 10803, loss = 0.06887844\n",
      "Iteration 10804, loss = 0.06887717\n",
      "Iteration 10805, loss = 0.06887588\n",
      "Iteration 10806, loss = 0.06887460\n",
      "Iteration 10807, loss = 0.06887333\n",
      "Iteration 10808, loss = 0.06887204\n",
      "Iteration 10809, loss = 0.06887079\n",
      "Iteration 10810, loss = 0.06886949\n",
      "Iteration 10811, loss = 0.06886822\n",
      "Iteration 10812, loss = 0.06886695\n",
      "Iteration 10813, loss = 0.06886566\n",
      "Iteration 10814, loss = 0.06886441\n",
      "Iteration 10815, loss = 0.06886311\n",
      "Iteration 10816, loss = 0.06886184\n",
      "Iteration 10817, loss = 0.06886059\n",
      "Iteration 10818, loss = 0.06885926\n",
      "Iteration 10819, loss = 0.06885805\n",
      "Iteration 10820, loss = 0.06885675\n",
      "Iteration 10821, loss = 0.06885545\n",
      "Iteration 10822, loss = 0.06885421\n",
      "Iteration 10823, loss = 0.06885292\n",
      "Iteration 10824, loss = 0.06885166\n",
      "Iteration 10825, loss = 0.06885038\n",
      "Iteration 10826, loss = 0.06884912\n",
      "Iteration 10827, loss = 0.06884785\n",
      "Iteration 10828, loss = 0.06884655\n",
      "Iteration 10829, loss = 0.06884528\n",
      "Iteration 10830, loss = 0.06884401\n",
      "Iteration 10831, loss = 0.06884273\n",
      "Iteration 10832, loss = 0.06884145\n",
      "Iteration 10833, loss = 0.06884017\n",
      "Iteration 10834, loss = 0.06883893\n",
      "Iteration 10835, loss = 0.06883763\n",
      "Iteration 10836, loss = 0.06883638\n",
      "Iteration 10837, loss = 0.06883511\n",
      "Iteration 10838, loss = 0.06883383\n",
      "Iteration 10839, loss = 0.06883255\n",
      "Iteration 10840, loss = 0.06883127\n",
      "Iteration 10841, loss = 0.06883000\n",
      "Iteration 10842, loss = 0.06882872\n",
      "Iteration 10843, loss = 0.06882747\n",
      "Iteration 10844, loss = 0.06882619\n",
      "Iteration 10845, loss = 0.06882492\n",
      "Iteration 10846, loss = 0.06882367\n",
      "Iteration 10847, loss = 0.06882238\n",
      "Iteration 10848, loss = 0.06882108\n",
      "Iteration 10849, loss = 0.06881985\n",
      "Iteration 10850, loss = 0.06881854\n",
      "Iteration 10851, loss = 0.06881731\n",
      "Iteration 10852, loss = 0.06881602\n",
      "Iteration 10853, loss = 0.06881474\n",
      "Iteration 10854, loss = 0.06881350\n",
      "Iteration 10855, loss = 0.06881219\n",
      "Iteration 10856, loss = 0.06881095\n",
      "Iteration 10857, loss = 0.06880967\n",
      "Iteration 10858, loss = 0.06880841\n",
      "Iteration 10859, loss = 0.06880715\n",
      "Iteration 10860, loss = 0.06880586\n",
      "Iteration 10861, loss = 0.06880459\n",
      "Iteration 10862, loss = 0.06880333\n",
      "Iteration 10863, loss = 0.06880203\n",
      "Iteration 10864, loss = 0.06880081\n",
      "Iteration 10865, loss = 0.06879950\n",
      "Iteration 10866, loss = 0.06879825\n",
      "Iteration 10867, loss = 0.06879699\n",
      "Iteration 10868, loss = 0.06879569\n",
      "Iteration 10869, loss = 0.06879447\n",
      "Iteration 10870, loss = 0.06879316\n",
      "Iteration 10871, loss = 0.06879191\n",
      "Iteration 10872, loss = 0.06879066\n",
      "Iteration 10873, loss = 0.06878937\n",
      "Iteration 10874, loss = 0.06878809\n",
      "Iteration 10875, loss = 0.06878683\n",
      "Iteration 10876, loss = 0.06878555\n",
      "Iteration 10877, loss = 0.06878431\n",
      "Iteration 10878, loss = 0.06878303\n",
      "Iteration 10879, loss = 0.06878175\n",
      "Iteration 10880, loss = 0.06878050\n",
      "Iteration 10881, loss = 0.06877920\n",
      "Iteration 10882, loss = 0.06877800\n",
      "Iteration 10883, loss = 0.06877668\n",
      "Iteration 10884, loss = 0.06877545\n",
      "Iteration 10885, loss = 0.06877418\n",
      "Iteration 10886, loss = 0.06877288\n",
      "Iteration 10887, loss = 0.06877166\n",
      "Iteration 10888, loss = 0.06877035\n",
      "Iteration 10889, loss = 0.06876912\n",
      "Iteration 10890, loss = 0.06876783\n",
      "Iteration 10891, loss = 0.06876655\n",
      "Iteration 10892, loss = 0.06876533\n",
      "Iteration 10893, loss = 0.06876403\n",
      "Iteration 10894, loss = 0.06876277\n",
      "Iteration 10895, loss = 0.06876151\n",
      "Iteration 10896, loss = 0.06876023\n",
      "Iteration 10897, loss = 0.06875899\n",
      "Iteration 10898, loss = 0.06875770\n",
      "Iteration 10899, loss = 0.06875645\n",
      "Iteration 10900, loss = 0.06875519\n",
      "Iteration 10901, loss = 0.06875391\n",
      "Iteration 10902, loss = 0.06875265\n",
      "Iteration 10903, loss = 0.06875142\n",
      "Iteration 10904, loss = 0.06875012\n",
      "Iteration 10905, loss = 0.06874888\n",
      "Iteration 10906, loss = 0.06874759\n",
      "Iteration 10907, loss = 0.06874635\n",
      "Iteration 10908, loss = 0.06874507\n",
      "Iteration 10909, loss = 0.06874382\n",
      "Iteration 10910, loss = 0.06874254\n",
      "Iteration 10911, loss = 0.06874127\n",
      "Iteration 10912, loss = 0.06874004\n",
      "Iteration 10913, loss = 0.06873878\n",
      "Iteration 10914, loss = 0.06873748\n",
      "Iteration 10915, loss = 0.06873626\n",
      "Iteration 10916, loss = 0.06873497\n",
      "Iteration 10917, loss = 0.06873373\n",
      "Iteration 10918, loss = 0.06873244\n",
      "Iteration 10919, loss = 0.06873119\n",
      "Iteration 10920, loss = 0.06872991\n",
      "Iteration 10921, loss = 0.06872866\n",
      "Iteration 10922, loss = 0.06872742\n",
      "Iteration 10923, loss = 0.06872612\n",
      "Iteration 10924, loss = 0.06872490\n",
      "Iteration 10925, loss = 0.06872363\n",
      "Iteration 10926, loss = 0.06872237\n",
      "Iteration 10927, loss = 0.06872109\n",
      "Iteration 10928, loss = 0.06871983\n",
      "Iteration 10929, loss = 0.06871856\n",
      "Iteration 10930, loss = 0.06871732\n",
      "Iteration 10931, loss = 0.06871605\n",
      "Iteration 10932, loss = 0.06871480\n",
      "Iteration 10933, loss = 0.06871353\n",
      "Iteration 10934, loss = 0.06871228\n",
      "Iteration 10935, loss = 0.06871102\n",
      "Iteration 10936, loss = 0.06870973\n",
      "Iteration 10937, loss = 0.06870852\n",
      "Iteration 10938, loss = 0.06870720\n",
      "Iteration 10939, loss = 0.06870600\n",
      "Iteration 10940, loss = 0.06870470\n",
      "Iteration 10941, loss = 0.06870344\n",
      "Iteration 10942, loss = 0.06870221\n",
      "Iteration 10943, loss = 0.06870093\n",
      "Iteration 10944, loss = 0.06869968\n",
      "Iteration 10945, loss = 0.06869845\n",
      "Iteration 10946, loss = 0.06869715\n",
      "Iteration 10947, loss = 0.06869592\n",
      "Iteration 10948, loss = 0.06869464\n",
      "Iteration 10949, loss = 0.06869338\n",
      "Iteration 10950, loss = 0.06869212\n",
      "Iteration 10951, loss = 0.06869087\n",
      "Iteration 10952, loss = 0.06868961\n",
      "Iteration 10953, loss = 0.06868834\n",
      "Iteration 10954, loss = 0.06868710\n",
      "Iteration 10955, loss = 0.06868584\n",
      "Iteration 10956, loss = 0.06868456\n",
      "Iteration 10957, loss = 0.06868334\n",
      "Iteration 10958, loss = 0.06868204\n",
      "Iteration 10959, loss = 0.06868081\n",
      "Iteration 10960, loss = 0.06867955\n",
      "Iteration 10961, loss = 0.06867827\n",
      "Iteration 10962, loss = 0.06867706\n",
      "Iteration 10963, loss = 0.06867576\n",
      "Iteration 10964, loss = 0.06867452\n",
      "Iteration 10965, loss = 0.06867327\n",
      "Iteration 10966, loss = 0.06867198\n",
      "Iteration 10967, loss = 0.06867078\n",
      "Iteration 10968, loss = 0.06866949\n",
      "Iteration 10969, loss = 0.06866823\n",
      "Iteration 10970, loss = 0.06866700\n",
      "Iteration 10971, loss = 0.06866573\n",
      "Iteration 10972, loss = 0.06866450\n",
      "Iteration 10973, loss = 0.06866321\n",
      "Iteration 10974, loss = 0.06866195\n",
      "Iteration 10975, loss = 0.06866072\n",
      "Iteration 10976, loss = 0.06865944\n",
      "Iteration 10977, loss = 0.06865820\n",
      "Iteration 10978, loss = 0.06865694\n",
      "Iteration 10979, loss = 0.06865567\n",
      "Iteration 10980, loss = 0.06865445\n",
      "Iteration 10981, loss = 0.06865316\n",
      "Iteration 10982, loss = 0.06865193\n",
      "Iteration 10983, loss = 0.06865067\n",
      "Iteration 10984, loss = 0.06864941\n",
      "Iteration 10985, loss = 0.06864817\n",
      "Iteration 10986, loss = 0.06864688\n",
      "Iteration 10987, loss = 0.06864568\n",
      "Iteration 10988, loss = 0.06864439\n",
      "Iteration 10989, loss = 0.06864314\n",
      "Iteration 10990, loss = 0.06864191\n",
      "Iteration 10991, loss = 0.06864062\n",
      "Iteration 10992, loss = 0.06863940\n",
      "Iteration 10993, loss = 0.06863814\n",
      "Iteration 10994, loss = 0.06863687\n",
      "Iteration 10995, loss = 0.06863565\n",
      "Iteration 10996, loss = 0.06863438\n",
      "Iteration 10997, loss = 0.06863313\n",
      "Iteration 10998, loss = 0.06863188\n",
      "Iteration 10999, loss = 0.06863061\n",
      "Iteration 11000, loss = 0.06862938\n",
      "Iteration 11001, loss = 0.06862812\n",
      "Iteration 11002, loss = 0.06862687\n",
      "Iteration 11003, loss = 0.06862560\n",
      "Iteration 11004, loss = 0.06862436\n",
      "Iteration 11005, loss = 0.06862310\n",
      "Iteration 11006, loss = 0.06862185\n",
      "Iteration 11007, loss = 0.06862062\n",
      "Iteration 11008, loss = 0.06861935\n",
      "Iteration 11009, loss = 0.06861813\n",
      "Iteration 11010, loss = 0.06861687\n",
      "Iteration 11011, loss = 0.06861561\n",
      "Iteration 11012, loss = 0.06861435\n",
      "Iteration 11013, loss = 0.06861309\n",
      "Iteration 11014, loss = 0.06861185\n",
      "Iteration 11015, loss = 0.06861060\n",
      "Iteration 11016, loss = 0.06860935\n",
      "Iteration 11017, loss = 0.06860812\n",
      "Iteration 11018, loss = 0.06860684\n",
      "Iteration 11019, loss = 0.06860563\n",
      "Iteration 11020, loss = 0.06860435\n",
      "Iteration 11021, loss = 0.06860308\n",
      "Iteration 11022, loss = 0.06860188\n",
      "Iteration 11023, loss = 0.06860061\n",
      "Iteration 11024, loss = 0.06859934\n",
      "Iteration 11025, loss = 0.06859810\n",
      "Iteration 11026, loss = 0.06859686\n",
      "Iteration 11027, loss = 0.06859561\n",
      "Iteration 11028, loss = 0.06859435\n",
      "Iteration 11029, loss = 0.06859313\n",
      "Iteration 11030, loss = 0.06859185\n",
      "Iteration 11031, loss = 0.06859061\n",
      "Iteration 11032, loss = 0.06858938\n",
      "Iteration 11033, loss = 0.06858811\n",
      "Iteration 11034, loss = 0.06858687\n",
      "Iteration 11035, loss = 0.06858563\n",
      "Iteration 11036, loss = 0.06858435\n",
      "Iteration 11037, loss = 0.06858315\n",
      "Iteration 11038, loss = 0.06858186\n",
      "Iteration 11039, loss = 0.06858065\n",
      "Iteration 11040, loss = 0.06857938\n",
      "Iteration 11041, loss = 0.06857814\n",
      "Iteration 11042, loss = 0.06857690\n",
      "Iteration 11043, loss = 0.06857563\n",
      "Iteration 11044, loss = 0.06857441\n",
      "Iteration 11045, loss = 0.06857314\n",
      "Iteration 11046, loss = 0.06857189\n",
      "Iteration 11047, loss = 0.06857069\n",
      "Iteration 11048, loss = 0.06856939\n",
      "Iteration 11049, loss = 0.06856820\n",
      "Iteration 11050, loss = 0.06856692\n",
      "Iteration 11051, loss = 0.06856568\n",
      "Iteration 11052, loss = 0.06856445\n",
      "Iteration 11053, loss = 0.06856316\n",
      "Iteration 11054, loss = 0.06856197\n",
      "Iteration 11055, loss = 0.06856069\n",
      "Iteration 11056, loss = 0.06855944\n",
      "Iteration 11057, loss = 0.06855822\n",
      "Iteration 11058, loss = 0.06855696\n",
      "Iteration 11059, loss = 0.06855574\n",
      "Iteration 11060, loss = 0.06855447\n",
      "Iteration 11061, loss = 0.06855322\n",
      "Iteration 11062, loss = 0.06855199\n",
      "Iteration 11063, loss = 0.06855074\n",
      "Iteration 11064, loss = 0.06854949\n",
      "Iteration 11065, loss = 0.06854826\n",
      "Iteration 11066, loss = 0.06854699\n",
      "Iteration 11067, loss = 0.06854578\n",
      "Iteration 11068, loss = 0.06854451\n",
      "Iteration 11069, loss = 0.06854327\n",
      "Iteration 11070, loss = 0.06854204\n",
      "Iteration 11071, loss = 0.06854077\n",
      "Iteration 11072, loss = 0.06853956\n",
      "Iteration 11073, loss = 0.06853830\n",
      "Iteration 11074, loss = 0.06853706\n",
      "Iteration 11075, loss = 0.06853582\n",
      "Iteration 11076, loss = 0.06853458\n",
      "Iteration 11077, loss = 0.06853333\n",
      "Iteration 11078, loss = 0.06853209\n",
      "Iteration 11079, loss = 0.06853086\n",
      "Iteration 11080, loss = 0.06852960\n",
      "Iteration 11081, loss = 0.06852836\n",
      "Iteration 11082, loss = 0.06852713\n",
      "Iteration 11083, loss = 0.06852588\n",
      "Iteration 11084, loss = 0.06852465\n",
      "Iteration 11085, loss = 0.06852339\n",
      "Iteration 11086, loss = 0.06852217\n",
      "Iteration 11087, loss = 0.06852092\n",
      "Iteration 11088, loss = 0.06851966\n",
      "Iteration 11089, loss = 0.06851844\n",
      "Iteration 11090, loss = 0.06851722\n",
      "Iteration 11091, loss = 0.06851592\n",
      "Iteration 11092, loss = 0.06851476\n",
      "Iteration 11093, loss = 0.06851348\n",
      "Iteration 11094, loss = 0.06851223\n",
      "Iteration 11095, loss = 0.06851100\n",
      "Iteration 11096, loss = 0.06850974\n",
      "Iteration 11097, loss = 0.06850851\n",
      "Iteration 11098, loss = 0.06850727\n",
      "Iteration 11099, loss = 0.06850605\n",
      "Iteration 11100, loss = 0.06850477\n",
      "Iteration 11101, loss = 0.06850358\n",
      "Iteration 11102, loss = 0.06850233\n",
      "Iteration 11103, loss = 0.06850108\n",
      "Iteration 11104, loss = 0.06849984\n",
      "Iteration 11105, loss = 0.06849859\n",
      "Iteration 11106, loss = 0.06849737\n",
      "Iteration 11107, loss = 0.06849611\n",
      "Iteration 11108, loss = 0.06849488\n",
      "Iteration 11109, loss = 0.06849366\n",
      "Iteration 11110, loss = 0.06849239\n",
      "Iteration 11111, loss = 0.06849118\n",
      "Iteration 11112, loss = 0.06848992\n",
      "Iteration 11113, loss = 0.06848868\n",
      "Iteration 11114, loss = 0.06848748\n",
      "Iteration 11115, loss = 0.06848620\n",
      "Iteration 11116, loss = 0.06848500\n",
      "Iteration 11117, loss = 0.06848374\n",
      "Iteration 11118, loss = 0.06848250\n",
      "Iteration 11119, loss = 0.06848129\n",
      "Iteration 11120, loss = 0.06848002\n",
      "Iteration 11121, loss = 0.06847882\n",
      "Iteration 11122, loss = 0.06847755\n",
      "Iteration 11123, loss = 0.06847631\n",
      "Iteration 11124, loss = 0.06847510\n",
      "Iteration 11125, loss = 0.06847383\n",
      "Iteration 11126, loss = 0.06847263\n",
      "Iteration 11127, loss = 0.06847138\n",
      "Iteration 11128, loss = 0.06847011\n",
      "Iteration 11129, loss = 0.06846894\n",
      "Iteration 11130, loss = 0.06846766\n",
      "Iteration 11131, loss = 0.06846645\n",
      "Iteration 11132, loss = 0.06846518\n",
      "Iteration 11133, loss = 0.06846395\n",
      "Iteration 11134, loss = 0.06846273\n",
      "Iteration 11135, loss = 0.06846147\n",
      "Iteration 11136, loss = 0.06846028\n",
      "Iteration 11137, loss = 0.06845900\n",
      "Iteration 11138, loss = 0.06845779\n",
      "Iteration 11139, loss = 0.06845655\n",
      "Iteration 11140, loss = 0.06845529\n",
      "Iteration 11141, loss = 0.06845410\n",
      "Iteration 11142, loss = 0.06845284\n",
      "Iteration 11143, loss = 0.06845160\n",
      "Iteration 11144, loss = 0.06845039\n",
      "Iteration 11145, loss = 0.06844913\n",
      "Iteration 11146, loss = 0.06844792\n",
      "Iteration 11147, loss = 0.06844666\n",
      "Iteration 11148, loss = 0.06844544\n",
      "Iteration 11149, loss = 0.06844421\n",
      "Iteration 11150, loss = 0.06844297\n",
      "Iteration 11151, loss = 0.06844173\n",
      "Iteration 11152, loss = 0.06844052\n",
      "Iteration 11153, loss = 0.06843927\n",
      "Iteration 11154, loss = 0.06843805\n",
      "Iteration 11155, loss = 0.06843679\n",
      "Iteration 11156, loss = 0.06843559\n",
      "Iteration 11157, loss = 0.06843435\n",
      "Iteration 11158, loss = 0.06843309\n",
      "Iteration 11159, loss = 0.06843190\n",
      "Iteration 11160, loss = 0.06843062\n",
      "Iteration 11161, loss = 0.06842944\n",
      "Iteration 11162, loss = 0.06842818\n",
      "Iteration 11163, loss = 0.06842694\n",
      "Iteration 11164, loss = 0.06842573\n",
      "Iteration 11165, loss = 0.06842447\n",
      "Iteration 11166, loss = 0.06842327\n",
      "Iteration 11167, loss = 0.06842201\n",
      "Iteration 11168, loss = 0.06842080\n",
      "Iteration 11169, loss = 0.06841956\n",
      "Iteration 11170, loss = 0.06841833\n",
      "Iteration 11171, loss = 0.06841709\n",
      "Iteration 11172, loss = 0.06841586\n",
      "Iteration 11173, loss = 0.06841466\n",
      "Iteration 11174, loss = 0.06841339\n",
      "Iteration 11175, loss = 0.06841220\n",
      "Iteration 11176, loss = 0.06841093\n",
      "Iteration 11177, loss = 0.06840973\n",
      "Iteration 11178, loss = 0.06840850\n",
      "Iteration 11179, loss = 0.06840726\n",
      "Iteration 11180, loss = 0.06840602\n",
      "Iteration 11181, loss = 0.06840480\n",
      "Iteration 11182, loss = 0.06840357\n",
      "Iteration 11183, loss = 0.06840235\n",
      "Iteration 11184, loss = 0.06840111\n",
      "Iteration 11185, loss = 0.06839990\n",
      "Iteration 11186, loss = 0.06839867\n",
      "Iteration 11187, loss = 0.06839744\n",
      "Iteration 11188, loss = 0.06839621\n",
      "Iteration 11189, loss = 0.06839496\n",
      "Iteration 11190, loss = 0.06839377\n",
      "Iteration 11191, loss = 0.06839250\n",
      "Iteration 11192, loss = 0.06839129\n",
      "Iteration 11193, loss = 0.06839007\n",
      "Iteration 11194, loss = 0.06838880\n",
      "Iteration 11195, loss = 0.06838764\n",
      "Iteration 11196, loss = 0.06838636\n",
      "Iteration 11197, loss = 0.06838515\n",
      "Iteration 11198, loss = 0.06838392\n",
      "Iteration 11199, loss = 0.06838266\n",
      "Iteration 11200, loss = 0.06838150\n",
      "Iteration 11201, loss = 0.06838021\n",
      "Iteration 11202, loss = 0.06837903\n",
      "Iteration 11203, loss = 0.06837778\n",
      "Iteration 11204, loss = 0.06837653\n",
      "Iteration 11205, loss = 0.06837536\n",
      "Iteration 11206, loss = 0.06837409\n",
      "Iteration 11207, loss = 0.06837288\n",
      "Iteration 11208, loss = 0.06837165\n",
      "Iteration 11209, loss = 0.06837041\n",
      "Iteration 11210, loss = 0.06836921\n",
      "Iteration 11211, loss = 0.06836795\n",
      "Iteration 11212, loss = 0.06836675\n",
      "Iteration 11213, loss = 0.06836551\n",
      "Iteration 11214, loss = 0.06836429\n",
      "Iteration 11215, loss = 0.06836309\n",
      "Iteration 11216, loss = 0.06836183\n",
      "Iteration 11217, loss = 0.06836062\n",
      "Iteration 11218, loss = 0.06835939\n",
      "Iteration 11219, loss = 0.06835815\n",
      "Iteration 11220, loss = 0.06835696\n",
      "Iteration 11221, loss = 0.06835572\n",
      "Iteration 11222, loss = 0.06835448\n",
      "Iteration 11223, loss = 0.06835329\n",
      "Iteration 11224, loss = 0.06835203\n",
      "Iteration 11225, loss = 0.06835085\n",
      "Iteration 11226, loss = 0.06834960\n",
      "Iteration 11227, loss = 0.06834837\n",
      "Iteration 11228, loss = 0.06834716\n",
      "Iteration 11229, loss = 0.06834593\n",
      "Iteration 11230, loss = 0.06834471\n",
      "Iteration 11231, loss = 0.06834346\n",
      "Iteration 11232, loss = 0.06834228\n",
      "Iteration 11233, loss = 0.06834102\n",
      "Iteration 11234, loss = 0.06833983\n",
      "Iteration 11235, loss = 0.06833858\n",
      "Iteration 11236, loss = 0.06833736\n",
      "Iteration 11237, loss = 0.06833615\n",
      "Iteration 11238, loss = 0.06833493\n",
      "Iteration 11239, loss = 0.06833370\n",
      "Iteration 11240, loss = 0.06833247\n",
      "Iteration 11241, loss = 0.06833125\n",
      "Iteration 11242, loss = 0.06833006\n",
      "Iteration 11243, loss = 0.06832882\n",
      "Iteration 11244, loss = 0.06832761\n",
      "Iteration 11245, loss = 0.06832639\n",
      "Iteration 11246, loss = 0.06832515\n",
      "Iteration 11247, loss = 0.06832394\n",
      "Iteration 11248, loss = 0.06832270\n",
      "Iteration 11249, loss = 0.06832149\n",
      "Iteration 11250, loss = 0.06832028\n",
      "Iteration 11251, loss = 0.06831903\n",
      "Iteration 11252, loss = 0.06831783\n",
      "Iteration 11253, loss = 0.06831662\n",
      "Iteration 11254, loss = 0.06831540\n",
      "Iteration 11255, loss = 0.06831416\n",
      "Iteration 11256, loss = 0.06831294\n",
      "Iteration 11257, loss = 0.06831174\n",
      "Iteration 11258, loss = 0.06831049\n",
      "Iteration 11259, loss = 0.06830929\n",
      "Iteration 11260, loss = 0.06830804\n",
      "Iteration 11261, loss = 0.06830685\n",
      "Iteration 11262, loss = 0.06830562\n",
      "Iteration 11263, loss = 0.06830439\n",
      "Iteration 11264, loss = 0.06830320\n",
      "Iteration 11265, loss = 0.06830197\n",
      "Iteration 11266, loss = 0.06830074\n",
      "Iteration 11267, loss = 0.06829953\n",
      "Iteration 11268, loss = 0.06829830\n",
      "Iteration 11269, loss = 0.06829710\n",
      "Iteration 11270, loss = 0.06829587\n",
      "Iteration 11271, loss = 0.06829466\n",
      "Iteration 11272, loss = 0.06829343\n",
      "Iteration 11273, loss = 0.06829222\n",
      "Iteration 11274, loss = 0.06829101\n",
      "Iteration 11275, loss = 0.06828977\n",
      "Iteration 11276, loss = 0.06828858\n",
      "Iteration 11277, loss = 0.06828733\n",
      "Iteration 11278, loss = 0.06828613\n",
      "Iteration 11279, loss = 0.06828491\n",
      "Iteration 11280, loss = 0.06828368\n",
      "Iteration 11281, loss = 0.06828250\n",
      "Iteration 11282, loss = 0.06828124\n",
      "Iteration 11283, loss = 0.06828007\n",
      "Iteration 11284, loss = 0.06827884\n",
      "Iteration 11285, loss = 0.06827761\n",
      "Iteration 11286, loss = 0.06827641\n",
      "Iteration 11287, loss = 0.06827516\n",
      "Iteration 11288, loss = 0.06827398\n",
      "Iteration 11289, loss = 0.06827275\n",
      "Iteration 11290, loss = 0.06827152\n",
      "Iteration 11291, loss = 0.06827032\n",
      "Iteration 11292, loss = 0.06826909\n",
      "Iteration 11293, loss = 0.06826789\n",
      "Iteration 11294, loss = 0.06826667\n",
      "Iteration 11295, loss = 0.06826544\n",
      "Iteration 11296, loss = 0.06826425\n",
      "Iteration 11297, loss = 0.06826302\n",
      "Iteration 11298, loss = 0.06826180\n",
      "Iteration 11299, loss = 0.06826060\n",
      "Iteration 11300, loss = 0.06825938\n",
      "Iteration 11301, loss = 0.06825817\n",
      "Iteration 11302, loss = 0.06825692\n",
      "Iteration 11303, loss = 0.06825576\n",
      "Iteration 11304, loss = 0.06825450\n",
      "Iteration 11305, loss = 0.06825332\n",
      "Iteration 11306, loss = 0.06825212\n",
      "Iteration 11307, loss = 0.06825086\n",
      "Iteration 11308, loss = 0.06824968\n",
      "Iteration 11309, loss = 0.06824843\n",
      "Iteration 11310, loss = 0.06824724\n",
      "Iteration 11311, loss = 0.06824603\n",
      "Iteration 11312, loss = 0.06824479\n",
      "Iteration 11313, loss = 0.06824361\n",
      "Iteration 11314, loss = 0.06824237\n",
      "Iteration 11315, loss = 0.06824117\n",
      "Iteration 11316, loss = 0.06823995\n",
      "Iteration 11317, loss = 0.06823874\n",
      "Iteration 11318, loss = 0.06823754\n",
      "Iteration 11319, loss = 0.06823630\n",
      "Iteration 11320, loss = 0.06823511\n",
      "Iteration 11321, loss = 0.06823388\n",
      "Iteration 11322, loss = 0.06823269\n",
      "Iteration 11323, loss = 0.06823147\n",
      "Iteration 11324, loss = 0.06823025\n",
      "Iteration 11325, loss = 0.06822905\n",
      "Iteration 11326, loss = 0.06822784\n",
      "Iteration 11327, loss = 0.06822661\n",
      "Iteration 11328, loss = 0.06822541\n",
      "Iteration 11329, loss = 0.06822420\n",
      "Iteration 11330, loss = 0.06822300\n",
      "Iteration 11331, loss = 0.06822178\n",
      "Iteration 11332, loss = 0.06822058\n",
      "Iteration 11333, loss = 0.06821936\n",
      "Iteration 11334, loss = 0.06821815\n",
      "Iteration 11335, loss = 0.06821695\n",
      "Iteration 11336, loss = 0.06821572\n",
      "Iteration 11337, loss = 0.06821453\n",
      "Iteration 11338, loss = 0.06821331\n",
      "Iteration 11339, loss = 0.06821209\n",
      "Iteration 11340, loss = 0.06821092\n",
      "Iteration 11341, loss = 0.06820969\n",
      "Iteration 11342, loss = 0.06820847\n",
      "Iteration 11343, loss = 0.06820725\n",
      "Iteration 11344, loss = 0.06820604\n",
      "Iteration 11345, loss = 0.06820484\n",
      "Iteration 11346, loss = 0.06820362\n",
      "Iteration 11347, loss = 0.06820244\n",
      "Iteration 11348, loss = 0.06820119\n",
      "Iteration 11349, loss = 0.06820003\n",
      "Iteration 11350, loss = 0.06819880\n",
      "Iteration 11351, loss = 0.06819756\n",
      "Iteration 11352, loss = 0.06819640\n",
      "Iteration 11353, loss = 0.06819514\n",
      "Iteration 11354, loss = 0.06819399\n",
      "Iteration 11355, loss = 0.06819274\n",
      "Iteration 11356, loss = 0.06819154\n",
      "Iteration 11357, loss = 0.06819036\n",
      "Iteration 11358, loss = 0.06818911\n",
      "Iteration 11359, loss = 0.06818794\n",
      "Iteration 11360, loss = 0.06818671\n",
      "Iteration 11361, loss = 0.06818550\n",
      "Iteration 11362, loss = 0.06818430\n",
      "Iteration 11363, loss = 0.06818308\n",
      "Iteration 11364, loss = 0.06818191\n",
      "Iteration 11365, loss = 0.06818065\n",
      "Iteration 11366, loss = 0.06817949\n",
      "Iteration 11367, loss = 0.06817827\n",
      "Iteration 11368, loss = 0.06817705\n",
      "Iteration 11369, loss = 0.06817588\n",
      "Iteration 11370, loss = 0.06817462\n",
      "Iteration 11371, loss = 0.06817347\n",
      "Iteration 11372, loss = 0.06817225\n",
      "Iteration 11373, loss = 0.06817101\n",
      "Iteration 11374, loss = 0.06816983\n",
      "Iteration 11375, loss = 0.06816861\n",
      "Iteration 11376, loss = 0.06816741\n",
      "Iteration 11377, loss = 0.06816621\n",
      "Iteration 11378, loss = 0.06816497\n",
      "Iteration 11379, loss = 0.06816381\n",
      "Iteration 11380, loss = 0.06816257\n",
      "Iteration 11381, loss = 0.06816139\n",
      "Iteration 11382, loss = 0.06816016\n",
      "Iteration 11383, loss = 0.06815898\n",
      "Iteration 11384, loss = 0.06815777\n",
      "Iteration 11385, loss = 0.06815655\n",
      "Iteration 11386, loss = 0.06815537\n",
      "Iteration 11387, loss = 0.06815414\n",
      "Iteration 11388, loss = 0.06815294\n",
      "Iteration 11389, loss = 0.06815175\n",
      "Iteration 11390, loss = 0.06815052\n",
      "Iteration 11391, loss = 0.06814935\n",
      "Iteration 11392, loss = 0.06814812\n",
      "Iteration 11393, loss = 0.06814694\n",
      "Iteration 11394, loss = 0.06814572\n",
      "Iteration 11395, loss = 0.06814452\n",
      "Iteration 11396, loss = 0.06814331\n",
      "Iteration 11397, loss = 0.06814210\n",
      "Iteration 11398, loss = 0.06814092\n",
      "Iteration 11399, loss = 0.06813971\n",
      "Iteration 11400, loss = 0.06813850\n",
      "Iteration 11401, loss = 0.06813730\n",
      "Iteration 11402, loss = 0.06813609\n",
      "Iteration 11403, loss = 0.06813491\n",
      "Iteration 11404, loss = 0.06813370\n",
      "Iteration 11405, loss = 0.06813250\n",
      "Iteration 11406, loss = 0.06813129\n",
      "Iteration 11407, loss = 0.06813009\n",
      "Iteration 11408, loss = 0.06812888\n",
      "Iteration 11409, loss = 0.06812769\n",
      "Iteration 11410, loss = 0.06812649\n",
      "Iteration 11411, loss = 0.06812527\n",
      "Iteration 11412, loss = 0.06812409\n",
      "Iteration 11413, loss = 0.06812287\n",
      "Iteration 11414, loss = 0.06812168\n",
      "Iteration 11415, loss = 0.06812048\n",
      "Iteration 11416, loss = 0.06811928\n",
      "Iteration 11417, loss = 0.06811807\n",
      "Iteration 11418, loss = 0.06811688\n",
      "Iteration 11419, loss = 0.06811566\n",
      "Iteration 11420, loss = 0.06811449\n",
      "Iteration 11421, loss = 0.06811327\n",
      "Iteration 11422, loss = 0.06811208\n",
      "Iteration 11423, loss = 0.06811088\n",
      "Iteration 11424, loss = 0.06810967\n",
      "Iteration 11425, loss = 0.06810849\n",
      "Iteration 11426, loss = 0.06810726\n",
      "Iteration 11427, loss = 0.06810610\n",
      "Iteration 11428, loss = 0.06810487\n",
      "Iteration 11429, loss = 0.06810369\n",
      "Iteration 11430, loss = 0.06810247\n",
      "Iteration 11431, loss = 0.06810126\n",
      "Iteration 11432, loss = 0.06810010\n",
      "Iteration 11433, loss = 0.06809887\n",
      "Iteration 11434, loss = 0.06809768\n",
      "Iteration 11435, loss = 0.06809648\n",
      "Iteration 11436, loss = 0.06809527\n",
      "Iteration 11437, loss = 0.06809412\n",
      "Iteration 11438, loss = 0.06809287\n",
      "Iteration 11439, loss = 0.06809171\n",
      "Iteration 11440, loss = 0.06809048\n",
      "Iteration 11441, loss = 0.06808929\n",
      "Iteration 11442, loss = 0.06808811\n",
      "Iteration 11443, loss = 0.06808687\n",
      "Iteration 11444, loss = 0.06808573\n",
      "Iteration 11445, loss = 0.06808451\n",
      "Iteration 11446, loss = 0.06808330\n",
      "Iteration 11447, loss = 0.06808212\n",
      "Iteration 11448, loss = 0.06808090\n",
      "Iteration 11449, loss = 0.06807975\n",
      "Iteration 11450, loss = 0.06807850\n",
      "Iteration 11451, loss = 0.06807735\n",
      "Iteration 11452, loss = 0.06807613\n",
      "Iteration 11453, loss = 0.06807491\n",
      "Iteration 11454, loss = 0.06807376\n",
      "Iteration 11455, loss = 0.06807254\n",
      "Iteration 11456, loss = 0.06807134\n",
      "Iteration 11457, loss = 0.06807015\n",
      "Iteration 11458, loss = 0.06806895\n",
      "Iteration 11459, loss = 0.06806774\n",
      "Iteration 11460, loss = 0.06806655\n",
      "Iteration 11461, loss = 0.06806537\n",
      "Iteration 11462, loss = 0.06806418\n",
      "Iteration 11463, loss = 0.06806298\n",
      "Iteration 11464, loss = 0.06806179\n",
      "Iteration 11465, loss = 0.06806057\n",
      "Iteration 11466, loss = 0.06805940\n",
      "Iteration 11467, loss = 0.06805820\n",
      "Iteration 11468, loss = 0.06805700\n",
      "Iteration 11469, loss = 0.06805582\n",
      "Iteration 11470, loss = 0.06805460\n",
      "Iteration 11471, loss = 0.06805342\n",
      "Iteration 11472, loss = 0.06805221\n",
      "Iteration 11473, loss = 0.06805103\n",
      "Iteration 11474, loss = 0.06804983\n",
      "Iteration 11475, loss = 0.06804864\n",
      "Iteration 11476, loss = 0.06804744\n",
      "Iteration 11477, loss = 0.06804625\n",
      "Iteration 11478, loss = 0.06804507\n",
      "Iteration 11479, loss = 0.06804386\n",
      "Iteration 11480, loss = 0.06804267\n",
      "Iteration 11481, loss = 0.06804147\n",
      "Iteration 11482, loss = 0.06804029\n",
      "Iteration 11483, loss = 0.06803909\n",
      "Iteration 11484, loss = 0.06803787\n",
      "Iteration 11485, loss = 0.06803673\n",
      "Iteration 11486, loss = 0.06803548\n",
      "Iteration 11487, loss = 0.06803436\n",
      "Iteration 11488, loss = 0.06803314\n",
      "Iteration 11489, loss = 0.06803193\n",
      "Iteration 11490, loss = 0.06803076\n",
      "Iteration 11491, loss = 0.06802953\n",
      "Iteration 11492, loss = 0.06802839\n",
      "Iteration 11493, loss = 0.06802714\n",
      "Iteration 11494, loss = 0.06802599\n",
      "Iteration 11495, loss = 0.06802479\n",
      "Iteration 11496, loss = 0.06802356\n",
      "Iteration 11497, loss = 0.06802244\n",
      "Iteration 11498, loss = 0.06802118\n",
      "Iteration 11499, loss = 0.06802006\n",
      "Iteration 11500, loss = 0.06801883\n",
      "Iteration 11501, loss = 0.06801764\n",
      "Iteration 11502, loss = 0.06801645\n",
      "Iteration 11503, loss = 0.06801524\n",
      "Iteration 11504, loss = 0.06801408\n",
      "Iteration 11505, loss = 0.06801286\n",
      "Iteration 11506, loss = 0.06801169\n",
      "Iteration 11507, loss = 0.06801050\n",
      "Iteration 11508, loss = 0.06800930\n",
      "Iteration 11509, loss = 0.06800813\n",
      "Iteration 11510, loss = 0.06800690\n",
      "Iteration 11511, loss = 0.06800575\n",
      "Iteration 11512, loss = 0.06800455\n",
      "Iteration 11513, loss = 0.06800335\n",
      "Iteration 11514, loss = 0.06800218\n",
      "Iteration 11515, loss = 0.06800098\n",
      "Iteration 11516, loss = 0.06799977\n",
      "Iteration 11517, loss = 0.06799861\n",
      "Iteration 11518, loss = 0.06799739\n",
      "Iteration 11519, loss = 0.06799623\n",
      "Iteration 11520, loss = 0.06799504\n",
      "Iteration 11521, loss = 0.06799384\n",
      "Iteration 11522, loss = 0.06799266\n",
      "Iteration 11523, loss = 0.06799146\n",
      "Iteration 11524, loss = 0.06799028\n",
      "Iteration 11525, loss = 0.06798908\n",
      "Iteration 11526, loss = 0.06798791\n",
      "Iteration 11527, loss = 0.06798671\n",
      "Iteration 11528, loss = 0.06798555\n",
      "Iteration 11529, loss = 0.06798436\n",
      "Iteration 11530, loss = 0.06798314\n",
      "Iteration 11531, loss = 0.06798198\n",
      "Iteration 11532, loss = 0.06798077\n",
      "Iteration 11533, loss = 0.06797961\n",
      "Iteration 11534, loss = 0.06797842\n",
      "Iteration 11535, loss = 0.06797720\n",
      "Iteration 11536, loss = 0.06797606\n",
      "Iteration 11537, loss = 0.06797485\n",
      "Iteration 11538, loss = 0.06797367\n",
      "Iteration 11539, loss = 0.06797248\n",
      "Iteration 11540, loss = 0.06797128\n",
      "Iteration 11541, loss = 0.06797011\n",
      "Iteration 11542, loss = 0.06796890\n",
      "Iteration 11543, loss = 0.06796774\n",
      "Iteration 11544, loss = 0.06796655\n",
      "Iteration 11545, loss = 0.06796536\n",
      "Iteration 11546, loss = 0.06796417\n",
      "Iteration 11547, loss = 0.06796299\n",
      "Iteration 11548, loss = 0.06796180\n",
      "Iteration 11549, loss = 0.06796061\n",
      "Iteration 11550, loss = 0.06795945\n",
      "Iteration 11551, loss = 0.06795824\n",
      "Iteration 11552, loss = 0.06795708\n",
      "Iteration 11553, loss = 0.06795586\n",
      "Iteration 11554, loss = 0.06795471\n",
      "Iteration 11555, loss = 0.06795351\n",
      "Iteration 11556, loss = 0.06795232\n",
      "Iteration 11557, loss = 0.06795116\n",
      "Iteration 11558, loss = 0.06794994\n",
      "Iteration 11559, loss = 0.06794879\n",
      "Iteration 11560, loss = 0.06794760\n",
      "Iteration 11561, loss = 0.06794639\n",
      "Iteration 11562, loss = 0.06794527\n",
      "Iteration 11563, loss = 0.06794403\n",
      "Iteration 11564, loss = 0.06794289\n",
      "Iteration 11565, loss = 0.06794169\n",
      "Iteration 11566, loss = 0.06794049\n",
      "Iteration 11567, loss = 0.06793932\n",
      "Iteration 11568, loss = 0.06793812\n",
      "Iteration 11569, loss = 0.06793697\n",
      "Iteration 11570, loss = 0.06793577\n",
      "Iteration 11571, loss = 0.06793458\n",
      "Iteration 11572, loss = 0.06793342\n",
      "Iteration 11573, loss = 0.06793222\n",
      "Iteration 11574, loss = 0.06793106\n",
      "Iteration 11575, loss = 0.06792986\n",
      "Iteration 11576, loss = 0.06792867\n",
      "Iteration 11577, loss = 0.06792750\n",
      "Iteration 11578, loss = 0.06792630\n",
      "Iteration 11579, loss = 0.06792516\n",
      "Iteration 11580, loss = 0.06792395\n",
      "Iteration 11581, loss = 0.06792277\n",
      "Iteration 11582, loss = 0.06792159\n",
      "Iteration 11583, loss = 0.06792039\n",
      "Iteration 11584, loss = 0.06791925\n",
      "Iteration 11585, loss = 0.06791804\n",
      "Iteration 11586, loss = 0.06791687\n",
      "Iteration 11587, loss = 0.06791568\n",
      "Iteration 11588, loss = 0.06791451\n",
      "Iteration 11589, loss = 0.06791333\n",
      "Iteration 11590, loss = 0.06791213\n",
      "Iteration 11591, loss = 0.06791099\n",
      "Iteration 11592, loss = 0.06790979\n",
      "Iteration 11593, loss = 0.06790861\n",
      "Iteration 11594, loss = 0.06790743\n",
      "Iteration 11595, loss = 0.06790624\n",
      "Iteration 11596, loss = 0.06790508\n",
      "Iteration 11597, loss = 0.06790386\n",
      "Iteration 11598, loss = 0.06790273\n",
      "Iteration 11599, loss = 0.06790154\n",
      "Iteration 11600, loss = 0.06790034\n",
      "Iteration 11601, loss = 0.06789918\n",
      "Iteration 11602, loss = 0.06789797\n",
      "Iteration 11603, loss = 0.06789684\n",
      "Iteration 11604, loss = 0.06789564\n",
      "Iteration 11605, loss = 0.06789448\n",
      "Iteration 11606, loss = 0.06789328\n",
      "Iteration 11607, loss = 0.06789209\n",
      "Iteration 11608, loss = 0.06789093\n",
      "Iteration 11609, loss = 0.06788974\n",
      "Iteration 11610, loss = 0.06788859\n",
      "Iteration 11611, loss = 0.06788738\n",
      "Iteration 11612, loss = 0.06788623\n",
      "Iteration 11613, loss = 0.06788502\n",
      "Iteration 11614, loss = 0.06788387\n",
      "Iteration 11615, loss = 0.06788269\n",
      "Iteration 11616, loss = 0.06788149\n",
      "Iteration 11617, loss = 0.06788035\n",
      "Iteration 11618, loss = 0.06787913\n",
      "Iteration 11619, loss = 0.06787800\n",
      "Iteration 11620, loss = 0.06787680\n",
      "Iteration 11621, loss = 0.06787562\n",
      "Iteration 11622, loss = 0.06787447\n",
      "Iteration 11623, loss = 0.06787327\n",
      "Iteration 11624, loss = 0.06787210\n",
      "Iteration 11625, loss = 0.06787091\n",
      "Iteration 11626, loss = 0.06786975\n",
      "Iteration 11627, loss = 0.06786857\n",
      "Iteration 11628, loss = 0.06786740\n",
      "Iteration 11629, loss = 0.06786622\n",
      "Iteration 11630, loss = 0.06786502\n",
      "Iteration 11631, loss = 0.06786389\n",
      "Iteration 11632, loss = 0.06786270\n",
      "Iteration 11633, loss = 0.06786150\n",
      "Iteration 11634, loss = 0.06786037\n",
      "Iteration 11635, loss = 0.06785917\n",
      "Iteration 11636, loss = 0.06785799\n",
      "Iteration 11637, loss = 0.06785682\n",
      "Iteration 11638, loss = 0.06785564\n",
      "Iteration 11639, loss = 0.06785448\n",
      "Iteration 11640, loss = 0.06785328\n",
      "Iteration 11641, loss = 0.06785215\n",
      "Iteration 11642, loss = 0.06785093\n",
      "Iteration 11643, loss = 0.06784980\n",
      "Iteration 11644, loss = 0.06784861\n",
      "Iteration 11645, loss = 0.06784743\n",
      "Iteration 11646, loss = 0.06784626\n",
      "Iteration 11647, loss = 0.06784508\n",
      "Iteration 11648, loss = 0.06784391\n",
      "Iteration 11649, loss = 0.06784274\n",
      "Iteration 11650, loss = 0.06784156\n",
      "Iteration 11651, loss = 0.06784040\n",
      "Iteration 11652, loss = 0.06783921\n",
      "Iteration 11653, loss = 0.06783806\n",
      "Iteration 11654, loss = 0.06783687\n",
      "Iteration 11655, loss = 0.06783571\n",
      "Iteration 11656, loss = 0.06783456\n",
      "Iteration 11657, loss = 0.06783334\n",
      "Iteration 11658, loss = 0.06783221\n",
      "Iteration 11659, loss = 0.06783101\n",
      "Iteration 11660, loss = 0.06782984\n",
      "Iteration 11661, loss = 0.06782870\n",
      "Iteration 11662, loss = 0.06782747\n",
      "Iteration 11663, loss = 0.06782635\n",
      "Iteration 11664, loss = 0.06782515\n",
      "Iteration 11665, loss = 0.06782399\n",
      "Iteration 11666, loss = 0.06782282\n",
      "Iteration 11667, loss = 0.06782166\n",
      "Iteration 11668, loss = 0.06782047\n",
      "Iteration 11669, loss = 0.06781930\n",
      "Iteration 11670, loss = 0.06781812\n",
      "Iteration 11671, loss = 0.06781695\n",
      "Iteration 11672, loss = 0.06781581\n",
      "Iteration 11673, loss = 0.06781461\n",
      "Iteration 11674, loss = 0.06781347\n",
      "Iteration 11675, loss = 0.06781229\n",
      "Iteration 11676, loss = 0.06781111\n",
      "Iteration 11677, loss = 0.06780994\n",
      "Iteration 11678, loss = 0.06780876\n",
      "Iteration 11679, loss = 0.06780761\n",
      "Iteration 11680, loss = 0.06780641\n",
      "Iteration 11681, loss = 0.06780527\n",
      "Iteration 11682, loss = 0.06780408\n",
      "Iteration 11683, loss = 0.06780292\n",
      "Iteration 11684, loss = 0.06780176\n",
      "Iteration 11685, loss = 0.06780059\n",
      "Iteration 11686, loss = 0.06779942\n",
      "Iteration 11687, loss = 0.06779825\n",
      "Iteration 11688, loss = 0.06779706\n",
      "Iteration 11689, loss = 0.06779593\n",
      "Iteration 11690, loss = 0.06779474\n",
      "Iteration 11691, loss = 0.06779359\n",
      "Iteration 11692, loss = 0.06779239\n",
      "Iteration 11693, loss = 0.06779124\n",
      "Iteration 11694, loss = 0.06779007\n",
      "Iteration 11695, loss = 0.06778889\n",
      "Iteration 11696, loss = 0.06778776\n",
      "Iteration 11697, loss = 0.06778654\n",
      "Iteration 11698, loss = 0.06778544\n",
      "Iteration 11699, loss = 0.06778423\n",
      "Iteration 11700, loss = 0.06778307\n",
      "Iteration 11701, loss = 0.06778193\n",
      "Iteration 11702, loss = 0.06778072\n",
      "Iteration 11703, loss = 0.06777960\n",
      "Iteration 11704, loss = 0.06777839\n",
      "Iteration 11705, loss = 0.06777724\n",
      "Iteration 11706, loss = 0.06777607\n",
      "Iteration 11707, loss = 0.06777487\n",
      "Iteration 11708, loss = 0.06777376\n",
      "Iteration 11709, loss = 0.06777256\n",
      "Iteration 11710, loss = 0.06777141\n",
      "Iteration 11711, loss = 0.06777022\n",
      "Iteration 11712, loss = 0.06776906\n",
      "Iteration 11713, loss = 0.06776791\n",
      "Iteration 11714, loss = 0.06776673\n",
      "Iteration 11715, loss = 0.06776560\n",
      "Iteration 11716, loss = 0.06776438\n",
      "Iteration 11717, loss = 0.06776327\n",
      "Iteration 11718, loss = 0.06776209\n",
      "Iteration 11719, loss = 0.06776089\n",
      "Iteration 11720, loss = 0.06775978\n",
      "Iteration 11721, loss = 0.06775857\n",
      "Iteration 11722, loss = 0.06775744\n",
      "Iteration 11723, loss = 0.06775627\n",
      "Iteration 11724, loss = 0.06775508\n",
      "Iteration 11725, loss = 0.06775393\n",
      "Iteration 11726, loss = 0.06775274\n",
      "Iteration 11727, loss = 0.06775162\n",
      "Iteration 11728, loss = 0.06775042\n",
      "Iteration 11729, loss = 0.06774927\n",
      "Iteration 11730, loss = 0.06774812\n",
      "Iteration 11731, loss = 0.06774692\n",
      "Iteration 11732, loss = 0.06774578\n",
      "Iteration 11733, loss = 0.06774461\n",
      "Iteration 11734, loss = 0.06774345\n",
      "Iteration 11735, loss = 0.06774229\n",
      "Iteration 11736, loss = 0.06774113\n",
      "Iteration 11737, loss = 0.06773996\n",
      "Iteration 11738, loss = 0.06773881\n",
      "Iteration 11739, loss = 0.06773762\n",
      "Iteration 11740, loss = 0.06773648\n",
      "Iteration 11741, loss = 0.06773532\n",
      "Iteration 11742, loss = 0.06773414\n",
      "Iteration 11743, loss = 0.06773301\n",
      "Iteration 11744, loss = 0.06773183\n",
      "Iteration 11745, loss = 0.06773068\n",
      "Iteration 11746, loss = 0.06772951\n",
      "Iteration 11747, loss = 0.06772833\n",
      "Iteration 11748, loss = 0.06772719\n",
      "Iteration 11749, loss = 0.06772600\n",
      "Iteration 11750, loss = 0.06772486\n",
      "Iteration 11751, loss = 0.06772369\n",
      "Iteration 11752, loss = 0.06772253\n",
      "Iteration 11753, loss = 0.06772138\n",
      "Iteration 11754, loss = 0.06772021\n",
      "Iteration 11755, loss = 0.06771904\n",
      "Iteration 11756, loss = 0.06771789\n",
      "Iteration 11757, loss = 0.06771672\n",
      "Iteration 11758, loss = 0.06771559\n",
      "Iteration 11759, loss = 0.06771440\n",
      "Iteration 11760, loss = 0.06771326\n",
      "Iteration 11761, loss = 0.06771207\n",
      "Iteration 11762, loss = 0.06771095\n",
      "Iteration 11763, loss = 0.06770976\n",
      "Iteration 11764, loss = 0.06770861\n",
      "Iteration 11765, loss = 0.06770746\n",
      "Iteration 11766, loss = 0.06770628\n",
      "Iteration 11767, loss = 0.06770514\n",
      "Iteration 11768, loss = 0.06770396\n",
      "Iteration 11769, loss = 0.06770283\n",
      "Iteration 11770, loss = 0.06770166\n",
      "Iteration 11771, loss = 0.06770047\n",
      "Iteration 11772, loss = 0.06769937\n",
      "Iteration 11773, loss = 0.06769817\n",
      "Iteration 11774, loss = 0.06769703\n",
      "Iteration 11775, loss = 0.06769586\n",
      "Iteration 11776, loss = 0.06769468\n",
      "Iteration 11777, loss = 0.06769358\n",
      "Iteration 11778, loss = 0.06769238\n",
      "Iteration 11779, loss = 0.06769124\n",
      "Iteration 11780, loss = 0.06769006\n",
      "Iteration 11781, loss = 0.06768892\n",
      "Iteration 11782, loss = 0.06768775\n",
      "Iteration 11783, loss = 0.06768659\n",
      "Iteration 11784, loss = 0.06768546\n",
      "Iteration 11785, loss = 0.06768426\n",
      "Iteration 11786, loss = 0.06768316\n",
      "Iteration 11787, loss = 0.06768196\n",
      "Iteration 11788, loss = 0.06768081\n",
      "Iteration 11789, loss = 0.06767968\n",
      "Iteration 11790, loss = 0.06767853\n",
      "Iteration 11791, loss = 0.06767742\n",
      "Iteration 11792, loss = 0.06767627\n",
      "Iteration 11793, loss = 0.06767515\n",
      "Iteration 11794, loss = 0.06767402\n",
      "Iteration 11795, loss = 0.06767289\n",
      "Iteration 11796, loss = 0.06767176\n",
      "Iteration 11797, loss = 0.06767061\n",
      "Iteration 11798, loss = 0.06766952\n",
      "Iteration 11799, loss = 0.06766837\n",
      "Iteration 11800, loss = 0.06766727\n",
      "Iteration 11801, loss = 0.06766613\n",
      "Iteration 11802, loss = 0.06766497\n",
      "Iteration 11803, loss = 0.06766388\n",
      "Iteration 11804, loss = 0.06766272\n",
      "Iteration 11805, loss = 0.06766162\n",
      "Iteration 11806, loss = 0.06766048\n",
      "Iteration 11807, loss = 0.06765933\n",
      "Iteration 11808, loss = 0.06765822\n",
      "Iteration 11809, loss = 0.06765706\n",
      "Iteration 11810, loss = 0.06765598\n",
      "Iteration 11811, loss = 0.06765483\n",
      "Iteration 11812, loss = 0.06765372\n",
      "Iteration 11813, loss = 0.06765257\n",
      "Iteration 11814, loss = 0.06765144\n",
      "Iteration 11815, loss = 0.06765031\n",
      "Iteration 11816, loss = 0.06764918\n",
      "Iteration 11817, loss = 0.06764807\n",
      "Iteration 11818, loss = 0.06764693\n",
      "Iteration 11819, loss = 0.06764584\n",
      "Iteration 11820, loss = 0.06764468\n",
      "Iteration 11821, loss = 0.06764354\n",
      "Iteration 11822, loss = 0.06764244\n",
      "Iteration 11823, loss = 0.06764131\n",
      "Iteration 11824, loss = 0.06764016\n",
      "Iteration 11825, loss = 0.06763903\n",
      "Iteration 11826, loss = 0.06763792\n",
      "Iteration 11827, loss = 0.06763677\n",
      "Iteration 11828, loss = 0.06763566\n",
      "Iteration 11829, loss = 0.06763453\n",
      "Iteration 11830, loss = 0.06763340\n",
      "Iteration 11831, loss = 0.06763229\n",
      "Iteration 11832, loss = 0.06763112\n",
      "Iteration 11833, loss = 0.06763006\n",
      "Iteration 11834, loss = 0.06762890\n",
      "Iteration 11835, loss = 0.06762776\n",
      "Iteration 11836, loss = 0.06762668\n",
      "Iteration 11837, loss = 0.06762552\n",
      "Iteration 11838, loss = 0.06762441\n",
      "Iteration 11839, loss = 0.06762325\n",
      "Iteration 11840, loss = 0.06762216\n",
      "Iteration 11841, loss = 0.06762102\n",
      "Iteration 11842, loss = 0.06761988\n",
      "Iteration 11843, loss = 0.06761880\n",
      "Iteration 11844, loss = 0.06761765\n",
      "Iteration 11845, loss = 0.06761652\n",
      "Iteration 11846, loss = 0.06761540\n",
      "Iteration 11847, loss = 0.06761427\n",
      "Iteration 11848, loss = 0.06761317\n",
      "Iteration 11849, loss = 0.06761202\n",
      "Iteration 11850, loss = 0.06761093\n",
      "Iteration 11851, loss = 0.06760976\n",
      "Iteration 11852, loss = 0.06760867\n",
      "Iteration 11853, loss = 0.06760756\n",
      "Iteration 11854, loss = 0.06760641\n",
      "Iteration 11855, loss = 0.06760530\n",
      "Iteration 11856, loss = 0.06760416\n",
      "Iteration 11857, loss = 0.06760305\n",
      "Iteration 11858, loss = 0.06760192\n",
      "Iteration 11859, loss = 0.06760079\n",
      "Iteration 11860, loss = 0.06759968\n",
      "Iteration 11861, loss = 0.06759853\n",
      "Iteration 11862, loss = 0.06759745\n",
      "Iteration 11863, loss = 0.06759630\n",
      "Iteration 11864, loss = 0.06759518\n",
      "Iteration 11865, loss = 0.06759407\n",
      "Iteration 11866, loss = 0.06759292\n",
      "Iteration 11867, loss = 0.06759181\n",
      "Iteration 11868, loss = 0.06759069\n",
      "Iteration 11869, loss = 0.06758958\n",
      "Iteration 11870, loss = 0.06758845\n",
      "Iteration 11871, loss = 0.06758734\n",
      "Iteration 11872, loss = 0.06758622\n",
      "Iteration 11873, loss = 0.06758508\n",
      "Iteration 11874, loss = 0.06758398\n",
      "Iteration 11875, loss = 0.06758287\n",
      "Iteration 11876, loss = 0.06758171\n",
      "Iteration 11877, loss = 0.06758061\n",
      "Iteration 11878, loss = 0.06757948\n",
      "Iteration 11879, loss = 0.06757834\n",
      "Iteration 11880, loss = 0.06757725\n",
      "Iteration 11881, loss = 0.06757612\n",
      "Iteration 11882, loss = 0.06757500\n",
      "Iteration 11883, loss = 0.06757389\n",
      "Iteration 11884, loss = 0.06757275\n",
      "Iteration 11885, loss = 0.06757166\n",
      "Iteration 11886, loss = 0.06757049\n",
      "Iteration 11887, loss = 0.06756942\n",
      "Iteration 11888, loss = 0.06756827\n",
      "Iteration 11889, loss = 0.06756717\n",
      "Iteration 11890, loss = 0.06756607\n",
      "Iteration 11891, loss = 0.06756490\n",
      "Iteration 11892, loss = 0.06756382\n",
      "Iteration 11893, loss = 0.06756266\n",
      "Iteration 11894, loss = 0.06756158\n",
      "Iteration 11895, loss = 0.06756045\n",
      "Iteration 11896, loss = 0.06755932\n",
      "Iteration 11897, loss = 0.06755822\n",
      "Iteration 11898, loss = 0.06755707\n",
      "Iteration 11899, loss = 0.06755598\n",
      "Iteration 11900, loss = 0.06755484\n",
      "Iteration 11901, loss = 0.06755373\n",
      "Iteration 11902, loss = 0.06755263\n",
      "Iteration 11903, loss = 0.06755150\n",
      "Iteration 11904, loss = 0.06755039\n",
      "Iteration 11905, loss = 0.06754925\n",
      "Iteration 11906, loss = 0.06754815\n",
      "Iteration 11907, loss = 0.06754703\n",
      "Iteration 11908, loss = 0.06754590\n",
      "Iteration 11909, loss = 0.06754481\n",
      "Iteration 11910, loss = 0.06754367\n",
      "Iteration 11911, loss = 0.06754258\n",
      "Iteration 11912, loss = 0.06754143\n",
      "Iteration 11913, loss = 0.06754034\n",
      "Iteration 11914, loss = 0.06753923\n",
      "Iteration 11915, loss = 0.06753807\n",
      "Iteration 11916, loss = 0.06753701\n",
      "Iteration 11917, loss = 0.06753586\n",
      "Iteration 11918, loss = 0.06753477\n",
      "Iteration 11919, loss = 0.06753366\n",
      "Iteration 11920, loss = 0.06753251\n",
      "Iteration 11921, loss = 0.06753143\n",
      "Iteration 11922, loss = 0.06753029\n",
      "Iteration 11923, loss = 0.06752918\n",
      "Iteration 11924, loss = 0.06752809\n",
      "Iteration 11925, loss = 0.06752693\n",
      "Iteration 11926, loss = 0.06752584\n",
      "Iteration 11927, loss = 0.06752471\n",
      "Iteration 11928, loss = 0.06752360\n",
      "Iteration 11929, loss = 0.06752248\n",
      "Iteration 11930, loss = 0.06752138\n",
      "Iteration 11931, loss = 0.06752026\n",
      "Iteration 11932, loss = 0.06751912\n",
      "Iteration 11933, loss = 0.06751804\n",
      "Iteration 11934, loss = 0.06751691\n",
      "Iteration 11935, loss = 0.06751580\n",
      "Iteration 11936, loss = 0.06751468\n",
      "Iteration 11937, loss = 0.06751359\n",
      "Iteration 11938, loss = 0.06751246\n",
      "Iteration 11939, loss = 0.06751133\n",
      "Iteration 11940, loss = 0.06751024\n",
      "Iteration 11941, loss = 0.06750913\n",
      "Iteration 11942, loss = 0.06750800\n",
      "Iteration 11943, loss = 0.06750689\n",
      "Iteration 11944, loss = 0.06750578\n",
      "Iteration 11945, loss = 0.06750464\n",
      "Iteration 11946, loss = 0.06750356\n",
      "Iteration 11947, loss = 0.06750243\n",
      "Iteration 11948, loss = 0.06750132\n",
      "Iteration 11949, loss = 0.06750022\n",
      "Iteration 11950, loss = 0.06749908\n",
      "Iteration 11951, loss = 0.06749801\n",
      "Iteration 11952, loss = 0.06749688\n",
      "Iteration 11953, loss = 0.06749577\n",
      "Iteration 11954, loss = 0.06749466\n",
      "Iteration 11955, loss = 0.06749352\n",
      "Iteration 11956, loss = 0.06749244\n",
      "Iteration 11957, loss = 0.06749132\n",
      "Iteration 11958, loss = 0.06749021\n",
      "Iteration 11959, loss = 0.06748908\n",
      "Iteration 11960, loss = 0.06748798\n",
      "Iteration 11961, loss = 0.06748689\n",
      "Iteration 11962, loss = 0.06748575\n",
      "Iteration 11963, loss = 0.06748467\n",
      "Iteration 11964, loss = 0.06748352\n",
      "Iteration 11965, loss = 0.06748243\n",
      "Iteration 11966, loss = 0.06748132\n",
      "Iteration 11967, loss = 0.06748018\n",
      "Iteration 11968, loss = 0.06747913\n",
      "Iteration 11969, loss = 0.06747798\n",
      "Iteration 11970, loss = 0.06747690\n",
      "Iteration 11971, loss = 0.06747576\n",
      "Iteration 11972, loss = 0.06747464\n",
      "Iteration 11973, loss = 0.06747356\n",
      "Iteration 11974, loss = 0.06747242\n",
      "Iteration 11975, loss = 0.06747134\n",
      "Iteration 11976, loss = 0.06747019\n",
      "Iteration 11977, loss = 0.06746914\n",
      "Iteration 11978, loss = 0.06746801\n",
      "Iteration 11979, loss = 0.06746687\n",
      "Iteration 11980, loss = 0.06746581\n",
      "Iteration 11981, loss = 0.06746467\n",
      "Iteration 11982, loss = 0.06746357\n",
      "Iteration 11983, loss = 0.06746244\n",
      "Iteration 11984, loss = 0.06746135\n",
      "Iteration 11985, loss = 0.06746022\n",
      "Iteration 11986, loss = 0.06745912\n",
      "Iteration 11987, loss = 0.06745802\n",
      "Iteration 11988, loss = 0.06745689\n",
      "Iteration 11989, loss = 0.06745583\n",
      "Iteration 11990, loss = 0.06745467\n",
      "Iteration 11991, loss = 0.06745360\n",
      "Iteration 11992, loss = 0.06745249\n",
      "Iteration 11993, loss = 0.06745137\n",
      "Iteration 11994, loss = 0.06745027\n",
      "Iteration 11995, loss = 0.06744916\n",
      "Iteration 11996, loss = 0.06744805\n",
      "Iteration 11997, loss = 0.06744694\n",
      "Iteration 11998, loss = 0.06744583\n",
      "Iteration 11999, loss = 0.06744474\n",
      "Iteration 12000, loss = 0.06744362\n",
      "Iteration 12001, loss = 0.06744251\n",
      "Iteration 12002, loss = 0.06744141\n",
      "Iteration 12003, loss = 0.06744029\n",
      "Iteration 12004, loss = 0.06743919\n",
      "Iteration 12005, loss = 0.06743810\n",
      "Iteration 12006, loss = 0.06743696\n",
      "Iteration 12007, loss = 0.06743589\n",
      "Iteration 12008, loss = 0.06743476\n",
      "Iteration 12009, loss = 0.06743367\n",
      "Iteration 12010, loss = 0.06743257\n",
      "Iteration 12011, loss = 0.06743142\n",
      "Iteration 12012, loss = 0.06743036\n",
      "Iteration 12013, loss = 0.06742922\n",
      "Iteration 12014, loss = 0.06742814\n",
      "Iteration 12015, loss = 0.06742703\n",
      "Iteration 12016, loss = 0.06742592\n",
      "Iteration 12017, loss = 0.06742482\n",
      "Iteration 12018, loss = 0.06742370\n",
      "Iteration 12019, loss = 0.06742262\n",
      "Iteration 12020, loss = 0.06742149\n",
      "Iteration 12021, loss = 0.06742040\n",
      "Iteration 12022, loss = 0.06741931\n",
      "Iteration 12023, loss = 0.06741818\n",
      "Iteration 12024, loss = 0.06741710\n",
      "Iteration 12025, loss = 0.06741597\n",
      "Iteration 12026, loss = 0.06741489\n",
      "Iteration 12027, loss = 0.06741376\n",
      "Iteration 12028, loss = 0.06741266\n",
      "Iteration 12029, loss = 0.06741158\n",
      "Iteration 12030, loss = 0.06741045\n",
      "Iteration 12031, loss = 0.06740937\n",
      "Iteration 12032, loss = 0.06740825\n",
      "Iteration 12033, loss = 0.06740714\n",
      "Iteration 12034, loss = 0.06740605\n",
      "Iteration 12035, loss = 0.06740493\n",
      "Iteration 12036, loss = 0.06740387\n",
      "Iteration 12037, loss = 0.06740272\n",
      "Iteration 12038, loss = 0.06740167\n",
      "Iteration 12039, loss = 0.06740055\n",
      "Iteration 12040, loss = 0.06739941\n",
      "Iteration 12041, loss = 0.06739837\n",
      "Iteration 12042, loss = 0.06739723\n",
      "Iteration 12043, loss = 0.06739614\n",
      "Iteration 12044, loss = 0.06739501\n",
      "Iteration 12045, loss = 0.06739393\n",
      "Iteration 12046, loss = 0.06739282\n",
      "Iteration 12047, loss = 0.06739176\n",
      "Iteration 12048, loss = 0.06739067\n",
      "Iteration 12049, loss = 0.06738956\n",
      "Iteration 12050, loss = 0.06738853\n",
      "Iteration 12051, loss = 0.06738743\n",
      "Iteration 12052, loss = 0.06738633\n",
      "Iteration 12053, loss = 0.06738529\n",
      "Iteration 12054, loss = 0.06738418\n",
      "Iteration 12055, loss = 0.06738314\n",
      "Iteration 12056, loss = 0.06738204\n",
      "Iteration 12057, loss = 0.06738098\n",
      "Iteration 12058, loss = 0.06737989\n",
      "Iteration 12059, loss = 0.06737884\n",
      "Iteration 12060, loss = 0.06737774\n",
      "Iteration 12061, loss = 0.06737669\n",
      "Iteration 12062, loss = 0.06737559\n",
      "Iteration 12063, loss = 0.06737453\n",
      "Iteration 12064, loss = 0.06737349\n",
      "Iteration 12065, loss = 0.06737240\n",
      "Iteration 12066, loss = 0.06737131\n",
      "Iteration 12067, loss = 0.06737025\n",
      "Iteration 12068, loss = 0.06736916\n",
      "Iteration 12069, loss = 0.06736812\n",
      "Iteration 12070, loss = 0.06736704\n",
      "Iteration 12071, loss = 0.06736597\n",
      "Iteration 12072, loss = 0.06736489\n",
      "Iteration 12073, loss = 0.06736384\n",
      "Iteration 12074, loss = 0.06736274\n",
      "Iteration 12075, loss = 0.06736169\n",
      "Iteration 12076, loss = 0.06736061\n",
      "Iteration 12077, loss = 0.06735954\n",
      "Iteration 12078, loss = 0.06735850\n",
      "Iteration 12079, loss = 0.06735738\n",
      "Iteration 12080, loss = 0.06735637\n",
      "Iteration 12081, loss = 0.06735528\n",
      "Iteration 12082, loss = 0.06735418\n",
      "Iteration 12083, loss = 0.06735315\n",
      "Iteration 12084, loss = 0.06735205\n",
      "Iteration 12085, loss = 0.06735100\n",
      "Iteration 12086, loss = 0.06734991\n",
      "Iteration 12087, loss = 0.06734885\n",
      "Iteration 12088, loss = 0.06734778\n",
      "Iteration 12089, loss = 0.06734671\n",
      "Iteration 12090, loss = 0.06734567\n",
      "Iteration 12091, loss = 0.06734456\n",
      "Iteration 12092, loss = 0.06734354\n",
      "Iteration 12093, loss = 0.06734242\n",
      "Iteration 12094, loss = 0.06734139\n",
      "Iteration 12095, loss = 0.06734031\n",
      "Iteration 12096, loss = 0.06733922\n",
      "Iteration 12097, loss = 0.06733820\n",
      "Iteration 12098, loss = 0.06733709\n",
      "Iteration 12099, loss = 0.06733606\n",
      "Iteration 12100, loss = 0.06733495\n",
      "Iteration 12101, loss = 0.06733392\n",
      "Iteration 12102, loss = 0.06733286\n",
      "Iteration 12103, loss = 0.06733175\n",
      "Iteration 12104, loss = 0.06733074\n",
      "Iteration 12105, loss = 0.06732962\n",
      "Iteration 12106, loss = 0.06732860\n",
      "Iteration 12107, loss = 0.06732752\n",
      "Iteration 12108, loss = 0.06732642\n",
      "Iteration 12109, loss = 0.06732540\n",
      "Iteration 12110, loss = 0.06732428\n",
      "Iteration 12111, loss = 0.06732327\n",
      "Iteration 12112, loss = 0.06732217\n",
      "Iteration 12113, loss = 0.06732110\n",
      "Iteration 12114, loss = 0.06732006\n",
      "Iteration 12115, loss = 0.06731898\n",
      "Iteration 12116, loss = 0.06731791\n",
      "Iteration 12117, loss = 0.06731683\n",
      "Iteration 12118, loss = 0.06731579\n",
      "Iteration 12119, loss = 0.06731471\n",
      "Iteration 12120, loss = 0.06731365\n",
      "Iteration 12121, loss = 0.06731259\n",
      "Iteration 12122, loss = 0.06731154\n",
      "Iteration 12123, loss = 0.06731044\n",
      "Iteration 12124, loss = 0.06730939\n",
      "Iteration 12125, loss = 0.06730832\n",
      "Iteration 12126, loss = 0.06730726\n",
      "Iteration 12127, loss = 0.06730621\n",
      "Iteration 12128, loss = 0.06730512\n",
      "Iteration 12129, loss = 0.06730409\n",
      "Iteration 12130, loss = 0.06730303\n",
      "Iteration 12131, loss = 0.06730193\n",
      "Iteration 12132, loss = 0.06730089\n",
      "Iteration 12133, loss = 0.06729982\n",
      "Iteration 12134, loss = 0.06729876\n",
      "Iteration 12135, loss = 0.06729771\n",
      "Iteration 12136, loss = 0.06729663\n",
      "Iteration 12137, loss = 0.06729556\n",
      "Iteration 12138, loss = 0.06729449\n",
      "Iteration 12139, loss = 0.06729343\n",
      "Iteration 12140, loss = 0.06729239\n",
      "Iteration 12141, loss = 0.06729131\n",
      "Iteration 12142, loss = 0.06729025\n",
      "Iteration 12143, loss = 0.06728920\n",
      "Iteration 12144, loss = 0.06728810\n",
      "Iteration 12145, loss = 0.06728708\n",
      "Iteration 12146, loss = 0.06728599\n",
      "Iteration 12147, loss = 0.06728494\n",
      "Iteration 12148, loss = 0.06728388\n",
      "Iteration 12149, loss = 0.06728278\n",
      "Iteration 12150, loss = 0.06728178\n",
      "Iteration 12151, loss = 0.06728070\n",
      "Iteration 12152, loss = 0.06727962\n",
      "Iteration 12153, loss = 0.06727858\n",
      "Iteration 12154, loss = 0.06727749\n",
      "Iteration 12155, loss = 0.06727646\n",
      "Iteration 12156, loss = 0.06727537\n",
      "Iteration 12157, loss = 0.06727432\n",
      "Iteration 12158, loss = 0.06727325\n",
      "Iteration 12159, loss = 0.06727218\n",
      "Iteration 12160, loss = 0.06727115\n",
      "Iteration 12161, loss = 0.06727006\n",
      "Iteration 12162, loss = 0.06726903\n",
      "Iteration 12163, loss = 0.06726792\n",
      "Iteration 12164, loss = 0.06726692\n",
      "Iteration 12165, loss = 0.06726581\n",
      "Iteration 12166, loss = 0.06726477\n",
      "Iteration 12167, loss = 0.06726371\n",
      "Iteration 12168, loss = 0.06726263\n",
      "Iteration 12169, loss = 0.06726161\n",
      "Iteration 12170, loss = 0.06726053\n",
      "Iteration 12171, loss = 0.06725946\n",
      "Iteration 12172, loss = 0.06725842\n",
      "Iteration 12173, loss = 0.06725733\n",
      "Iteration 12174, loss = 0.06725630\n",
      "Iteration 12175, loss = 0.06725521\n",
      "Iteration 12176, loss = 0.06725419\n",
      "Iteration 12177, loss = 0.06725309\n",
      "Iteration 12178, loss = 0.06725206\n",
      "Iteration 12179, loss = 0.06725099\n",
      "Iteration 12180, loss = 0.06724994\n",
      "Iteration 12181, loss = 0.06724888\n",
      "Iteration 12182, loss = 0.06724781\n",
      "Iteration 12183, loss = 0.06724677\n",
      "Iteration 12184, loss = 0.06724571\n",
      "Iteration 12185, loss = 0.06724463\n",
      "Iteration 12186, loss = 0.06724359\n",
      "Iteration 12187, loss = 0.06724251\n",
      "Iteration 12188, loss = 0.06724148\n",
      "Iteration 12189, loss = 0.06724039\n",
      "Iteration 12190, loss = 0.06723937\n",
      "Iteration 12191, loss = 0.06723829\n",
      "Iteration 12192, loss = 0.06723727\n",
      "Iteration 12193, loss = 0.06723618\n",
      "Iteration 12194, loss = 0.06723513\n",
      "Iteration 12195, loss = 0.06723411\n",
      "Iteration 12196, loss = 0.06723302\n",
      "Iteration 12197, loss = 0.06723196\n",
      "Iteration 12198, loss = 0.06723090\n",
      "Iteration 12199, loss = 0.06722985\n",
      "Iteration 12200, loss = 0.06722878\n",
      "Iteration 12201, loss = 0.06722774\n",
      "Iteration 12202, loss = 0.06722668\n",
      "Iteration 12203, loss = 0.06722561\n",
      "Iteration 12204, loss = 0.06722458\n",
      "Iteration 12205, loss = 0.06722352\n",
      "Iteration 12206, loss = 0.06722245\n",
      "Iteration 12207, loss = 0.06722140\n",
      "Iteration 12208, loss = 0.06722036\n",
      "Iteration 12209, loss = 0.06721930\n",
      "Iteration 12210, loss = 0.06721822\n",
      "Iteration 12211, loss = 0.06721721\n",
      "Iteration 12212, loss = 0.06721612\n",
      "Iteration 12213, loss = 0.06721509\n",
      "Iteration 12214, loss = 0.06721402\n",
      "Iteration 12215, loss = 0.06721298\n",
      "Iteration 12216, loss = 0.06721192\n",
      "Iteration 12217, loss = 0.06721084\n",
      "Iteration 12218, loss = 0.06720984\n",
      "Iteration 12219, loss = 0.06720875\n",
      "Iteration 12220, loss = 0.06720771\n",
      "Iteration 12221, loss = 0.06720663\n",
      "Iteration 12222, loss = 0.06720559\n",
      "Iteration 12223, loss = 0.06720453\n",
      "Iteration 12224, loss = 0.06720346\n",
      "Iteration 12225, loss = 0.06720245\n",
      "Iteration 12226, loss = 0.06720135\n",
      "Iteration 12227, loss = 0.06720033\n",
      "Iteration 12228, loss = 0.06719925\n",
      "Iteration 12229, loss = 0.06719820\n",
      "Iteration 12230, loss = 0.06719714\n",
      "Iteration 12231, loss = 0.06719607\n",
      "Iteration 12232, loss = 0.06719504\n",
      "Iteration 12233, loss = 0.06719396\n",
      "Iteration 12234, loss = 0.06719293\n",
      "Iteration 12235, loss = 0.06719185\n",
      "Iteration 12236, loss = 0.06719082\n",
      "Iteration 12237, loss = 0.06718976\n",
      "Iteration 12238, loss = 0.06718867\n",
      "Iteration 12239, loss = 0.06718767\n",
      "Iteration 12240, loss = 0.06718657\n",
      "Iteration 12241, loss = 0.06718554\n",
      "Iteration 12242, loss = 0.06718447\n",
      "Iteration 12243, loss = 0.06718340\n",
      "Iteration 12244, loss = 0.06718236\n",
      "Iteration 12245, loss = 0.06718129\n",
      "Iteration 12246, loss = 0.06718026\n",
      "Iteration 12247, loss = 0.06717918\n",
      "Iteration 12248, loss = 0.06717815\n",
      "Iteration 12249, loss = 0.06717708\n",
      "Iteration 12250, loss = 0.06717604\n",
      "Iteration 12251, loss = 0.06717499\n",
      "Iteration 12252, loss = 0.06717390\n",
      "Iteration 12253, loss = 0.06717289\n",
      "Iteration 12254, loss = 0.06717182\n",
      "Iteration 12255, loss = 0.06717074\n",
      "Iteration 12256, loss = 0.06716970\n",
      "Iteration 12257, loss = 0.06716865\n",
      "Iteration 12258, loss = 0.06716758\n",
      "Iteration 12259, loss = 0.06716654\n",
      "Iteration 12260, loss = 0.06716547\n",
      "Iteration 12261, loss = 0.06716443\n",
      "Iteration 12262, loss = 0.06716339\n",
      "Iteration 12263, loss = 0.06716230\n",
      "Iteration 12264, loss = 0.06716128\n",
      "Iteration 12265, loss = 0.06716019\n",
      "Iteration 12266, loss = 0.06715918\n",
      "Iteration 12267, loss = 0.06715809\n",
      "Iteration 12268, loss = 0.06715706\n",
      "Iteration 12269, loss = 0.06715602\n",
      "Iteration 12270, loss = 0.06715493\n",
      "Iteration 12271, loss = 0.06715391\n",
      "Iteration 12272, loss = 0.06715282\n",
      "Iteration 12273, loss = 0.06715180\n",
      "Iteration 12274, loss = 0.06715073\n",
      "Iteration 12275, loss = 0.06714967\n",
      "Iteration 12276, loss = 0.06714864\n",
      "Iteration 12277, loss = 0.06714756\n",
      "Iteration 12278, loss = 0.06714655\n",
      "Iteration 12279, loss = 0.06714544\n",
      "Iteration 12280, loss = 0.06714445\n",
      "Iteration 12281, loss = 0.06714338\n",
      "Iteration 12282, loss = 0.06714231\n",
      "Iteration 12283, loss = 0.06714129\n",
      "Iteration 12284, loss = 0.06714020\n",
      "Iteration 12285, loss = 0.06713918\n",
      "Iteration 12286, loss = 0.06713809\n",
      "Iteration 12287, loss = 0.06713706\n",
      "Iteration 12288, loss = 0.06713600\n",
      "Iteration 12289, loss = 0.06713494\n",
      "Iteration 12290, loss = 0.06713392\n",
      "Iteration 12291, loss = 0.06713284\n",
      "Iteration 12292, loss = 0.06713182\n",
      "Iteration 12293, loss = 0.06713074\n",
      "Iteration 12294, loss = 0.06712970\n",
      "Iteration 12295, loss = 0.06712868\n",
      "Iteration 12296, loss = 0.06712758\n",
      "Iteration 12297, loss = 0.06712657\n",
      "Iteration 12298, loss = 0.06712549\n",
      "Iteration 12299, loss = 0.06712445\n",
      "Iteration 12300, loss = 0.06712339\n",
      "Iteration 12301, loss = 0.06712236\n",
      "Iteration 12302, loss = 0.06712132\n",
      "Iteration 12303, loss = 0.06712022\n",
      "Iteration 12304, loss = 0.06711922\n",
      "Iteration 12305, loss = 0.06711813\n",
      "Iteration 12306, loss = 0.06711711\n",
      "Iteration 12307, loss = 0.06711605\n",
      "Iteration 12308, loss = 0.06711500\n",
      "Iteration 12309, loss = 0.06711396\n",
      "Iteration 12310, loss = 0.06711289\n",
      "Iteration 12311, loss = 0.06711186\n",
      "Iteration 12312, loss = 0.06711079\n",
      "Iteration 12313, loss = 0.06710976\n",
      "Iteration 12314, loss = 0.06710871\n",
      "Iteration 12315, loss = 0.06710765\n",
      "Iteration 12316, loss = 0.06710662\n",
      "Iteration 12317, loss = 0.06710558\n",
      "Iteration 12318, loss = 0.06710450\n",
      "Iteration 12319, loss = 0.06710347\n",
      "Iteration 12320, loss = 0.06710240\n",
      "Iteration 12321, loss = 0.06710136\n",
      "Iteration 12322, loss = 0.06710032\n",
      "Iteration 12323, loss = 0.06709928\n",
      "Iteration 12324, loss = 0.06709821\n",
      "Iteration 12325, loss = 0.06709717\n",
      "Iteration 12326, loss = 0.06709613\n",
      "Iteration 12327, loss = 0.06709509\n",
      "Iteration 12328, loss = 0.06709401\n",
      "Iteration 12329, loss = 0.06709301\n",
      "Iteration 12330, loss = 0.06709192\n",
      "Iteration 12331, loss = 0.06709092\n",
      "Iteration 12332, loss = 0.06708984\n",
      "Iteration 12333, loss = 0.06708880\n",
      "Iteration 12334, loss = 0.06708777\n",
      "Iteration 12335, loss = 0.06708668\n",
      "Iteration 12336, loss = 0.06708569\n",
      "Iteration 12337, loss = 0.06708460\n",
      "Iteration 12338, loss = 0.06708359\n",
      "Iteration 12339, loss = 0.06708252\n",
      "Iteration 12340, loss = 0.06708146\n",
      "Iteration 12341, loss = 0.06708043\n",
      "Iteration 12342, loss = 0.06707935\n",
      "Iteration 12343, loss = 0.06707836\n",
      "Iteration 12344, loss = 0.06707727\n",
      "Iteration 12345, loss = 0.06707627\n",
      "Iteration 12346, loss = 0.06707519\n",
      "Iteration 12347, loss = 0.06707415\n",
      "Iteration 12348, loss = 0.06707311\n",
      "Iteration 12349, loss = 0.06707206\n",
      "Iteration 12350, loss = 0.06707102\n",
      "Iteration 12351, loss = 0.06706995\n",
      "Iteration 12352, loss = 0.06706894\n",
      "Iteration 12353, loss = 0.06706788\n",
      "Iteration 12354, loss = 0.06706682\n",
      "Iteration 12355, loss = 0.06706581\n",
      "Iteration 12356, loss = 0.06706474\n",
      "Iteration 12357, loss = 0.06706371\n",
      "Iteration 12358, loss = 0.06706264\n",
      "Iteration 12359, loss = 0.06706163\n",
      "Iteration 12360, loss = 0.06706058\n",
      "Iteration 12361, loss = 0.06705954\n",
      "Iteration 12362, loss = 0.06705849\n",
      "Iteration 12363, loss = 0.06705742\n",
      "Iteration 12364, loss = 0.06705639\n",
      "Iteration 12365, loss = 0.06705533\n",
      "Iteration 12366, loss = 0.06705433\n",
      "Iteration 12367, loss = 0.06705327\n",
      "Iteration 12368, loss = 0.06705223\n",
      "Iteration 12369, loss = 0.06705117\n",
      "Iteration 12370, loss = 0.06705013\n",
      "Iteration 12371, loss = 0.06704907\n",
      "Iteration 12372, loss = 0.06704804\n",
      "Iteration 12373, loss = 0.06704701\n",
      "Iteration 12374, loss = 0.06704595\n",
      "Iteration 12375, loss = 0.06704492\n",
      "Iteration 12376, loss = 0.06704386\n",
      "Iteration 12377, loss = 0.06704285\n",
      "Iteration 12378, loss = 0.06704179\n",
      "Iteration 12379, loss = 0.06704073\n",
      "Iteration 12380, loss = 0.06703973\n",
      "Iteration 12381, loss = 0.06703866\n",
      "Iteration 12382, loss = 0.06703762\n",
      "Iteration 12383, loss = 0.06703656\n",
      "Iteration 12384, loss = 0.06703555\n",
      "Iteration 12385, loss = 0.06703447\n",
      "Iteration 12386, loss = 0.06703347\n",
      "Iteration 12387, loss = 0.06703239\n",
      "Iteration 12388, loss = 0.06703137\n",
      "Iteration 12389, loss = 0.06703033\n",
      "Iteration 12390, loss = 0.06702927\n",
      "Iteration 12391, loss = 0.06702826\n",
      "Iteration 12392, loss = 0.06702718\n",
      "Iteration 12393, loss = 0.06702618\n",
      "Iteration 12394, loss = 0.06702510\n",
      "Iteration 12395, loss = 0.06702408\n",
      "Iteration 12396, loss = 0.06702304\n",
      "Iteration 12397, loss = 0.06702197\n",
      "Iteration 12398, loss = 0.06702099\n",
      "Iteration 12399, loss = 0.06701990\n",
      "Iteration 12400, loss = 0.06701891\n",
      "Iteration 12401, loss = 0.06701785\n",
      "Iteration 12402, loss = 0.06701678\n",
      "Iteration 12403, loss = 0.06701578\n",
      "Iteration 12404, loss = 0.06701470\n",
      "Iteration 12405, loss = 0.06701370\n",
      "Iteration 12406, loss = 0.06701263\n",
      "Iteration 12407, loss = 0.06701160\n",
      "Iteration 12408, loss = 0.06701058\n",
      "Iteration 12409, loss = 0.06700950\n",
      "Iteration 12410, loss = 0.06700850\n",
      "Iteration 12411, loss = 0.06700742\n",
      "Iteration 12412, loss = 0.06700642\n",
      "Iteration 12413, loss = 0.06700535\n",
      "Iteration 12414, loss = 0.06700432\n",
      "Iteration 12415, loss = 0.06700329\n",
      "Iteration 12416, loss = 0.06700224\n",
      "Iteration 12417, loss = 0.06700120\n",
      "Iteration 12418, loss = 0.06700015\n",
      "Iteration 12419, loss = 0.06699913\n",
      "Iteration 12420, loss = 0.06699808\n",
      "Iteration 12421, loss = 0.06699705\n",
      "Iteration 12422, loss = 0.06699601\n",
      "Iteration 12423, loss = 0.06699500\n",
      "Iteration 12424, loss = 0.06699396\n",
      "Iteration 12425, loss = 0.06699289\n",
      "Iteration 12426, loss = 0.06699188\n",
      "Iteration 12427, loss = 0.06699084\n",
      "Iteration 12428, loss = 0.06698977\n",
      "Iteration 12429, loss = 0.06698874\n",
      "Iteration 12430, loss = 0.06698774\n",
      "Iteration 12431, loss = 0.06698665\n",
      "Iteration 12432, loss = 0.06698567\n",
      "Iteration 12433, loss = 0.06698461\n",
      "Iteration 12434, loss = 0.06698356\n",
      "Iteration 12435, loss = 0.06698253\n",
      "Iteration 12436, loss = 0.06698149\n",
      "Iteration 12437, loss = 0.06698048\n",
      "Iteration 12438, loss = 0.06697941\n",
      "Iteration 12439, loss = 0.06697839\n",
      "Iteration 12440, loss = 0.06697734\n",
      "Iteration 12441, loss = 0.06697631\n",
      "Iteration 12442, loss = 0.06697528\n",
      "Iteration 12443, loss = 0.06697421\n",
      "Iteration 12444, loss = 0.06697322\n",
      "Iteration 12445, loss = 0.06697215\n",
      "Iteration 12446, loss = 0.06697116\n",
      "Iteration 12447, loss = 0.06697009\n",
      "Iteration 12448, loss = 0.06696906\n",
      "Iteration 12449, loss = 0.06696804\n",
      "Iteration 12450, loss = 0.06696696\n",
      "Iteration 12451, loss = 0.06696599\n",
      "Iteration 12452, loss = 0.06696492\n",
      "Iteration 12453, loss = 0.06696390\n",
      "Iteration 12454, loss = 0.06696285\n",
      "Iteration 12455, loss = 0.06696180\n",
      "Iteration 12456, loss = 0.06696078\n",
      "Iteration 12457, loss = 0.06695972\n",
      "Iteration 12458, loss = 0.06695874\n",
      "Iteration 12459, loss = 0.06695767\n",
      "Iteration 12460, loss = 0.06695667\n",
      "Iteration 12461, loss = 0.06695559\n",
      "Iteration 12462, loss = 0.06695459\n",
      "Iteration 12463, loss = 0.06695356\n",
      "Iteration 12464, loss = 0.06695248\n",
      "Iteration 12465, loss = 0.06695151\n",
      "Iteration 12466, loss = 0.06695044\n",
      "Iteration 12467, loss = 0.06694941\n",
      "Iteration 12468, loss = 0.06694835\n",
      "Iteration 12469, loss = 0.06694736\n",
      "Iteration 12470, loss = 0.06694629\n",
      "Iteration 12471, loss = 0.06694529\n",
      "Iteration 12472, loss = 0.06694426\n",
      "Iteration 12473, loss = 0.06694320\n",
      "Iteration 12474, loss = 0.06694219\n",
      "Iteration 12475, loss = 0.06694113\n",
      "Iteration 12476, loss = 0.06694011\n",
      "Iteration 12477, loss = 0.06693908\n",
      "Iteration 12478, loss = 0.06693804\n",
      "Iteration 12479, loss = 0.06693700\n",
      "Iteration 12480, loss = 0.06693598\n",
      "Iteration 12481, loss = 0.06693494\n",
      "Iteration 12482, loss = 0.06693390\n",
      "Iteration 12483, loss = 0.06693291\n",
      "Iteration 12484, loss = 0.06693185\n",
      "Iteration 12485, loss = 0.06693082\n",
      "Iteration 12486, loss = 0.06692979\n",
      "Iteration 12487, loss = 0.06692875\n",
      "Iteration 12488, loss = 0.06692772\n",
      "Iteration 12489, loss = 0.06692669\n",
      "Iteration 12490, loss = 0.06692568\n",
      "Iteration 12491, loss = 0.06692463\n",
      "Iteration 12492, loss = 0.06692360\n",
      "Iteration 12493, loss = 0.06692255\n",
      "Iteration 12494, loss = 0.06692155\n",
      "Iteration 12495, loss = 0.06692050\n",
      "Iteration 12496, loss = 0.06691949\n",
      "Iteration 12497, loss = 0.06691843\n",
      "Iteration 12498, loss = 0.06691741\n",
      "Iteration 12499, loss = 0.06691639\n",
      "Iteration 12500, loss = 0.06691535\n",
      "Iteration 12501, loss = 0.06691433\n",
      "Iteration 12502, loss = 0.06691328\n",
      "Iteration 12503, loss = 0.06691227\n",
      "Iteration 12504, loss = 0.06691122\n",
      "Iteration 12505, loss = 0.06691020\n",
      "Iteration 12506, loss = 0.06690918\n",
      "Iteration 12507, loss = 0.06690812\n",
      "Iteration 12508, loss = 0.06690713\n",
      "Iteration 12509, loss = 0.06690607\n",
      "Iteration 12510, loss = 0.06690507\n",
      "Iteration 12511, loss = 0.06690401\n",
      "Iteration 12512, loss = 0.06690299\n",
      "Iteration 12513, loss = 0.06690197\n",
      "Iteration 12514, loss = 0.06690093\n",
      "Iteration 12515, loss = 0.06689992\n",
      "Iteration 12516, loss = 0.06689888\n",
      "Iteration 12517, loss = 0.06689786\n",
      "Iteration 12518, loss = 0.06689681\n",
      "Iteration 12519, loss = 0.06689579\n",
      "Iteration 12520, loss = 0.06689477\n",
      "Iteration 12521, loss = 0.06689373\n",
      "Iteration 12522, loss = 0.06689272\n",
      "Iteration 12523, loss = 0.06689168\n",
      "Iteration 12524, loss = 0.06689066\n",
      "Iteration 12525, loss = 0.06688961\n",
      "Iteration 12526, loss = 0.06688862\n",
      "Iteration 12527, loss = 0.06688760\n",
      "Iteration 12528, loss = 0.06688652\n",
      "Iteration 12529, loss = 0.06688555\n",
      "Iteration 12530, loss = 0.06688448\n",
      "Iteration 12531, loss = 0.06688347\n",
      "Iteration 12532, loss = 0.06688243\n",
      "Iteration 12533, loss = 0.06688141\n",
      "Iteration 12534, loss = 0.06688038\n",
      "Iteration 12535, loss = 0.06687936\n",
      "Iteration 12536, loss = 0.06687832\n",
      "Iteration 12537, loss = 0.06687729\n",
      "Iteration 12538, loss = 0.06687628\n",
      "Iteration 12539, loss = 0.06687524\n",
      "Iteration 12540, loss = 0.06687424\n",
      "Iteration 12541, loss = 0.06687318\n",
      "Iteration 12542, loss = 0.06687218\n",
      "Iteration 12543, loss = 0.06687114\n",
      "Iteration 12544, loss = 0.06687010\n",
      "Iteration 12545, loss = 0.06686912\n",
      "Iteration 12546, loss = 0.06686808\n",
      "Iteration 12547, loss = 0.06686703\n",
      "Iteration 12548, loss = 0.06686600\n",
      "Iteration 12549, loss = 0.06686500\n",
      "Iteration 12550, loss = 0.06686394\n",
      "Iteration 12551, loss = 0.06686296\n",
      "Iteration 12552, loss = 0.06686193\n",
      "Iteration 12553, loss = 0.06686088\n",
      "Iteration 12554, loss = 0.06685987\n",
      "Iteration 12555, loss = 0.06685883\n",
      "Iteration 12556, loss = 0.06685782\n",
      "Iteration 12557, loss = 0.06685677\n",
      "Iteration 12558, loss = 0.06685577\n",
      "Iteration 12559, loss = 0.06685473\n",
      "Iteration 12560, loss = 0.06685370\n",
      "Iteration 12561, loss = 0.06685269\n",
      "Iteration 12562, loss = 0.06685164\n",
      "Iteration 12563, loss = 0.06685066\n",
      "Iteration 12564, loss = 0.06684961\n",
      "Iteration 12565, loss = 0.06684859\n",
      "Iteration 12566, loss = 0.06684755\n",
      "Iteration 12567, loss = 0.06684655\n",
      "Iteration 12568, loss = 0.06684551\n",
      "Iteration 12569, loss = 0.06684449\n",
      "Iteration 12570, loss = 0.06684348\n",
      "Iteration 12571, loss = 0.06684242\n",
      "Iteration 12572, loss = 0.06684145\n",
      "Iteration 12573, loss = 0.06684039\n",
      "Iteration 12574, loss = 0.06683940\n",
      "Iteration 12575, loss = 0.06683836\n",
      "Iteration 12576, loss = 0.06683731\n",
      "Iteration 12577, loss = 0.06683633\n",
      "Iteration 12578, loss = 0.06683526\n",
      "Iteration 12579, loss = 0.06683430\n",
      "Iteration 12580, loss = 0.06683325\n",
      "Iteration 12581, loss = 0.06683222\n",
      "Iteration 12582, loss = 0.06683120\n",
      "Iteration 12583, loss = 0.06683017\n",
      "Iteration 12584, loss = 0.06682915\n",
      "Iteration 12585, loss = 0.06682813\n",
      "Iteration 12586, loss = 0.06682710\n",
      "Iteration 12587, loss = 0.06682607\n",
      "Iteration 12588, loss = 0.06682508\n",
      "Iteration 12589, loss = 0.06682405\n",
      "Iteration 12590, loss = 0.06682303\n",
      "Iteration 12591, loss = 0.06682201\n",
      "Iteration 12592, loss = 0.06682097\n",
      "Iteration 12593, loss = 0.06681994\n",
      "Iteration 12594, loss = 0.06681893\n",
      "Iteration 12595, loss = 0.06681793\n",
      "Iteration 12596, loss = 0.06681688\n",
      "Iteration 12597, loss = 0.06681587\n",
      "Iteration 12598, loss = 0.06681483\n",
      "Iteration 12599, loss = 0.06681384\n",
      "Iteration 12600, loss = 0.06681278\n",
      "Iteration 12601, loss = 0.06681180\n",
      "Iteration 12602, loss = 0.06681075\n",
      "Iteration 12603, loss = 0.06680975\n",
      "Iteration 12604, loss = 0.06680872\n",
      "Iteration 12605, loss = 0.06680770\n",
      "Iteration 12606, loss = 0.06680670\n",
      "Iteration 12607, loss = 0.06680563\n",
      "Iteration 12608, loss = 0.06680467\n",
      "Iteration 12609, loss = 0.06680363\n",
      "Iteration 12610, loss = 0.06680260\n",
      "Iteration 12611, loss = 0.06680160\n",
      "Iteration 12612, loss = 0.06680056\n",
      "Iteration 12613, loss = 0.06679955\n",
      "Iteration 12614, loss = 0.06679850\n",
      "Iteration 12615, loss = 0.06679753\n",
      "Iteration 12616, loss = 0.06679646\n",
      "Iteration 12617, loss = 0.06679549\n",
      "Iteration 12618, loss = 0.06679445\n",
      "Iteration 12619, loss = 0.06679342\n",
      "Iteration 12620, loss = 0.06679243\n",
      "Iteration 12621, loss = 0.06679137\n",
      "Iteration 12622, loss = 0.06679040\n",
      "Iteration 12623, loss = 0.06678934\n",
      "Iteration 12624, loss = 0.06678835\n",
      "Iteration 12625, loss = 0.06678733\n",
      "Iteration 12626, loss = 0.06678627\n",
      "Iteration 12627, loss = 0.06678531\n",
      "Iteration 12628, loss = 0.06678425\n",
      "Iteration 12629, loss = 0.06678327\n",
      "Iteration 12630, loss = 0.06678221\n",
      "Iteration 12631, loss = 0.06678121\n",
      "Iteration 12632, loss = 0.06678020\n",
      "Iteration 12633, loss = 0.06677916\n",
      "Iteration 12634, loss = 0.06677816\n",
      "Iteration 12635, loss = 0.06677712\n",
      "Iteration 12636, loss = 0.06677614\n",
      "Iteration 12637, loss = 0.06677509\n",
      "Iteration 12638, loss = 0.06677410\n",
      "Iteration 12639, loss = 0.06677307\n",
      "Iteration 12640, loss = 0.06677206\n",
      "Iteration 12641, loss = 0.06677105\n",
      "Iteration 12642, loss = 0.06677001\n",
      "Iteration 12643, loss = 0.06676900\n",
      "Iteration 12644, loss = 0.06676797\n",
      "Iteration 12645, loss = 0.06676697\n",
      "Iteration 12646, loss = 0.06676597\n",
      "Iteration 12647, loss = 0.06676494\n",
      "Iteration 12648, loss = 0.06676391\n",
      "Iteration 12649, loss = 0.06676292\n",
      "Iteration 12650, loss = 0.06676186\n",
      "Iteration 12651, loss = 0.06676088\n",
      "Iteration 12652, loss = 0.06675983\n",
      "Iteration 12653, loss = 0.06675885\n",
      "Iteration 12654, loss = 0.06675782\n",
      "Iteration 12655, loss = 0.06675681\n",
      "Iteration 12656, loss = 0.06675582\n",
      "Iteration 12657, loss = 0.06675475\n",
      "Iteration 12658, loss = 0.06675380\n",
      "Iteration 12659, loss = 0.06675276\n",
      "Iteration 12660, loss = 0.06675172\n",
      "Iteration 12661, loss = 0.06675074\n",
      "Iteration 12662, loss = 0.06674971\n",
      "Iteration 12663, loss = 0.06674869\n",
      "Iteration 12664, loss = 0.06674765\n",
      "Iteration 12665, loss = 0.06674668\n",
      "Iteration 12666, loss = 0.06674562\n",
      "Iteration 12667, loss = 0.06674465\n",
      "Iteration 12668, loss = 0.06674362\n",
      "Iteration 12669, loss = 0.06674259\n",
      "Iteration 12670, loss = 0.06674161\n",
      "Iteration 12671, loss = 0.06674056\n",
      "Iteration 12672, loss = 0.06673957\n",
      "Iteration 12673, loss = 0.06673853\n",
      "Iteration 12674, loss = 0.06673754\n",
      "Iteration 12675, loss = 0.06673651\n",
      "Iteration 12676, loss = 0.06673551\n",
      "Iteration 12677, loss = 0.06673448\n",
      "Iteration 12678, loss = 0.06673346\n",
      "Iteration 12679, loss = 0.06673248\n",
      "Iteration 12680, loss = 0.06673142\n",
      "Iteration 12681, loss = 0.06673047\n",
      "Iteration 12682, loss = 0.06672942\n",
      "Iteration 12683, loss = 0.06672842\n",
      "Iteration 12684, loss = 0.06672742\n",
      "Iteration 12685, loss = 0.06672637\n",
      "Iteration 12686, loss = 0.06672539\n",
      "Iteration 12687, loss = 0.06672434\n",
      "Iteration 12688, loss = 0.06672336\n",
      "Iteration 12689, loss = 0.06672233\n",
      "Iteration 12690, loss = 0.06672132\n",
      "Iteration 12691, loss = 0.06672031\n",
      "Iteration 12692, loss = 0.06671930\n",
      "Iteration 12693, loss = 0.06671828\n",
      "Iteration 12694, loss = 0.06671726\n",
      "Iteration 12695, loss = 0.06671626\n",
      "Iteration 12696, loss = 0.06671523\n",
      "Iteration 12697, loss = 0.06671425\n",
      "Iteration 12698, loss = 0.06671324\n",
      "Iteration 12699, loss = 0.06671222\n",
      "Iteration 12700, loss = 0.06671120\n",
      "Iteration 12701, loss = 0.06671020\n",
      "Iteration 12702, loss = 0.06670917\n",
      "Iteration 12703, loss = 0.06670818\n",
      "Iteration 12704, loss = 0.06670716\n",
      "Iteration 12705, loss = 0.06670615\n",
      "Iteration 12706, loss = 0.06670516\n",
      "Iteration 12707, loss = 0.06670415\n",
      "Iteration 12708, loss = 0.06670313\n",
      "Iteration 12709, loss = 0.06670210\n",
      "Iteration 12710, loss = 0.06670112\n",
      "Iteration 12711, loss = 0.06670010\n",
      "Iteration 12712, loss = 0.06669909\n",
      "Iteration 12713, loss = 0.06669808\n",
      "Iteration 12714, loss = 0.06669705\n",
      "Iteration 12715, loss = 0.06669608\n",
      "Iteration 12716, loss = 0.06669503\n",
      "Iteration 12717, loss = 0.06669406\n",
      "Iteration 12718, loss = 0.06669304\n",
      "Iteration 12719, loss = 0.06669201\n",
      "Iteration 12720, loss = 0.06669101\n",
      "Iteration 12721, loss = 0.06668999\n",
      "Iteration 12722, loss = 0.06668902\n",
      "Iteration 12723, loss = 0.06668798\n",
      "Iteration 12724, loss = 0.06668699\n",
      "Iteration 12725, loss = 0.06668597\n",
      "Iteration 12726, loss = 0.06668497\n",
      "Iteration 12727, loss = 0.06668394\n",
      "Iteration 12728, loss = 0.06668295\n",
      "Iteration 12729, loss = 0.06668193\n",
      "Iteration 12730, loss = 0.06668091\n",
      "Iteration 12731, loss = 0.06667994\n",
      "Iteration 12732, loss = 0.06667890\n",
      "Iteration 12733, loss = 0.06667794\n",
      "Iteration 12734, loss = 0.06667689\n",
      "Iteration 12735, loss = 0.06667591\n",
      "Iteration 12736, loss = 0.06667492\n",
      "Iteration 12737, loss = 0.06667386\n",
      "Iteration 12738, loss = 0.06667291\n",
      "Iteration 12739, loss = 0.06667187\n",
      "Iteration 12740, loss = 0.06667086\n",
      "Iteration 12741, loss = 0.06666983\n",
      "Iteration 12742, loss = 0.06666885\n",
      "Iteration 12743, loss = 0.06666785\n",
      "Iteration 12744, loss = 0.06666681\n",
      "Iteration 12745, loss = 0.06666583\n",
      "Iteration 12746, loss = 0.06666479\n",
      "Iteration 12747, loss = 0.06666384\n",
      "Iteration 12748, loss = 0.06666278\n",
      "Iteration 12749, loss = 0.06666182\n",
      "Iteration 12750, loss = 0.06666079\n",
      "Iteration 12751, loss = 0.06665979\n",
      "Iteration 12752, loss = 0.06665878\n",
      "Iteration 12753, loss = 0.06665776\n",
      "Iteration 12754, loss = 0.06665678\n",
      "Iteration 12755, loss = 0.06665576\n",
      "Iteration 12756, loss = 0.06665476\n",
      "Iteration 12757, loss = 0.06665374\n",
      "Iteration 12758, loss = 0.06665277\n",
      "Iteration 12759, loss = 0.06665174\n",
      "Iteration 12760, loss = 0.06665074\n",
      "Iteration 12761, loss = 0.06664972\n",
      "Iteration 12762, loss = 0.06664873\n",
      "Iteration 12763, loss = 0.06664771\n",
      "Iteration 12764, loss = 0.06664673\n",
      "Iteration 12765, loss = 0.06664571\n",
      "Iteration 12766, loss = 0.06664471\n",
      "Iteration 12767, loss = 0.06664373\n",
      "Iteration 12768, loss = 0.06664268\n",
      "Iteration 12769, loss = 0.06664173\n",
      "Iteration 12770, loss = 0.06664069\n",
      "Iteration 12771, loss = 0.06663969\n",
      "Iteration 12772, loss = 0.06663868\n",
      "Iteration 12773, loss = 0.06663767\n",
      "Iteration 12774, loss = 0.06663668\n",
      "Iteration 12775, loss = 0.06663565\n",
      "Iteration 12776, loss = 0.06663470\n",
      "Iteration 12777, loss = 0.06663365\n",
      "Iteration 12778, loss = 0.06663269\n",
      "Iteration 12779, loss = 0.06663166\n",
      "Iteration 12780, loss = 0.06663065\n",
      "Iteration 12781, loss = 0.06662965\n",
      "Iteration 12782, loss = 0.06662863\n",
      "Iteration 12783, loss = 0.06662767\n",
      "Iteration 12784, loss = 0.06662663\n",
      "Iteration 12785, loss = 0.06662566\n",
      "Iteration 12786, loss = 0.06662462\n",
      "Iteration 12787, loss = 0.06662366\n",
      "Iteration 12788, loss = 0.06662264\n",
      "Iteration 12789, loss = 0.06662161\n",
      "Iteration 12790, loss = 0.06662064\n",
      "Iteration 12791, loss = 0.06661961\n",
      "Iteration 12792, loss = 0.06661865\n",
      "Iteration 12793, loss = 0.06661762\n",
      "Iteration 12794, loss = 0.06661662\n",
      "Iteration 12795, loss = 0.06661561\n",
      "Iteration 12796, loss = 0.06661462\n",
      "Iteration 12797, loss = 0.06661361\n",
      "Iteration 12798, loss = 0.06661263\n",
      "Iteration 12799, loss = 0.06661161\n",
      "Iteration 12800, loss = 0.06661062\n",
      "Iteration 12801, loss = 0.06660960\n",
      "Iteration 12802, loss = 0.06660861\n",
      "Iteration 12803, loss = 0.06660763\n",
      "Iteration 12804, loss = 0.06660661\n",
      "Iteration 12805, loss = 0.06660560\n",
      "Iteration 12806, loss = 0.06660460\n",
      "Iteration 12807, loss = 0.06660360\n",
      "Iteration 12808, loss = 0.06660260\n",
      "Iteration 12809, loss = 0.06660160\n",
      "Iteration 12810, loss = 0.06660060\n",
      "Iteration 12811, loss = 0.06659960\n",
      "Iteration 12812, loss = 0.06659860\n",
      "Iteration 12813, loss = 0.06659761\n",
      "Iteration 12814, loss = 0.06659661\n",
      "Iteration 12815, loss = 0.06659561\n",
      "Iteration 12816, loss = 0.06659459\n",
      "Iteration 12817, loss = 0.06659360\n",
      "Iteration 12818, loss = 0.06659259\n",
      "Iteration 12819, loss = 0.06659162\n",
      "Iteration 12820, loss = 0.06659062\n",
      "Iteration 12821, loss = 0.06658961\n",
      "Iteration 12822, loss = 0.06658859\n",
      "Iteration 12823, loss = 0.06658763\n",
      "Iteration 12824, loss = 0.06658659\n",
      "Iteration 12825, loss = 0.06658563\n",
      "Iteration 12826, loss = 0.06658459\n",
      "Iteration 12827, loss = 0.06658361\n",
      "Iteration 12828, loss = 0.06658262\n",
      "Iteration 12829, loss = 0.06658159\n",
      "Iteration 12830, loss = 0.06658065\n",
      "Iteration 12831, loss = 0.06657961\n",
      "Iteration 12832, loss = 0.06657863\n",
      "Iteration 12833, loss = 0.06657762\n",
      "Iteration 12834, loss = 0.06657662\n",
      "Iteration 12835, loss = 0.06657563\n",
      "Iteration 12836, loss = 0.06657461\n",
      "Iteration 12837, loss = 0.06657366\n",
      "Iteration 12838, loss = 0.06657262\n",
      "Iteration 12839, loss = 0.06657166\n",
      "Iteration 12840, loss = 0.06657063\n",
      "Iteration 12841, loss = 0.06656965\n",
      "Iteration 12842, loss = 0.06656868\n",
      "Iteration 12843, loss = 0.06656764\n",
      "Iteration 12844, loss = 0.06656666\n",
      "Iteration 12845, loss = 0.06656563\n",
      "Iteration 12846, loss = 0.06656469\n",
      "Iteration 12847, loss = 0.06656366\n",
      "Iteration 12848, loss = 0.06656266\n",
      "Iteration 12849, loss = 0.06656166\n",
      "Iteration 12850, loss = 0.06656068\n",
      "Iteration 12851, loss = 0.06655967\n",
      "Iteration 12852, loss = 0.06655869\n",
      "Iteration 12853, loss = 0.06655769\n",
      "Iteration 12854, loss = 0.06655670\n",
      "Iteration 12855, loss = 0.06655568\n",
      "Iteration 12856, loss = 0.06655469\n",
      "Iteration 12857, loss = 0.06655372\n",
      "Iteration 12858, loss = 0.06655271\n",
      "Iteration 12859, loss = 0.06655171\n",
      "Iteration 12860, loss = 0.06655070\n",
      "Iteration 12861, loss = 0.06654972\n",
      "Iteration 12862, loss = 0.06654870\n",
      "Iteration 12863, loss = 0.06654773\n",
      "Iteration 12864, loss = 0.06654674\n",
      "Iteration 12865, loss = 0.06654572\n",
      "Iteration 12866, loss = 0.06654477\n",
      "Iteration 12867, loss = 0.06654375\n",
      "Iteration 12868, loss = 0.06654276\n",
      "Iteration 12869, loss = 0.06654175\n",
      "Iteration 12870, loss = 0.06654077\n",
      "Iteration 12871, loss = 0.06653976\n",
      "Iteration 12872, loss = 0.06653878\n",
      "Iteration 12873, loss = 0.06653778\n",
      "Iteration 12874, loss = 0.06653677\n",
      "Iteration 12875, loss = 0.06653581\n",
      "Iteration 12876, loss = 0.06653478\n",
      "Iteration 12877, loss = 0.06653382\n",
      "Iteration 12878, loss = 0.06653281\n",
      "Iteration 12879, loss = 0.06653182\n",
      "Iteration 12880, loss = 0.06653082\n",
      "Iteration 12881, loss = 0.06652983\n",
      "Iteration 12882, loss = 0.06652884\n",
      "Iteration 12883, loss = 0.06652781\n",
      "Iteration 12884, loss = 0.06652689\n",
      "Iteration 12885, loss = 0.06652585\n",
      "Iteration 12886, loss = 0.06652488\n",
      "Iteration 12887, loss = 0.06652386\n",
      "Iteration 12888, loss = 0.06652288\n",
      "Iteration 12889, loss = 0.06652188\n",
      "Iteration 12890, loss = 0.06652088\n",
      "Iteration 12891, loss = 0.06651991\n",
      "Iteration 12892, loss = 0.06651888\n",
      "Iteration 12893, loss = 0.06651794\n",
      "Iteration 12894, loss = 0.06651693\n",
      "Iteration 12895, loss = 0.06651593\n",
      "Iteration 12896, loss = 0.06651494\n",
      "Iteration 12897, loss = 0.06651396\n",
      "Iteration 12898, loss = 0.06651294\n",
      "Iteration 12899, loss = 0.06651196\n",
      "Iteration 12900, loss = 0.06651096\n",
      "Iteration 12901, loss = 0.06650998\n",
      "Iteration 12902, loss = 0.06650899\n",
      "Iteration 12903, loss = 0.06650798\n",
      "Iteration 12904, loss = 0.06650701\n",
      "Iteration 12905, loss = 0.06650601\n",
      "Iteration 12906, loss = 0.06650503\n",
      "Iteration 12907, loss = 0.06650405\n",
      "Iteration 12908, loss = 0.06650303\n",
      "Iteration 12909, loss = 0.06650205\n",
      "Iteration 12910, loss = 0.06650106\n",
      "Iteration 12911, loss = 0.06650009\n",
      "Iteration 12912, loss = 0.06649908\n",
      "Iteration 12913, loss = 0.06649809\n",
      "Iteration 12914, loss = 0.06649708\n",
      "Iteration 12915, loss = 0.06649611\n",
      "Iteration 12916, loss = 0.06649510\n",
      "Iteration 12917, loss = 0.06649413\n",
      "Iteration 12918, loss = 0.06649312\n",
      "Iteration 12919, loss = 0.06649215\n",
      "Iteration 12920, loss = 0.06649114\n",
      "Iteration 12921, loss = 0.06649015\n",
      "Iteration 12922, loss = 0.06648919\n",
      "Iteration 12923, loss = 0.06648818\n",
      "Iteration 12924, loss = 0.06648721\n",
      "Iteration 12925, loss = 0.06648620\n",
      "Iteration 12926, loss = 0.06648522\n",
      "Iteration 12927, loss = 0.06648422\n",
      "Iteration 12928, loss = 0.06648324\n",
      "Iteration 12929, loss = 0.06648226\n",
      "Iteration 12930, loss = 0.06648124\n",
      "Iteration 12931, loss = 0.06648030\n",
      "Iteration 12932, loss = 0.06647927\n",
      "Iteration 12933, loss = 0.06647831\n",
      "Iteration 12934, loss = 0.06647730\n",
      "Iteration 12935, loss = 0.06647630\n",
      "Iteration 12936, loss = 0.06647532\n",
      "Iteration 12937, loss = 0.06647431\n",
      "Iteration 12938, loss = 0.06647337\n",
      "Iteration 12939, loss = 0.06647234\n",
      "Iteration 12940, loss = 0.06647140\n",
      "Iteration 12941, loss = 0.06647038\n",
      "Iteration 12942, loss = 0.06646940\n",
      "Iteration 12943, loss = 0.06646840\n",
      "Iteration 12944, loss = 0.06646741\n",
      "Iteration 12945, loss = 0.06646644\n",
      "Iteration 12946, loss = 0.06646544\n",
      "Iteration 12947, loss = 0.06646446\n",
      "Iteration 12948, loss = 0.06646345\n",
      "Iteration 12949, loss = 0.06646250\n",
      "Iteration 12950, loss = 0.06646148\n",
      "Iteration 12951, loss = 0.06646051\n",
      "Iteration 12952, loss = 0.06645951\n",
      "Iteration 12953, loss = 0.06645855\n",
      "Iteration 12954, loss = 0.06645756\n",
      "Iteration 12955, loss = 0.06645653\n",
      "Iteration 12956, loss = 0.06645558\n",
      "Iteration 12957, loss = 0.06645456\n",
      "Iteration 12958, loss = 0.06645362\n",
      "Iteration 12959, loss = 0.06645261\n",
      "Iteration 12960, loss = 0.06645163\n",
      "Iteration 12961, loss = 0.06645063\n",
      "Iteration 12962, loss = 0.06644966\n",
      "Iteration 12963, loss = 0.06644866\n",
      "Iteration 12964, loss = 0.06644769\n",
      "Iteration 12965, loss = 0.06644667\n",
      "Iteration 12966, loss = 0.06644571\n",
      "Iteration 12967, loss = 0.06644471\n",
      "Iteration 12968, loss = 0.06644373\n",
      "Iteration 12969, loss = 0.06644276\n",
      "Iteration 12970, loss = 0.06644176\n",
      "Iteration 12971, loss = 0.06644078\n",
      "Iteration 12972, loss = 0.06643977\n",
      "Iteration 12973, loss = 0.06643882\n",
      "Iteration 12974, loss = 0.06643781\n",
      "Iteration 12975, loss = 0.06643685\n",
      "Iteration 12976, loss = 0.06643585\n",
      "Iteration 12977, loss = 0.06643486\n",
      "Iteration 12978, loss = 0.06643389\n",
      "Iteration 12979, loss = 0.06643288\n",
      "Iteration 12980, loss = 0.06643193\n",
      "Iteration 12981, loss = 0.06643090\n",
      "Iteration 12982, loss = 0.06642998\n",
      "Iteration 12983, loss = 0.06642897\n",
      "Iteration 12984, loss = 0.06642796\n",
      "Iteration 12985, loss = 0.06642702\n",
      "Iteration 12986, loss = 0.06642601\n",
      "Iteration 12987, loss = 0.06642503\n",
      "Iteration 12988, loss = 0.06642402\n",
      "Iteration 12989, loss = 0.06642308\n",
      "Iteration 12990, loss = 0.06642206\n",
      "Iteration 12991, loss = 0.06642109\n",
      "Iteration 12992, loss = 0.06642010\n",
      "Iteration 12993, loss = 0.06641911\n",
      "Iteration 12994, loss = 0.06641814\n",
      "Iteration 12995, loss = 0.06641715\n",
      "Iteration 12996, loss = 0.06641617\n",
      "Iteration 12997, loss = 0.06641517\n",
      "Iteration 12998, loss = 0.06641423\n",
      "Iteration 12999, loss = 0.06641322\n",
      "Iteration 13000, loss = 0.06641224\n",
      "Iteration 13001, loss = 0.06641126\n",
      "Iteration 13002, loss = 0.06641027\n",
      "Iteration 13003, loss = 0.06640928\n",
      "Iteration 13004, loss = 0.06640831\n",
      "Iteration 13005, loss = 0.06640733\n",
      "Iteration 13006, loss = 0.06640633\n",
      "Iteration 13007, loss = 0.06640537\n",
      "Iteration 13008, loss = 0.06640439\n",
      "Iteration 13009, loss = 0.06640341\n",
      "Iteration 13010, loss = 0.06640239\n",
      "Iteration 13011, loss = 0.06640145\n",
      "Iteration 13012, loss = 0.06640043\n",
      "Iteration 13013, loss = 0.06639949\n",
      "Iteration 13014, loss = 0.06639847\n",
      "Iteration 13015, loss = 0.06639752\n",
      "Iteration 13016, loss = 0.06639653\n",
      "Iteration 13017, loss = 0.06639552\n",
      "Iteration 13018, loss = 0.06639458\n",
      "Iteration 13019, loss = 0.06639359\n",
      "Iteration 13020, loss = 0.06639260\n",
      "Iteration 13021, loss = 0.06639160\n",
      "Iteration 13022, loss = 0.06639065\n",
      "Iteration 13023, loss = 0.06638965\n",
      "Iteration 13024, loss = 0.06638867\n",
      "Iteration 13025, loss = 0.06638769\n",
      "Iteration 13026, loss = 0.06638671\n",
      "Iteration 13027, loss = 0.06638575\n",
      "Iteration 13028, loss = 0.06638474\n",
      "Iteration 13029, loss = 0.06638379\n",
      "Iteration 13030, loss = 0.06638278\n",
      "Iteration 13031, loss = 0.06638182\n",
      "Iteration 13032, loss = 0.06638082\n",
      "Iteration 13033, loss = 0.06637986\n",
      "Iteration 13034, loss = 0.06637887\n",
      "Iteration 13035, loss = 0.06637788\n",
      "Iteration 13036, loss = 0.06637694\n",
      "Iteration 13037, loss = 0.06637592\n",
      "Iteration 13038, loss = 0.06637498\n",
      "Iteration 13039, loss = 0.06637396\n",
      "Iteration 13040, loss = 0.06637301\n",
      "Iteration 13041, loss = 0.06637202\n",
      "Iteration 13042, loss = 0.06637102\n",
      "Iteration 13043, loss = 0.06637008\n",
      "Iteration 13044, loss = 0.06636906\n",
      "Iteration 13045, loss = 0.06636812\n",
      "Iteration 13046, loss = 0.06636711\n",
      "Iteration 13047, loss = 0.06636616\n",
      "Iteration 13048, loss = 0.06636516\n",
      "Iteration 13049, loss = 0.06636420\n",
      "Iteration 13050, loss = 0.06636322\n",
      "Iteration 13051, loss = 0.06636223\n",
      "Iteration 13052, loss = 0.06636126\n",
      "Iteration 13053, loss = 0.06636026\n",
      "Iteration 13054, loss = 0.06635930\n",
      "Iteration 13055, loss = 0.06635830\n",
      "Iteration 13056, loss = 0.06635736\n",
      "Iteration 13057, loss = 0.06635636\n",
      "Iteration 13058, loss = 0.06635539\n",
      "Iteration 13059, loss = 0.06635440\n",
      "Iteration 13060, loss = 0.06635344\n",
      "Iteration 13061, loss = 0.06635244\n",
      "Iteration 13062, loss = 0.06635150\n",
      "Iteration 13063, loss = 0.06635052\n",
      "Iteration 13064, loss = 0.06634952\n",
      "Iteration 13065, loss = 0.06634856\n",
      "Iteration 13066, loss = 0.06634758\n",
      "Iteration 13067, loss = 0.06634659\n",
      "Iteration 13068, loss = 0.06634561\n",
      "Iteration 13069, loss = 0.06634464\n",
      "Iteration 13070, loss = 0.06634364\n",
      "Iteration 13071, loss = 0.06634272\n",
      "Iteration 13072, loss = 0.06634171\n",
      "Iteration 13073, loss = 0.06634073\n",
      "Iteration 13074, loss = 0.06633978\n",
      "Iteration 13075, loss = 0.06633880\n",
      "Iteration 13076, loss = 0.06633779\n",
      "Iteration 13077, loss = 0.06633681\n",
      "Iteration 13078, loss = 0.06633587\n",
      "Iteration 13079, loss = 0.06633487\n",
      "Iteration 13080, loss = 0.06633391\n",
      "Iteration 13081, loss = 0.06633291\n",
      "Iteration 13082, loss = 0.06633197\n",
      "Iteration 13083, loss = 0.06633098\n",
      "Iteration 13084, loss = 0.06632998\n",
      "Iteration 13085, loss = 0.06632904\n",
      "Iteration 13086, loss = 0.06632803\n",
      "Iteration 13087, loss = 0.06632711\n",
      "Iteration 13088, loss = 0.06632610\n",
      "Iteration 13089, loss = 0.06632513\n",
      "Iteration 13090, loss = 0.06632414\n",
      "Iteration 13091, loss = 0.06632317\n",
      "Iteration 13092, loss = 0.06632220\n",
      "Iteration 13093, loss = 0.06632122\n",
      "Iteration 13094, loss = 0.06632026\n",
      "Iteration 13095, loss = 0.06631925\n",
      "Iteration 13096, loss = 0.06631832\n",
      "Iteration 13097, loss = 0.06631731\n",
      "Iteration 13098, loss = 0.06631637\n",
      "Iteration 13099, loss = 0.06631536\n",
      "Iteration 13100, loss = 0.06631442\n",
      "Iteration 13101, loss = 0.06631345\n",
      "Iteration 13102, loss = 0.06631245\n",
      "Iteration 13103, loss = 0.06631149\n",
      "Iteration 13104, loss = 0.06631050\n",
      "Iteration 13105, loss = 0.06630954\n",
      "Iteration 13106, loss = 0.06630855\n",
      "Iteration 13107, loss = 0.06630760\n",
      "Iteration 13108, loss = 0.06630663\n",
      "Iteration 13109, loss = 0.06630565\n",
      "Iteration 13110, loss = 0.06630468\n",
      "Iteration 13111, loss = 0.06630371\n",
      "Iteration 13112, loss = 0.06630271\n",
      "Iteration 13113, loss = 0.06630177\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('E:\\\\淺度機器學習\\\\data\\\\wine.csv')\n",
    "X = np.array(df.iloc[:, :-1]) # 排 除 最 後 一 欄 標 籤\n",
    "y = np.array(df.iloc[:, -1])\n",
    "\n",
    "data,LR,LR_CV,SVM,MLP,LR_pca,LR_CV_pca,SVM_pca,MLP_pca =classifier(X,y,\"wine.csv\",30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataname =  wine.csv \n",
      " LR =  [0.98148148 0.96296296 0.98148148] \n",
      " LR_CV=  [0.90740741 0.96296296 0.90740741] \n",
      " SVM=  [[0.94444444 0.35185185 0.98148148]\n",
      " [0.94444444 0.35185185 0.98148148]] \n",
      " MLP=  [[0.90740741 0.2962963 ]\n",
      " [0.2962963  0.33333333]] \n",
      "--------------\n",
      " LR_pca=  [0.88888889 0.87037037 0.88888889] \n",
      " LR_CV_pca=  [0.87037037 0.87037037 0.87037037] \n",
      " SVM_pca=  [[0.85185185 0.7962963  0.77777778]\n",
      " [0.85185185 0.7962963  0.77777778]] \n",
      " MLP_pca=  [[0.85185185 0.90740741]\n",
      " [0.83333333 0.90740741]]\n"
     ]
    }
   ],
   "source": [
    "print(\"dataname = \",data,\"\\n\",\"LR = \",LR,\"\\n\",\"LR_CV= \",LR_CV,\"\\n\",\"SVM= \",SVM,\"\\n\",\"MLP= \",MLP,\\\n",
    "      \"\\n--------------\\n\",\\\n",
    "      \"LR_pca= \",LR_pca,\"\\n\",\"LR_CV_pca= \",LR_CV_pca,\"\\n\",\"SVM_pca= \",SVM_pca,\"\\n\",\"MLP_pca= \",MLP_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>正確率最高的模型</h3>\n",
    "將各個模型中不同參數下，正確率最高的模型挑選出來作為代表，進行比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>LR_CV</th>\n",
       "      <th>SVM</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca</th>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.9074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              LR   LR_CV     SVM     MLP\n",
       "original  0.9815  0.9074  0.9815  0.9074\n",
       "pca       0.8889  0.8704  0.8519  0.9074"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = {'LR': [0.9815, 0.8889],\n",
    "        'LR_CV': [ 0.9074,0.8704 ],\n",
    "        'SVM': [ 0.9815, 0.8519 ],\n",
    "        'MLP': [0.9074,0.9074]       \n",
    "        }\n",
    "index = ['original', 'pca']\n",
    "df = pd.DataFrame(acc, index=index)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>實驗結果觀察</h3>\n",
    "根據以上實驗結果，對於沒有進行PCA的資料集，羅吉斯迴歸模型皆有0.9以上的正確率，其中以　solver = ['lbfgs','newton-cg'] 兩模型獲得最高正確率，而SVM模型中除了 kernel=\"rbf\" 的模型正確率明顯較低之外，其他四種模型正確率皆在0.95左右，MLP模型則是唯一以 activation='logistic'、solver='adam' 為最高正確率模型。<br>\n",
    "當 n_components＝2 時，可以發現雖然大部份沒有做PCA的資料集正確率比有做PCA的資料集來得低，但有做PCA的資料集在每個模型的不同參數搭配下，有更相近（穩定）的正確率，此外，可以特別注意到，在SVM模型中 kernel=\"rbf\" 的模型，以及MLP模型中 activation='logistic'、solver='sgd' 和 activation='relu' 搭配 solver = ['adam','sgd'] 的模型，在有做PCA的情況下，有更高的正確率，其中以 activation='relu' 的模型有最高正確率。<br>\n",
    "綜合以上觀察結果，對於此資料集，會建議使用經過PCA的資料集，藉由 MLP(activation='relu') 的模型進行建模預測會更有效率!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
